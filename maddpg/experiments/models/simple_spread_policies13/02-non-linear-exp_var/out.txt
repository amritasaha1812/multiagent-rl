# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 15.0 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies13/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies13/02-non-linear-exp_var/
Job <1092149> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc164>>
arglist.u_estimation True
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -519.597556735976, agent episode reward: [-173.19918557865867, -173.19918557865867, -173.19918557865867], time: 148.935
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 148.936
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -767.9352798479799, agent episode reward: [-255.9784266159933, -255.9784266159933, -255.9784266159933], time: 169.812
steps: 49975, episodes: 2000, mean episode variance: 8.8880753352046, agent episode variance: [3.014538957118988, 2.9451610519289972, 2.9283753261566163], time: 169.812
Running avgs for agent 0: q_loss: 109.73954772949219, p_loss: -5.766327381134033, mean_rew: -7.939682586112074, variance: 12.354667857045033, lamda: 1.0105326175689697
Running avgs for agent 1: q_loss: 112.26692962646484, p_loss: -5.576189994812012, mean_rew: -7.9312696095428, variance: 12.070332180036873, lamda: 1.0107696056365967
Running avgs for agent 2: q_loss: 113.98674011230469, p_loss: -5.503586292266846, mean_rew: -7.93055906952898, variance: 12.001538221953345, lamda: 1.0108922719955444

steps: 74975, episodes: 3000, mean episode reward: -832.6584506442389, agent episode reward: [-277.5528168814129, -277.5528168814129, -277.5528168814129], time: 165.131
steps: 74975, episodes: 3000, mean episode variance: 6.635399838209152, agent episode variance: [2.2170314779281615, 2.2142004277706144, 2.204167932510376], time: 165.132
Running avgs for agent 0: q_loss: 47.19541931152344, p_loss: -4.983115196228027, mean_rew: -9.032687772830576, variance: 8.868125915527344, lamda: 1.0349620580673218
Running avgs for agent 1: q_loss: 46.28377914428711, p_loss: -4.837008953094482, mean_rew: -9.037519003762489, variance: 8.856801986694336, lamda: 1.0342533588409424
Running avgs for agent 2: q_loss: 56.353546142578125, p_loss: -4.90572452545166, mean_rew: -9.038668390495701, variance: 8.816672325134277, lamda: 1.034653663635254

steps: 99975, episodes: 4000, mean episode reward: -903.9482266503265, agent episode reward: [-301.3160755501088, -301.3160755501088, -301.3160755501088], time: 168.947
steps: 99975, episodes: 4000, mean episode variance: 6.821025787115097, agent episode variance: [2.2305053369998933, 2.29191237449646, 2.298608075618744], time: 168.947
Running avgs for agent 0: q_loss: 95.75357818603516, p_loss: -5.05901575088501, mean_rew: -9.830587442689422, variance: 8.922022819519043, lamda: 1.0599867105484009
Running avgs for agent 1: q_loss: 88.12720489501953, p_loss: -5.110257625579834, mean_rew: -9.83495951788263, variance: 9.16765022277832, lamda: 1.0591286420822144
Running avgs for agent 2: q_loss: 73.68150329589844, p_loss: -5.020240306854248, mean_rew: -9.819369919368972, variance: 9.194432258605957, lamda: 1.059615969657898

steps: 124975, episodes: 5000, mean episode reward: -799.383012088453, agent episode reward: [-266.46100402948434, -266.46100402948434, -266.46100402948434], time: 167.255
steps: 124975, episodes: 5000, mean episode variance: 6.976014833927154, agent episode variance: [2.3126124606132508, 2.3112037138938906, 2.3521986594200133], time: 167.256
Running avgs for agent 0: q_loss: 60.462860107421875, p_loss: -5.13160514831543, mean_rew: -10.185944316951224, variance: 9.250449180603027, lamda: 1.085011601448059
Running avgs for agent 1: q_loss: 83.4488525390625, p_loss: -5.145720481872559, mean_rew: -10.185538553659963, variance: 9.2448148727417, lamda: 1.0841327905654907
Running avgs for agent 2: q_loss: 66.85543823242188, p_loss: -5.188809394836426, mean_rew: -10.206928628378742, variance: 9.408795356750488, lamda: 1.0846201181411743

steps: 149975, episodes: 6000, mean episode reward: -714.5504874625915, agent episode reward: [-238.18349582086387, -238.18349582086387, -238.18349582086387], time: 166.743
steps: 149975, episodes: 6000, mean episode variance: 6.861521067142487, agent episode variance: [2.293475841999054, 2.2893675084114076, 2.2786777167320253], time: 166.744
Running avgs for agent 0: q_loss: 50.993804931640625, p_loss: -5.162243843078613, mean_rew: -10.160128860032975, variance: 9.173904418945312, lamda: 1.1100164651870728
Running avgs for agent 1: q_loss: 53.53684997558594, p_loss: -5.118286609649658, mean_rew: -10.141319870647182, variance: 9.157470703125, lamda: 1.1091368198394775
Running avgs for agent 2: q_loss: 58.14704513549805, p_loss: -5.183780193328857, mean_rew: -10.166148115929047, variance: 9.114710807800293, lamda: 1.1096242666244507

steps: 174975, episodes: 7000, mean episode reward: -701.899246586002, agent episode reward: [-233.96641552866737, -233.96641552866737, -233.96641552866737], time: 164.036
steps: 174975, episodes: 7000, mean episode variance: 6.62188415145874, agent episode variance: [2.194759938955307, 2.2323671894073485, 2.1947570230960847], time: 164.037
Running avgs for agent 0: q_loss: 65.96823120117188, p_loss: -5.088199138641357, mean_rew: -10.045933185990958, variance: 8.779040336608887, lamda: 1.13472580909729
Running avgs for agent 1: q_loss: 46.54454040527344, p_loss: -5.102899551391602, mean_rew: -10.034854241646014, variance: 8.929469108581543, lamda: 1.134128451347351
Running avgs for agent 2: q_loss: 76.24848937988281, p_loss: -5.104776382446289, mean_rew: -10.062924183890132, variance: 8.779027938842773, lamda: 1.134628415107727

steps: 199975, episodes: 8000, mean episode reward: -703.6486823282875, agent episode reward: [-234.54956077609583, -234.54956077609583, -234.54956077609583], time: 164.569
steps: 199975, episodes: 8000, mean episode variance: 6.488211263895034, agent episode variance: [2.159437581539154, 2.1773292107582094, 2.1514444715976717], time: 164.569
Running avgs for agent 0: q_loss: 61.86076354980469, p_loss: -5.043044090270996, mean_rew: -9.937109412532447, variance: 8.637749671936035, lamda: 1.1587224006652832
Running avgs for agent 1: q_loss: 38.914249420166016, p_loss: -5.042136192321777, mean_rew: -9.949980533752504, variance: 8.709317207336426, lamda: 1.1591187715530396
Running avgs for agent 2: q_loss: 62.036231994628906, p_loss: -5.048944473266602, mean_rew: -9.937662763878125, variance: 8.605777740478516, lamda: 1.1596324443817139

steps: 224975, episodes: 9000, mean episode reward: -688.6529717506055, agent episode reward: [-229.55099058353517, -229.55099058353517, -229.55099058353517], time: 164.97
steps: 224975, episodes: 9000, mean episode variance: 6.289825978517532, agent episode variance: [2.0906729681491854, 2.120460823535919, 2.078692186832428], time: 164.971
Running avgs for agent 0: q_loss: 44.03714370727539, p_loss: -5.019658088684082, mean_rew: -9.876015848028699, variance: 8.362692832946777, lamda: 1.1823816299438477
Running avgs for agent 1: q_loss: 34.1066780090332, p_loss: -5.005273818969727, mean_rew: -9.892490858371053, variance: 8.481843948364258, lamda: 1.1838628053665161
Running avgs for agent 2: q_loss: 47.37416458129883, p_loss: -4.9873247146606445, mean_rew: -9.877395847258995, variance: 8.31476879119873, lamda: 1.1846117973327637

steps: 249975, episodes: 10000, mean episode reward: -699.7633112212578, agent episode reward: [-233.25443707375263, -233.25443707375263, -233.25443707375263], time: 165.553
steps: 249975, episodes: 10000, mean episode variance: 6.121423768997192, agent episode variance: [2.0355791912078858, 2.059495536327362, 2.0263490414619447], time: 165.554
Running avgs for agent 0: q_loss: 33.19388961791992, p_loss: -4.970342636108398, mean_rew: -9.79578764562485, variance: 8.142316818237305, lamda: 1.2056828737258911
Running avgs for agent 1: q_loss: 48.09611511230469, p_loss: -4.945638179779053, mean_rew: -9.789038212142295, variance: 8.237982749938965, lamda: 1.203702688217163
Running avgs for agent 2: q_loss: 43.684288024902344, p_loss: -4.960020542144775, mean_rew: -9.79907984942897, variance: 8.105396270751953, lamda: 1.2091633081436157

steps: 274975, episodes: 11000, mean episode reward: -768.1040729552763, agent episode reward: [-256.0346909850921, -256.0346909850921, -256.0346909850921], time: 167.519
steps: 274975, episodes: 11000, mean episode variance: 6.01674888586998, agent episode variance: [1.9871682467460632, 2.035020041465759, 1.9945605976581573], time: 167.519
Running avgs for agent 0: q_loss: 34.8416633605957, p_loss: -4.970840930938721, mean_rew: -9.812732597679382, variance: 7.948673248291016, lamda: 1.2257661819458008
Running avgs for agent 1: q_loss: 34.760189056396484, p_loss: -4.949549674987793, mean_rew: -9.79838565834869, variance: 8.140080451965332, lamda: 1.2190873622894287
Running avgs for agent 2: q_loss: 41.122901916503906, p_loss: -4.97320556640625, mean_rew: -9.807651379518104, variance: 7.97824239730835, lamda: 1.2315884828567505

steps: 299975, episodes: 12000, mean episode reward: -735.497884521038, agent episode reward: [-245.16596150701264, -245.16596150701264, -245.16596150701264], time: 167.843
steps: 299975, episodes: 12000, mean episode variance: 5.932942262172699, agent episode variance: [1.9709798798561096, 2.0029699149131774, 1.9589924674034118], time: 167.844
Running avgs for agent 0: q_loss: 43.419063568115234, p_loss: -4.98365592956543, mean_rew: -9.825104401985456, variance: 7.883919715881348, lamda: 1.2490359544754028
Running avgs for agent 1: q_loss: 37.27397155761719, p_loss: -4.96942663192749, mean_rew: -9.828162105897576, variance: 8.011878967285156, lamda: 1.239189624786377
Running avgs for agent 2: q_loss: 57.42434310913086, p_loss: -4.967421531677246, mean_rew: -9.81450080528648, variance: 7.835969924926758, lamda: 1.251876950263977

steps: 324975, episodes: 13000, mean episode reward: -764.6044904459424, agent episode reward: [-254.8681634819808, -254.8681634819808, -254.8681634819808], time: 167.405
steps: 324975, episodes: 13000, mean episode variance: 5.897407039165497, agent episode variance: [1.954331776857376, 1.9905397398471831, 1.9525355224609375], time: 167.406
Running avgs for agent 0: q_loss: 50.65580749511719, p_loss: -4.971465110778809, mean_rew: -9.842971998082868, variance: 7.81732702255249, lamda: 1.264129638671875
Running avgs for agent 1: q_loss: 40.16838073730469, p_loss: -4.967069149017334, mean_rew: -9.837095332841253, variance: 7.962158679962158, lamda: 1.247082233428955
Running avgs for agent 2: q_loss: 35.0792350769043, p_loss: -4.968850612640381, mean_rew: -9.82748439483237, variance: 7.810141563415527, lamda: 1.2714985609054565

steps: 349975, episodes: 14000, mean episode reward: -772.5195363236817, agent episode reward: [-257.5065121078939, -257.5065121078939, -257.5065121078939], time: 163.214
steps: 349975, episodes: 14000, mean episode variance: 5.822068711996079, agent episode variance: [1.913360823392868, 1.990254623889923, 1.9184532647132873], time: 163.214
Running avgs for agent 0: q_loss: 47.31709289550781, p_loss: -5.003209114074707, mean_rew: -9.873616136404134, variance: 7.653443336486816, lamda: 1.281680703163147
Running avgs for agent 1: q_loss: 29.58572769165039, p_loss: -4.983283042907715, mean_rew: -9.876124785627137, variance: 7.9610185623168945, lamda: 1.257599115371704
Running avgs for agent 2: q_loss: 40.73649978637695, p_loss: -4.987672805786133, mean_rew: -9.864528781620134, variance: 7.673813343048096, lamda: 1.292360544204712

steps: 374975, episodes: 15000, mean episode reward: -796.9853334524682, agent episode reward: [-265.6617778174894, -265.6617778174894, -265.6617778174894], time: 170.374
steps: 374975, episodes: 15000, mean episode variance: 5.751846795320511, agent episode variance: [1.8992005960941314, 1.954092580318451, 1.8985536189079284], time: 170.375
Running avgs for agent 0: q_loss: 37.92497634887695, p_loss: -5.010796070098877, mean_rew: -9.910889973985574, variance: 7.596802711486816, lamda: 1.3007333278656006
Running avgs for agent 1: q_loss: 29.651418685913086, p_loss: -4.986090660095215, mean_rew: -9.895121353427061, variance: 7.816370487213135, lamda: 1.2793306112289429
Running avgs for agent 2: q_loss: 34.887271881103516, p_loss: -4.973519802093506, mean_rew: -9.88925046647529, variance: 7.594214916229248, lamda: 1.312446117401123

steps: 399975, episodes: 16000, mean episode reward: -811.7039868268404, agent episode reward: [-270.5679956089468, -270.5679956089468, -270.5679956089468], time: 170.855
steps: 399975, episodes: 16000, mean episode variance: 5.755453386306763, agent episode variance: [1.9068081135749817, 1.9528650197982789, 1.8957802529335022], time: 170.855
Running avgs for agent 0: q_loss: 33.31563949584961, p_loss: -5.042393207550049, mean_rew: -9.981780771577103, variance: 7.627233028411865, lamda: 1.3233017921447754
Running avgs for agent 1: q_loss: 36.97574996948242, p_loss: -5.032514572143555, mean_rew: -9.95237338518122, variance: 7.811460018157959, lamda: 1.2903656959533691
Running avgs for agent 2: q_loss: 53.781070709228516, p_loss: -5.04292106628418, mean_rew: -9.97276963549419, variance: 7.583121299743652, lamda: 1.3293927907943726

steps: 424975, episodes: 17000, mean episode reward: -812.9178156910668, agent episode reward: [-270.9726052303556, -270.9726052303556, -270.9726052303556], time: 169.98
steps: 424975, episodes: 17000, mean episode variance: 5.6942338728904724, agent episode variance: [1.8795284852981566, 1.9447331085205077, 1.8699722790718079], time: 169.981
Running avgs for agent 0: q_loss: 42.66400909423828, p_loss: -5.059515476226807, mean_rew: -10.011234552096921, variance: 7.518113613128662, lamda: 1.3453365564346313
Running avgs for agent 1: q_loss: 36.160491943359375, p_loss: -5.059154510498047, mean_rew: -10.011888227347596, variance: 7.778932571411133, lamda: 1.2955156564712524
Running avgs for agent 2: q_loss: 46.1661491394043, p_loss: -5.074786186218262, mean_rew: -10.025393755293134, variance: 7.479889392852783, lamda: 1.3434529304504395

steps: 449975, episodes: 18000, mean episode reward: -820.095463780762, agent episode reward: [-273.3651545935873, -273.3651545935873, -273.3651545935873], time: 169.801
steps: 449975, episodes: 18000, mean episode variance: 5.701037139415741, agent episode variance: [1.864920201063156, 1.9479574637413024, 1.8881594746112824], time: 169.802
Running avgs for agent 0: q_loss: 46.419334411621094, p_loss: -5.079647541046143, mean_rew: -10.060338688268319, variance: 7.459680557250977, lamda: 1.3561301231384277
Running avgs for agent 1: q_loss: 33.02871322631836, p_loss: -5.07951545715332, mean_rew: -10.064629456326267, variance: 7.791830062866211, lamda: 1.3011614084243774
Running avgs for agent 2: q_loss: 50.90119552612305, p_loss: -5.100183486938477, mean_rew: -10.084820068369488, variance: 7.552637577056885, lamda: 1.3530614376068115

steps: 474975, episodes: 19000, mean episode reward: -842.1080804625428, agent episode reward: [-280.70269348751424, -280.70269348751424, -280.70269348751424], time: 169.365
steps: 474975, episodes: 19000, mean episode variance: 5.677555671691895, agent episode variance: [1.8601446845531464, 1.9314149060249328, 1.8859960811138152], time: 169.365
Running avgs for agent 0: q_loss: 46.41353225708008, p_loss: -5.116381645202637, mean_rew: -10.12315332784447, variance: 7.440578937530518, lamda: 1.3648616075515747
Running avgs for agent 1: q_loss: 30.695852279663086, p_loss: -5.111982345581055, mean_rew: -10.119591853388396, variance: 7.725659370422363, lamda: 1.3189003467559814
Running avgs for agent 2: q_loss: 47.851219177246094, p_loss: -5.125009536743164, mean_rew: -10.122319995543945, variance: 7.543984413146973, lamda: 1.356207013130188

steps: 499975, episodes: 20000, mean episode reward: -851.6565632337342, agent episode reward: [-283.8855210779114, -283.8855210779114, -283.8855210779114], time: 164.397
steps: 499975, episodes: 20000, mean episode variance: 5.689115713596344, agent episode variance: [1.8685913059711456, 1.933284851551056, 1.8872395560741424], time: 164.398
Running avgs for agent 0: q_loss: 47.394493103027344, p_loss: -5.1304612159729, mean_rew: -10.174531256915335, variance: 7.474365234375, lamda: 1.3737138509750366
Running avgs for agent 1: q_loss: 35.57978439331055, p_loss: -5.137432098388672, mean_rew: -10.169555255725305, variance: 7.7331390380859375, lamda: 1.3293781280517578
Running avgs for agent 2: q_loss: 46.9709358215332, p_loss: -5.14857816696167, mean_rew: -10.18023574254301, variance: 7.5489583015441895, lamda: 1.3587737083435059

steps: 524975, episodes: 21000, mean episode reward: -852.7128827760793, agent episode reward: [-284.2376275920265, -284.2376275920265, -284.2376275920265], time: 165.915
steps: 524975, episodes: 21000, mean episode variance: 5.690650113105774, agent episode variance: [1.874662882089615, 1.9255501384735108, 1.8904370925426484], time: 165.916
Running avgs for agent 0: q_loss: 45.68972396850586, p_loss: -5.175042152404785, mean_rew: -10.249335731583638, variance: 7.498651504516602, lamda: 1.381673812866211
Running avgs for agent 1: q_loss: 32.139583587646484, p_loss: -5.16220235824585, mean_rew: -10.232283658107148, variance: 7.702200412750244, lamda: 1.334781289100647
Running avgs for agent 2: q_loss: 45.369998931884766, p_loss: -5.177490234375, mean_rew: -10.228024920295427, variance: 7.561748027801514, lamda: 1.3606054782867432

steps: 549975, episodes: 22000, mean episode reward: -872.3729311449928, agent episode reward: [-290.79097704833094, -290.79097704833094, -290.79097704833094], time: 167.099
steps: 549975, episodes: 22000, mean episode variance: 5.694303181171417, agent episode variance: [1.8560712275505067, 1.9186151671409606, 1.91961678647995], time: 167.099
Running avgs for agent 0: q_loss: 46.036094665527344, p_loss: -5.188991069793701, mean_rew: -10.28934346800266, variance: 7.424284934997559, lamda: 1.3856172561645508
Running avgs for agent 1: q_loss: 31.491527557373047, p_loss: -5.190492153167725, mean_rew: -10.28573589992828, variance: 7.6744608879089355, lamda: 1.350277066230774
Running avgs for agent 2: q_loss: 45.720916748046875, p_loss: -5.211455821990967, mean_rew: -10.302970021299501, variance: 7.678467273712158, lamda: 1.3632524013519287

steps: 574975, episodes: 23000, mean episode reward: -878.2498841392874, agent episode reward: [-292.7499613797625, -292.7499613797625, -292.7499613797625], time: 165.707
steps: 574975, episodes: 23000, mean episode variance: 5.718459929227829, agent episode variance: [1.8835230746269227, 1.9139332265853881, 1.9210036280155183], time: 165.707
Running avgs for agent 0: q_loss: 47.415042877197266, p_loss: -5.242759704589844, mean_rew: -10.372262646992061, variance: 7.534092426300049, lamda: 1.392137885093689
Running avgs for agent 1: q_loss: 30.482879638671875, p_loss: -5.232284069061279, mean_rew: -10.355712806755333, variance: 7.655733585357666, lamda: 1.3604363203048706
Running avgs for agent 2: q_loss: 44.42500305175781, p_loss: -5.251255512237549, mean_rew: -10.358519351375636, variance: 7.684014797210693, lamda: 1.3651716709136963

steps: 599975, episodes: 24000, mean episode reward: -882.9577518977658, agent episode reward: [-294.31925063258865, -294.31925063258865, -294.31925063258865], time: 165.332
steps: 599975, episodes: 24000, mean episode variance: 5.7262335689067845, agent episode variance: [1.8817659947872163, 1.9229671692848205, 1.9215004048347473], time: 165.332
Running avgs for agent 0: q_loss: 47.13938522338867, p_loss: -5.261700630187988, mean_rew: -10.434863657493311, variance: 7.527063846588135, lamda: 1.3987699747085571
Running avgs for agent 1: q_loss: 35.94350814819336, p_loss: -5.264406204223633, mean_rew: -10.425213146197398, variance: 7.691868782043457, lamda: 1.3694344758987427
Running avgs for agent 2: q_loss: 44.056373596191406, p_loss: -5.29307222366333, mean_rew: -10.439462649798042, variance: 7.686001300811768, lamda: 1.3677070140838623

steps: 624975, episodes: 25000, mean episode reward: -875.8303069085388, agent episode reward: [-291.94343563617963, -291.94343563617963, -291.94343563617963], time: 166.126
steps: 624975, episodes: 25000, mean episode variance: 5.772771564722061, agent episode variance: [1.8794355828762055, 1.9338341093063354, 1.9595018725395204], time: 166.127
Running avgs for agent 0: q_loss: 47.993675231933594, p_loss: -5.280331611633301, mean_rew: -10.477831356865526, variance: 7.517742156982422, lamda: 1.4053999185562134
Running avgs for agent 1: q_loss: 35.8789176940918, p_loss: -5.28762674331665, mean_rew: -10.48127545091914, variance: 7.7353363037109375, lamda: 1.372751235961914
Running avgs for agent 2: q_loss: 42.729923248291016, p_loss: -5.29880428314209, mean_rew: -10.481061582166676, variance: 7.838007926940918, lamda: 1.3699473142623901

steps: 649975, episodes: 26000, mean episode reward: -887.3430303678138, agent episode reward: [-295.7810101226045, -295.7810101226045, -295.7810101226045], time: 165.123
steps: 649975, episodes: 26000, mean episode variance: 5.706775655269623, agent episode variance: [1.8662273516654968, 1.9064589745998382, 1.9340893290042878], time: 165.124
Running avgs for agent 0: q_loss: 47.20332717895508, p_loss: -5.2930521965026855, mean_rew: -10.501073455856742, variance: 7.464909076690674, lamda: 1.4134243726730347
Running avgs for agent 1: q_loss: 36.093536376953125, p_loss: -5.3214569091796875, mean_rew: -10.521599430546095, variance: 7.62583589553833, lamda: 1.3765889406204224
Running avgs for agent 2: q_loss: 43.050323486328125, p_loss: -5.333347320556641, mean_rew: -10.529216631474695, variance: 7.73635721206665, lamda: 1.3732136487960815

steps: 674975, episodes: 27000, mean episode reward: -867.9182906144797, agent episode reward: [-289.3060968714932, -289.3060968714932, -289.3060968714932], time: 165.836
steps: 674975, episodes: 27000, mean episode variance: 5.818472458124161, agent episode variance: [1.900289155483246, 1.9561184287071227, 1.962064873933792], time: 165.836
Running avgs for agent 0: q_loss: 47.9098014831543, p_loss: -5.3332438468933105, mean_rew: -10.585271378519366, variance: 7.601156711578369, lamda: 1.4199351072311401
Running avgs for agent 1: q_loss: 35.06312561035156, p_loss: -5.323238849639893, mean_rew: -10.56509486399501, variance: 7.824473857879639, lamda: 1.3780725002288818
Running avgs for agent 2: q_loss: 41.65758514404297, p_loss: -5.350990295410156, mean_rew: -10.573281698457096, variance: 7.848259925842285, lamda: 1.3757305145263672

steps: 699975, episodes: 28000, mean episode reward: -866.6433987062494, agent episode reward: [-288.8811329020831, -288.8811329020831, -288.8811329020831], time: 162.779
steps: 699975, episodes: 28000, mean episode variance: 5.763460476398468, agent episode variance: [1.8643683989048003, 1.931957192182541, 1.9671348853111268], time: 162.78
Running avgs for agent 0: q_loss: 46.99075698852539, p_loss: -5.338812828063965, mean_rew: -10.601496597467914, variance: 7.4574737548828125, lamda: 1.4261467456817627
Running avgs for agent 1: q_loss: 35.381507873535156, p_loss: -5.345049858093262, mean_rew: -10.59887104243869, variance: 7.727828502655029, lamda: 1.3796952962875366
Running avgs for agent 2: q_loss: 39.36067199707031, p_loss: -5.3660054206848145, mean_rew: -10.614458485778306, variance: 7.868539810180664, lamda: 1.3793944120407104

steps: 724975, episodes: 29000, mean episode reward: -883.5818197158981, agent episode reward: [-294.52727323863263, -294.52727323863263, -294.52727323863263], time: 163.272
steps: 724975, episodes: 29000, mean episode variance: 5.731885047912598, agent episode variance: [1.8794563417434693, 1.9333837018013, 1.9190450043678284], time: 163.272
Running avgs for agent 0: q_loss: 46.9619026184082, p_loss: -5.365316390991211, mean_rew: -10.652442802177193, variance: 7.517825126647949, lamda: 1.431884527206421
Running avgs for agent 1: q_loss: 28.632986068725586, p_loss: -5.373105525970459, mean_rew: -10.641548260494972, variance: 7.733535289764404, lamda: 1.385599136352539
Running avgs for agent 2: q_loss: 36.50510787963867, p_loss: -5.386348247528076, mean_rew: -10.65597829072786, variance: 7.676179885864258, lamda: 1.3951032161712646

steps: 749975, episodes: 30000, mean episode reward: -868.2174728624257, agent episode reward: [-289.4058242874752, -289.4058242874752, -289.4058242874752], time: 170.348
steps: 749975, episodes: 30000, mean episode variance: 5.701935339212418, agent episode variance: [1.8749891633987428, 1.9081637358665466, 1.9187824399471283], time: 170.348
Running avgs for agent 0: q_loss: 41.67207717895508, p_loss: -5.379345893859863, mean_rew: -10.682897887064206, variance: 7.4999566078186035, lamda: 1.4366105794906616
Running avgs for agent 1: q_loss: 26.917892456054688, p_loss: -5.401182651519775, mean_rew: -10.675262302982237, variance: 7.632655143737793, lamda: 1.4030003547668457
Running avgs for agent 2: q_loss: 40.62879180908203, p_loss: -5.38742208480835, mean_rew: -10.667585976042686, variance: 7.675129413604736, lamda: 1.4021801948547363

steps: 774975, episodes: 31000, mean episode reward: -911.2619005419114, agent episode reward: [-303.7539668473038, -303.7539668473038, -303.7539668473038], time: 170.735
steps: 774975, episodes: 31000, mean episode variance: 5.75696481871605, agent episode variance: [1.885098839521408, 1.9209678225517273, 1.9508981566429138], time: 170.736
Running avgs for agent 0: q_loss: 44.76852798461914, p_loss: -5.397352695465088, mean_rew: -10.716234939256845, variance: 7.540395736694336, lamda: 1.4456870555877686
Running avgs for agent 1: q_loss: 26.996206283569336, p_loss: -5.409526348114014, mean_rew: -10.734366821363581, variance: 7.683871746063232, lamda: 1.4178786277770996
Running avgs for agent 2: q_loss: 40.18153762817383, p_loss: -5.421596527099609, mean_rew: -10.73254723590724, variance: 7.803592681884766, lamda: 1.4043924808502197

steps: 799975, episodes: 32000, mean episode reward: -901.2709056606013, agent episode reward: [-300.4236352202004, -300.4236352202004, -300.4236352202004], time: 167.207
steps: 799975, episodes: 32000, mean episode variance: 5.686354102134705, agent episode variance: [1.8765111441612243, 1.8849237384796143, 1.924919219493866], time: 167.207
Running avgs for agent 0: q_loss: 43.98992156982422, p_loss: -5.402344703674316, mean_rew: -10.749597622777772, variance: 7.506044864654541, lamda: 1.4481556415557861
Running avgs for agent 1: q_loss: 27.970304489135742, p_loss: -5.4161553382873535, mean_rew: -10.763351198648612, variance: 7.539694786071777, lamda: 1.4339172840118408
Running avgs for agent 2: q_loss: 40.1657600402832, p_loss: -5.445419788360596, mean_rew: -10.764702658301816, variance: 7.699676990509033, lamda: 1.4067004919052124

steps: 824975, episodes: 33000, mean episode reward: -914.1079689017022, agent episode reward: [-304.70265630056736, -304.70265630056736, -304.70265630056736], time: 166.298
steps: 824975, episodes: 33000, mean episode variance: 5.718717863798141, agent episode variance: [1.8983338034152986, 1.8788762149810792, 1.941507845401764], time: 166.299
Running avgs for agent 0: q_loss: 43.73988342285156, p_loss: -5.446811676025391, mean_rew: -10.812586877475589, variance: 7.593335151672363, lamda: 1.4509257078170776
Running avgs for agent 1: q_loss: 37.200469970703125, p_loss: -5.441804885864258, mean_rew: -10.816354310136637, variance: 7.515504837036133, lamda: 1.4449580907821655
Running avgs for agent 2: q_loss: 39.986209869384766, p_loss: -5.4495768547058105, mean_rew: -10.794460447086275, variance: 7.766031265258789, lamda: 1.4096500873565674

steps: 849975, episodes: 34000, mean episode reward: -906.2865890406038, agent episode reward: [-302.0955296802012, -302.0955296802012, -302.0955296802012], time: 163.824
steps: 849975, episodes: 34000, mean episode variance: 5.716290681123733, agent episode variance: [1.8757571852207184, 1.905106431722641, 1.9354270641803741], time: 163.824
Running avgs for agent 0: q_loss: 41.42154312133789, p_loss: -5.46378231048584, mean_rew: -10.8459482886589, variance: 7.5030293464660645, lamda: 1.4563913345336914
Running avgs for agent 1: q_loss: 37.6385383605957, p_loss: -5.455551624298096, mean_rew: -10.837157001497808, variance: 7.620425701141357, lamda: 1.446591854095459
Running avgs for agent 2: q_loss: 39.60283660888672, p_loss: -5.486546039581299, mean_rew: -10.848758992373767, variance: 7.741707801818848, lamda: 1.4123692512512207

steps: 874975, episodes: 35000, mean episode reward: -892.0340776221685, agent episode reward: [-297.3446925407229, -297.3446925407229, -297.3446925407229], time: 167.447
steps: 874975, episodes: 35000, mean episode variance: 5.734255185842514, agent episode variance: [1.8947954168319703, 1.9134116325378419, 1.926048136472702], time: 167.448
Running avgs for agent 0: q_loss: 42.63290786743164, p_loss: -5.473229885101318, mean_rew: -10.872336046405696, variance: 7.579182147979736, lamda: 1.4613847732543945
Running avgs for agent 1: q_loss: 38.08888244628906, p_loss: -5.482709884643555, mean_rew: -10.894438884215099, variance: 7.653646469116211, lamda: 1.4485195875167847
Running avgs for agent 2: q_loss: 40.270103454589844, p_loss: -5.493574142456055, mean_rew: -10.887321675891778, variance: 7.704192161560059, lamda: 1.4165799617767334

steps: 899975, episodes: 36000, mean episode reward: -883.7250494513678, agent episode reward: [-294.57501648378917, -294.57501648378917, -294.57501648378917], time: 165.691
steps: 899975, episodes: 36000, mean episode variance: 5.709274846792221, agent episode variance: [1.876092691898346, 1.8941371500492097, 1.9390450048446655], time: 165.692
Running avgs for agent 0: q_loss: 42.27659606933594, p_loss: -5.482621669769287, mean_rew: -10.892817497589382, variance: 7.504371166229248, lamda: 1.4633430242538452
Running avgs for agent 1: q_loss: 38.30808639526367, p_loss: -5.496670722961426, mean_rew: -10.913910822647628, variance: 7.576549053192139, lamda: 1.4499316215515137
Running avgs for agent 2: q_loss: 38.41596221923828, p_loss: -5.506802558898926, mean_rew: -10.90219402781333, variance: 7.7561798095703125, lamda: 1.4188810586929321

steps: 924975, episodes: 37000, mean episode reward: -907.456004847414, agent episode reward: [-302.485334949138, -302.485334949138, -302.485334949138], time: 165.49
steps: 924975, episodes: 37000, mean episode variance: 5.738176012516022, agent episode variance: [1.8888004264831544, 1.8995454287528992, 1.9498301572799683], time: 165.491
Running avgs for agent 0: q_loss: 43.2996826171875, p_loss: -5.515689849853516, mean_rew: -10.939286679825727, variance: 7.555201530456543, lamda: 1.4655412435531616
Running avgs for agent 1: q_loss: 37.3394889831543, p_loss: -5.496355056762695, mean_rew: -10.923270243085575, variance: 7.59818172454834, lamda: 1.4517956972122192
Running avgs for agent 2: q_loss: 37.95654296875, p_loss: -5.527750015258789, mean_rew: -10.929869953246603, variance: 7.799320697784424, lamda: 1.420080304145813

steps: 949975, episodes: 38000, mean episode reward: -900.796865891124, agent episode reward: [-300.26562196370804, -300.26562196370804, -300.26562196370804], time: 168.284
steps: 949975, episodes: 38000, mean episode variance: 5.713631525993347, agent episode variance: [1.8569179701805114, 1.920099202632904, 1.9366143531799316], time: 168.284
Running avgs for agent 0: q_loss: 42.754188537597656, p_loss: -5.5115485191345215, mean_rew: -10.948951856643827, variance: 7.427671909332275, lamda: 1.468522071838379
Running avgs for agent 1: q_loss: 30.314510345458984, p_loss: -5.5229597091674805, mean_rew: -10.975580036471497, variance: 7.680397033691406, lamda: 1.4582065343856812
Running avgs for agent 2: q_loss: 38.221961975097656, p_loss: -5.540692329406738, mean_rew: -10.959241127381238, variance: 7.746457576751709, lamda: 1.4209080934524536

steps: 974975, episodes: 39000, mean episode reward: -903.258442589019, agent episode reward: [-301.0861475296729, -301.0861475296729, -301.0861475296729], time: 167.894
steps: 974975, episodes: 39000, mean episode variance: 5.705985936164856, agent episode variance: [1.8955354194641114, 1.8823246369361877, 1.9281258797645568], time: 167.895
Running avgs for agent 0: q_loss: 42.53676986694336, p_loss: -5.537618160247803, mean_rew: -10.994394052756958, variance: 7.582141399383545, lamda: 1.4705945253372192
Running avgs for agent 1: q_loss: 27.090211868286133, p_loss: -5.542811393737793, mean_rew: -10.987575035098883, variance: 7.529298782348633, lamda: 1.4752988815307617
Running avgs for agent 2: q_loss: 38.745906829833984, p_loss: -5.5548505783081055, mean_rew: -10.985999318367472, variance: 7.712503433227539, lamda: 1.4230048656463623

steps: 999975, episodes: 40000, mean episode reward: -891.8423643149464, agent episode reward: [-297.2807881049821, -297.2807881049821, -297.2807881049821], time: 168.111
steps: 999975, episodes: 40000, mean episode variance: 5.739466748952865, agent episode variance: [1.8928965137004852, 1.8854167203903198, 1.9611535148620605], time: 168.111
Running avgs for agent 0: q_loss: 42.922508239746094, p_loss: -5.5513153076171875, mean_rew: -11.028220286695985, variance: 7.5715861320495605, lamda: 1.4744423627853394
Running avgs for agent 1: q_loss: 38.846370697021484, p_loss: -5.551388263702393, mean_rew: -11.04033612845991, variance: 7.5416669845581055, lamda: 1.4872255325317383
Running avgs for agent 2: q_loss: 37.69340896606445, p_loss: -5.562576770782471, mean_rew: -11.018184391410186, variance: 7.844614028930664, lamda: 1.4252961874008179

steps: 1024975, episodes: 41000, mean episode reward: -915.6647374838645, agent episode reward: [-305.2215791612882, -305.2215791612882, -305.2215791612882], time: 169.26
steps: 1024975, episodes: 41000, mean episode variance: 5.737061227083206, agent episode variance: [1.8946325888633728, 1.8848469808101653, 1.957581657409668], time: 169.261
Running avgs for agent 0: q_loss: 42.194671630859375, p_loss: -5.592784404754639, mean_rew: -11.090957367979467, variance: 7.578530311584473, lamda: 1.4764577150344849
Running avgs for agent 1: q_loss: 38.683162689208984, p_loss: -5.569021701812744, mean_rew: -11.083216801410817, variance: 7.539388179779053, lamda: 1.489270806312561
Running avgs for agent 2: q_loss: 38.33428955078125, p_loss: -5.608676433563232, mean_rew: -11.10363972641404, variance: 7.830326557159424, lamda: 1.4270588159561157

steps: 1049975, episodes: 42000, mean episode reward: -894.7562165733723, agent episode reward: [-298.25207219112406, -298.25207219112406, -298.25207219112406], time: 164.043
steps: 1049975, episodes: 42000, mean episode variance: 5.775552724838257, agent episode variance: [1.9027987265586852, 1.9117481245994568, 1.9610058736801148], time: 164.044
Running avgs for agent 0: q_loss: 41.821807861328125, p_loss: -5.642220497131348, mean_rew: -11.200637704964562, variance: 7.611195087432861, lamda: 1.4777798652648926
Running avgs for agent 1: q_loss: 39.864830017089844, p_loss: -5.6198906898498535, mean_rew: -11.186042234483654, variance: 7.646992206573486, lamda: 1.4909437894821167
Running avgs for agent 2: q_loss: 38.14407730102539, p_loss: -5.645941257476807, mean_rew: -11.173703958592977, variance: 7.84402322769165, lamda: 1.429668664932251

steps: 1074975, episodes: 43000, mean episode reward: -927.7387843140948, agent episode reward: [-309.2462614380316, -309.2462614380316, -309.2462614380316], time: 167.901
steps: 1074975, episodes: 43000, mean episode variance: 5.780357349872589, agent episode variance: [1.9120559730529785, 1.9092108330726623, 1.9590905437469481], time: 167.901
Running avgs for agent 0: q_loss: 41.30629348754883, p_loss: -5.644229412078857, mean_rew: -11.206287782166031, variance: 7.648223400115967, lamda: 1.4790897369384766
Running avgs for agent 1: q_loss: 37.82595443725586, p_loss: -5.636887073516846, mean_rew: -11.228669613112157, variance: 7.636843204498291, lamda: 1.492498517036438
Running avgs for agent 2: q_loss: 36.68735122680664, p_loss: -5.666680335998535, mean_rew: -11.23192960499466, variance: 7.836362361907959, lamda: 1.431150197982788

steps: 1099975, episodes: 44000, mean episode reward: -931.7447410920093, agent episode reward: [-310.58158036400306, -310.58158036400306, -310.58158036400306], time: 169.587
steps: 1099975, episodes: 44000, mean episode variance: 5.789189588546753, agent episode variance: [1.9369237840175628, 1.8849501163959503, 1.9673156881332396], time: 169.587
Running avgs for agent 0: q_loss: 40.77024459838867, p_loss: -5.666983604431152, mean_rew: -11.255689151703557, variance: 7.747695446014404, lamda: 1.4803584814071655
Running avgs for agent 1: q_loss: 37.892154693603516, p_loss: -5.656014442443848, mean_rew: -11.247472691480016, variance: 7.539800643920898, lamda: 1.4931864738464355
Running avgs for agent 2: q_loss: 36.135921478271484, p_loss: -5.6798319816589355, mean_rew: -11.25295337475963, variance: 7.8692626953125, lamda: 1.4329699277877808

steps: 1124975, episodes: 45000, mean episode reward: -946.3686905418352, agent episode reward: [-315.4562301806118, -315.4562301806118, -315.4562301806118], time: 168.347
steps: 1124975, episodes: 45000, mean episode variance: 5.8110805239677426, agent episode variance: [1.9277106885910034, 1.904478687286377, 1.9788911480903626], time: 168.348
Running avgs for agent 0: q_loss: 40.01013946533203, p_loss: -5.670515537261963, mean_rew: -11.267551905606867, variance: 7.710842609405518, lamda: 1.4810981750488281
Running avgs for agent 1: q_loss: 38.20998001098633, p_loss: -5.653500080108643, mean_rew: -11.257834552663507, variance: 7.617915153503418, lamda: 1.494431972503662
Running avgs for agent 2: q_loss: 35.966365814208984, p_loss: -5.673676013946533, mean_rew: -11.25466093039399, variance: 7.915565013885498, lamda: 1.4339168071746826

steps: 1149975, episodes: 46000, mean episode reward: -934.4377667244252, agent episode reward: [-311.4792555748084, -311.4792555748084, -311.4792555748084], time: 167.806
steps: 1149975, episodes: 46000, mean episode variance: 5.827260754823684, agent episode variance: [1.932623125076294, 1.9193781492710114, 1.9752594804763794], time: 167.807
Running avgs for agent 0: q_loss: 38.50841522216797, p_loss: -5.717041969299316, mean_rew: -11.330179549667376, variance: 7.73049259185791, lamda: 1.481887936592102
Running avgs for agent 1: q_loss: 38.857139587402344, p_loss: -5.683309078216553, mean_rew: -11.316560493163855, variance: 7.6775126457214355, lamda: 1.4963266849517822
Running avgs for agent 2: q_loss: 36.851531982421875, p_loss: -5.706789493560791, mean_rew: -11.323684369601754, variance: 7.901037216186523, lamda: 1.43605637550354

steps: 1174975, episodes: 47000, mean episode reward: -942.4227845275976, agent episode reward: [-314.14092817586584, -314.14092817586584, -314.14092817586584], time: 166.486
steps: 1174975, episodes: 47000, mean episode variance: 5.831479405403138, agent episode variance: [1.9296279635429383, 1.9004411516189574, 2.0014102902412416], time: 166.487
Running avgs for agent 0: q_loss: 28.483169555664062, p_loss: -5.764100074768066, mean_rew: -11.418120025484976, variance: 7.718511581420898, lamda: 1.4917279481887817
Running avgs for agent 1: q_loss: 39.600311279296875, p_loss: -5.723887920379639, mean_rew: -11.396969978249665, variance: 7.601764678955078, lamda: 1.497444987297058
Running avgs for agent 2: q_loss: 36.887943267822266, p_loss: -5.742522239685059, mean_rew: -11.411700305407383, variance: 8.005640983581543, lamda: 1.437188744544983

steps: 1199975, episodes: 48000, mean episode reward: -944.3107420401652, agent episode reward: [-314.7702473467217, -314.7702473467217, -314.7702473467217], time: 161.857
steps: 1199975, episodes: 48000, mean episode variance: 5.865159517288208, agent episode variance: [1.9303610835075378, 1.9382748212814331, 1.9965236124992372], time: 161.857
Running avgs for agent 0: q_loss: 28.2854061126709, p_loss: -5.79229736328125, mean_rew: -11.48412406711433, variance: 7.721444129943848, lamda: 1.5035288333892822
Running avgs for agent 1: q_loss: 39.9124870300293, p_loss: -5.759659290313721, mean_rew: -11.491856997521491, variance: 7.753098964691162, lamda: 1.498827338218689
Running avgs for agent 2: q_loss: 37.07807159423828, p_loss: -5.776360988616943, mean_rew: -11.464435126465212, variance: 7.9860944747924805, lamda: 1.4378265142440796

steps: 1224975, episodes: 49000, mean episode reward: -942.0035011096248, agent episode reward: [-314.00116703654163, -314.00116703654163, -314.00116703654163], time: 164.163
steps: 1224975, episodes: 49000, mean episode variance: 5.880685104608536, agent episode variance: [1.9203713109493257, 1.9349792375564576, 2.0253345561027527], time: 164.163
Running avgs for agent 0: q_loss: 36.98918151855469, p_loss: -5.827345848083496, mean_rew: -11.570099838464463, variance: 7.681485176086426, lamda: 1.513217568397522
Running avgs for agent 1: q_loss: 40.5919303894043, p_loss: -5.8185930252075195, mean_rew: -11.574042397840822, variance: 7.739916801452637, lamda: 1.5003553628921509
Running avgs for agent 2: q_loss: 37.781837463378906, p_loss: -5.805605888366699, mean_rew: -11.563570224322165, variance: 8.101338386535645, lamda: 1.4383188486099243

steps: 1249975, episodes: 50000, mean episode reward: -936.8912785939448, agent episode reward: [-312.2970928646483, -312.2970928646483, -312.2970928646483], time: 162.813
steps: 1249975, episodes: 50000, mean episode variance: 5.874012805938721, agent episode variance: [1.91974622297287, 1.952417718410492, 2.0018488645553587], time: 162.813
Running avgs for agent 0: q_loss: 43.07583999633789, p_loss: -5.865041732788086, mean_rew: -11.647773104910033, variance: 7.67898416519165, lamda: 1.5156879425048828
Running avgs for agent 1: q_loss: 41.94255447387695, p_loss: -5.838862895965576, mean_rew: -11.635077753901522, variance: 7.809670925140381, lamda: 1.5026296377182007
Running avgs for agent 2: q_loss: 38.758174896240234, p_loss: -5.861151218414307, mean_rew: -11.646430717069844, variance: 8.00739574432373, lamda: 1.4395004510879517

steps: 1274975, episodes: 51000, mean episode reward: -916.1421577525826, agent episode reward: [-305.3807192508609, -305.3807192508609, -305.3807192508609], time: 162.432
steps: 1274975, episodes: 51000, mean episode variance: 5.919989475488663, agent episode variance: [1.9423755042552948, 1.9315197324752809, 2.046094238758087], time: 162.433
Running avgs for agent 0: q_loss: 42.859031677246094, p_loss: -5.902293682098389, mean_rew: -11.727247728863574, variance: 7.76950216293335, lamda: 1.516573190689087
Running avgs for agent 1: q_loss: 41.95621109008789, p_loss: -5.881535530090332, mean_rew: -11.717492010890146, variance: 7.726078987121582, lamda: 1.5052627325057983
Running avgs for agent 2: q_loss: 39.26613235473633, p_loss: -5.889090061187744, mean_rew: -11.718344154474249, variance: 8.184377670288086, lamda: 1.4402120113372803

steps: 1299975, episodes: 52000, mean episode reward: -934.4453619444122, agent episode reward: [-311.481787314804, -311.481787314804, -311.481787314804], time: 163.05
steps: 1299975, episodes: 52000, mean episode variance: 5.983885803222656, agent episode variance: [1.9667902426719666, 1.9681636090278625, 2.0489319515228273], time: 163.051
Running avgs for agent 0: q_loss: 42.820919036865234, p_loss: -5.927652359008789, mean_rew: -11.773672706315693, variance: 7.867161273956299, lamda: 1.51678466796875
Running avgs for agent 1: q_loss: 42.21120071411133, p_loss: -5.907738208770752, mean_rew: -11.782078892146764, variance: 7.872654914855957, lamda: 1.5067667961120605
Running avgs for agent 2: q_loss: 39.90586853027344, p_loss: -5.927839279174805, mean_rew: -11.779567942650742, variance: 8.19572639465332, lamda: 1.4411890506744385

steps: 1324975, episodes: 53000, mean episode reward: -896.9264219645158, agent episode reward: [-298.9754739881719, -298.9754739881719, -298.9754739881719], time: 173.257
steps: 1324975, episodes: 53000, mean episode variance: 5.964448543548584, agent episode variance: [1.9511466884613038, 1.9576224155426025, 2.0556794395446776], time: 173.258
Running avgs for agent 0: q_loss: 43.64373016357422, p_loss: -5.96212100982666, mean_rew: -11.83155279171517, variance: 7.804586887359619, lamda: 1.517116904258728
Running avgs for agent 1: q_loss: 43.448360443115234, p_loss: -5.9554619789123535, mean_rew: -11.850957303426199, variance: 7.830489158630371, lamda: 1.5079271793365479
Running avgs for agent 2: q_loss: 39.7739143371582, p_loss: -5.949090957641602, mean_rew: -11.837546430946261, variance: 8.222716331481934, lamda: 1.4417243003845215

steps: 1349975, episodes: 54000, mean episode reward: -899.663560666382, agent episode reward: [-299.8878535554607, -299.8878535554607, -299.8878535554607], time: 180.235
steps: 1349975, episodes: 54000, mean episode variance: 6.004742199897766, agent episode variance: [1.9716243467330932, 1.9808548216819764, 2.0522630314826964], time: 180.235
Running avgs for agent 0: q_loss: 43.59014129638672, p_loss: -5.981027126312256, mean_rew: -11.882006370343083, variance: 7.8864970207214355, lamda: 1.5173344612121582
Running avgs for agent 1: q_loss: 43.45793151855469, p_loss: -5.9548139572143555, mean_rew: -11.867945051654177, variance: 7.923419952392578, lamda: 1.5094455480575562
Running avgs for agent 2: q_loss: 39.765872955322266, p_loss: -5.968451976776123, mean_rew: -11.877655915145649, variance: 8.209052085876465, lamda: 1.4422364234924316

steps: 1374975, episodes: 55000, mean episode reward: -904.6584785792476, agent episode reward: [-301.5528261930826, -301.5528261930826, -301.5528261930826], time: 174.899
steps: 1374975, episodes: 55000, mean episode variance: 6.043515309333801, agent episode variance: [1.9779955887794494, 1.9649785442352294, 2.100541176319122], time: 174.9
Running avgs for agent 0: q_loss: 44.033573150634766, p_loss: -5.9918742179870605, mean_rew: -11.911064976804969, variance: 7.911982536315918, lamda: 1.5175124406814575
Running avgs for agent 1: q_loss: 43.60683059692383, p_loss: -5.982432842254639, mean_rew: -11.910747716950192, variance: 7.8599138259887695, lamda: 1.5102702379226685
Running avgs for agent 2: q_loss: 40.493839263916016, p_loss: -5.97830867767334, mean_rew: -11.919418085147312, variance: 8.402165412902832, lamda: 1.4438159465789795

steps: 1399975, episodes: 56000, mean episode reward: -895.2291479427715, agent episode reward: [-298.40971598092386, -298.40971598092386, -298.40971598092386], time: 177.129
steps: 1399975, episodes: 56000, mean episode variance: 6.028157191753388, agent episode variance: [1.9709562931060791, 1.9824400787353516, 2.074760819911957], time: 177.129
Running avgs for agent 0: q_loss: 44.26383590698242, p_loss: -6.025350093841553, mean_rew: -11.95914366885793, variance: 7.883825302124023, lamda: 1.5178751945495605
Running avgs for agent 1: q_loss: 43.28650665283203, p_loss: -5.997148036956787, mean_rew: -11.947920506743223, variance: 7.929760456085205, lamda: 1.5116370916366577
Running avgs for agent 2: q_loss: 40.38399124145508, p_loss: -6.00286340713501, mean_rew: -11.940825903509669, variance: 8.299042701721191, lamda: 1.4446982145309448

steps: 1424975, episodes: 57000, mean episode reward: -920.2080661927438, agent episode reward: [-306.736022064248, -306.736022064248, -306.736022064248], time: 168.187
steps: 1424975, episodes: 57000, mean episode variance: 6.0345157129764555, agent episode variance: [1.9842706649303437, 1.9714244837760926, 2.0788205642700195], time: 168.187
Running avgs for agent 0: q_loss: 44.038734436035156, p_loss: -6.025447368621826, mean_rew: -11.991532326575962, variance: 7.937082290649414, lamda: 1.5189499855041504
Running avgs for agent 1: q_loss: 43.075382232666016, p_loss: -6.006256580352783, mean_rew: -11.95645407271877, variance: 7.885698318481445, lamda: 1.5134062767028809
Running avgs for agent 2: q_loss: 40.34020233154297, p_loss: -6.021354675292969, mean_rew: -11.988567689281913, variance: 8.315281867980957, lamda: 1.4450392723083496

steps: 1449975, episodes: 58000, mean episode reward: -904.3438761361172, agent episode reward: [-301.44795871203905, -301.44795871203905, -301.44795871203905], time: 167.227
steps: 1449975, episodes: 58000, mean episode variance: 6.083303764820099, agent episode variance: [1.9816807808876038, 1.9997263102531433, 2.101896673679352], time: 167.227
Running avgs for agent 0: q_loss: 45.21802520751953, p_loss: -6.0362868309021, mean_rew: -12.008281373625827, variance: 7.926723003387451, lamda: 1.5197700262069702
Running avgs for agent 1: q_loss: 43.91646957397461, p_loss: -6.029696464538574, mean_rew: -12.005062279740503, variance: 7.998905181884766, lamda: 1.5146605968475342
Running avgs for agent 2: q_loss: 40.10849380493164, p_loss: -6.027030944824219, mean_rew: -12.021863140608257, variance: 8.407586097717285, lamda: 1.445306420326233

steps: 1474975, episodes: 59000, mean episode reward: -895.5125938699488, agent episode reward: [-298.5041979566496, -298.5041979566496, -298.5041979566496], time: 166.592
steps: 1474975, episodes: 59000, mean episode variance: 6.039844373226166, agent episode variance: [1.9911813831329346, 1.9652143650054932, 2.083448625087738], time: 166.593
Running avgs for agent 0: q_loss: 44.69921875, p_loss: -6.049070358276367, mean_rew: -12.029276364541532, variance: 7.964725494384766, lamda: 1.5217311382293701
Running avgs for agent 1: q_loss: 43.76219177246094, p_loss: -6.031598091125488, mean_rew: -12.019203736390898, variance: 7.860857963562012, lamda: 1.5168856382369995
Running avgs for agent 2: q_loss: 40.81875228881836, p_loss: -6.023403167724609, mean_rew: -12.01503848926281, variance: 8.333794593811035, lamda: 1.4459407329559326

steps: 1499975, episodes: 60000, mean episode reward: -891.8767807357747, agent episode reward: [-297.2922602452582, -297.2922602452582, -297.2922602452582], time: 160.629
steps: 1499975, episodes: 60000, mean episode variance: 6.050957363128662, agent episode variance: [1.9927500417232513, 1.9786572272777558, 2.0795500941276552], time: 160.629
Running avgs for agent 0: q_loss: 44.415382385253906, p_loss: -6.047786235809326, mean_rew: -12.042088807277715, variance: 7.9710001945495605, lamda: 1.5228395462036133
Running avgs for agent 1: q_loss: 43.86081314086914, p_loss: -6.040053367614746, mean_rew: -12.035845453217823, variance: 7.914628982543945, lamda: 1.5188838243484497
Running avgs for agent 2: q_loss: 40.94815444946289, p_loss: -6.054567337036133, mean_rew: -12.057981121944062, variance: 8.31820011138916, lamda: 1.4472270011901855

steps: 1524975, episodes: 61000, mean episode reward: -885.2941042762927, agent episode reward: [-295.09803475876424, -295.09803475876424, -295.09803475876424], time: 163.25
steps: 1524975, episodes: 61000, mean episode variance: 6.065947066783905, agent episode variance: [1.9756917991638183, 1.9941726508140565, 2.0960826168060303], time: 163.251
Running avgs for agent 0: q_loss: 44.18784713745117, p_loss: -6.057706356048584, mean_rew: -12.057767650507838, variance: 7.902767181396484, lamda: 1.524011254310608
Running avgs for agent 1: q_loss: 45.12233352661133, p_loss: -6.054084300994873, mean_rew: -12.050905415648387, variance: 7.976690292358398, lamda: 1.5210238695144653
Running avgs for agent 2: q_loss: 40.91497039794922, p_loss: -6.055194854736328, mean_rew: -12.064358082333088, variance: 8.384329795837402, lamda: 1.447980523109436

steps: 1549975, episodes: 62000, mean episode reward: -870.6757924231896, agent episode reward: [-290.2252641410632, -290.2252641410632, -290.2252641410632], time: 164.389
steps: 1549975, episodes: 62000, mean episode variance: 6.0369135789871216, agent episode variance: [1.9825354018211365, 1.988933539390564, 2.065444637775421], time: 164.39
Running avgs for agent 0: q_loss: 30.162948608398438, p_loss: -6.074272155761719, mean_rew: -12.08237802572761, variance: 7.930141448974609, lamda: 1.533676266670227
Running avgs for agent 1: q_loss: 44.43100357055664, p_loss: -6.0555100440979, mean_rew: -12.063300454146603, variance: 7.955733776092529, lamda: 1.5236153602600098
Running avgs for agent 2: q_loss: 40.67148971557617, p_loss: -6.057031154632568, mean_rew: -12.061148512933215, variance: 8.261777877807617, lamda: 1.448655605316162

steps: 1574975, episodes: 63000, mean episode reward: -870.0589489001295, agent episode reward: [-290.0196496333765, -290.0196496333765, -290.0196496333765], time: 166.923
steps: 1574975, episodes: 63000, mean episode variance: 6.000845778942108, agent episode variance: [1.95729878616333, 1.9789094457626342, 2.0646375470161438], time: 166.923
Running avgs for agent 0: q_loss: 29.1986141204834, p_loss: -6.038027286529541, mean_rew: -12.048832319151142, variance: 7.829195499420166, lamda: 1.5445055961608887
Running avgs for agent 1: q_loss: 44.61334991455078, p_loss: -6.054630756378174, mean_rew: -12.057125747162967, variance: 7.915637969970703, lamda: 1.5254372358322144
Running avgs for agent 2: q_loss: 41.28273391723633, p_loss: -6.055202007293701, mean_rew: -12.062845815313194, variance: 8.258549690246582, lamda: 1.4497929811477661

steps: 1599975, episodes: 64000, mean episode reward: -863.9586206555185, agent episode reward: [-287.98620688517286, -287.98620688517286, -287.98620688517286], time: 165.321
steps: 1599975, episodes: 64000, mean episode variance: 6.009005048036576, agent episode variance: [1.9425932326316833, 1.9810664732456207, 2.0853453421592714], time: 165.322
Running avgs for agent 0: q_loss: 34.683589935302734, p_loss: -6.026425361633301, mean_rew: -12.049769546482963, variance: 7.7703728675842285, lamda: 1.5557903051376343
Running avgs for agent 1: q_loss: 43.9819450378418, p_loss: -6.052517414093018, mean_rew: -12.067150499059306, variance: 7.924266338348389, lamda: 1.5264328718185425
Running avgs for agent 2: q_loss: 40.89640808105469, p_loss: -6.050825119018555, mean_rew: -12.059805981447676, variance: 8.341381072998047, lamda: 1.45100736618042

steps: 1624975, episodes: 65000, mean episode reward: -870.2882386354536, agent episode reward: [-290.0960795451512, -290.0960795451512, -290.0960795451512], time: 159.938
steps: 1624975, episodes: 65000, mean episode variance: 5.986867947340012, agent episode variance: [1.9343663084506988, 1.9659588012695313, 2.0865428376197817], time: 159.938
Running avgs for agent 0: q_loss: 43.522865295410156, p_loss: -6.039261817932129, mean_rew: -12.06048579420675, variance: 7.7374653816223145, lamda: 1.5586713552474976
Running avgs for agent 1: q_loss: 43.62616729736328, p_loss: -6.054981231689453, mean_rew: -12.06247470871521, variance: 7.863834857940674, lamda: 1.5278596878051758
Running avgs for agent 2: q_loss: 40.85516357421875, p_loss: -6.05550479888916, mean_rew: -12.064987783894297, variance: 8.346170425415039, lamda: 1.4525986909866333

steps: 1649975, episodes: 66000, mean episode reward: -877.1456468665909, agent episode reward: [-292.38188228886366, -292.38188228886366, -292.38188228886366], time: 160.414
steps: 1649975, episodes: 66000, mean episode variance: 5.996515772342682, agent episode variance: [1.939291835308075, 1.980912085056305, 2.076311851978302], time: 160.414
Running avgs for agent 0: q_loss: 43.64173889160156, p_loss: -6.053196907043457, mean_rew: -12.062644050200564, variance: 7.757167339324951, lamda: 1.5590224266052246
Running avgs for agent 1: q_loss: 43.89350891113281, p_loss: -6.058180332183838, mean_rew: -12.064284846847162, variance: 7.923648834228516, lamda: 1.5299386978149414
Running avgs for agent 2: q_loss: 41.63905334472656, p_loss: -6.058626651763916, mean_rew: -12.048978736396688, variance: 8.30524730682373, lamda: 1.4549181461334229

steps: 1674975, episodes: 67000, mean episode reward: -893.6799502568508, agent episode reward: [-297.8933167522836, -297.8933167522836, -297.8933167522836], time: 164.236
steps: 1674975, episodes: 67000, mean episode variance: 5.986704890727997, agent episode variance: [1.9547027893066407, 1.9621081771850586, 2.0698939242362977], time: 164.236
Running avgs for agent 0: q_loss: 43.94460678100586, p_loss: -6.049269676208496, mean_rew: -12.073413363882999, variance: 7.818811416625977, lamda: 1.5590548515319824
Running avgs for agent 1: q_loss: 43.48744583129883, p_loss: -6.055417060852051, mean_rew: -12.064706805127262, variance: 7.848432540893555, lamda: 1.531832218170166
Running avgs for agent 2: q_loss: 40.22294998168945, p_loss: -6.048847675323486, mean_rew: -12.047519363126241, variance: 8.27957534790039, lamda: 1.456706166267395

steps: 1699975, episodes: 68000, mean episode reward: -871.1264133549155, agent episode reward: [-290.37547111830514, -290.37547111830514, -290.37547111830514], time: 162.353
steps: 1699975, episodes: 68000, mean episode variance: 5.973464724540711, agent episode variance: [1.934759617805481, 1.9677959504127502, 2.0709091563224793], time: 162.353
Running avgs for agent 0: q_loss: 44.27572250366211, p_loss: -6.0529866218566895, mean_rew: -12.05476100205679, variance: 7.739037990570068, lamda: 1.5593783855438232
Running avgs for agent 1: q_loss: 43.67803192138672, p_loss: -6.05379581451416, mean_rew: -12.063193061642771, variance: 7.8711838722229, lamda: 1.532867193222046
Running avgs for agent 2: q_loss: 40.60275650024414, p_loss: -6.053104400634766, mean_rew: -12.058512257579627, variance: 8.283637046813965, lamda: 1.458042860031128

steps: 1724975, episodes: 69000, mean episode reward: -881.2031985410068, agent episode reward: [-293.7343995136689, -293.7343995136689, -293.7343995136689], time: 159.515
steps: 1724975, episodes: 69000, mean episode variance: 5.950005363941193, agent episode variance: [1.9266837601661682, 1.9621935229301453, 2.0611280808448793], time: 159.516
Running avgs for agent 0: q_loss: 43.38174819946289, p_loss: -6.0512919425964355, mean_rew: -12.056394781940897, variance: 7.706734657287598, lamda: 1.5596985816955566
Running avgs for agent 1: q_loss: 43.25833511352539, p_loss: -6.073731899261475, mean_rew: -12.068574146833068, variance: 7.848773956298828, lamda: 1.5348628759384155
Running avgs for agent 2: q_loss: 40.426231384277344, p_loss: -6.0574049949646, mean_rew: -12.052067599797919, variance: 8.244512557983398, lamda: 1.4602326154708862

steps: 1749975, episodes: 70000, mean episode reward: -890.0299047445658, agent episode reward: [-296.67663491485524, -296.67663491485524, -296.67663491485524], time: 159.096
steps: 1749975, episodes: 70000, mean episode variance: 5.980704447507859, agent episode variance: [1.9326447987556457, 1.9756652781963349, 2.0723943705558776], time: 159.096
Running avgs for agent 0: q_loss: 42.90636444091797, p_loss: -6.051768779754639, mean_rew: -12.067446748864066, variance: 7.730578899383545, lamda: 1.5606077909469604
Running avgs for agent 1: q_loss: 43.570838928222656, p_loss: -6.055395603179932, mean_rew: -12.052053230328687, variance: 7.902661323547363, lamda: 1.5364270210266113
Running avgs for agent 2: q_loss: 40.16701889038086, p_loss: -6.063412189483643, mean_rew: -12.077882265458571, variance: 8.289578437805176, lamda: 1.4611538648605347

steps: 1774975, episodes: 71000, mean episode reward: -892.5788455414944, agent episode reward: [-297.52628184716485, -297.52628184716485, -297.52628184716485], time: 165.606
steps: 1774975, episodes: 71000, mean episode variance: 5.961236770868301, agent episode variance: [1.932064175605774, 1.9672890508174896, 2.061883544445038], time: 165.607
Running avgs for agent 0: q_loss: 43.327693939208984, p_loss: -6.043156147003174, mean_rew: -12.049733860247832, variance: 7.728256702423096, lamda: 1.5615788698196411
Running avgs for agent 1: q_loss: 43.3791618347168, p_loss: -6.070882320404053, mean_rew: -12.069718862124011, variance: 7.869156360626221, lamda: 1.538818120956421
Running avgs for agent 2: q_loss: 31.581880569458008, p_loss: -6.066440582275391, mean_rew: -12.07305422779591, variance: 8.247533798217773, lamda: 1.4702945947647095

steps: 1799975, episodes: 72000, mean episode reward: -871.2693288549268, agent episode reward: [-290.42310961830896, -290.42310961830896, -290.42310961830896], time: 167.009
steps: 1799975, episodes: 72000, mean episode variance: 5.935658314943313, agent episode variance: [1.9363448021411895, 1.9549548716545104, 2.0443586411476136], time: 167.009
Running avgs for agent 0: q_loss: 43.35895919799805, p_loss: -6.04831075668335, mean_rew: -12.047827092041752, variance: 7.7453789710998535, lamda: 1.5631171464920044
Running avgs for agent 1: q_loss: 43.22886657714844, p_loss: -6.060384750366211, mean_rew: -12.051591714722882, variance: 7.819819450378418, lamda: 1.5407795906066895
Running avgs for agent 2: q_loss: 40.832244873046875, p_loss: -6.058801651000977, mean_rew: -12.066175160208644, variance: 8.177434921264648, lamda: 1.4803693294525146

steps: 1824975, episodes: 73000, mean episode reward: -862.8829110215604, agent episode reward: [-287.62763700718676, -287.62763700718676, -287.62763700718676], time: 166.056
steps: 1824975, episodes: 73000, mean episode variance: 5.932371978998185, agent episode variance: [1.9352887053489685, 1.9663218061923982, 2.0307614674568177], time: 166.056
Running avgs for agent 0: q_loss: 35.00434112548828, p_loss: -6.043850421905518, mean_rew: -12.037597751230845, variance: 7.741154670715332, lamda: 1.5672880411148071
Running avgs for agent 1: q_loss: 40.75366973876953, p_loss: -6.062035083770752, mean_rew: -12.052319293813763, variance: 7.8652873039245605, lamda: 1.5449168682098389
Running avgs for agent 2: q_loss: 39.73064041137695, p_loss: -6.036351680755615, mean_rew: -12.016838462877097, variance: 8.123045921325684, lamda: 1.4817662239074707

steps: 1849975, episodes: 74000, mean episode reward: -868.6428345305649, agent episode reward: [-289.54761151018823, -289.54761151018823, -289.54761151018823], time: 163.546
steps: 1849975, episodes: 74000, mean episode variance: 5.874333343505859, agent episode variance: [1.9181400227546692, 1.9368711886405945, 2.0193221321105956], time: 163.546
Running avgs for agent 0: q_loss: 36.07729721069336, p_loss: -6.0344133377075195, mean_rew: -12.031987948326488, variance: 7.672560214996338, lamda: 1.5793341398239136
Running avgs for agent 1: q_loss: 37.58110046386719, p_loss: -6.055345058441162, mean_rew: -12.025027917238814, variance: 7.7474846839904785, lamda: 1.5585136413574219
Running avgs for agent 2: q_loss: 39.963993072509766, p_loss: -6.037511348724365, mean_rew: -12.001429459265418, variance: 8.077287673950195, lamda: 1.4834567308425903

steps: 1874975, episodes: 75000, mean episode reward: -875.1126686779995, agent episode reward: [-291.70422289266645, -291.70422289266645, -291.70422289266645], time: 171.126
steps: 1874975, episodes: 75000, mean episode variance: 5.858272483587265, agent episode variance: [1.9044866075515747, 1.9235843164920807, 2.0302015595436096], time: 171.126
Running avgs for agent 0: q_loss: 42.901912689208984, p_loss: -6.029446601867676, mean_rew: -12.005063567247646, variance: 7.617946624755859, lamda: 1.5820505619049072
Running avgs for agent 1: q_loss: 42.53926086425781, p_loss: -6.052674770355225, mean_rew: -12.025660806303836, variance: 7.694337368011475, lamda: 1.5620821714401245
Running avgs for agent 2: q_loss: 39.33849334716797, p_loss: -6.036569118499756, mean_rew: -12.012419264000965, variance: 8.120805740356445, lamda: 1.485187292098999

steps: 1899975, episodes: 76000, mean episode reward: -894.9366565764332, agent episode reward: [-298.3122188588111, -298.3122188588111, -298.3122188588111], time: 181.375
steps: 1899975, episodes: 76000, mean episode variance: 5.8309072020053865, agent episode variance: [1.8960828564167023, 1.917165786266327, 2.0176585593223573], time: 181.375
Running avgs for agent 0: q_loss: 42.18239974975586, p_loss: -6.027254104614258, mean_rew: -12.008271379469264, variance: 7.584331512451172, lamda: 1.5853413343429565
Running avgs for agent 1: q_loss: 41.42809295654297, p_loss: -6.0497822761535645, mean_rew: -12.01102843942091, variance: 7.668663024902344, lamda: 1.563220500946045
Running avgs for agent 2: q_loss: 39.940006256103516, p_loss: -6.04994010925293, mean_rew: -12.02214412424474, variance: 8.070633888244629, lamda: 1.486708402633667

steps: 1924975, episodes: 77000, mean episode reward: -880.7145688691656, agent episode reward: [-293.57152295638855, -293.57152295638855, -293.57152295638855], time: 179.7
steps: 1924975, episodes: 77000, mean episode variance: 5.8384768085479735, agent episode variance: [1.9006866416931152, 1.9274267497062683, 2.0103634171485902], time: 179.701
Running avgs for agent 0: q_loss: 42.871986389160156, p_loss: -6.035582065582275, mean_rew: -12.014708283576708, variance: 7.602746486663818, lamda: 1.5874906778335571
Running avgs for agent 1: q_loss: 41.8283805847168, p_loss: -6.049495220184326, mean_rew: -12.018606944421519, variance: 7.709707260131836, lamda: 1.564420461654663
Running avgs for agent 2: q_loss: 39.988094329833984, p_loss: -6.041199207305908, mean_rew: -12.007115661027827, variance: 8.04145336151123, lamda: 1.4876552820205688

steps: 1949975, episodes: 78000, mean episode reward: -889.9199455739025, agent episode reward: [-296.6399818579674, -296.6399818579674, -296.6399818579674], time: 180.024
steps: 1949975, episodes: 78000, mean episode variance: 5.824509182929993, agent episode variance: [1.8838904576301574, 1.9194036302566528, 2.0212150950431824], time: 180.025
Running avgs for agent 0: q_loss: 35.28285217285156, p_loss: -6.031303882598877, mean_rew: -12.0092210741934, variance: 7.535562038421631, lamda: 1.5961803197860718
Running avgs for agent 1: q_loss: 41.83865737915039, p_loss: -6.042020797729492, mean_rew: -12.001247562580865, variance: 7.677614688873291, lamda: 1.565734624862671
Running avgs for agent 2: q_loss: 39.40123748779297, p_loss: -6.044806003570557, mean_rew: -12.012490232274438, variance: 8.084860801696777, lamda: 1.4890931844711304

steps: 1974975, episodes: 79000, mean episode reward: -898.9004810099528, agent episode reward: [-299.63349366998426, -299.63349366998426, -299.63349366998426], time: 174.136
steps: 1974975, episodes: 79000, mean episode variance: 5.818954498291015, agent episode variance: [1.8631025142669677, 1.9187338852882385, 2.0371180987358093], time: 174.136
Running avgs for agent 0: q_loss: 29.518983840942383, p_loss: -6.0228986740112305, mean_rew: -12.00960392123099, variance: 7.4524102210998535, lamda: 1.6123477220535278
Running avgs for agent 1: q_loss: 42.387142181396484, p_loss: -6.050439357757568, mean_rew: -12.016478920330114, variance: 7.674935340881348, lamda: 1.5671521425247192
Running avgs for agent 2: q_loss: 39.864601135253906, p_loss: -6.044398307800293, mean_rew: -12.014472895873086, variance: 8.14847183227539, lamda: 1.490888237953186

steps: 1999975, episodes: 80000, mean episode reward: -897.3353470633386, agent episode reward: [-299.1117823544462, -299.1117823544462, -299.1117823544462], time: 183.443
steps: 1999975, episodes: 80000, mean episode variance: 5.790476665258407, agent episode variance: [1.8545976412296294, 1.9225840363502502, 2.013294987678528], time: 183.443
Running avgs for agent 0: q_loss: 37.137760162353516, p_loss: -6.039750099182129, mean_rew: -12.01608412656072, variance: 7.41839075088501, lamda: 1.6213183403015137
Running avgs for agent 1: q_loss: 43.05257797241211, p_loss: -6.045772552490234, mean_rew: -12.005220233233972, variance: 7.690335750579834, lamda: 1.5691311359405518
Running avgs for agent 2: q_loss: 39.630043029785156, p_loss: -6.041704177856445, mean_rew: -11.999319805290122, variance: 8.053180694580078, lamda: 1.4936925172805786

steps: 2024975, episodes: 81000, mean episode reward: -886.6614977512312, agent episode reward: [-295.5538325837437, -295.5538325837437, -295.5538325837437], time: 184.295
steps: 2024975, episodes: 81000, mean episode variance: 5.761470987796783, agent episode variance: [1.842079516172409, 1.9025024836063384, 2.016888988018036], time: 184.296
Running avgs for agent 0: q_loss: 40.19691848754883, p_loss: -6.03350305557251, mean_rew: -12.007011236407015, variance: 7.368318557739258, lamda: 1.6302058696746826
Running avgs for agent 1: q_loss: 42.290802001953125, p_loss: -6.032168865203857, mean_rew: -11.984336345455395, variance: 7.610009670257568, lamda: 1.5707489252090454
Running avgs for agent 2: q_loss: 39.726200103759766, p_loss: -6.0432257652282715, mean_rew: -12.001703526066043, variance: 8.06755542755127, lamda: 1.4959383010864258

steps: 2049975, episodes: 82000, mean episode reward: -882.2960446992219, agent episode reward: [-294.0986815664073, -294.0986815664073, -294.0986815664073], time: 177.859
steps: 2049975, episodes: 82000, mean episode variance: 5.760594362735748, agent episode variance: [1.8399989376068115, 1.918359121799469, 2.0022363033294677], time: 177.859
Running avgs for agent 0: q_loss: 27.206830978393555, p_loss: -6.028129577636719, mean_rew: -12.004593342178108, variance: 7.3599958419799805, lamda: 1.6368122100830078
Running avgs for agent 1: q_loss: 41.88547134399414, p_loss: -6.044344425201416, mean_rew: -12.004262992801872, variance: 7.673436164855957, lamda: 1.571780800819397
Running avgs for agent 2: q_loss: 39.221683502197266, p_loss: -6.046356678009033, mean_rew: -11.99827376362031, variance: 8.00894546508789, lamda: 1.4970639944076538

steps: 2074975, episodes: 83000, mean episode reward: -900.5788577580233, agent episode reward: [-300.1929525860078, -300.1929525860078, -300.1929525860078], time: 173.626
steps: 2074975, episodes: 83000, mean episode variance: 5.729055149555206, agent episode variance: [1.8246485424041747, 1.9033987078666688, 2.001007899284363], time: 173.626
Running avgs for agent 0: q_loss: 30.236181259155273, p_loss: -6.019151210784912, mean_rew: -11.985551092843592, variance: 7.2985944747924805, lamda: 1.6483197212219238
Running avgs for agent 1: q_loss: 32.48961639404297, p_loss: -6.042285919189453, mean_rew: -11.982830189148034, variance: 7.6135945320129395, lamda: 1.5817487239837646
Running avgs for agent 2: q_loss: 38.66591262817383, p_loss: -6.038372039794922, mean_rew: -11.983547634532318, variance: 8.004032135009766, lamda: 1.4977943897247314

steps: 2099975, episodes: 84000, mean episode reward: -901.0122446095312, agent episode reward: [-300.33741486984377, -300.33741486984377, -300.33741486984377], time: 162.2
steps: 2099975, episodes: 84000, mean episode variance: 5.695259953975677, agent episode variance: [1.7957087545394896, 1.884589584350586, 2.014961615085602], time: 162.201
Running avgs for agent 0: q_loss: 27.664249420166016, p_loss: -6.008120059967041, mean_rew: -11.958929582750192, variance: 7.182835102081299, lamda: 1.6629557609558105
Running avgs for agent 1: q_loss: 29.27233123779297, p_loss: -6.035187244415283, mean_rew: -11.97790219567214, variance: 7.538358211517334, lamda: 1.5967656373977661
Running avgs for agent 2: q_loss: 38.78925323486328, p_loss: -6.0412373542785645, mean_rew: -11.989583645364423, variance: 8.059846878051758, lamda: 1.4998663663864136

steps: 2124975, episodes: 85000, mean episode reward: -911.4173429700918, agent episode reward: [-303.8057809900306, -303.8057809900306, -303.8057809900306], time: 165.486
steps: 2124975, episodes: 85000, mean episode variance: 5.635177210569382, agent episode variance: [1.7927198948860168, 1.8569378368854523, 1.9855194787979127], time: 165.486
Running avgs for agent 0: q_loss: 28.074039459228516, p_loss: -6.010616302490234, mean_rew: -11.977469756551741, variance: 7.170879364013672, lamda: 1.6744952201843262
Running avgs for agent 1: q_loss: 37.5898551940918, p_loss: -6.03773307800293, mean_rew: -11.972581589045939, variance: 7.427751541137695, lamda: 1.611748456954956
Running avgs for agent 2: q_loss: 38.72986602783203, p_loss: -6.027045726776123, mean_rew: -11.957465615351854, variance: 7.942078113555908, lamda: 1.5032446384429932

steps: 2149975, episodes: 86000, mean episode reward: -905.3526991601882, agent episode reward: [-301.7842330533961, -301.7842330533961, -301.7842330533961], time: 165.947
steps: 2149975, episodes: 86000, mean episode variance: 5.607896733999253, agent episode variance: [1.7685851333141327, 1.8498612051010133, 1.9894503955841065], time: 165.948
Running avgs for agent 0: q_loss: 31.242671966552734, p_loss: -6.005170822143555, mean_rew: -11.95833524858391, variance: 7.0743408203125, lamda: 1.684621810913086
Running avgs for agent 1: q_loss: 42.512046813964844, p_loss: -6.0222039222717285, mean_rew: -11.954176254090092, variance: 7.399445056915283, lamda: 1.6158766746520996
Running avgs for agent 2: q_loss: 28.911359786987305, p_loss: -6.02905797958374, mean_rew: -11.95388948750494, variance: 7.957801342010498, lamda: 1.5085179805755615

steps: 2174975, episodes: 87000, mean episode reward: -914.7489582433067, agent episode reward: [-304.9163194144355, -304.9163194144355, -304.9163194144355], time: 166.285
steps: 2174975, episodes: 87000, mean episode variance: 5.576939984798432, agent episode variance: [1.761127296924591, 1.843571578025818, 1.9722411098480224], time: 166.285
Running avgs for agent 0: q_loss: 27.648788452148438, p_loss: -5.998480319976807, mean_rew: -11.94402051054869, variance: 7.044509410858154, lamda: 1.6956442594528198
Running avgs for agent 1: q_loss: 34.69209289550781, p_loss: -6.020766735076904, mean_rew: -11.940509183379115, variance: 7.37428617477417, lamda: 1.6185908317565918
Running avgs for agent 2: q_loss: 25.052204132080078, p_loss: -6.029061317443848, mean_rew: -11.940161022807592, variance: 7.88896369934082, lamda: 1.5189059972763062

steps: 2199975, episodes: 88000, mean episode reward: -918.8616533142767, agent episode reward: [-306.28721777142556, -306.28721777142556, -306.28721777142556], time: 162.135
steps: 2199975, episodes: 88000, mean episode variance: 5.548359168529511, agent episode variance: [1.7547476320266724, 1.834223813533783, 1.9593877229690553], time: 162.136
Running avgs for agent 0: q_loss: 29.903179168701172, p_loss: -6.004269599914551, mean_rew: -11.946230575793166, variance: 7.0189900398254395, lamda: 1.706612467765808
Running avgs for agent 1: q_loss: 28.686038970947266, p_loss: -6.0234694480896, mean_rew: -11.942967700656087, variance: 7.33689546585083, lamda: 1.6274340152740479
Running avgs for agent 2: q_loss: 26.626096725463867, p_loss: -6.019682884216309, mean_rew: -11.935526502659334, variance: 7.837550640106201, lamda: 1.528668999671936

steps: 2224975, episodes: 89000, mean episode reward: -923.9698649616439, agent episode reward: [-307.9899549872147, -307.9899549872147, -307.9899549872147], time: 162.548
steps: 2224975, episodes: 89000, mean episode variance: 5.502062583446503, agent episode variance: [1.741234965801239, 1.8276354193687439, 1.9331921982765197], time: 162.548
Running avgs for agent 0: q_loss: 44.289737701416016, p_loss: -5.994567394256592, mean_rew: -11.927167866568984, variance: 6.964940071105957, lamda: 1.7144291400909424
Running avgs for agent 1: q_loss: 29.95734214782715, p_loss: -6.010408401489258, mean_rew: -11.92873746994948, variance: 7.31054162979126, lamda: 1.6396039724349976
Running avgs for agent 2: q_loss: 37.504547119140625, p_loss: -6.020595550537109, mean_rew: -11.925845956923954, variance: 7.732769012451172, lamda: 1.5407421588897705

steps: 2249975, episodes: 90000, mean episode reward: -924.0508803751571, agent episode reward: [-308.0169601250524, -308.0169601250524, -308.0169601250524], time: 166.002
steps: 2249975, episodes: 90000, mean episode variance: 5.489202039241791, agent episode variance: [1.744094898700714, 1.7997947802543641, 1.9453123602867126], time: 166.003
Running avgs for agent 0: q_loss: 39.846649169921875, p_loss: -5.985284328460693, mean_rew: -11.914304516679419, variance: 6.976379871368408, lamda: 1.7167627811431885
Running avgs for agent 1: q_loss: 28.731468200683594, p_loss: -6.012516498565674, mean_rew: -11.918264430300313, variance: 7.199178695678711, lamda: 1.6549981832504272
Running avgs for agent 2: q_loss: 37.44922637939453, p_loss: -6.011839866638184, mean_rew: -11.912839864226754, variance: 7.781249523162842, lamda: 1.542419195175171

steps: 2274975, episodes: 91000, mean episode reward: -918.1601401695951, agent episode reward: [-306.05338005653175, -306.05338005653175, -306.05338005653175], time: 161.2
steps: 2274975, episodes: 91000, mean episode variance: 5.440137881755829, agent episode variance: [1.7280775866508484, 1.794017958164215, 1.9180423369407653], time: 161.201
Running avgs for agent 0: q_loss: 27.936573028564453, p_loss: -6.000298500061035, mean_rew: -11.937141961192795, variance: 6.912310600280762, lamda: 1.7252061367034912
Running avgs for agent 1: q_loss: 29.305753707885742, p_loss: -6.003404140472412, mean_rew: -11.921801841687243, variance: 7.176071643829346, lamda: 1.6697126626968384
Running avgs for agent 2: q_loss: 36.650054931640625, p_loss: -6.015430450439453, mean_rew: -11.922688346523495, variance: 7.672168731689453, lamda: 1.5438307523727417

steps: 2299975, episodes: 92000, mean episode reward: -922.1494573417958, agent episode reward: [-307.3831524472653, -307.3831524472653, -307.3831524472653], time: 159.652
steps: 2299975, episodes: 92000, mean episode variance: 5.426005709409714, agent episode variance: [1.7169343116283418, 1.7721652231216432, 1.936906174659729], time: 159.653
Running avgs for agent 0: q_loss: 43.004661560058594, p_loss: -6.004107475280762, mean_rew: -11.937459151745022, variance: 6.867737293243408, lamda: 1.7352045774459839
Running avgs for agent 1: q_loss: 30.219228744506836, p_loss: -6.0133538246154785, mean_rew: -11.919358354320519, variance: 7.088661193847656, lamda: 1.683102011680603
Running avgs for agent 2: q_loss: 38.4705696105957, p_loss: -6.011178016662598, mean_rew: -11.9257824678971, variance: 7.74762487411499, lamda: 1.546844482421875

steps: 2324975, episodes: 93000, mean episode reward: -916.7188284447541, agent episode reward: [-305.57294281491806, -305.57294281491806, -305.57294281491806], time: 163.613
steps: 2324975, episodes: 93000, mean episode variance: 5.411574542284012, agent episode variance: [1.7236209869384767, 1.7624353659152985, 1.925518189430237], time: 163.614
Running avgs for agent 0: q_loss: 30.577293395996094, p_loss: -5.990896224975586, mean_rew: -11.917194819974263, variance: 6.89448356628418, lamda: 1.7432186603546143
Running avgs for agent 1: q_loss: 30.713829040527344, p_loss: -6.017026424407959, mean_rew: -11.938337965825376, variance: 7.049741268157959, lamda: 1.695889949798584
Running avgs for agent 2: q_loss: 38.389015197753906, p_loss: -6.018759250640869, mean_rew: -11.919328086140787, variance: 7.702072620391846, lamda: 1.5479482412338257

steps: 2349975, episodes: 94000, mean episode reward: -928.5667836517734, agent episode reward: [-309.52226121725784, -309.52226121725784, -309.52226121725784], time: 165.061
steps: 2349975, episodes: 94000, mean episode variance: 5.370125331401825, agent episode variance: [1.7001380462646485, 1.7399722306728362, 1.9300150544643402], time: 165.062
Running avgs for agent 0: q_loss: 30.068445205688477, p_loss: -6.000369071960449, mean_rew: -11.927581135692765, variance: 6.800551891326904, lamda: 1.7575974464416504
Running avgs for agent 1: q_loss: 39.79262161254883, p_loss: -6.021972179412842, mean_rew: -11.930901112792753, variance: 6.959888935089111, lamda: 1.7093595266342163
Running avgs for agent 2: q_loss: 37.78742980957031, p_loss: -6.017024517059326, mean_rew: -11.930261364992784, variance: 7.720060348510742, lamda: 1.5486693382263184

steps: 2374975, episodes: 95000, mean episode reward: -918.8380246130131, agent episode reward: [-306.27934153767103, -306.27934153767103, -306.27934153767103], time: 145.873
steps: 2374975, episodes: 95000, mean episode variance: 5.378765244483947, agent episode variance: [1.700823745250702, 1.7425584573745727, 1.935383041858673], time: 145.874
Running avgs for agent 0: q_loss: 37.77839660644531, p_loss: -6.010603904724121, mean_rew: -11.959150988340163, variance: 6.803295135498047, lamda: 1.7689318656921387
Running avgs for agent 1: q_loss: 28.960527420043945, p_loss: -6.0147385597229, mean_rew: -11.924194163540006, variance: 6.970233917236328, lamda: 1.7198642492294312
Running avgs for agent 2: q_loss: 38.5825080871582, p_loss: -6.023277282714844, mean_rew: -11.93758966125569, variance: 7.741532325744629, lamda: 1.5499398708343506

steps: 2399975, episodes: 96000, mean episode reward: -909.7469238489549, agent episode reward: [-303.2489746163184, -303.2489746163184, -303.2489746163184], time: 124.124
steps: 2399975, episodes: 96000, mean episode variance: 5.335964665412903, agent episode variance: [1.6955192997455597, 1.7169206607341767, 1.9235247049331665], time: 124.124
Running avgs for agent 0: q_loss: 44.51995849609375, p_loss: -6.004092693328857, mean_rew: -11.942566605970576, variance: 6.782076835632324, lamda: 1.7728171348571777
Running avgs for agent 1: q_loss: 37.61307144165039, p_loss: -6.019764423370361, mean_rew: -11.943394207959395, variance: 6.867682456970215, lamda: 1.73467218875885
Running avgs for agent 2: q_loss: 37.968326568603516, p_loss: -6.025335788726807, mean_rew: -11.943220846446827, variance: 7.694098472595215, lamda: 1.5512423515319824

steps: 2424975, episodes: 97000, mean episode reward: -912.0210470335128, agent episode reward: [-304.0070156778376, -304.0070156778376, -304.0070156778376], time: 121.389
steps: 2424975, episodes: 97000, mean episode variance: 5.372641406297683, agent episode variance: [1.6953320686817168, 1.7474335470199585, 1.9298757905960082], time: 121.39
Running avgs for agent 0: q_loss: 41.0144157409668, p_loss: -6.014472484588623, mean_rew: -11.956408375765637, variance: 6.781328201293945, lamda: 1.776916742324829
Running avgs for agent 1: q_loss: 46.645240783691406, p_loss: -6.026309967041016, mean_rew: -11.958494337775697, variance: 6.989734172821045, lamda: 1.739883542060852
Running avgs for agent 2: q_loss: 37.578590393066406, p_loss: -6.031101226806641, mean_rew: -11.952799504279296, variance: 7.719502925872803, lamda: 1.5524671077728271

steps: 2449975, episodes: 98000, mean episode reward: -904.9372076295172, agent episode reward: [-301.64573587650574, -301.64573587650574, -301.64573587650574], time: 124.939
steps: 2449975, episodes: 98000, mean episode variance: 5.322772690296173, agent episode variance: [1.6906438055038453, 1.6987702474594115, 1.9333586373329164], time: 124.94
Running avgs for agent 0: q_loss: 45.333351135253906, p_loss: -6.005132675170898, mean_rew: -11.934835993396957, variance: 6.762575149536133, lamda: 1.7856035232543945
Running avgs for agent 1: q_loss: 46.44118118286133, p_loss: -6.029672622680664, mean_rew: -11.939358106714412, variance: 6.79508113861084, lamda: 1.7411662340164185
Running avgs for agent 2: q_loss: 37.457767486572266, p_loss: -6.038576602935791, mean_rew: -11.958747894020515, variance: 7.733434677124023, lamda: 1.5535237789154053

steps: 2474975, episodes: 99000, mean episode reward: -914.0380297116684, agent episode reward: [-304.6793432372228, -304.6793432372228, -304.6793432372228], time: 115.879
steps: 2474975, episodes: 99000, mean episode variance: 5.3254412379264835, agent episode variance: [1.6822994425296784, 1.7118533346652984, 1.9312884607315064], time: 115.879
Running avgs for agent 0: q_loss: 44.466224670410156, p_loss: -6.0143280029296875, mean_rew: -11.944391677217714, variance: 6.729198455810547, lamda: 1.7892423868179321
Running avgs for agent 1: q_loss: 47.13351058959961, p_loss: -6.03242301940918, mean_rew: -11.962662986766425, variance: 6.847413063049316, lamda: 1.7443140745162964
Running avgs for agent 2: q_loss: 37.96836853027344, p_loss: -6.026249885559082, mean_rew: -11.93836474461586, variance: 7.725154399871826, lamda: 1.554890751838684

steps: 2499975, episodes: 100000, mean episode reward: -909.6000389616075, agent episode reward: [-303.20001298720257, -303.20001298720257, -303.20001298720257], time: 109.293
steps: 2499975, episodes: 100000, mean episode variance: 5.311373102903366, agent episode variance: [1.665130241394043, 1.7159454920291901, 1.930297369480133], time: 109.293
Running avgs for agent 0: q_loss: 31.34783935546875, p_loss: -6.011575222015381, mean_rew: -11.938547113393543, variance: 6.660521030426025, lamda: 1.7978256940841675
Running avgs for agent 1: q_loss: 44.21845245361328, p_loss: -6.040552616119385, mean_rew: -11.964117228639447, variance: 6.863781929016113, lamda: 1.7470046281814575
Running avgs for agent 2: q_loss: 37.50764846801758, p_loss: -6.028418064117432, mean_rew: -11.947169170826164, variance: 7.721189498901367, lamda: 1.5557960271835327

steps: 2524975, episodes: 101000, mean episode reward: -910.1799645286792, agent episode reward: [-303.39332150955977, -303.39332150955977, -303.39332150955977], time: 103.16
steps: 2524975, episodes: 101000, mean episode variance: 5.295091125011444, agent episode variance: [1.6667723703384398, 1.706546694278717, 1.9217720603942872], time: 103.161
Running avgs for agent 0: q_loss: 29.189838409423828, p_loss: -6.022933483123779, mean_rew: -11.961121029563675, variance: 6.667089462280273, lamda: 1.8116395473480225
Running avgs for agent 1: q_loss: 46.52210235595703, p_loss: -6.0242533683776855, mean_rew: -11.945755244349664, variance: 6.826186656951904, lamda: 1.7529816627502441
Running avgs for agent 2: q_loss: 38.21342086791992, p_loss: -6.0341925621032715, mean_rew: -11.952375882108685, variance: 7.6870880126953125, lamda: 1.5572654008865356

steps: 2549975, episodes: 102000, mean episode reward: -920.7398592519786, agent episode reward: [-306.9132864173262, -306.9132864173262, -306.9132864173262], time: 103.678
steps: 2549975, episodes: 102000, mean episode variance: 5.283026274681092, agent episode variance: [1.6491885480880737, 1.7116773977279662, 1.9221603288650513], time: 103.678
Running avgs for agent 0: q_loss: 31.913986206054688, p_loss: -6.027915954589844, mean_rew: -11.968531099264329, variance: 6.59675407409668, lamda: 1.8257800340652466
Running avgs for agent 1: q_loss: 45.725032806396484, p_loss: -6.039236545562744, mean_rew: -11.962198343263502, variance: 6.846709728240967, lamda: 1.7553550004959106
Running avgs for agent 2: q_loss: 38.546600341796875, p_loss: -6.04288387298584, mean_rew: -11.977110819365516, variance: 7.68864107131958, lamda: 1.5587085485458374

steps: 2574975, episodes: 103000, mean episode reward: -932.8405401674104, agent episode reward: [-310.9468467224701, -310.9468467224701, -310.9468467224701], time: 95.081
steps: 2574975, episodes: 103000, mean episode variance: 5.270432960033417, agent episode variance: [1.630899839401245, 1.7031702036857606, 1.9363629169464112], time: 95.081
Running avgs for agent 0: q_loss: 32.085304260253906, p_loss: -6.039522647857666, mean_rew: -11.988732526203295, variance: 6.523599624633789, lamda: 1.8386412858963013
Running avgs for agent 1: q_loss: 40.198795318603516, p_loss: -6.031029224395752, mean_rew: -11.9778368603542, variance: 6.812680721282959, lamda: 1.7608071565628052
Running avgs for agent 2: q_loss: 38.360755920410156, p_loss: -6.051527976989746, mean_rew: -11.990717910462255, variance: 7.7454514503479, lamda: 1.560174584388733

steps: 2599975, episodes: 104000, mean episode reward: -912.3584226651853, agent episode reward: [-304.1194742217284, -304.1194742217284, -304.1194742217284], time: 93.302
steps: 2599975, episodes: 104000, mean episode variance: 5.248960629463196, agent episode variance: [1.6244945955276489, 1.6936187825202942, 1.9308472514152526], time: 93.303
Running avgs for agent 0: q_loss: 32.10101318359375, p_loss: -6.051167964935303, mean_rew: -12.00834907562476, variance: 6.497978210449219, lamda: 1.8553202152252197
Running avgs for agent 1: q_loss: 43.41403579711914, p_loss: -6.054028511047363, mean_rew: -12.00833783403056, variance: 6.77447509765625, lamda: 1.7714229822158813
Running avgs for agent 2: q_loss: 37.970890045166016, p_loss: -6.066710948944092, mean_rew: -12.010001548925862, variance: 7.723388671875, lamda: 1.5612388849258423

steps: 2624975, episodes: 105000, mean episode reward: -913.724707601378, agent episode reward: [-304.5749025337927, -304.5749025337927, -304.5749025337927], time: 93.193
steps: 2624975, episodes: 105000, mean episode variance: 5.2601889927387235, agent episode variance: [1.6205154056549071, 1.7086677989959718, 1.9310057880878448], time: 93.194
Running avgs for agent 0: q_loss: 31.520517349243164, p_loss: -6.060503005981445, mean_rew: -12.023479305638805, variance: 6.482061386108398, lamda: 1.8673774003982544
Running avgs for agent 1: q_loss: 41.7327880859375, p_loss: -6.069852352142334, mean_rew: -12.031723340832665, variance: 6.834671497344971, lamda: 1.7761750221252441
Running avgs for agent 2: q_loss: 37.97087860107422, p_loss: -6.052848815917969, mean_rew: -12.013931408341076, variance: 7.724023342132568, lamda: 1.5621774196624756

steps: 2649975, episodes: 106000, mean episode reward: -909.4001921760782, agent episode reward: [-303.13339739202615, -303.13339739202615, -303.13339739202615], time: 93.142
steps: 2649975, episodes: 106000, mean episode variance: 5.229450292825699, agent episode variance: [1.6113335592746734, 1.6917728157043457, 1.9263439178466797], time: 93.142
Running avgs for agent 0: q_loss: 32.704139709472656, p_loss: -6.08017110824585, mean_rew: -12.053812179149762, variance: 6.445334434509277, lamda: 1.880138635635376
Running avgs for agent 1: q_loss: 31.860754013061523, p_loss: -6.06839656829834, mean_rew: -12.042992048108498, variance: 6.767091751098633, lamda: 1.7861814498901367
Running avgs for agent 2: q_loss: 38.52117919921875, p_loss: -6.066366672515869, mean_rew: -12.030797552005753, variance: 7.705375671386719, lamda: 1.5631687641143799

steps: 2674975, episodes: 107000, mean episode reward: -904.6619293131251, agent episode reward: [-301.55397643770834, -301.55397643770834, -301.55397643770834], time: 92.477
steps: 2674975, episodes: 107000, mean episode variance: 5.237700342893601, agent episode variance: [1.6102894783020019, 1.6927630379199983, 1.9346478266716003], time: 92.477
Running avgs for agent 0: q_loss: 31.584701538085938, p_loss: -6.06535530090332, mean_rew: -12.038359617460737, variance: 6.441158294677734, lamda: 1.8926314115524292
Running avgs for agent 1: q_loss: 46.251949310302734, p_loss: -6.082103729248047, mean_rew: -12.05369277780709, variance: 6.771052360534668, lamda: 1.796083927154541
Running avgs for agent 2: q_loss: 38.27229309082031, p_loss: -6.062816619873047, mean_rew: -12.023191158060852, variance: 7.73859167098999, lamda: 1.5645008087158203

steps: 2699975, episodes: 108000, mean episode reward: -921.0067411969675, agent episode reward: [-307.00224706565587, -307.00224706565587, -307.00224706565587], time: 92.226
steps: 2699975, episodes: 108000, mean episode variance: 5.191957796096802, agent episode variance: [1.5782790155410766, 1.6806673424243928, 1.9330114381313324], time: 92.226
Running avgs for agent 0: q_loss: 31.92428207397461, p_loss: -6.084254264831543, mean_rew: -12.047075329515476, variance: 6.313116073608398, lamda: 1.9047579765319824
Running avgs for agent 1: q_loss: 40.94282150268555, p_loss: -6.073549270629883, mean_rew: -12.056142795887526, variance: 6.7226691246032715, lamda: 1.8033257722854614
Running avgs for agent 2: q_loss: 38.823726654052734, p_loss: -6.077281475067139, mean_rew: -12.064563830918713, variance: 7.732046127319336, lamda: 1.5662766695022583

steps: 2724975, episodes: 109000, mean episode reward: -898.4547671344089, agent episode reward: [-299.4849223781364, -299.4849223781364, -299.4849223781364], time: 95.761
steps: 2724975, episodes: 109000, mean episode variance: 5.17187455701828, agent episode variance: [1.5746348900794982, 1.670736186504364, 1.9265034804344177], time: 95.762
Running avgs for agent 0: q_loss: 31.36817169189453, p_loss: -6.084427833557129, mean_rew: -12.052912314894693, variance: 6.298540115356445, lamda: 1.9135698080062866
Running avgs for agent 1: q_loss: 51.11589431762695, p_loss: -6.090392589569092, mean_rew: -12.083451436805984, variance: 6.6829447746276855, lamda: 1.8113417625427246
Running avgs for agent 2: q_loss: 39.048240661621094, p_loss: -6.0821404457092285, mean_rew: -12.068286381185125, variance: 7.7060136795043945, lamda: 1.5675994157791138

steps: 2749975, episodes: 110000, mean episode reward: -894.9151943831873, agent episode reward: [-298.3050647943958, -298.3050647943958, -298.3050647943958], time: 93.975
steps: 2749975, episodes: 110000, mean episode variance: 5.177474870443344, agent episode variance: [1.5716645283699036, 1.6671500823497771, 1.9386602597236633], time: 93.976
Running avgs for agent 0: q_loss: 31.909757614135742, p_loss: -6.082827091217041, mean_rew: -12.05277491252035, variance: 6.28665828704834, lamda: 1.9237502813339233
Running avgs for agent 1: q_loss: 51.11565017700195, p_loss: -6.099877834320068, mean_rew: -12.091360245661129, variance: 6.668600559234619, lamda: 1.8151679039001465
Running avgs for agent 2: q_loss: 38.83039093017578, p_loss: -6.080918312072754, mean_rew: -12.071760068161947, variance: 7.754641056060791, lamda: 1.5686404705047607

steps: 2774975, episodes: 111000, mean episode reward: -899.1554167569832, agent episode reward: [-299.71847225232773, -299.71847225232773, -299.71847225232773], time: 81.075
steps: 2774975, episodes: 111000, mean episode variance: 5.154898803234101, agent episode variance: [1.562931911945343, 1.6618803005218505, 1.9300865907669067], time: 81.075
Running avgs for agent 0: q_loss: 34.6928825378418, p_loss: -6.092776775360107, mean_rew: -12.062445568814846, variance: 6.25172758102417, lamda: 1.9367097616195679
Running avgs for agent 1: q_loss: 50.56657028198242, p_loss: -6.086615085601807, mean_rew: -12.071101292882132, variance: 6.647521495819092, lamda: 1.8189613819122314
Running avgs for agent 2: q_loss: 39.56467819213867, p_loss: -6.06714391708374, mean_rew: -12.056142850286529, variance: 7.720346450805664, lamda: 1.5707029104232788

steps: 2799975, episodes: 112000, mean episode reward: -905.3081039897351, agent episode reward: [-301.76936799657835, -301.76936799657835, -301.76936799657835], time: 79.219
steps: 2799975, episodes: 112000, mean episode variance: 5.131901939868927, agent episode variance: [1.5436036396026611, 1.6681808609962463, 1.9201174392700195], time: 79.219
Running avgs for agent 0: q_loss: 38.13146209716797, p_loss: -6.0992021560668945, mean_rew: -12.071116675626904, variance: 6.17441463470459, lamda: 1.948394775390625
Running avgs for agent 1: q_loss: 50.98395919799805, p_loss: -6.098363399505615, mean_rew: -12.099867257698952, variance: 6.672723770141602, lamda: 1.821756362915039
Running avgs for agent 2: q_loss: 40.51767349243164, p_loss: -6.0844011306762695, mean_rew: -12.080137602351085, variance: 7.680469512939453, lamda: 1.572757601737976

steps: 2824975, episodes: 113000, mean episode reward: -903.1915908721994, agent episode reward: [-301.06386362406647, -301.06386362406647, -301.06386362406647], time: 80.539
steps: 2824975, episodes: 113000, mean episode variance: 5.135288007497787, agent episode variance: [1.5514448297023773, 1.6562388243675232, 1.927604353427887], time: 80.539
Running avgs for agent 0: q_loss: 39.89701843261719, p_loss: -6.112876892089844, mean_rew: -12.097876906583606, variance: 6.205779075622559, lamda: 1.9641159772872925
Running avgs for agent 1: q_loss: 48.13887405395508, p_loss: -6.096498012542725, mean_rew: -12.097796843814077, variance: 6.624955177307129, lamda: 1.830573558807373
Running avgs for agent 2: q_loss: 39.93425750732422, p_loss: -6.096289157867432, mean_rew: -12.11073606542392, variance: 7.710416793823242, lamda: 1.573898434638977

steps: 2849975, episodes: 114000, mean episode reward: -909.040767193131, agent episode reward: [-303.013589064377, -303.013589064377, -303.013589064377], time: 84.873
steps: 2849975, episodes: 114000, mean episode variance: 5.112137401580811, agent episode variance: [1.5445189158916472, 1.6462929236888886, 1.9213255620002747], time: 84.873
Running avgs for agent 0: q_loss: 52.60565948486328, p_loss: -6.104824066162109, mean_rew: -12.096908254453073, variance: 6.178075313568115, lamda: 1.972185492515564
Running avgs for agent 1: q_loss: 45.267181396484375, p_loss: -6.10643196105957, mean_rew: -12.098382743531118, variance: 6.585171699523926, lamda: 1.8384919166564941
Running avgs for agent 2: q_loss: 39.34088134765625, p_loss: -6.087388038635254, mean_rew: -12.088115318575504, variance: 7.685302734375, lamda: 1.5753405094146729

steps: 2874975, episodes: 115000, mean episode reward: -901.0130927828087, agent episode reward: [-300.3376975942695, -300.3376975942695, -300.3376975942695], time: 82.761
steps: 2874975, episodes: 115000, mean episode variance: 5.086144299983978, agent episode variance: [1.5314341516494752, 1.6348260111808777, 1.9198841371536255], time: 82.762
Running avgs for agent 0: q_loss: 53.73259735107422, p_loss: -6.123925685882568, mean_rew: -12.107900587418913, variance: 6.125736236572266, lamda: 1.9749388694763184
Running avgs for agent 1: q_loss: 39.16830062866211, p_loss: -6.105951309204102, mean_rew: -12.106716389403115, variance: 6.539303779602051, lamda: 1.8516093492507935
Running avgs for agent 2: q_loss: 40.03071212768555, p_loss: -6.101708889007568, mean_rew: -12.110754207966998, variance: 7.67953634262085, lamda: 1.576033353805542

steps: 2899975, episodes: 116000, mean episode reward: -894.4974977436643, agent episode reward: [-298.1658325812214, -298.1658325812214, -298.1658325812214], time: 75.701
steps: 2899975, episodes: 116000, mean episode variance: 5.0788779098987575, agent episode variance: [1.5266895685195923, 1.6251766328811645, 1.9270117084980012], time: 75.701
Running avgs for agent 0: q_loss: 53.38811492919922, p_loss: -6.1303486824035645, mean_rew: -12.13548863952879, variance: 6.106757640838623, lamda: 1.9774636030197144
Running avgs for agent 1: q_loss: 36.953365325927734, p_loss: -6.110919952392578, mean_rew: -12.129680700537028, variance: 6.500706672668457, lamda: 1.868646264076233
Running avgs for agent 2: q_loss: 40.375946044921875, p_loss: -6.097390651702881, mean_rew: -12.110234544122486, variance: 7.708046913146973, lamda: 1.5772873163223267

steps: 2924975, episodes: 117000, mean episode reward: -910.8724384097065, agent episode reward: [-303.62414613656887, -303.62414613656887, -303.62414613656887], time: 73.107
steps: 2924975, episodes: 117000, mean episode variance: 5.0949756565093995, agent episode variance: [1.544252624988556, 1.6125995907783508, 1.9381234407424928], time: 73.108
Running avgs for agent 0: q_loss: 55.06039047241211, p_loss: -6.123173713684082, mean_rew: -12.120979234515413, variance: 6.177010536193848, lamda: 1.981324315071106
Running avgs for agent 1: q_loss: 45.07976531982422, p_loss: -6.1109490394592285, mean_rew: -12.115496865351634, variance: 6.4503984451293945, lamda: 1.881338119506836
Running avgs for agent 2: q_loss: 42.053733825683594, p_loss: -6.111686706542969, mean_rew: -12.128722369906486, variance: 7.752493858337402, lamda: 1.579077959060669

steps: 2949975, episodes: 118000, mean episode reward: -898.8599300958897, agent episode reward: [-299.6199766986299, -299.6199766986299, -299.6199766986299], time: 71.776
steps: 2949975, episodes: 118000, mean episode variance: 5.065306154489517, agent episode variance: [1.5254396743774414, 1.6130484077930451, 1.9268180723190307], time: 71.777
Running avgs for agent 0: q_loss: 48.539039611816406, p_loss: -6.135611534118652, mean_rew: -12.135692186365276, variance: 6.10175895690918, lamda: 1.9858168363571167
Running avgs for agent 1: q_loss: 54.01368713378906, p_loss: -6.127087116241455, mean_rew: -12.151610238338243, variance: 6.452193260192871, lamda: 1.8847639560699463
Running avgs for agent 2: q_loss: 40.8139762878418, p_loss: -6.106070041656494, mean_rew: -12.139216157162421, variance: 7.707272529602051, lamda: 1.5824626684188843

steps: 2974975, episodes: 119000, mean episode reward: -921.089631684611, agent episode reward: [-307.02987722820376, -307.02987722820376, -307.02987722820376], time: 80.069
steps: 2974975, episodes: 119000, mean episode variance: 5.070329299926758, agent episode variance: [1.5263285641670228, 1.6157425842285156, 1.9282581515312194], time: 80.069
Running avgs for agent 0: q_loss: 38.483646392822266, p_loss: -6.135732650756836, mean_rew: -12.139350502160088, variance: 6.105314254760742, lamda: 2.002274751663208
Running avgs for agent 1: q_loss: 42.76197814941406, p_loss: -6.121434211730957, mean_rew: -12.13673421317122, variance: 6.46297025680542, lamda: 1.890770673751831
Running avgs for agent 2: q_loss: 40.36920928955078, p_loss: -6.117039203643799, mean_rew: -12.1439472907619, variance: 7.713032245635986, lamda: 1.584723711013794

steps: 2999975, episodes: 120000, mean episode reward: -903.6503706700618, agent episode reward: [-301.21679022335394, -301.21679022335394, -301.21679022335394], time: 77.647
steps: 2999975, episodes: 120000, mean episode variance: 5.042365056991577, agent episode variance: [1.5225246987342835, 1.5981093878746033, 1.9217309703826904], time: 77.648
Running avgs for agent 0: q_loss: 56.43287658691406, p_loss: -6.133774757385254, mean_rew: -12.135168519616773, variance: 6.090098857879639, lamda: 2.015709400177002
Running avgs for agent 1: q_loss: 38.50681686401367, p_loss: -6.1038126945495605, mean_rew: -12.124371444463668, variance: 6.39243745803833, lamda: 1.9061132669448853
Running avgs for agent 2: q_loss: 40.97663116455078, p_loss: -6.103148460388184, mean_rew: -12.134138752060528, variance: 7.686924457550049, lamda: 1.5892233848571777

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -901.0752544455545, agent episode reward: [-300.3584181485181, -300.3584181485181, -300.3584181485181], time: 52.331
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 52.331
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -904.5660014785442, agent episode reward: [-301.5220004928481, -301.5220004928481, -301.5220004928481], time: 63.411
steps: 49975, episodes: 2000, mean episode variance: 4.39380454993248, agent episode variance: [0.9458660500049592, 1.9662068214416504, 1.4817316784858703], time: 63.411
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.03987248519813, variance: 3.876500368118286, lamda: 2.0184547901153564
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.044029478814052, variance: 8.05822467803955, lamda: 1.9141672849655151
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.038331524851932, variance: 6.072670936584473, lamda: 1.5910470485687256

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567761470.1092149: line 9: --exp_var_alpha: command not found
