# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 40.0 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies9/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies9/02-non-linear-exp_var/
Job <1091589> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc227>>
arglist.u_estimation True
2019-09-06 05:07:50.029165: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -528.9437295979507, agent episode reward: [-176.31457653265022, -176.31457653265022, -176.31457653265022], time: 88.242
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 88.242
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -793.1503130396777, agent episode reward: [-264.3834376798925, -264.3834376798925, -264.3834376798925], time: 124.268
steps: 49975, episodes: 2000, mean episode variance: 6.935069718390703, agent episode variance: [2.96469611620903, 1.0877819277346135, 2.8825916744470597], time: 124.268
Running avgs for agent 0: q_loss: 105.58604431152344, p_loss: -5.806055068969727, mean_rew: -7.957654996364149, variance: 12.150393918889467, lamda: 1.0111491680145264
Running avgs for agent 1: q_loss: 787.8184204101562, p_loss: 12.68164348602295, mean_rew: -7.961539830002585, variance: 4.458122654650055, lamda: 1.010532259941101
Running avgs for agent 2: q_loss: 92.46430206298828, p_loss: -5.591443061828613, mean_rew: -7.96231290496531, variance: 11.8139003051109, lamda: 1.010233759880066

steps: 74975, episodes: 3000, mean episode reward: -752.7418574862904, agent episode reward: [-250.91395249543012, -250.91395249543012, -250.91395249543012], time: 114.874
steps: 74975, episodes: 3000, mean episode variance: 10.31900836443901, agent episode variance: [2.201874044418335, 5.885018772363662, 2.232115547657013], time: 114.874
Running avgs for agent 0: q_loss: 50.98302459716797, p_loss: -4.938359260559082, mean_rew: -9.052225308367037, variance: 8.807496070861816, lamda: 1.0352303981781006
Running avgs for agent 1: q_loss: 15300.8740234375, p_loss: 32.19288635253906, mean_rew: -9.035985061408004, variance: 23.54007508945465, lamda: 1.0351879596710205
Running avgs for agent 2: q_loss: 43.661685943603516, p_loss: -4.919008731842041, mean_rew: -9.058856101277598, variance: 8.928462982177734, lamda: 1.0333315134048462

steps: 99975, episodes: 4000, mean episode reward: -804.3537951184497, agent episode reward: [-268.11793170614993, -268.11793170614993, -268.11793170614993], time: 112.452
steps: 99975, episodes: 4000, mean episode variance: 16.465140299081803, agent episode variance: [2.2342531220912933, 11.931874602079391, 2.2990125749111177], time: 112.453
Running avgs for agent 0: q_loss: 44.633705139160156, p_loss: -4.919501781463623, mean_rew: -9.45165603541234, variance: 8.93701171875, lamda: 1.0600391626358032
Running avgs for agent 1: q_loss: 66155.515625, p_loss: 53.803985595703125, mean_rew: -9.47149100486791, variance: 47.727498408317565, lamda: 1.0601922273635864
Running avgs for agent 2: q_loss: 59.087066650390625, p_loss: -4.797097682952881, mean_rew: -9.461592384205426, variance: 9.196050643920898, lamda: 1.0578066110610962

steps: 124975, episodes: 5000, mean episode reward: -725.9182161522907, agent episode reward: [-241.97273871743022, -241.97273871743022, -241.97273871743022], time: 113.142
steps: 124975, episodes: 5000, mean episode variance: 33.91193921232224, agent episode variance: [2.232466453075409, 29.38398562717438, 2.2954871320724486], time: 113.143
Running avgs for agent 0: q_loss: 57.02671813964844, p_loss: -5.000133037567139, mean_rew: -9.641053703975953, variance: 8.929864883422852, lamda: 1.0831763744354248
Running avgs for agent 1: q_loss: 224925.15625, p_loss: 73.27717590332031, mean_rew: -9.646780816128208, variance: 117.53594250869752, lamda: 1.0851962566375732
Running avgs for agent 2: q_loss: 52.94439697265625, p_loss: -4.9129462242126465, mean_rew: -9.6496668191608, variance: 9.1819486618042, lamda: 1.0773450136184692

steps: 149975, episodes: 6000, mean episode reward: -664.8334632376466, agent episode reward: [-221.61115441254887, -221.61115441254887, -221.61115441254887], time: 144.311
steps: 149975, episodes: 6000, mean episode variance: 57.148349285840986, agent episode variance: [2.143007497549057, 52.79674169826507, 2.2086000900268554], time: 144.311
Running avgs for agent 0: q_loss: 68.31581115722656, p_loss: -4.944737911224365, mean_rew: -9.568757290562381, variance: 8.572030067443848, lamda: 1.1077221632003784
Running avgs for agent 1: q_loss: 807914.125, p_loss: 90.79551696777344, mean_rew: -9.602818981307108, variance: 211.1869667930603, lamda: 1.1102005243301392
Running avgs for agent 2: q_loss: 49.518638610839844, p_loss: -4.90573787689209, mean_rew: -9.588019513652032, variance: 8.834400177001953, lamda: 1.0969408750534058

steps: 174975, episodes: 7000, mean episode reward: -603.3606115779862, agent episode reward: [-201.12020385932877, -201.12020385932877, -201.12020385932877], time: 161.431
steps: 174975, episodes: 7000, mean episode variance: 41.85411881899834, agent episode variance: [2.122150037765503, 37.585999507904056, 2.145969273328781], time: 161.431
Running avgs for agent 0: q_loss: 69.23843383789062, p_loss: -4.844849586486816, mean_rew: -9.406315871339096, variance: 8.488600730895996, lamda: 1.132756233215332
Running avgs for agent 1: q_loss: 496915.28125, p_loss: 104.43428802490234, mean_rew: -9.386620861972325, variance: 150.34399803161622, lamda: 1.1352046728134155
Running avgs for agent 2: q_loss: 42.21546936035156, p_loss: -4.805666446685791, mean_rew: -9.403230491362475, variance: 8.583877563476562, lamda: 1.1158479452133179

steps: 199975, episodes: 8000, mean episode reward: -652.0147763125187, agent episode reward: [-217.33825877083956, -217.33825877083956, -217.33825877083956], time: 161.562
steps: 199975, episodes: 8000, mean episode variance: 44.55208100950718, agent episode variance: [2.0176306921243667, 40.4898885269165, 2.0445617904663087], time: 161.563
Running avgs for agent 0: q_loss: 65.93670654296875, p_loss: -4.770342826843262, mean_rew: -9.24202466559795, variance: 8.070523262023926, lamda: 1.1577861309051514
Running avgs for agent 1: q_loss: 513153.53125, p_loss: 116.06781768798828, mean_rew: -9.233566905659373, variance: 161.959554107666, lamda: 1.1602087020874023
Running avgs for agent 2: q_loss: 29.522024154663086, p_loss: -4.69312047958374, mean_rew: -9.242676374362953, variance: 8.178247451782227, lamda: 1.1382802724838257

steps: 224975, episodes: 9000, mean episode reward: -661.3714034724145, agent episode reward: [-220.4571344908048, -220.4571344908048, -220.4571344908048], time: 161.069
steps: 224975, episodes: 9000, mean episode variance: 52.57744247078895, agent episode variance: [1.9675692131519318, 48.61694105529785, 1.9929322023391725], time: 161.07
Running avgs for agent 0: q_loss: 45.54631042480469, p_loss: -4.723537921905518, mean_rew: -9.197015446213372, variance: 7.870276927947998, lamda: 1.1827934980392456
Running avgs for agent 1: q_loss: 663592.3125, p_loss: 127.49989318847656, mean_rew: -9.210286375789277, variance: 194.4677642211914, lamda: 1.1852128505706787
Running avgs for agent 2: q_loss: 25.268054962158203, p_loss: -4.664259910583496, mean_rew: -9.1884446655414, variance: 7.971729278564453, lamda: 1.1594098806381226

steps: 249975, episodes: 10000, mean episode reward: -627.8489432932779, agent episode reward: [-209.2829810977593, -209.2829810977593, -209.2829810977593], time: 160.348
steps: 249975, episodes: 10000, mean episode variance: 55.5376987657547, agent episode variance: [1.9092667853832246, 51.666028533935545, 1.9624034464359283], time: 160.349
Running avgs for agent 0: q_loss: 40.869415283203125, p_loss: -4.66588020324707, mean_rew: -9.139728696926785, variance: 7.6370673179626465, lamda: 1.207797646522522
Running avgs for agent 1: q_loss: 844433.8125, p_loss: 138.34494018554688, mean_rew: -9.13975681726991, variance: 206.66411413574218, lamda: 1.2102171182632446
Running avgs for agent 2: q_loss: 36.63871765136719, p_loss: -4.654285430908203, mean_rew: -9.152513973879453, variance: 7.849613666534424, lamda: 1.1748014688491821

steps: 274975, episodes: 11000, mean episode reward: -641.6734262057904, agent episode reward: [-213.8911420685968, -213.8911420685968, -213.8911420685968], time: 159.886
steps: 274975, episodes: 11000, mean episode variance: 57.79514793920517, agent episode variance: [1.8643892755508422, 53.965036140441896, 1.965722523212433], time: 159.887
Running avgs for agent 0: q_loss: 45.79368591308594, p_loss: -4.62973690032959, mean_rew: -9.076431358810943, variance: 7.457557201385498, lamda: 1.2327286005020142
Running avgs for agent 1: q_loss: 1010996.25, p_loss: 148.13189697265625, mean_rew: -9.062601320417588, variance: 215.86014456176758, lamda: 1.2352211475372314
Running avgs for agent 2: q_loss: 34.96297836303711, p_loss: -4.589596748352051, mean_rew: -9.069261608119627, variance: 7.862890243530273, lamda: 1.1800708770751953

steps: 299975, episodes: 12000, mean episode reward: -626.5592794072687, agent episode reward: [-208.85309313575624, -208.85309313575624, -208.85309313575624], time: 158.988
steps: 299975, episodes: 12000, mean episode variance: 66.82386534976959, agent episode variance: [1.8087239446640015, 63.08989094543457, 1.9252504596710205], time: 158.989
Running avgs for agent 0: q_loss: 43.29109191894531, p_loss: -4.597309112548828, mean_rew: -9.022465277569633, variance: 7.234896183013916, lamda: 1.25741708278656
Running avgs for agent 1: q_loss: 1153716.5, p_loss: 157.37124633789062, mean_rew: -9.020244820085136, variance: 252.35956378173827, lamda: 1.2602252960205078
Running avgs for agent 2: q_loss: 33.818084716796875, p_loss: -4.5923614501953125, mean_rew: -9.029496833791793, variance: 7.70100212097168, lamda: 1.1861058473587036

steps: 324975, episodes: 13000, mean episode reward: -630.8291149687954, agent episode reward: [-210.27637165626513, -210.27637165626513, -210.27637165626513], time: 158.943
steps: 324975, episodes: 13000, mean episode variance: 70.89893990969658, agent episode variance: [1.7623028614521026, 67.22972045898437, 1.9069165892601012], time: 158.944
Running avgs for agent 0: q_loss: 42.471649169921875, p_loss: -4.558017730712891, mean_rew: -8.964816285774319, variance: 7.049211025238037, lamda: 1.2816063165664673
Running avgs for agent 1: q_loss: 1317365.875, p_loss: 165.8475341796875, mean_rew: -8.964444685144738, variance: 268.9188818359375, lamda: 1.2852294445037842
Running avgs for agent 2: q_loss: 25.62057876586914, p_loss: -4.555151462554932, mean_rew: -8.97435381896341, variance: 7.627665996551514, lamda: 1.197253704071045

steps: 349975, episodes: 14000, mean episode reward: -617.9518506289803, agent episode reward: [-205.98395020966012, -205.98395020966012, -205.98395020966012], time: 163.057
steps: 349975, episodes: 14000, mean episode variance: 72.89485165309905, agent episode variance: [1.7400559630393981, 69.27538433837891, 1.8794113516807556], time: 163.057
Running avgs for agent 0: q_loss: 33.96995162963867, p_loss: -4.524092674255371, mean_rew: -8.920778005050348, variance: 6.960224151611328, lamda: 1.3061379194259644
Running avgs for agent 1: q_loss: 1444909.875, p_loss: 173.3894500732422, mean_rew: -8.922170993250171, variance: 277.10153735351565, lamda: 1.3102335929870605
Running avgs for agent 2: q_loss: 32.62352752685547, p_loss: -4.533245086669922, mean_rew: -8.92928631409332, variance: 7.517645359039307, lamda: 1.2133418321609497

steps: 374975, episodes: 15000, mean episode reward: -597.5853266265888, agent episode reward: [-199.1951088755296, -199.1951088755296, -199.1951088755296], time: 158.574
steps: 374975, episodes: 15000, mean episode variance: 72.90710093855859, agent episode variance: [1.6738014423847198, 69.38830694580078, 1.8449925503730773], time: 158.575
Running avgs for agent 0: q_loss: 36.56122970581055, p_loss: -4.513901710510254, mean_rew: -8.876509649069732, variance: 6.6952056884765625, lamda: 1.331200361251831
Running avgs for agent 1: q_loss: 1532881.875, p_loss: 178.65908813476562, mean_rew: -8.875783329157759, variance: 277.5532277832031, lamda: 1.335237741470337
Running avgs for agent 2: q_loss: 27.600351333618164, p_loss: -4.496882438659668, mean_rew: -8.867154800304776, variance: 7.379970550537109, lamda: 1.2230143547058105

steps: 399975, episodes: 16000, mean episode reward: -600.5823168227807, agent episode reward: [-200.1941056075936, -200.1941056075936, -200.1941056075936], time: 160.211
steps: 399975, episodes: 16000, mean episode variance: 76.7130048737526, agent episode variance: [1.6239121496677398, 73.29580847167969, 1.7932842524051666], time: 160.212
Running avgs for agent 0: q_loss: 30.842512130737305, p_loss: -4.466697692871094, mean_rew: -8.799694484361, variance: 6.495648384094238, lamda: 1.3560709953308105
Running avgs for agent 1: q_loss: 1619687.125, p_loss: 181.41152954101562, mean_rew: -8.811765957416855, variance: 293.18323388671877, lamda: 1.3602418899536133
Running avgs for agent 2: q_loss: 23.350366592407227, p_loss: -4.453637599945068, mean_rew: -8.801202646662787, variance: 7.1731367111206055, lamda: 1.2428388595581055

steps: 424975, episodes: 17000, mean episode reward: -631.0624123319561, agent episode reward: [-210.35413744398537, -210.35413744398537, -210.35413744398537], time: 160.829
steps: 424975, episodes: 17000, mean episode variance: 77.80402504086494, agent episode variance: [1.6101042869091033, 74.43063186645507, 1.7632888875007628], time: 160.83
Running avgs for agent 0: q_loss: 29.433748245239258, p_loss: -4.443457126617432, mean_rew: -8.775130001740756, variance: 6.4404168128967285, lamda: 1.3810811042785645
Running avgs for agent 1: q_loss: 1688094.625, p_loss: 183.77243041992188, mean_rew: -8.780352486201807, variance: 297.7225274658203, lamda: 1.3852460384368896
Running avgs for agent 2: q_loss: 22.902645111083984, p_loss: -4.45639181137085, mean_rew: -8.774769546224524, variance: 7.053155422210693, lamda: 1.262239933013916

steps: 449975, episodes: 18000, mean episode reward: -622.5858879198748, agent episode reward: [-207.52862930662494, -207.52862930662494, -207.52862930662494], time: 161.382
steps: 449975, episodes: 18000, mean episode variance: 77.32149056339264, agent episode variance: [1.576514524936676, 74.03140969848633, 1.713566339969635], time: 161.383
Running avgs for agent 0: q_loss: 28.394012451171875, p_loss: -4.443774223327637, mean_rew: -8.749974553019035, variance: 6.306057929992676, lamda: 1.4060089588165283
Running avgs for agent 1: q_loss: 1752710.625, p_loss: 186.6431884765625, mean_rew: -8.758038315515229, variance: 296.1256387939453, lamda: 1.410250186920166
Running avgs for agent 2: q_loss: 21.60053062438965, p_loss: -4.437966823577881, mean_rew: -8.737654201693909, variance: 6.8542656898498535, lamda: 1.283214807510376

steps: 474975, episodes: 19000, mean episode reward: -590.2930736933301, agent episode reward: [-196.7643578977767, -196.7643578977767, -196.7643578977767], time: 163.427
steps: 474975, episodes: 19000, mean episode variance: 77.85230717229844, agent episode variance: [1.5537989540100097, 74.60372036743163, 1.694787850856781], time: 163.427
Running avgs for agent 0: q_loss: 32.630210876464844, p_loss: -4.423669815063477, mean_rew: -8.714245814247937, variance: 6.215195655822754, lamda: 1.4265565872192383
Running avgs for agent 1: q_loss: 1900869.125, p_loss: 189.00001525878906, mean_rew: -8.714330467541366, variance: 298.41488146972654, lamda: 1.4352545738220215
Running avgs for agent 2: q_loss: 19.29852294921875, p_loss: -4.416267395019531, mean_rew: -8.713757823855081, variance: 6.779151439666748, lamda: 1.3023735284805298

steps: 499975, episodes: 20000, mean episode reward: -560.8312778476399, agent episode reward: [-186.9437592825466, -186.9437592825466, -186.9437592825466], time: 165.361
steps: 499975, episodes: 20000, mean episode variance: 77.62279005479813, agent episode variance: [1.5216906442642213, 74.44593600463867, 1.6551634058952331], time: 165.362
Running avgs for agent 0: q_loss: 29.537525177001953, p_loss: -4.418737411499023, mean_rew: -8.679767908136933, variance: 6.08676290512085, lamda: 1.4453835487365723
Running avgs for agent 1: q_loss: 1985050.75, p_loss: 190.02830505371094, mean_rew: -8.655987382591174, variance: 297.78374401855467, lamda: 1.4602586030960083
Running avgs for agent 2: q_loss: 18.08069610595703, p_loss: -4.390397548675537, mean_rew: -8.65859066502594, variance: 6.62065315246582, lamda: 1.315458059310913

steps: 524975, episodes: 21000, mean episode reward: -559.9375592427018, agent episode reward: [-186.64585308090062, -186.64585308090062, -186.64585308090062], time: 163.082
steps: 524975, episodes: 21000, mean episode variance: 73.94132380247116, agent episode variance: [1.4917749235630036, 70.8038894958496, 1.645659383058548], time: 163.083
Running avgs for agent 0: q_loss: 24.11897850036621, p_loss: -4.375449180603027, mean_rew: -8.609559752823891, variance: 5.967099666595459, lamda: 1.4690124988555908
Running avgs for agent 1: q_loss: 2053776.125, p_loss: 192.062255859375, mean_rew: -8.615943899333649, variance: 283.2155579833984, lamda: 1.4852627515792847
Running avgs for agent 2: q_loss: 24.881900787353516, p_loss: -4.370173931121826, mean_rew: -8.607508808174527, variance: 6.582637786865234, lamda: 1.3252848386764526

steps: 549975, episodes: 22000, mean episode reward: -547.1277431709456, agent episode reward: [-182.37591439031522, -182.37591439031522, -182.37591439031522], time: 165.803
steps: 549975, episodes: 22000, mean episode variance: 71.3714750339985, agent episode variance: [1.4617165548801423, 68.28405781555176, 1.6257006635665894], time: 165.803
Running avgs for agent 0: q_loss: 24.052358627319336, p_loss: -4.356003761291504, mean_rew: -8.538790277498684, variance: 5.846866130828857, lamda: 1.4940197467803955
Running avgs for agent 1: q_loss: 2383173.75, p_loss: 193.5572967529297, mean_rew: -8.545075055212946, variance: 273.13623126220705, lamda: 1.510266900062561
Running avgs for agent 2: q_loss: 24.14137077331543, p_loss: -4.3476243019104, mean_rew: -8.547460261611153, variance: 6.502802848815918, lamda: 1.3291490077972412

steps: 574975, episodes: 23000, mean episode reward: -532.0079004385058, agent episode reward: [-177.33596681283524, -177.33596681283524, -177.33596681283524], time: 163.704
steps: 574975, episodes: 23000, mean episode variance: 71.7409611685276, agent episode variance: [1.4280142579078674, 68.71097694396973, 1.6019699666500091], time: 163.705
Running avgs for agent 0: q_loss: 26.342571258544922, p_loss: -4.299688816070557, mean_rew: -8.471535923678173, variance: 5.712057590484619, lamda: 1.5148574113845825
Running avgs for agent 1: q_loss: 2179383.25, p_loss: 194.42245483398438, mean_rew: -8.485520705449574, variance: 274.8439077758789, lamda: 1.5352710485458374
Running avgs for agent 2: q_loss: 19.558067321777344, p_loss: -4.306739330291748, mean_rew: -8.47345722358912, variance: 6.407879829406738, lamda: 1.3437683582305908

steps: 599975, episodes: 24000, mean episode reward: -528.7937523990488, agent episode reward: [-176.26458413301629, -176.26458413301629, -176.26458413301629], time: 165.518
steps: 599975, episodes: 24000, mean episode variance: 71.73241864609719, agent episode variance: [1.3924414346218108, 68.76596047973632, 1.574016731739044], time: 165.518
Running avgs for agent 0: q_loss: 26.75424575805664, p_loss: -4.293190956115723, mean_rew: -8.424083055183782, variance: 5.569765567779541, lamda: 1.5308849811553955
Running avgs for agent 1: q_loss: 2337354.25, p_loss: 195.0161590576172, mean_rew: -8.429330740757546, variance: 275.0638419189453, lamda: 1.5602751970291138
Running avgs for agent 2: q_loss: 22.079492568969727, p_loss: -4.281875133514404, mean_rew: -8.432173466134442, variance: 6.296066761016846, lamda: 1.3575549125671387

steps: 624975, episodes: 25000, mean episode reward: -517.5095883450758, agent episode reward: [-172.50319611502525, -172.50319611502525, -172.50319611502525], time: 164.613
steps: 624975, episodes: 25000, mean episode variance: 69.27123526716233, agent episode variance: [1.3811474969387054, 66.3227177886963, 1.5673699815273285], time: 164.613
Running avgs for agent 0: q_loss: 24.164892196655273, p_loss: -4.261849403381348, mean_rew: -8.362115040445257, variance: 5.524590015411377, lamda: 1.54239821434021
Running avgs for agent 1: q_loss: 2359986.5, p_loss: 194.77713012695312, mean_rew: -8.374554530054665, variance: 265.2908711547852, lamda: 1.5852793455123901
Running avgs for agent 2: q_loss: 22.269437789916992, p_loss: -4.250404357910156, mean_rew: -8.36889185332248, variance: 6.269479274749756, lamda: 1.3617991209030151

steps: 649975, episodes: 26000, mean episode reward: -507.665767135645, agent episode reward: [-169.22192237854833, -169.22192237854833, -169.22192237854833], time: 163.708
steps: 649975, episodes: 26000, mean episode variance: 71.28434029722214, agent episode variance: [1.3570320389270782, 68.38539181518554, 1.5419164431095123], time: 163.708
Running avgs for agent 0: q_loss: 20.472740173339844, p_loss: -4.228147506713867, mean_rew: -8.296351098593151, variance: 5.428127765655518, lamda: 1.5636643171310425
Running avgs for agent 1: q_loss: 2339855.75, p_loss: 193.69537353515625, mean_rew: -8.31030038352054, variance: 273.54156726074217, lamda: 1.6102834939956665
Running avgs for agent 2: q_loss: 20.53526496887207, p_loss: -4.216948986053467, mean_rew: -8.29237131275978, variance: 6.167665958404541, lamda: 1.3647810220718384

steps: 674975, episodes: 27000, mean episode reward: -511.82542336655627, agent episode reward: [-170.60847445551877, -170.60847445551877, -170.60847445551877], time: 164.843
steps: 674975, episodes: 27000, mean episode variance: 71.60531596374511, agent episode variance: [1.3283434381484986, 68.75478538513184, 1.5221871404647827], time: 164.844
Running avgs for agent 0: q_loss: 18.517030715942383, p_loss: -4.214323043823242, mean_rew: -8.247643918863858, variance: 5.313373565673828, lamda: 1.5878231525421143
Running avgs for agent 1: q_loss: 2377484.25, p_loss: 192.65113830566406, mean_rew: -8.25865371655525, variance: 275.01914154052736, lamda: 1.6352876424789429
Running avgs for agent 2: q_loss: 19.824796676635742, p_loss: -4.187217712402344, mean_rew: -8.256205879693992, variance: 6.088748455047607, lamda: 1.3686821460723877

steps: 699975, episodes: 28000, mean episode reward: -507.58957297473034, agent episode reward: [-169.19652432491014, -169.19652432491014, -169.19652432491014], time: 162.72
steps: 699975, episodes: 28000, mean episode variance: 69.14765973854065, agent episode variance: [1.3003583595752717, 66.34588291931152, 1.5014184596538545], time: 162.72
Running avgs for agent 0: q_loss: 19.638080596923828, p_loss: -4.182951927185059, mean_rew: -8.203311879918997, variance: 5.2014336585998535, lamda: 1.611280918121338
Running avgs for agent 1: q_loss: 2343170.5, p_loss: 191.11090087890625, mean_rew: -8.19850937116361, variance: 265.3835316772461, lamda: 1.6602917909622192
Running avgs for agent 2: q_loss: 15.739425659179688, p_loss: -4.181302547454834, mean_rew: -8.194312380576562, variance: 6.005673885345459, lamda: 1.3793425559997559

steps: 724975, episodes: 29000, mean episode reward: -507.1374981899973, agent episode reward: [-169.0458327299991, -169.0458327299991, -169.0458327299991], time: 164.498
steps: 724975, episodes: 29000, mean episode variance: 65.00563036966324, agent episode variance: [1.284563178062439, 62.23077249145508, 1.4902947001457214], time: 164.498
Running avgs for agent 0: q_loss: 20.593914031982422, p_loss: -4.1508965492248535, mean_rew: -8.142524890963436, variance: 5.138252258300781, lamda: 1.6342412233352661
Running avgs for agent 1: q_loss: 2350117.5, p_loss: 190.1624755859375, mean_rew: -8.15289419543815, variance: 248.9230899658203, lamda: 1.6852959394454956
Running avgs for agent 2: q_loss: 16.93861961364746, p_loss: -4.149905204772949, mean_rew: -8.14788116301212, variance: 5.961178779602051, lamda: 1.3940551280975342

steps: 749975, episodes: 30000, mean episode reward: -506.9561229279652, agent episode reward: [-168.9853743093217, -168.9853743093217, -168.9853743093217], time: 162.622
steps: 749975, episodes: 30000, mean episode variance: 67.19228599357605, agent episode variance: [1.2534133689403535, 64.47436516284942, 1.4645074617862701], time: 162.623
Running avgs for agent 0: q_loss: 20.94269561767578, p_loss: -4.14268159866333, mean_rew: -8.105326146497713, variance: 5.01365327835083, lamda: 1.6491564512252808
Running avgs for agent 1: q_loss: 2358343.0, p_loss: 188.64111328125, mean_rew: -8.104710062320336, variance: 257.8974606513977, lamda: 1.710300087928772
Running avgs for agent 2: q_loss: 14.472067832946777, p_loss: -4.1320390701293945, mean_rew: -8.103307376065546, variance: 5.858029842376709, lamda: 1.4057248830795288

steps: 774975, episodes: 31000, mean episode reward: -509.8005663684885, agent episode reward: [-169.9335221228295, -169.9335221228295, -169.9335221228295], time: 163.815
steps: 774975, episodes: 31000, mean episode variance: 60.64441237425804, agent episode variance: [1.2219057323932647, 57.98051986694336, 1.4419867749214172], time: 163.816
Running avgs for agent 0: q_loss: 20.790132522583008, p_loss: -4.134269714355469, mean_rew: -8.054157352753839, variance: 4.887622833251953, lamda: 1.6634092330932617
Running avgs for agent 1: q_loss: 2360776.25, p_loss: 187.14694213867188, mean_rew: -8.051920418436541, variance: 231.92207946777344, lamda: 1.7353042364120483
Running avgs for agent 2: q_loss: 16.682132720947266, p_loss: -4.107645034790039, mean_rew: -8.05772088328648, variance: 5.767946720123291, lamda: 1.4186238050460815

steps: 799975, episodes: 32000, mean episode reward: -506.14245929619284, agent episode reward: [-168.71415309873095, -168.71415309873095, -168.71415309873095], time: 164.387
steps: 799975, episodes: 32000, mean episode variance: 59.615561762809754, agent episode variance: [1.2192049162387848, 56.96582827758789, 1.430528568983078], time: 164.387
Running avgs for agent 0: q_loss: 18.974668502807617, p_loss: -4.087655544281006, mean_rew: -8.014952035733929, variance: 4.876819133758545, lamda: 1.6762079000473022
Running avgs for agent 1: q_loss: 2417951.0, p_loss: 184.4888458251953, mean_rew: -8.015269145928697, variance: 227.86331311035156, lamda: 1.7603083848953247
Running avgs for agent 2: q_loss: 12.122842788696289, p_loss: -4.08455753326416, mean_rew: -8.027532494513839, variance: 5.722114086151123, lamda: 1.42800772190094

steps: 824975, episodes: 33000, mean episode reward: -499.68304590302677, agent episode reward: [-166.56101530100895, -166.56101530100895, -166.56101530100895], time: 164.798
steps: 824975, episodes: 33000, mean episode variance: 59.10092985987663, agent episode variance: [1.2052679579257966, 56.496123951911926, 1.39953795003891], time: 164.798
Running avgs for agent 0: q_loss: 17.88607406616211, p_loss: -4.060760974884033, mean_rew: -7.968324291470863, variance: 4.821071147918701, lamda: 1.6994309425354004
Running avgs for agent 1: q_loss: 2371164.75, p_loss: 182.26194763183594, mean_rew: -7.981471999698364, variance: 225.9844958076477, lamda: 1.785312533378601
Running avgs for agent 2: q_loss: 12.800655364990234, p_loss: -4.07003116607666, mean_rew: -7.966484468819194, variance: 5.598152160644531, lamda: 1.4426250457763672

steps: 849975, episodes: 34000, mean episode reward: -502.4390875106747, agent episode reward: [-167.47969583689158, -167.47969583689158, -167.47969583689158], time: 165.121
steps: 849975, episodes: 34000, mean episode variance: 63.907323058843616, agent episode variance: [1.1818556272983551, 61.343562644958496, 1.3819047865867615], time: 165.121
Running avgs for agent 0: q_loss: 16.72982406616211, p_loss: -4.052213668823242, mean_rew: -7.934648255600597, variance: 4.727422714233398, lamda: 1.7184116840362549
Running avgs for agent 1: q_loss: 3123849.75, p_loss: 179.5969696044922, mean_rew: -7.931285369009942, variance: 245.37425057983398, lamda: 1.8103166818618774
Running avgs for agent 2: q_loss: 16.804304122924805, p_loss: -4.049314975738525, mean_rew: -7.941344846066594, variance: 5.527619361877441, lamda: 1.450557827949524

steps: 874975, episodes: 35000, mean episode reward: -508.3919410890479, agent episode reward: [-169.46398036301596, -169.46398036301596, -169.46398036301596], time: 163.395
steps: 874975, episodes: 35000, mean episode variance: 95.93540389680862, agent episode variance: [1.155419493675232, 93.3988572692871, 1.381127133846283], time: 163.395
Running avgs for agent 0: q_loss: 16.028812408447266, p_loss: -4.037581443786621, mean_rew: -7.902682225554907, variance: 4.621677875518799, lamda: 1.7413408756256104
Running avgs for agent 1: q_loss: 5350401.0, p_loss: 178.0933380126953, mean_rew: -7.8949366768888956, variance: 373.5954290771484, lamda: 1.8353208303451538
Running avgs for agent 2: q_loss: 15.348641395568848, p_loss: -4.036640644073486, mean_rew: -7.908265134112249, variance: 5.524508953094482, lamda: 1.4523217678070068

steps: 899975, episodes: 36000, mean episode reward: -507.3902230039218, agent episode reward: [-169.1300743346406, -169.1300743346406, -169.1300743346406], time: 170.353
steps: 899975, episodes: 36000, mean episode variance: 91.83914363694191, agent episode variance: [1.1401104376316071, 89.3249479675293, 1.374085231781006], time: 170.354
Running avgs for agent 0: q_loss: 15.053969383239746, p_loss: -4.023606300354004, mean_rew: -7.878192096517474, variance: 4.560441493988037, lamda: 1.763205647468567
Running avgs for agent 1: q_loss: 5354220.5, p_loss: 176.45924377441406, mean_rew: -7.8805052871717365, variance: 357.2997918701172, lamda: 1.8603249788284302
Running avgs for agent 2: q_loss: 15.235419273376465, p_loss: -4.022063255310059, mean_rew: -7.888008957540071, variance: 5.496340751647949, lamda: 1.4608614444732666

steps: 924975, episodes: 37000, mean episode reward: -507.37661206556265, agent episode reward: [-169.12553735518756, -169.12553735518756, -169.12553735518756], time: 170.639
steps: 924975, episodes: 37000, mean episode variance: 90.21050502705575, agent episode variance: [1.1223494338989257, 87.73278366088867, 1.3553719322681428], time: 170.64
Running avgs for agent 0: q_loss: 15.96501636505127, p_loss: -3.994323492050171, mean_rew: -7.832599023206611, variance: 4.4893975257873535, lamda: 1.7847346067428589
Running avgs for agent 1: q_loss: 5348038.5, p_loss: 174.19497680664062, mean_rew: -7.848814422752494, variance: 350.9311346435547, lamda: 1.8853291273117065
Running avgs for agent 2: q_loss: 15.703518867492676, p_loss: -3.9994211196899414, mean_rew: -7.841114406556723, variance: 5.421487808227539, lamda: 1.4653148651123047

steps: 949975, episodes: 38000, mean episode reward: -506.57403221787547, agent episode reward: [-168.85801073929184, -168.85801073929184, -168.85801073929184], time: 164.656
steps: 949975, episodes: 38000, mean episode variance: 87.70231101393699, agent episode variance: [1.1029622120857239, 85.26350714111328, 1.3358416607379913], time: 164.657
Running avgs for agent 0: q_loss: 18.79913902282715, p_loss: -3.9776792526245117, mean_rew: -7.811823077910531, variance: 4.411848545074463, lamda: 1.7968131303787231
Running avgs for agent 1: q_loss: 5102138.5, p_loss: 172.34225463867188, mean_rew: -7.818642954819641, variance: 341.0540285644531, lamda: 1.910333275794983
Running avgs for agent 2: q_loss: 13.096602439880371, p_loss: -3.984236240386963, mean_rew: -7.799534648476339, variance: 5.343366622924805, lamda: 1.4732518196105957

steps: 974975, episodes: 39000, mean episode reward: -511.5522464568786, agent episode reward: [-170.51741548562623, -170.51741548562623, -170.51741548562623], time: 171.624
steps: 974975, episodes: 39000, mean episode variance: 89.33180209827422, agent episode variance: [1.0957719447612762, 86.90821299743652, 1.3278171560764314], time: 171.624
Running avgs for agent 0: q_loss: 18.434558868408203, p_loss: -3.966541051864624, mean_rew: -7.784394618630331, variance: 4.383088111877441, lamda: 1.802014946937561
Running avgs for agent 1: q_loss: 5105352.5, p_loss: 170.9386749267578, mean_rew: -7.793091988156333, variance: 347.6328519897461, lamda: 1.9353374242782593
Running avgs for agent 2: q_loss: 13.034066200256348, p_loss: -3.9715182781219482, mean_rew: -7.7854523088040875, variance: 5.311268329620361, lamda: 1.4878607988357544

steps: 999975, episodes: 40000, mean episode reward: -508.03496420417923, agent episode reward: [-169.34498806805976, -169.34498806805976, -169.34498806805976], time: 169.867
steps: 999975, episodes: 40000, mean episode variance: 85.6028870651722, agent episode variance: [1.0958166496753692, 83.19359317016601, 1.3134772453308106], time: 169.868
Running avgs for agent 0: q_loss: 15.926826477050781, p_loss: -3.9594435691833496, mean_rew: -7.766017675144001, variance: 4.383266448974609, lamda: 1.8117939233779907
Running avgs for agent 1: q_loss: 4977039.5, p_loss: 169.0889129638672, mean_rew: -7.758536272014415, variance: 332.77437268066404, lamda: 1.9603415727615356
Running avgs for agent 2: q_loss: 12.119004249572754, p_loss: -3.9650847911834717, mean_rew: -7.770306402510487, variance: 5.253909111022949, lamda: 1.503042459487915

steps: 1024975, episodes: 41000, mean episode reward: -505.60657544092334, agent episode reward: [-168.53552514697444, -168.53552514697444, -168.53552514697444], time: 168.652
steps: 1024975, episodes: 41000, mean episode variance: 81.64678774023056, agent episode variance: [1.0802057983875275, 79.2617505645752, 1.3048313772678375], time: 168.652
Running avgs for agent 0: q_loss: 17.945941925048828, p_loss: -3.9505300521850586, mean_rew: -7.754838154308628, variance: 4.3208231925964355, lamda: 1.8304768800735474
Running avgs for agent 1: q_loss: 4852355.0, p_loss: 167.4901885986328, mean_rew: -7.7428016922795395, variance: 317.0470022583008, lamda: 1.985345721244812
Running avgs for agent 2: q_loss: 13.507929801940918, p_loss: -3.95575213432312, mean_rew: -7.745640977592086, variance: 5.219325065612793, lamda: 1.512207269668579

steps: 1049975, episodes: 42000, mean episode reward: -515.1101655394193, agent episode reward: [-171.70338851313977, -171.70338851313977, -171.70338851313977], time: 162.784
steps: 1049975, episodes: 42000, mean episode variance: 81.8427227320671, agent episode variance: [1.067210259437561, 79.48844390869141, 1.287068563938141], time: 162.785
Running avgs for agent 0: q_loss: 14.34498405456543, p_loss: -3.931614875793457, mean_rew: -7.699423345007282, variance: 4.268840789794922, lamda: 1.8411169052124023
Running avgs for agent 1: q_loss: 4618185.5, p_loss: 165.5152130126953, mean_rew: -7.703734341803881, variance: 317.95377563476563, lamda: 2.0103375911712646
Running avgs for agent 2: q_loss: 12.795947074890137, p_loss: -3.935058116912842, mean_rew: -7.703527757469608, variance: 5.1482744216918945, lamda: 1.5174570083618164

steps: 1074975, episodes: 43000, mean episode reward: -509.1403150409437, agent episode reward: [-169.71343834698123, -169.71343834698123, -169.71343834698123], time: 163.357
steps: 1074975, episodes: 43000, mean episode variance: 78.39086176276207, agent episode variance: [1.0481335504055023, 76.07552668762207, 1.267201524734497], time: 163.357
Running avgs for agent 0: q_loss: 15.239119529724121, p_loss: -3.8799943923950195, mean_rew: -7.606214156618114, variance: 4.19253396987915, lamda: 1.851966142654419
Running avgs for agent 1: q_loss: 4316976.0, p_loss: 162.87460327148438, mean_rew: -7.613969375704184, variance: 304.3021067504883, lamda: 2.0353119373321533
Running avgs for agent 2: q_loss: 10.928914070129395, p_loss: -3.8926663398742676, mean_rew: -7.61434500541346, variance: 5.068805694580078, lamda: 1.526647686958313

steps: 1099975, episodes: 44000, mean episode reward: -506.27097573848897, agent episode reward: [-168.75699191282968, -168.75699191282968, -168.75699191282968], time: 164.29
steps: 1099975, episodes: 44000, mean episode variance: 81.61304150795937, agent episode variance: [1.0268955144882201, 79.33817018127442, 1.2479758121967315], time: 164.291
Running avgs for agent 0: q_loss: 13.965714454650879, p_loss: -3.8425512313842773, mean_rew: -7.519899066589835, variance: 4.107582092285156, lamda: 1.8605157136917114
Running avgs for agent 1: q_loss: 3806894.0, p_loss: 159.6706085205078, mean_rew: -7.513023123990635, variance: 317.3526807250977, lamda: 2.060286045074463
Running avgs for agent 2: q_loss: 8.853019714355469, p_loss: -3.8384339809417725, mean_rew: -7.516522493835418, variance: 4.991903305053711, lamda: 1.5356429815292358

steps: 1124975, episodes: 45000, mean episode reward: -512.9628496994127, agent episode reward: [-170.98761656647088, -170.98761656647088, -170.98761656647088], time: 164.522
steps: 1124975, episodes: 45000, mean episode variance: 76.28639802336693, agent episode variance: [1.0221391994953155, 74.03717767333984, 1.2270811505317687], time: 164.523
Running avgs for agent 0: q_loss: 12.287137031555176, p_loss: -3.7959563732147217, mean_rew: -7.441280802539257, variance: 4.08855676651001, lamda: 1.8645683526992798
Running avgs for agent 1: q_loss: 3382991.75, p_loss: 157.22642517089844, mean_rew: -7.430109972565951, variance: 296.14871069335936, lamda: 2.0852606296539307
Running avgs for agent 2: q_loss: 7.311947822570801, p_loss: -3.8017544746398926, mean_rew: -7.4292955909521075, variance: 4.908324241638184, lamda: 1.5503877401351929

steps: 1149975, episodes: 46000, mean episode reward: -514.9792343146215, agent episode reward: [-171.65974477154052, -171.65974477154052, -171.65974477154052], time: 155.58
steps: 1149975, episodes: 46000, mean episode variance: 72.74432720637321, agent episode variance: [1.0025599646568297, 70.54287232971191, 1.1988949120044707], time: 155.581
Running avgs for agent 0: q_loss: 10.946959495544434, p_loss: -3.7640085220336914, mean_rew: -7.367892632787136, variance: 4.010240077972412, lamda: 1.865783452987671
Running avgs for agent 1: q_loss: 2897335.25, p_loss: 154.7767791748047, mean_rew: -7.375533305457669, variance: 282.17148931884765, lamda: 2.1102349758148193
Running avgs for agent 2: q_loss: 6.684670925140381, p_loss: -3.775937557220459, mean_rew: -7.3744079792063975, variance: 4.79557991027832, lamda: 1.561123251914978

steps: 1174975, episodes: 47000, mean episode reward: -519.7953794281516, agent episode reward: [-173.26512647605057, -173.26512647605057, -173.26512647605057], time: 159.642
steps: 1174975, episodes: 47000, mean episode variance: 68.13825411462784, agent episode variance: [1.0020120067596436, 65.94310089111327, 1.1931412167549134], time: 159.642
Running avgs for agent 0: q_loss: 8.036600112915039, p_loss: -3.751777172088623, mean_rew: -7.336321920074142, variance: 4.008048057556152, lamda: 1.8685897588729858
Running avgs for agent 1: q_loss: 2514398.25, p_loss: 152.51153564453125, mean_rew: -7.336519106134099, variance: 263.7724035644531, lamda: 2.135209321975708
Running avgs for agent 2: q_loss: 7.012001991271973, p_loss: -3.759422779083252, mean_rew: -7.339065152191462, variance: 4.772564888000488, lamda: 1.5669001340866089

steps: 1199975, episodes: 48000, mean episode reward: -524.7906445250875, agent episode reward: [-174.9302148416958, -174.9302148416958, -174.9302148416958], time: 158.401
steps: 1199975, episodes: 48000, mean episode variance: 63.08671146261692, agent episode variance: [0.9905039385557175, 60.91208355712891, 1.1841239669322967], time: 158.401
Running avgs for agent 0: q_loss: 10.952357292175293, p_loss: -3.729966163635254, mean_rew: -7.298284552030487, variance: 3.9620158672332764, lamda: 1.8786052465438843
Running avgs for agent 1: q_loss: 2315232.25, p_loss: 150.24913024902344, mean_rew: -7.3129279047562, variance: 243.64833422851564, lamda: 2.1601836681365967
Running avgs for agent 2: q_loss: 7.3778910636901855, p_loss: -3.73333740234375, mean_rew: -7.296630943603925, variance: 4.7364959716796875, lamda: 1.5678670406341553

steps: 1224975, episodes: 49000, mean episode reward: -524.6269394179124, agent episode reward: [-174.8756464726374, -174.8756464726374, -174.8756464726374], time: 161.029
steps: 1224975, episodes: 49000, mean episode variance: 59.33164476084709, agent episode variance: [0.9841320157051087, 57.17803164672851, 1.1694810984134674], time: 161.03
Running avgs for agent 0: q_loss: 8.52390193939209, p_loss: -3.720784902572632, mean_rew: -7.260427640763215, variance: 3.936527967453003, lamda: 1.880527377128601
Running avgs for agent 1: q_loss: 2065133.5, p_loss: 147.44287109375, mean_rew: -7.259748893729393, variance: 228.71212658691405, lamda: 2.1851580142974854
Running avgs for agent 2: q_loss: 6.463721752166748, p_loss: -3.719120502471924, mean_rew: -7.252667488721668, variance: 4.677924156188965, lamda: 1.5694018602371216

steps: 1249975, episodes: 50000, mean episode reward: -531.8811123188724, agent episode reward: [-177.29370410629076, -177.29370410629076, -177.29370410629076], time: 162.041
steps: 1249975, episodes: 50000, mean episode variance: 56.67214809560776, agent episode variance: [0.9794499876499176, 54.52080893707275, 1.171889170885086], time: 162.041
Running avgs for agent 0: q_loss: 8.083837509155273, p_loss: -3.695078134536743, mean_rew: -7.213978236785723, variance: 3.917799949645996, lamda: 1.8817538022994995
Running avgs for agent 1: q_loss: 1834691.0, p_loss: 144.61325073242188, mean_rew: -7.2127421168676, variance: 218.083235748291, lamda: 2.210132360458374
Running avgs for agent 2: q_loss: 6.778649806976318, p_loss: -3.7042179107666016, mean_rew: -7.217612234364794, variance: 4.687556743621826, lamda: 1.5735788345336914

steps: 1274975, episodes: 51000, mean episode reward: -543.0633583417487, agent episode reward: [-181.02111944724956, -181.02111944724956, -181.02111944724956], time: 158.777
steps: 1274975, episodes: 51000, mean episode variance: 53.81665254759788, agent episode variance: [0.969218522310257, 51.677511375427244, 1.169922649860382], time: 158.778
Running avgs for agent 0: q_loss: 7.657268524169922, p_loss: -3.680262804031372, mean_rew: -7.18023900790833, variance: 3.8768739700317383, lamda: 1.88299560546875
Running avgs for agent 1: q_loss: 1692048.75, p_loss: 142.5838623046875, mean_rew: -7.184065696957271, variance: 206.71004550170898, lamda: 2.2351067066192627
Running avgs for agent 2: q_loss: 6.424801349639893, p_loss: -3.6845932006835938, mean_rew: -7.186038436075496, variance: 4.679690361022949, lamda: 1.5740854740142822

steps: 1299975, episodes: 52000, mean episode reward: -552.4739008031573, agent episode reward: [-184.15796693438577, -184.15796693438577, -184.15796693438577], time: 166.008
steps: 1299975, episodes: 52000, mean episode variance: 52.36965935516358, agent episode variance: [0.9641005883216858, 50.24749974060059, 1.1580590262413024], time: 166.009
Running avgs for agent 0: q_loss: 7.101370811462402, p_loss: -3.668154716491699, mean_rew: -7.1587437659971185, variance: 3.8564023971557617, lamda: 1.883470058441162
Running avgs for agent 1: q_loss: 1518983.125, p_loss: 141.21485900878906, mean_rew: -7.150683876377179, variance: 200.98999896240235, lamda: 2.2600810527801514
Running avgs for agent 2: q_loss: 6.244972229003906, p_loss: -3.6839663982391357, mean_rew: -7.159976975800491, variance: 4.632236003875732, lamda: 1.574433445930481

steps: 1324975, episodes: 53000, mean episode reward: -559.9813080127258, agent episode reward: [-186.66043600424192, -186.66043600424192, -186.66043600424192], time: 173.548
steps: 1324975, episodes: 53000, mean episode variance: 48.9437650551796, agent episode variance: [0.9634021825790405, 46.825270553588865, 1.1550923190116882], time: 173.548
Running avgs for agent 0: q_loss: 5.409425735473633, p_loss: -3.6549692153930664, mean_rew: -7.130439341574776, variance: 3.8536088466644287, lamda: 1.892032265663147
Running avgs for agent 1: q_loss: 1433318.625, p_loss: 140.15724182128906, mean_rew: -7.129430828578428, variance: 187.30108221435546, lamda: 2.285055160522461
Running avgs for agent 2: q_loss: 4.849260330200195, p_loss: -3.6567347049713135, mean_rew: -7.135646073651913, variance: 4.620368957519531, lamda: 1.5796091556549072

steps: 1349975, episodes: 54000, mean episode reward: -560.2958060029273, agent episode reward: [-186.76526866764243, -186.76526866764243, -186.76526866764243], time: 168.673
steps: 1349975, episodes: 54000, mean episode variance: 49.19734088253975, agent episode variance: [0.9595025839805603, 47.09548338317871, 1.142354915380478], time: 168.673
Running avgs for agent 0: q_loss: 5.304608345031738, p_loss: -3.637955665588379, mean_rew: -7.103604765941798, variance: 3.838010311126709, lamda: 1.9014716148376465
Running avgs for agent 1: q_loss: 1324687.25, p_loss: 139.4221649169922, mean_rew: -7.114016757131451, variance: 188.38193353271484, lamda: 2.3100297451019287
Running avgs for agent 2: q_loss: 5.58454704284668, p_loss: -3.639885187149048, mean_rew: -7.0994041798878476, variance: 4.569419860839844, lamda: 1.5877150297164917

steps: 1374975, episodes: 55000, mean episode reward: -558.0429451786378, agent episode reward: [-186.0143150595459, -186.0143150595459, -186.0143150595459], time: 164.229
steps: 1374975, episodes: 55000, mean episode variance: 48.23997107839585, agent episode variance: [0.9503053891658783, 46.152296264648434, 1.1373694245815278], time: 164.229
Running avgs for agent 0: q_loss: 5.888339519500732, p_loss: -3.639333486557007, mean_rew: -7.097378326088869, variance: 3.8012216091156006, lamda: 1.9092161655426025
Running avgs for agent 1: q_loss: 1288508.5, p_loss: 139.30886840820312, mean_rew: -7.093251632330392, variance: 184.60918505859374, lamda: 2.3350038528442383
Running avgs for agent 2: q_loss: 4.481441497802734, p_loss: -3.640316963195801, mean_rew: -7.090435305944628, variance: 4.549477577209473, lamda: 1.5960837602615356

steps: 1399975, episodes: 56000, mean episode reward: -561.3443925411815, agent episode reward: [-187.1147975137272, -187.1147975137272, -187.1147975137272], time: 175.429
steps: 1399975, episodes: 56000, mean episode variance: 46.20102942061424, agent episode variance: [0.9466261966228485, 44.12356340026855, 1.1308398237228394], time: 175.429
Running avgs for agent 0: q_loss: 5.059569358825684, p_loss: -3.6193246841430664, mean_rew: -7.081810729491924, variance: 3.7865049839019775, lamda: 1.9131635427474976
Running avgs for agent 1: q_loss: 1284770.0, p_loss: 139.41458129882812, mean_rew: -7.081338556149778, variance: 176.4942536010742, lamda: 2.359978437423706
Running avgs for agent 2: q_loss: 4.886291027069092, p_loss: -3.6336522102355957, mean_rew: -7.083543162677142, variance: 4.523359298706055, lamda: 1.6061928272247314

steps: 1424975, episodes: 57000, mean episode reward: -566.5370188191705, agent episode reward: [-188.84567293972353, -188.84567293972353, -188.84567293972353], time: 169.71
steps: 1424975, episodes: 57000, mean episode variance: 46.84332656168937, agent episode variance: [0.9452273712158203, 44.77732991027832, 1.1207692801952363], time: 169.711
Running avgs for agent 0: q_loss: 5.890944480895996, p_loss: -3.612180233001709, mean_rew: -7.059097542684513, variance: 3.780909776687622, lamda: 1.9183415174484253
Running avgs for agent 1: q_loss: 1238982.875, p_loss: 139.47511291503906, mean_rew: -7.06281859951725, variance: 179.10931964111327, lamda: 2.3849525451660156
Running avgs for agent 2: q_loss: 4.618696689605713, p_loss: -3.6233925819396973, mean_rew: -7.060410122266692, variance: 4.483077049255371, lamda: 1.610823631286621

steps: 1449975, episodes: 58000, mean episode reward: -563.6725441362511, agent episode reward: [-187.89084804541702, -187.89084804541702, -187.89084804541702], time: 167.011
steps: 1449975, episodes: 58000, mean episode variance: 45.68046350312233, agent episode variance: [0.9403097746372223, 43.62886808776855, 1.1112856407165528], time: 167.012
Running avgs for agent 0: q_loss: 5.5529937744140625, p_loss: -3.6051278114318848, mean_rew: -7.042226984020703, variance: 3.7612392902374268, lamda: 1.9183999300003052
Running avgs for agent 1: q_loss: 1247886.125, p_loss: 139.40142822265625, mean_rew: -7.045187291883555, variance: 174.5154723510742, lamda: 2.4099271297454834
Running avgs for agent 2: q_loss: 4.101986408233643, p_loss: -3.618034601211548, mean_rew: -7.04006191350665, variance: 4.44514274597168, lamda: 1.6197223663330078

steps: 1474975, episodes: 59000, mean episode reward: -563.235243417402, agent episode reward: [-187.74508113913407, -187.74508113913407, -187.74508113913407], time: 171.853
steps: 1474975, episodes: 59000, mean episode variance: 46.22052595305443, agent episode variance: [0.9363239510059357, 44.18430158996582, 1.099900412082672], time: 171.854
Running avgs for agent 0: q_loss: 5.510503768920898, p_loss: -3.597485065460205, mean_rew: -7.024729944579479, variance: 3.745296001434326, lamda: 1.9183999300003052
Running avgs for agent 1: q_loss: 1184019.0, p_loss: 139.1818389892578, mean_rew: -7.018876002158506, variance: 176.73720635986328, lamda: 2.434901475906372
Running avgs for agent 2: q_loss: 3.607815742492676, p_loss: -3.6134417057037354, mean_rew: -7.0244677417211765, variance: 4.399601459503174, lamda: 1.6283588409423828

steps: 1499975, episodes: 60000, mean episode reward: -568.9577592971016, agent episode reward: [-189.65258643236714, -189.65258643236714, -189.65258643236714], time: 163.009
steps: 1499975, episodes: 60000, mean episode variance: 45.552613403320315, agent episode variance: [0.9410394835472107, 43.50966558837891, 1.1019083313941955], time: 163.01
Running avgs for agent 0: q_loss: 4.690520286560059, p_loss: -3.5956339836120605, mean_rew: -7.022762118725097, variance: 3.764158010482788, lamda: 1.920041799545288
Running avgs for agent 1: q_loss: 1245003.25, p_loss: 139.10487365722656, mean_rew: -7.021600578690756, variance: 174.03866235351563, lamda: 2.4598758220672607
Running avgs for agent 2: q_loss: 4.665868282318115, p_loss: -3.6124444007873535, mean_rew: -7.023476929924569, variance: 4.407633304595947, lamda: 1.632458209991455

steps: 1524975, episodes: 61000, mean episode reward: -572.7761858676737, agent episode reward: [-190.92539528922453, -190.92539528922453, -190.92539528922453], time: 175.968
steps: 1524975, episodes: 61000, mean episode variance: 44.041288610696796, agent episode variance: [0.9331056571006775, 42.00869355773926, 1.0994893958568572], time: 175.969
Running avgs for agent 0: q_loss: 4.1884894371032715, p_loss: -3.6066949367523193, mean_rew: -7.028575604124931, variance: 3.732422351837158, lamda: 1.9286941289901733
Running avgs for agent 1: q_loss: 1224832.5, p_loss: 138.90362548828125, mean_rew: -7.024309291916297, variance: 168.03477423095703, lamda: 2.4848501682281494
Running avgs for agent 2: q_loss: 4.033876419067383, p_loss: -3.6110997200012207, mean_rew: -7.0276508521326, variance: 4.3979573249816895, lamda: 1.636767864227295

steps: 1549975, episodes: 62000, mean episode reward: -573.4284184244783, agent episode reward: [-191.1428061414928, -191.1428061414928, -191.1428061414928], time: 176.097
steps: 1549975, episodes: 62000, mean episode variance: 43.93200890254975, agent episode variance: [0.9298828084468842, 41.911123916625975, 1.091002177476883], time: 176.097
Running avgs for agent 0: q_loss: 4.525416374206543, p_loss: -3.5947186946868896, mean_rew: -7.027814912549853, variance: 3.719531297683716, lamda: 1.9325798749923706
Running avgs for agent 1: q_loss: 1128239.25, p_loss: 138.93397521972656, mean_rew: -7.035561468542161, variance: 167.6444956665039, lamda: 2.509824514389038
Running avgs for agent 2: q_loss: 4.201057434082031, p_loss: -3.6150307655334473, mean_rew: -7.028291283113672, variance: 4.364008903503418, lamda: 1.6460754871368408

steps: 1574975, episodes: 63000, mean episode reward: -569.8207052150219, agent episode reward: [-189.94023507167398, -189.94023507167398, -189.94023507167398], time: 181.092
steps: 1574975, episodes: 63000, mean episode variance: 44.619538198709485, agent episode variance: [0.9262774963378906, 42.59823179626465, 1.095028906106949], time: 181.092
Running avgs for agent 0: q_loss: 5.595024585723877, p_loss: -3.6190454959869385, mean_rew: -7.039115399237584, variance: 3.7051100730895996, lamda: 1.9353910684585571
Running avgs for agent 1: q_loss: 1157523.625, p_loss: 139.1282501220703, mean_rew: -7.0468570245542175, variance: 170.3929271850586, lamda: 2.5347988605499268
Running avgs for agent 2: q_loss: 4.41737174987793, p_loss: -3.6201374530792236, mean_rew: -7.048024341310386, variance: 4.380115985870361, lamda: 1.651537299156189

steps: 1599975, episodes: 64000, mean episode reward: -565.9715393084464, agent episode reward: [-188.65717976948213, -188.65717976948213, -188.65717976948213], time: 178.765
steps: 1599975, episodes: 64000, mean episode variance: 42.59767752623558, agent episode variance: [0.9334473161697387, 40.57158084106445, 1.0926493690013885], time: 178.766
Running avgs for agent 0: q_loss: 5.588769435882568, p_loss: -3.6212997436523438, mean_rew: -7.057022375656821, variance: 3.7337894439697266, lamda: 1.9354785680770874
Running avgs for agent 1: q_loss: 1161872.375, p_loss: 139.06321716308594, mean_rew: -7.055748862254042, variance: 162.2863233642578, lamda: 2.5597729682922363
Running avgs for agent 2: q_loss: 4.80219841003418, p_loss: -3.622385263442993, mean_rew: -7.052846944683156, variance: 4.370597839355469, lamda: 1.6555074453353882

steps: 1624975, episodes: 65000, mean episode reward: -568.2342888024868, agent episode reward: [-189.41142960082897, -189.41142960082897, -189.41142960082897], time: 176.923
steps: 1624975, episodes: 65000, mean episode variance: 42.72995949602127, agent episode variance: [0.9370320518016815, 40.69717227935791, 1.095755164861679], time: 176.923
Running avgs for agent 0: q_loss: 5.656790256500244, p_loss: -3.6289618015289307, mean_rew: -7.07758948919387, variance: 3.7481281757354736, lamda: 1.9354944229125977
Running avgs for agent 1: q_loss: 1155386.25, p_loss: 139.13771057128906, mean_rew: -7.070158227270129, variance: 162.78868911743163, lamda: 2.584747552871704
Running avgs for agent 2: q_loss: 4.854873180389404, p_loss: -3.627566337585449, mean_rew: -7.067789116170697, variance: 4.383020401000977, lamda: 1.6556886434555054

steps: 1649975, episodes: 66000, mean episode reward: -569.0748659143256, agent episode reward: [-189.69162197144183, -189.69162197144183, -189.69162197144183], time: 180.377
steps: 1649975, episodes: 66000, mean episode variance: 44.45631416678429, agent episode variance: [0.9385900909900665, 42.420497344970705, 1.0972267308235168], time: 180.378
Running avgs for agent 0: q_loss: 5.75319242477417, p_loss: -3.6410133838653564, mean_rew: -7.089598584629265, variance: 3.7543604373931885, lamda: 1.9354945421218872
Running avgs for agent 1: q_loss: 1124802.125, p_loss: 139.20799255371094, mean_rew: -7.093011865661027, variance: 169.68198937988282, lamda: 2.6097216606140137
Running avgs for agent 2: q_loss: 4.515482425689697, p_loss: -3.644777297973633, mean_rew: -7.090537933107165, variance: 4.388907432556152, lamda: 1.6562424898147583

steps: 1674975, episodes: 67000, mean episode reward: -569.6323849985217, agent episode reward: [-189.87746166617387, -189.87746166617387, -189.87746166617387], time: 183.095
steps: 1674975, episodes: 67000, mean episode variance: 42.807683636665345, agent episode variance: [0.941947538614273, 40.76499252319336, 1.1007435748577117], time: 183.096
Running avgs for agent 0: q_loss: 5.770876407623291, p_loss: -3.6571178436279297, mean_rew: -7.113889346124704, variance: 3.7677905559539795, lamda: 1.935522198677063
Running avgs for agent 1: q_loss: 1155128.375, p_loss: 139.2746124267578, mean_rew: -7.114916785236064, variance: 163.05997009277343, lamda: 2.6346962451934814
Running avgs for agent 2: q_loss: 4.715231418609619, p_loss: -3.6499669551849365, mean_rew: -7.105919736060985, variance: 4.402974605560303, lamda: 1.6610631942749023

steps: 1699975, episodes: 68000, mean episode reward: -573.4226597751947, agent episode reward: [-191.14088659173157, -191.14088659173157, -191.14088659173157], time: 181.591
steps: 1699975, episodes: 68000, mean episode variance: 43.088464627742766, agent episode variance: [0.9450531914234162, 41.04354637145996, 1.0998650648593902], time: 181.592
Running avgs for agent 0: q_loss: 4.9676032066345215, p_loss: -3.663020610809326, mean_rew: -7.129617888851632, variance: 3.780212640762329, lamda: 1.9397873878479004
Running avgs for agent 1: q_loss: 1122645.5, p_loss: 139.23660278320312, mean_rew: -7.129372359083165, variance: 164.17418548583984, lamda: 2.659670352935791
Running avgs for agent 2: q_loss: 4.715449333190918, p_loss: -3.668276786804199, mean_rew: -7.130520315197, variance: 4.399460315704346, lamda: 1.6634386777877808

steps: 1724975, episodes: 69000, mean episode reward: -569.7346048698707, agent episode reward: [-189.91153495662357, -189.91153495662357, -189.91153495662357], time: 180.126
steps: 1724975, episodes: 69000, mean episode variance: 43.51747990894317, agent episode variance: [0.9418879103660583, 41.47659246826172, 1.0989995303153992], time: 180.127
Running avgs for agent 0: q_loss: 6.093770503997803, p_loss: -3.6785852909088135, mean_rew: -7.153020506885337, variance: 3.7675516605377197, lamda: 1.9453388452529907
Running avgs for agent 1: q_loss: 1128019.75, p_loss: 139.13385009765625, mean_rew: -7.149775466102297, variance: 165.90636987304688, lamda: 2.6846446990966797
Running avgs for agent 2: q_loss: 3.71600079536438, p_loss: -3.6774048805236816, mean_rew: -7.148030332132365, variance: 4.395998001098633, lamda: 1.670883059501648

steps: 1749975, episodes: 70000, mean episode reward: -574.7682742441795, agent episode reward: [-191.58942474805986, -191.58942474805986, -191.58942474805986], time: 179.639
steps: 1749975, episodes: 70000, mean episode variance: 42.981175784111024, agent episode variance: [0.9431975994110108, 40.941691131591796, 1.0962870531082154], time: 179.639
Running avgs for agent 0: q_loss: 6.0169219970703125, p_loss: -3.6944563388824463, mean_rew: -7.174298264678078, variance: 3.7727904319763184, lamda: 1.9460800886154175
Running avgs for agent 1: q_loss: 1134700.875, p_loss: 138.9586181640625, mean_rew: -7.173406685948219, variance: 163.76676452636718, lamda: 2.7096190452575684
Running avgs for agent 2: q_loss: 4.23037052154541, p_loss: -3.689145803451538, mean_rew: -7.1748099019306775, variance: 4.385148525238037, lamda: 1.677991509437561

steps: 1774975, episodes: 71000, mean episode reward: -577.0264548140287, agent episode reward: [-192.34215160467627, -192.34215160467627, -192.34215160467627], time: 182.567
steps: 1774975, episodes: 71000, mean episode variance: 41.92368554401398, agent episode variance: [0.947512440443039, 39.879563781738284, 1.096609321832657], time: 182.567
Running avgs for agent 0: q_loss: 6.2039384841918945, p_loss: -3.707728385925293, mean_rew: -7.199431177284087, variance: 3.7900497913360596, lamda: 1.9462631940841675
Running avgs for agent 1: q_loss: 1160080.625, p_loss: 138.75315856933594, mean_rew: -7.19693948940016, variance: 159.51825512695314, lamda: 2.734593391418457
Running avgs for agent 2: q_loss: 5.206754207611084, p_loss: -3.7069504261016846, mean_rew: -7.198512560051435, variance: 4.38643741607666, lamda: 1.6801261901855469

steps: 1799975, episodes: 72000, mean episode reward: -569.166850057727, agent episode reward: [-189.72228335257566, -189.72228335257566, -189.72228335257566], time: 179.711
steps: 1799975, episodes: 72000, mean episode variance: 42.66434214234352, agent episode variance: [0.946081592798233, 40.617173873901365, 1.1010866756439208], time: 179.711
Running avgs for agent 0: q_loss: 5.792269706726074, p_loss: -3.7216572761535645, mean_rew: -7.222800112135715, variance: 3.7843263149261475, lamda: 1.9473599195480347
Running avgs for agent 1: q_loss: 1119744.0, p_loss: 138.6907501220703, mean_rew: -7.226829334121606, variance: 162.46869549560546, lamda: 2.759567975997925
Running avgs for agent 2: q_loss: 5.231761932373047, p_loss: -3.7108895778656006, mean_rew: -7.22137797209601, variance: 4.404346466064453, lamda: 1.680404782295227

steps: 1824975, episodes: 73000, mean episode reward: -573.4048072927673, agent episode reward: [-191.1349357642557, -191.1349357642557, -191.1349357642557], time: 182.668
steps: 1824975, episodes: 73000, mean episode variance: 42.20330701708794, agent episode variance: [0.9461125156879425, 40.15829110717773, 1.0989033942222595], time: 182.668
Running avgs for agent 0: q_loss: 6.437944412231445, p_loss: -3.7295825481414795, mean_rew: -7.240929565783146, variance: 3.784450054168701, lamda: 1.9520248174667358
Running avgs for agent 1: q_loss: 1123303.875, p_loss: 138.62533569335938, mean_rew: -7.239793923230477, variance: 160.63316442871093, lamda: 2.7845423221588135
Running avgs for agent 2: q_loss: 4.4081244468688965, p_loss: -3.724788188934326, mean_rew: -7.239026193679019, variance: 4.395613670349121, lamda: 1.6848324537277222

steps: 1849975, episodes: 74000, mean episode reward: -570.9813032332803, agent episode reward: [-190.3271010777601, -190.3271010777601, -190.3271010777601], time: 177.845
steps: 1849975, episodes: 74000, mean episode variance: 42.38928664445877, agent episode variance: [0.9525261421203614, 40.33866394042969, 1.098096561908722], time: 177.846
Running avgs for agent 0: q_loss: 6.540895462036133, p_loss: -3.7500381469726562, mean_rew: -7.266795768852479, variance: 3.8101046085357666, lamda: 1.952290654182434
Running avgs for agent 1: q_loss: 1137978.125, p_loss: 138.69161987304688, mean_rew: -7.26081972583953, variance: 161.35465576171876, lamda: 2.809516668319702
Running avgs for agent 2: q_loss: 5.307721138000488, p_loss: -3.73226261138916, mean_rew: -7.261943563332822, variance: 4.3923869132995605, lamda: 1.6881182193756104

steps: 1874975, episodes: 75000, mean episode reward: -571.4664864573657, agent episode reward: [-190.4888288191219, -190.4888288191219, -190.4888288191219], time: 178.193
steps: 1874975, episodes: 75000, mean episode variance: 43.53134947180748, agent episode variance: [0.9568790602684021, 41.47091596984863, 1.103554441690445], time: 178.193
Running avgs for agent 0: q_loss: 6.752799034118652, p_loss: -3.7622482776641846, mean_rew: -7.2912807061830485, variance: 3.8275160789489746, lamda: 1.9523425102233887
Running avgs for agent 1: q_loss: 1136095.25, p_loss: 138.59710693359375, mean_rew: -7.28359459743497, variance: 165.88366387939453, lamda: 2.8344907760620117
Running avgs for agent 2: q_loss: 5.378003120422363, p_loss: -3.746127128601074, mean_rew: -7.2866045056702315, variance: 4.414217948913574, lamda: 1.6881234645843506

steps: 1899975, episodes: 76000, mean episode reward: -577.253038233286, agent episode reward: [-192.41767941109532, -192.41767941109532, -192.41767941109532], time: 181.368
steps: 1899975, episodes: 76000, mean episode variance: 42.97479680991173, agent episode variance: [0.9605297737121582, 40.90264971923828, 1.1116173169612884], time: 181.369
Running avgs for agent 0: q_loss: 6.347809314727783, p_loss: -3.769955635070801, mean_rew: -7.302266898172124, variance: 3.8421192169189453, lamda: 1.9534938335418701
Running avgs for agent 1: q_loss: 1129950.125, p_loss: 138.5596923828125, mean_rew: -7.30548868340194, variance: 163.61059887695313, lamda: 2.8594653606414795
Running avgs for agent 2: q_loss: 5.589828014373779, p_loss: -3.764129400253296, mean_rew: -7.317845580424004, variance: 4.446469306945801, lamda: 1.6882444620132446

steps: 1924975, episodes: 77000, mean episode reward: -570.6756780000718, agent episode reward: [-190.2252260000239, -190.2252260000239, -190.2252260000239], time: 181.517
steps: 1924975, episodes: 77000, mean episode variance: 41.57641994976997, agent episode variance: [0.9598389148712159, 39.508352462768556, 1.1082285721302032], time: 181.518
Running avgs for agent 0: q_loss: 6.507950305938721, p_loss: -3.7846226692199707, mean_rew: -7.331859162708337, variance: 3.839355707168579, lamda: 1.9600151777267456
Running avgs for agent 1: q_loss: 1144253.625, p_loss: 138.58287048339844, mean_rew: -7.330060067508959, variance: 158.03340985107423, lamda: 2.884439706802368
Running avgs for agent 2: q_loss: 6.060774803161621, p_loss: -3.7738184928894043, mean_rew: -7.332514592665373, variance: 4.4329142570495605, lamda: 1.6889196634292603

steps: 1949975, episodes: 78000, mean episode reward: -573.0883460979865, agent episode reward: [-191.0294486993288, -191.0294486993288, -191.0294486993288], time: 172.994
steps: 1949975, episodes: 78000, mean episode variance: 42.508795797586444, agent episode variance: [0.9565222795009614, 40.43902198791504, 1.1132515301704407], time: 172.995
Running avgs for agent 0: q_loss: 6.961118698120117, p_loss: -3.80376935005188, mean_rew: -7.352364542002789, variance: 3.8260889053344727, lamda: 1.960928201675415
Running avgs for agent 1: q_loss: 1154498.5, p_loss: 138.67257690429688, mean_rew: -7.350034080555448, variance: 161.75608795166016, lamda: 2.909414052963257
Running avgs for agent 2: q_loss: 4.550360679626465, p_loss: -3.767557382583618, mean_rew: -7.349571768161527, variance: 4.453005790710449, lamda: 1.6955324411392212

steps: 1974975, episodes: 79000, mean episode reward: -573.0233024017562, agent episode reward: [-191.00776746725208, -191.00776746725208, -191.00776746725208], time: 171.696
steps: 1974975, episodes: 79000, mean episode variance: 41.53044741296768, agent episode variance: [0.9606806144714356, 39.45480711364746, 1.1149596848487855], time: 171.696
Running avgs for agent 0: q_loss: 7.165411949157715, p_loss: -3.8092222213745117, mean_rew: -7.375828230061188, variance: 3.8427224159240723, lamda: 1.960970163345337
Running avgs for agent 1: q_loss: 1151611.625, p_loss: 138.8076629638672, mean_rew: -7.373806113450482, variance: 157.81922845458985, lamda: 2.9343883991241455
Running avgs for agent 2: q_loss: 5.243247032165527, p_loss: -3.791048526763916, mean_rew: -7.378939374960308, variance: 4.4598388671875, lamda: 1.699108600616455

steps: 1999975, episodes: 80000, mean episode reward: -577.1836723873035, agent episode reward: [-192.3945574624345, -192.3945574624345, -192.3945574624345], time: 166.242
steps: 1999975, episodes: 80000, mean episode variance: 41.13359140253067, agent episode variance: [0.9661292636394501, 39.05535917663574, 1.112102962255478], time: 166.243
Running avgs for agent 0: q_loss: 7.450567245483398, p_loss: -3.831148862838745, mean_rew: -7.394196973842229, variance: 3.8645169734954834, lamda: 1.9622024297714233
Running avgs for agent 1: q_loss: 1158386.375, p_loss: 138.91671752929688, mean_rew: -7.39337715330779, variance: 156.22143670654296, lamda: 2.959362745285034
Running avgs for agent 2: q_loss: 4.607300281524658, p_loss: -3.8050308227539062, mean_rew: -7.395844063011842, variance: 4.448411464691162, lamda: 1.7031649351119995

steps: 2024975, episodes: 81000, mean episode reward: -565.9757942129437, agent episode reward: [-188.6585980709812, -188.6585980709812, -188.6585980709812], time: 159.896
steps: 2024975, episodes: 81000, mean episode variance: 40.393807070732116, agent episode variance: [0.9624709372520447, 38.32215600585938, 1.109180127620697], time: 159.896
Running avgs for agent 0: q_loss: 6.450117588043213, p_loss: -3.839392900466919, mean_rew: -7.4167867951752, variance: 3.849883556365967, lamda: 1.9653387069702148
Running avgs for agent 1: q_loss: 1177686.625, p_loss: 139.18768310546875, mean_rew: -7.418486615914606, variance: 153.2886240234375, lamda: 2.984337091445923
Running avgs for agent 2: q_loss: 4.26785945892334, p_loss: -3.8163559436798096, mean_rew: -7.418573550193216, variance: 4.436720848083496, lamda: 1.7100648880004883

steps: 2049975, episodes: 82000, mean episode reward: -574.3834506711415, agent episode reward: [-191.46115022371384, -191.46115022371384, -191.46115022371384], time: 159.54
steps: 2049975, episodes: 82000, mean episode variance: 41.22007965493202, agent episode variance: [0.956598872423172, 39.15588604736328, 1.1075947351455688], time: 159.54
Running avgs for agent 0: q_loss: 6.862163066864014, p_loss: -3.8557558059692383, mean_rew: -7.434200243088884, variance: 3.8263955116271973, lamda: 1.973785161972046
Running avgs for agent 1: q_loss: 1190918.0, p_loss: 139.40296936035156, mean_rew: -7.433294418959286, variance: 156.62354418945313, lamda: 3.0093114376068115
Running avgs for agent 2: q_loss: 4.527471542358398, p_loss: -3.8239340782165527, mean_rew: -7.438691345433595, variance: 4.4303789138793945, lamda: 1.7163666486740112

steps: 2074975, episodes: 83000, mean episode reward: -573.7645891278039, agent episode reward: [-191.25486304260133, -191.25486304260133, -191.25486304260133], time: 160.267
steps: 2074975, episodes: 83000, mean episode variance: 39.335005016565326, agent episode variance: [0.9616751141548157, 37.268097457885744, 1.105232444524765], time: 160.268
Running avgs for agent 0: q_loss: 7.405150413513184, p_loss: -3.87770676612854, mean_rew: -7.460762769726946, variance: 3.846700429916382, lamda: 1.97442626953125
Running avgs for agent 1: q_loss: 1211338.625, p_loss: 139.74188232421875, mean_rew: -7.456880036427769, variance: 149.07238983154298, lamda: 3.034285545349121
Running avgs for agent 2: q_loss: 5.901742935180664, p_loss: -3.8257617950439453, mean_rew: -7.456538196534954, variance: 4.420929908752441, lamda: 1.720665454864502

steps: 2099975, episodes: 84000, mean episode reward: -576.8521924056231, agent episode reward: [-192.28406413520773, -192.28406413520773, -192.28406413520773], time: 166.413
steps: 2099975, episodes: 84000, mean episode variance: 41.01232768344879, agent episode variance: [0.9628750157356262, 38.94655581665039, 1.1028968510627746], time: 166.414
Running avgs for agent 0: q_loss: 7.6880717277526855, p_loss: -3.8871731758117676, mean_rew: -7.482816445928902, variance: 3.8515002727508545, lamda: 1.9746602773666382
Running avgs for agent 1: q_loss: 1213256.125, p_loss: 139.8939666748047, mean_rew: -7.4830666798055026, variance: 155.78622326660155, lamda: 3.059260368347168
Running avgs for agent 2: q_loss: 5.972999572753906, p_loss: -3.841899871826172, mean_rew: -7.473225537081947, variance: 4.411587715148926, lamda: 1.7210007905960083

steps: 2124975, episodes: 85000, mean episode reward: -571.3825364077453, agent episode reward: [-190.46084546924848, -190.46084546924848, -190.46084546924848], time: 180.709
steps: 2124975, episodes: 85000, mean episode variance: 38.982354527950285, agent episode variance: [0.9664341824054719, 36.895881072998044, 1.1200392725467683], time: 180.71
Running avgs for agent 0: q_loss: 7.882980823516846, p_loss: -3.8980181217193604, mean_rew: -7.49993534768147, variance: 3.865736722946167, lamda: 1.9750654697418213
Running avgs for agent 1: q_loss: 1207458.25, p_loss: 140.13571166992188, mean_rew: -7.496542894489025, variance: 147.58352429199218, lamda: 3.0842344760894775
Running avgs for agent 2: q_loss: 6.050464630126953, p_loss: -3.8521366119384766, mean_rew: -7.50827146286172, variance: 4.480157375335693, lamda: 1.7210010290145874

steps: 2149975, episodes: 86000, mean episode reward: -580.1749868300271, agent episode reward: [-193.3916622766757, -193.3916622766757, -193.3916622766757], time: 163.767
steps: 2149975, episodes: 86000, mean episode variance: 40.048177243471144, agent episode variance: [0.9745841400623322, 37.955197875976566, 1.118395227432251], time: 163.767
Running avgs for agent 0: q_loss: 7.861029148101807, p_loss: -3.911252975463867, mean_rew: -7.529940446868919, variance: 3.898336410522461, lamda: 1.9753634929656982
Running avgs for agent 1: q_loss: 1218372.875, p_loss: 140.1537628173828, mean_rew: -7.525135781158312, variance: 151.82079150390626, lamda: 3.1092090606689453
Running avgs for agent 2: q_loss: 6.035487651824951, p_loss: -3.8617475032806396, mean_rew: -7.528989551923447, variance: 4.473580360412598, lamda: 1.7217731475830078

steps: 2174975, episodes: 87000, mean episode reward: -581.2449678190811, agent episode reward: [-193.74832260636038, -193.74832260636038, -193.74832260636038], time: 160.856
steps: 2174975, episodes: 87000, mean episode variance: 41.51083309006691, agent episode variance: [0.9764745688438415, 39.41583221435547, 1.1185263068675995], time: 160.856
Running avgs for agent 0: q_loss: 6.668809413909912, p_loss: -3.92254638671875, mean_rew: -7.551884938464733, variance: 3.905898332595825, lamda: 1.9785207509994507
Running avgs for agent 1: q_loss: 1197617.375, p_loss: 140.11135864257812, mean_rew: -7.551247279011588, variance: 157.66332885742187, lamda: 3.134183168411255
Running avgs for agent 2: q_loss: 6.196262836456299, p_loss: -3.8667349815368652, mean_rew: -7.546926513742687, variance: 4.474105358123779, lamda: 1.7232285737991333

steps: 2199975, episodes: 88000, mean episode reward: -587.2612851380321, agent episode reward: [-195.75376171267737, -195.75376171267737, -195.75376171267737], time: 157.14
steps: 2199975, episodes: 88000, mean episode variance: 38.95229555153847, agent episode variance: [0.9705295677185058, 36.865419998168946, 1.1163459856510163], time: 157.141
Running avgs for agent 0: q_loss: 6.584452152252197, p_loss: -3.9172661304473877, mean_rew: -7.562321543426893, variance: 3.8821182250976562, lamda: 1.983428955078125
Running avgs for agent 1: q_loss: 1208049.625, p_loss: 140.16778564453125, mean_rew: -7.572829473010828, variance: 147.46167999267578, lamda: 3.1591575145721436
Running avgs for agent 2: q_loss: 6.299941539764404, p_loss: -3.8814146518707275, mean_rew: -7.573510704249205, variance: 4.465384006500244, lamda: 1.7233655452728271

steps: 2224975, episodes: 89000, mean episode reward: -589.3220073893758, agent episode reward: [-196.44066912979193, -196.44066912979193, -196.44066912979193], time: 157.898
steps: 2224975, episodes: 89000, mean episode variance: 38.66179457306862, agent episode variance: [0.9754158213138581, 36.569561225891114, 1.1168175258636475], time: 157.898
Running avgs for agent 0: q_loss: 6.467352867126465, p_loss: -3.9286937713623047, mean_rew: -7.589369901443903, variance: 3.901663303375244, lamda: 1.985962986946106
Running avgs for agent 1: q_loss: 1197932.25, p_loss: 140.10055541992188, mean_rew: -7.584783394462312, variance: 146.27824490356446, lamda: 3.1841318607330322
Running avgs for agent 2: q_loss: 6.300873756408691, p_loss: -3.8902475833892822, mean_rew: -7.585509878873994, variance: 4.4672698974609375, lamda: 1.7234392166137695

steps: 2249975, episodes: 90000, mean episode reward: -586.3116440378758, agent episode reward: [-195.43721467929197, -195.43721467929197, -195.43721467929197], time: 160.116
steps: 2249975, episodes: 90000, mean episode variance: 38.29664531087875, agent episode variance: [0.9714972898960114, 36.206876235961914, 1.1182717850208284], time: 160.116
Running avgs for agent 0: q_loss: 5.247787952423096, p_loss: -3.941540241241455, mean_rew: -7.6028631576905354, variance: 3.885989189147949, lamda: 1.9923006296157837
Running avgs for agent 1: q_loss: 1178493.75, p_loss: 140.10256958007812, mean_rew: -7.61202929450506, variance: 144.82750494384766, lamda: 3.209106206893921
Running avgs for agent 2: q_loss: 6.260006904602051, p_loss: -3.8881709575653076, mean_rew: -7.6001089070578685, variance: 4.473086833953857, lamda: 1.7234898805618286

steps: 2274975, episodes: 91000, mean episode reward: -589.0279570532656, agent episode reward: [-196.3426523510886, -196.3426523510886, -196.3426523510886], time: 158.944
steps: 2274975, episodes: 91000, mean episode variance: 37.9062776427269, agent episode variance: [0.9667316539287567, 35.81461019134522, 1.1249357974529266], time: 158.944
Running avgs for agent 0: q_loss: 6.002045154571533, p_loss: -3.950591802597046, mean_rew: -7.6227093364490734, variance: 3.866926431655884, lamda: 1.9981114864349365
Running avgs for agent 1: q_loss: 1183776.0, p_loss: 139.95811462402344, mean_rew: -7.628562868299483, variance: 143.25844076538087, lamda: 3.2340805530548096
Running avgs for agent 2: q_loss: 6.520301342010498, p_loss: -3.906524896621704, mean_rew: -7.618773000856141, variance: 4.4997429847717285, lamda: 1.724016547203064

steps: 2299975, episodes: 92000, mean episode reward: -586.7924447173498, agent episode reward: [-195.59748157244994, -195.59748157244994, -195.59748157244994], time: 164.402
steps: 2299975, episodes: 92000, mean episode variance: 37.02056395721436, agent episode variance: [0.976669527053833, 34.91826607513428, 1.1256283550262451], time: 164.403
Running avgs for agent 0: q_loss: 7.229018211364746, p_loss: -3.962367296218872, mean_rew: -7.644367035380334, variance: 3.9066781997680664, lamda: 2.0002732276916504
Running avgs for agent 1: q_loss: 1152585.25, p_loss: 139.70481872558594, mean_rew: -7.627526520109512, variance: 139.6730643005371, lamda: 3.259054660797119
Running avgs for agent 2: q_loss: 6.506522178649902, p_loss: -3.9113874435424805, mean_rew: -7.640179003617492, variance: 4.502513885498047, lamda: 1.7240322828292847

steps: 2324975, episodes: 93000, mean episode reward: -598.0061860695843, agent episode reward: [-199.3353953565281, -199.3353953565281, -199.3353953565281], time: 161.96
steps: 2324975, episodes: 93000, mean episode variance: 38.376341870307925, agent episode variance: [0.9739158189296723, 36.27164072418213, 1.1307853271961212], time: 161.96
Running avgs for agent 0: q_loss: 7.07421350479126, p_loss: -3.961393117904663, mean_rew: -7.642153761131978, variance: 3.895663261413574, lamda: 2.000425100326538
Running avgs for agent 1: q_loss: 1152532.25, p_loss: 139.58480834960938, mean_rew: -7.648466795034842, variance: 145.0865628967285, lamda: 3.284029245376587
Running avgs for agent 2: q_loss: 6.483154296875, p_loss: -3.910952091217041, mean_rew: -7.647045861925917, variance: 4.523141384124756, lamda: 1.7240324020385742

steps: 2349975, episodes: 94000, mean episode reward: -590.0377726340671, agent episode reward: [-196.67925754468905, -196.67925754468905, -196.67925754468905], time: 159.158
steps: 2349975, episodes: 94000, mean episode variance: 36.39540639042854, agent episode variance: [0.9765697193145751, 34.2885658416748, 1.1302708294391632], time: 159.158
Running avgs for agent 0: q_loss: 7.3673014640808105, p_loss: -3.9743404388427734, mean_rew: -7.663654971752276, variance: 3.9062788486480713, lamda: 2.000431537628174
Running avgs for agent 1: q_loss: 1124039.5, p_loss: 139.47401428222656, mean_rew: -7.661501899218012, variance: 137.1542633666992, lamda: 3.3090033531188965
Running avgs for agent 2: q_loss: 6.499850749969482, p_loss: -3.91827392578125, mean_rew: -7.656016223356961, variance: 4.521082878112793, lamda: 1.7240649461746216

steps: 2374975, episodes: 95000, mean episode reward: -600.1044438993481, agent episode reward: [-200.03481463311607, -200.03481463311607, -200.03481463311607], time: 158.965
steps: 2374975, episodes: 95000, mean episode variance: 35.69854792690277, agent episode variance: [0.978583729505539, 33.58648742675781, 1.1334767706394195], time: 158.965
Running avgs for agent 0: q_loss: 7.420417785644531, p_loss: -3.9818105697631836, mean_rew: -7.6744488162774624, variance: 3.914335012435913, lamda: 2.0004405975341797
Running avgs for agent 1: q_loss: 1109936.375, p_loss: 139.35655212402344, mean_rew: -7.673728191142205, variance: 134.34594970703125, lamda: 3.3339779376983643
Running avgs for agent 2: q_loss: 6.396645545959473, p_loss: -3.9183008670806885, mean_rew: -7.667600897142621, variance: 4.533907413482666, lamda: 1.7242127656936646

steps: 2399975, episodes: 96000, mean episode reward: -590.9412910339255, agent episode reward: [-196.98043034464186, -196.98043034464186, -196.98043034464186], time: 165.711
steps: 2399975, episodes: 96000, mean episode variance: 36.26388585209847, agent episode variance: [0.9757188215255738, 34.15731280517578, 1.13085422539711], time: 165.711
Running avgs for agent 0: q_loss: 7.309140205383301, p_loss: -3.98494815826416, mean_rew: -7.687094200559289, variance: 3.9028751850128174, lamda: 2.000440835952759
Running avgs for agent 1: q_loss: 1135123.5, p_loss: 139.29946899414062, mean_rew: -7.685178886771861, variance: 136.6292512207031, lamda: 3.358952045440674
Running avgs for agent 2: q_loss: 6.458923816680908, p_loss: -3.927386522293091, mean_rew: -7.686163567778151, variance: 4.523416996002197, lamda: 1.724393367767334

steps: 2424975, episodes: 97000, mean episode reward: -596.6224226944688, agent episode reward: [-198.87414089815624, -198.87414089815624, -198.87414089815624], time: 162.814
steps: 2424975, episodes: 97000, mean episode variance: 35.278117773771285, agent episode variance: [0.9768640944957733, 33.1657448425293, 1.1355088367462158], time: 162.815
Running avgs for agent 0: q_loss: 7.4492506980896, p_loss: -3.9878664016723633, mean_rew: -7.687331212655041, variance: 3.907456398010254, lamda: 2.000441074371338
Running avgs for agent 1: q_loss: 1101206.75, p_loss: 139.35491943359375, mean_rew: -7.696240110481157, variance: 132.6629793701172, lamda: 3.3839268684387207
Running avgs for agent 2: q_loss: 6.389060020446777, p_loss: -3.9366352558135986, mean_rew: -7.693528733763669, variance: 4.542035102844238, lamda: 1.7249001264572144

steps: 2449975, episodes: 98000, mean episode reward: -601.0231600018176, agent episode reward: [-200.34105333393921, -200.34105333393921, -200.34105333393921], time: 159.083
steps: 2449975, episodes: 98000, mean episode variance: 35.20428950786591, agent episode variance: [0.9839265296459198, 33.08149327087402, 1.1388697073459626], time: 159.084
Running avgs for agent 0: q_loss: 7.425265789031982, p_loss: -3.9924869537353516, mean_rew: -7.707196002268158, variance: 3.935706377029419, lamda: 2.0004637241363525
Running avgs for agent 1: q_loss: 1087607.5, p_loss: 139.33987426757812, mean_rew: -7.705948987046062, variance: 132.32597308349608, lamda: 3.4089012145996094
Running avgs for agent 2: q_loss: 6.52191686630249, p_loss: -3.9427621364593506, mean_rew: -7.711940276346652, variance: 4.555478572845459, lamda: 1.7249282598495483

steps: 2474975, episodes: 99000, mean episode reward: -592.1989946795449, agent episode reward: [-197.39966489318158, -197.39966489318158, -197.39966489318158], time: 164.996
steps: 2474975, episodes: 99000, mean episode variance: 35.08954056763649, agent episode variance: [0.9799623498916626, 32.97082136535644, 1.138756852388382], time: 164.996
Running avgs for agent 0: q_loss: 7.521304130554199, p_loss: -4.002493381500244, mean_rew: -7.724057903733947, variance: 3.919849395751953, lamda: 2.0009055137634277
Running avgs for agent 1: q_loss: 1080990.625, p_loss: 139.39700317382812, mean_rew: -7.71252570804057, variance: 131.88328546142577, lamda: 3.433875322341919
Running avgs for agent 2: q_loss: 6.465591907501221, p_loss: -3.9472475051879883, mean_rew: -7.721532025949332, variance: 4.555027484893799, lamda: 1.7249282598495483

steps: 2499975, episodes: 100000, mean episode reward: -591.1985822386783, agent episode reward: [-197.06619407955944, -197.06619407955944, -197.06619407955944], time: 170.048
steps: 2499975, episodes: 100000, mean episode variance: 34.196134526491164, agent episode variance: [0.9784415452480316, 32.08030635070801, 1.1373866305351257], time: 170.049
Running avgs for agent 0: q_loss: 6.6088361740112305, p_loss: -4.00471305847168, mean_rew: -7.723986745903809, variance: 3.9137661457061768, lamda: 2.0024027824401855
Running avgs for agent 1: q_loss: 1078133.125, p_loss: 139.5580596923828, mean_rew: -7.724861700732576, variance: 128.32122540283203, lamda: 3.4588496685028076
Running avgs for agent 2: q_loss: 6.156996726989746, p_loss: -3.9475810527801514, mean_rew: -7.726951062539881, variance: 4.549546241760254, lamda: 1.7266730070114136

steps: 2524975, episodes: 101000, mean episode reward: -592.5367507284266, agent episode reward: [-197.51225024280888, -197.51225024280888, -197.51225024280888], time: 134.076
steps: 2524975, episodes: 101000, mean episode variance: 34.780695728063584, agent episode variance: [0.9851073665618897, 32.6584920501709, 1.1370963113307953], time: 134.076
Running avgs for agent 0: q_loss: 6.3437275886535645, p_loss: -3.99615216255188, mean_rew: -7.731088764025768, variance: 3.9404296875, lamda: 2.007870674133301
Running avgs for agent 1: q_loss: 1079034.875, p_loss: 139.57168579101562, mean_rew: -7.734956409263476, variance: 130.6339682006836, lamda: 3.4838240146636963
Running avgs for agent 2: q_loss: 6.475432395935059, p_loss: -3.94490385055542, mean_rew: -7.731671762638371, variance: 4.548385143280029, lamda: 1.7270023822784424

steps: 2549975, episodes: 102000, mean episode reward: -599.5902255038619, agent episode reward: [-199.8634085012873, -199.8634085012873, -199.8634085012873], time: 116.877
steps: 2549975, episodes: 102000, mean episode variance: 34.65029514884949, agent episode variance: [0.9837838788032531, 32.52812649536133, 1.138384774684906], time: 116.877
Running avgs for agent 0: q_loss: 7.264802932739258, p_loss: -4.000464916229248, mean_rew: -7.740133487695654, variance: 3.9351353645324707, lamda: 2.0098299980163574
Running avgs for agent 1: q_loss: 1072356.25, p_loss: 139.6935272216797, mean_rew: -7.735706840254123, variance: 130.1125059814453, lamda: 3.508798360824585
Running avgs for agent 2: q_loss: 6.3952765464782715, p_loss: -3.9612913131713867, mean_rew: -7.747501626666696, variance: 4.553539276123047, lamda: 1.7270026206970215

steps: 2574975, episodes: 103000, mean episode reward: -597.620174345973, agent episode reward: [-199.206724781991, -199.206724781991, -199.206724781991], time: 118.189
steps: 2574975, episodes: 103000, mean episode variance: 33.46284283828735, agent episode variance: [0.9800230476856232, 31.34130049133301, 1.1415192992687226], time: 118.19
Running avgs for agent 0: q_loss: 7.253256797790527, p_loss: -4.001537322998047, mean_rew: -7.741485030627949, variance: 3.9200923442840576, lamda: 2.0100808143615723
Running avgs for agent 1: q_loss: 1064684.625, p_loss: 139.845947265625, mean_rew: -7.750506149965523, variance: 125.36520196533203, lamda: 3.5337724685668945
Running avgs for agent 2: q_loss: 6.3631439208984375, p_loss: -3.960319757461548, mean_rew: -7.755204542056482, variance: 4.56607723236084, lamda: 1.7270023822784424

steps: 2599975, episodes: 104000, mean episode reward: -594.9056473510389, agent episode reward: [-198.30188245034634, -198.30188245034634, -198.30188245034634], time: 110.945
steps: 2599975, episodes: 104000, mean episode variance: 33.90274451494217, agent episode variance: [0.9876126387119293, 31.777548446655274, 1.1375834295749665], time: 110.946
Running avgs for agent 0: q_loss: 5.929760456085205, p_loss: -4.015890598297119, mean_rew: -7.764984375598569, variance: 3.9504506587982178, lamda: 2.0111727714538574
Running avgs for agent 1: q_loss: 1079821.0, p_loss: 140.0561981201172, mean_rew: -7.757449111958458, variance: 127.1101937866211, lamda: 3.5587470531463623
Running avgs for agent 2: q_loss: 6.2992987632751465, p_loss: -3.959146499633789, mean_rew: -7.746397883829176, variance: 4.550333499908447, lamda: 1.7270023822784424

steps: 2624975, episodes: 105000, mean episode reward: -597.8432034115526, agent episode reward: [-199.28106780385087, -199.28106780385087, -199.28106780385087], time: 102.099
steps: 2624975, episodes: 105000, mean episode variance: 33.764373057603834, agent episode variance: [0.9851129174232482, 31.639969482421876, 1.1392906577587127], time: 102.1
Running avgs for agent 0: q_loss: 5.3548736572265625, p_loss: -4.011790752410889, mean_rew: -7.765838997777182, variance: 3.9404516220092773, lamda: 2.0167694091796875
Running avgs for agent 1: q_loss: 1087057.5, p_loss: 140.2879180908203, mean_rew: -7.767954989709586, variance: 126.5598779296875, lamda: 3.583721160888672
Running avgs for agent 2: q_loss: 6.358006954193115, p_loss: -3.97733211517334, mean_rew: -7.769915692997904, variance: 4.557162761688232, lamda: 1.7271276712417603

steps: 2649975, episodes: 106000, mean episode reward: -598.4120905744805, agent episode reward: [-199.47069685816024, -199.47069685816024, -199.47069685816024], time: 101.454
steps: 2649975, episodes: 106000, mean episode variance: 33.0580952205658, agent episode variance: [0.9862559640407562, 30.92932989501953, 1.1425093615055084], time: 101.454
Running avgs for agent 0: q_loss: 7.1278581619262695, p_loss: -4.011111259460449, mean_rew: -7.781460054608953, variance: 3.945024013519287, lamda: 2.0222198963165283
Running avgs for agent 1: q_loss: 1080724.25, p_loss: 140.56617736816406, mean_rew: -7.780295830887725, variance: 123.71731958007813, lamda: 3.6086957454681396
Running avgs for agent 2: q_loss: 6.366968631744385, p_loss: -3.977527141571045, mean_rew: -7.780222734596444, variance: 4.570037364959717, lamda: 1.7273333072662354

steps: 2674975, episodes: 107000, mean episode reward: -588.2395055313083, agent episode reward: [-196.0798351771027, -196.0798351771027, -196.0798351771027], time: 101.522
steps: 2674975, episodes: 107000, mean episode variance: 32.92476894187927, agent episode variance: [0.9863972775936126, 30.79580168914795, 1.1425699751377105], time: 101.522
Running avgs for agent 0: q_loss: 7.385463237762451, p_loss: -4.019852638244629, mean_rew: -7.789764459327526, variance: 3.945589065551758, lamda: 2.0229711532592773
Running avgs for agent 1: q_loss: 1096628.75, p_loss: 140.73971557617188, mean_rew: -7.78170518957198, variance: 123.1832067565918, lamda: 3.6336700916290283
Running avgs for agent 2: q_loss: 6.398034572601318, p_loss: -3.9826605319976807, mean_rew: -7.7885164571016, variance: 4.570280075073242, lamda: 1.7274131774902344

steps: 2699975, episodes: 108000, mean episode reward: -591.6780507036444, agent episode reward: [-197.22601690121482, -197.22601690121482, -197.22601690121482], time: 98.442
steps: 2699975, episodes: 108000, mean episode variance: 33.29627448153496, agent episode variance: [0.988183418750763, 31.15866268157959, 1.149428381204605], time: 98.443
Running avgs for agent 0: q_loss: 7.253904342651367, p_loss: -4.020112037658691, mean_rew: -7.798250323070079, variance: 3.9527337551116943, lamda: 2.023042917251587
Running avgs for agent 1: q_loss: 1101624.875, p_loss: 140.88328552246094, mean_rew: -7.786984567801417, variance: 124.63465072631836, lamda: 3.658644199371338
Running avgs for agent 2: q_loss: 6.309670925140381, p_loss: -3.984473705291748, mean_rew: -7.793708428854854, variance: 4.597713470458984, lamda: 1.7275725603103638

steps: 2724975, episodes: 109000, mean episode reward: -594.1238739677227, agent episode reward: [-198.0412913225742, -198.0412913225742, -198.0412913225742], time: 89.601
steps: 2724975, episodes: 109000, mean episode variance: 32.71161947488785, agent episode variance: [0.9824254596233368, 30.580053207397462, 1.1491408078670502], time: 89.601
Running avgs for agent 0: q_loss: 7.26918363571167, p_loss: -4.024511814117432, mean_rew: -7.796554920891518, variance: 3.929701566696167, lamda: 2.023043155670166
Running avgs for agent 1: q_loss: 1098587.375, p_loss: 141.1912841796875, mean_rew: -7.80081077687241, variance: 122.32021282958985, lamda: 3.6836187839508057
Running avgs for agent 2: q_loss: 6.211039066314697, p_loss: -3.990601062774658, mean_rew: -7.79562969445798, variance: 4.596563339233398, lamda: 1.7275725603103638

steps: 2749975, episodes: 110000, mean episode reward: -598.8451673782675, agent episode reward: [-199.61505579275587, -199.61505579275587, -199.61505579275587], time: 85.369
steps: 2749975, episodes: 110000, mean episode variance: 33.24588860821724, agent episode variance: [0.9843859233856201, 31.115748077392578, 1.1457546074390412], time: 85.369
Running avgs for agent 0: q_loss: 7.348116874694824, p_loss: -4.028347969055176, mean_rew: -7.8059947036687465, variance: 3.9375436305999756, lamda: 2.023043155670166
Running avgs for agent 1: q_loss: 1109632.25, p_loss: 141.4535675048828, mean_rew: -7.802684081594431, variance: 124.46299230957031, lamda: 3.7085931301116943
Running avgs for agent 2: q_loss: 6.247183799743652, p_loss: -3.995056390762329, mean_rew: -7.801957490618848, variance: 4.583018779754639, lamda: 1.7275726795196533

steps: 2774975, episodes: 111000, mean episode reward: -596.6446074301817, agent episode reward: [-198.88153581006057, -198.88153581006057, -198.88153581006057], time: 88.516
steps: 2774975, episodes: 111000, mean episode variance: 33.439427455186845, agent episode variance: [0.9923172442913055, 31.295576217651366, 1.151533993244171], time: 88.516
Running avgs for agent 0: q_loss: 7.314004898071289, p_loss: -4.025798797607422, mean_rew: -7.813654576988174, variance: 3.969268798828125, lamda: 2.023043155670166
Running avgs for agent 1: q_loss: 1124473.625, p_loss: 141.71780395507812, mean_rew: -7.8203936768049465, variance: 125.18230487060546, lamda: 3.733567476272583
Running avgs for agent 2: q_loss: 6.2899017333984375, p_loss: -3.9981870651245117, mean_rew: -7.8159628608367475, variance: 4.606135845184326, lamda: 1.7275727987289429

steps: 2799975, episodes: 112000, mean episode reward: -591.3166101448065, agent episode reward: [-197.1055367149355, -197.1055367149355, -197.1055367149355], time: 90.971
steps: 2799975, episodes: 112000, mean episode variance: 32.5640903069973, agent episode variance: [0.9881084473133087, 30.42384715270996, 1.1521347069740295], time: 90.971
Running avgs for agent 0: q_loss: 7.229487895965576, p_loss: -4.027965068817139, mean_rew: -7.813471465786426, variance: 3.9524338245391846, lamda: 2.0230841636657715
Running avgs for agent 1: q_loss: 1131015.25, p_loss: 142.0050048828125, mean_rew: -7.820728423325905, variance: 121.69538861083984, lamda: 3.758542060852051
Running avgs for agent 2: q_loss: 6.3228654861450195, p_loss: -3.9991109371185303, mean_rew: -7.820913758936926, variance: 4.60853910446167, lamda: 1.7275745868682861

steps: 2824975, episodes: 113000, mean episode reward: -585.8851713692608, agent episode reward: [-195.29505712308693, -195.29505712308693, -195.29505712308693], time: 89.733
steps: 2824975, episodes: 113000, mean episode variance: 32.43725826478004, agent episode variance: [0.9884198534488678, 30.295327880859375, 1.1535105304718019], time: 89.734
Running avgs for agent 0: q_loss: 7.3478779792785645, p_loss: -4.0356245040893555, mean_rew: -7.8227788187624645, variance: 3.9536795616149902, lamda: 2.02312970161438
Running avgs for agent 1: q_loss: 1149617.5, p_loss: 142.3463134765625, mean_rew: -7.830125758880558, variance: 121.1813115234375, lamda: 3.7835161685943604
Running avgs for agent 2: q_loss: 6.341331958770752, p_loss: -4.010196685791016, mean_rew: -7.827473315978499, variance: 4.614041805267334, lamda: 1.7275781631469727

steps: 2849975, episodes: 114000, mean episode reward: -592.4651067706602, agent episode reward: [-197.4883689235534, -197.4883689235534, -197.4883689235534], time: 90.965
steps: 2849975, episodes: 114000, mean episode variance: 33.360746950387956, agent episode variance: [0.9923182821273804, 31.21559348297119, 1.152835185289383], time: 90.965
Running avgs for agent 0: q_loss: 7.3025994300842285, p_loss: -4.031130790710449, mean_rew: -7.829081865592656, variance: 3.9692728519439697, lamda: 2.02312970161438
Running avgs for agent 1: q_loss: 1164800.5, p_loss: 142.60604858398438, mean_rew: -7.833366905273077, variance: 124.86237393188476, lamda: 3.808490514755249
Running avgs for agent 2: q_loss: 6.3519439697265625, p_loss: -4.018428802490234, mean_rew: -7.833445369638466, variance: 4.6113409996032715, lamda: 1.7275781631469727

steps: 2874975, episodes: 115000, mean episode reward: -597.5565393660411, agent episode reward: [-199.1855131220137, -199.1855131220137, -199.1855131220137], time: 92.023
steps: 2874975, episodes: 115000, mean episode variance: 32.27436613368988, agent episode variance: [0.9912914884090424, 30.120682662963866, 1.1623919823169708], time: 92.024
Running avgs for agent 0: q_loss: 7.345991611480713, p_loss: -4.039228916168213, mean_rew: -7.838596487305089, variance: 3.9651660919189453, lamda: 2.02312970161438
Running avgs for agent 1: q_loss: 1173218.875, p_loss: 143.01675415039062, mean_rew: -7.840431350388164, variance: 120.48273065185546, lamda: 3.8334648609161377
Running avgs for agent 2: q_loss: 5.677964687347412, p_loss: -4.013111591339111, mean_rew: -7.845607526875451, variance: 4.649567604064941, lamda: 1.7284306287765503

steps: 2899975, episodes: 116000, mean episode reward: -596.1332166854047, agent episode reward: [-198.71107222846823, -198.71107222846823, -198.71107222846823], time: 92.012
steps: 2899975, episodes: 116000, mean episode variance: 33.14006228137016, agent episode variance: [0.9890815079212188, 30.987832748413087, 1.1631480250358581], time: 92.013
Running avgs for agent 0: q_loss: 7.0284833908081055, p_loss: -4.052799701690674, mean_rew: -7.848307253404368, variance: 3.9563262462615967, lamda: 2.023693799972534
Running avgs for agent 1: q_loss: 1180456.5, p_loss: 143.2991180419922, mean_rew: -7.845489942823649, variance: 123.95133099365235, lamda: 3.8584389686584473
Running avgs for agent 2: q_loss: 4.706393718719482, p_loss: -4.0163493156433105, mean_rew: -7.8490094886448825, variance: 4.652591705322266, lamda: 1.7332607507705688

steps: 2924975, episodes: 117000, mean episode reward: -597.022904771834, agent episode reward: [-199.00763492394466, -199.00763492394466, -199.00763492394466], time: 78.795
steps: 2924975, episodes: 117000, mean episode variance: 33.31210533571243, agent episode variance: [0.9904957368373871, 31.163630935668944, 1.1579786632061004], time: 78.796
Running avgs for agent 0: q_loss: 5.729833126068115, p_loss: -4.04758882522583, mean_rew: -7.854307529757799, variance: 3.9619829654693604, lamda: 2.0289928913116455
Running avgs for agent 1: q_loss: 1213564.375, p_loss: 143.66937255859375, mean_rew: -7.8562353507261085, variance: 124.65452374267578, lamda: 3.883413553237915
Running avgs for agent 2: q_loss: 5.992513656616211, p_loss: -4.016066551208496, mean_rew: -7.860685932389801, variance: 4.631915092468262, lamda: 1.7417434453964233

steps: 2949975, episodes: 118000, mean episode reward: -595.3673599842953, agent episode reward: [-198.45578666143177, -198.45578666143177, -198.45578666143177], time: 74.468
steps: 2949975, episodes: 118000, mean episode variance: 33.0289362885952, agent episode variance: [0.9889243643283844, 30.887388290405273, 1.1526236338615417], time: 74.468
Running avgs for agent 0: q_loss: 7.3632097244262695, p_loss: -4.046538352966309, mean_rew: -7.85304082672013, variance: 3.955697774887085, lamda: 2.031177282333374
Running avgs for agent 1: q_loss: 1221442.375, p_loss: 144.14614868164062, mean_rew: -7.860899502963798, variance: 123.54955316162109, lamda: 3.9083878993988037
Running avgs for agent 2: q_loss: 6.4372663497924805, p_loss: -4.025079727172852, mean_rew: -7.858263693175612, variance: 4.610494136810303, lamda: 1.743451714515686

steps: 2974975, episodes: 119000, mean episode reward: -594.8613817074646, agent episode reward: [-198.28712723582157, -198.28712723582157, -198.28712723582157], time: 76.023
steps: 2974975, episodes: 119000, mean episode variance: 32.948023827552795, agent episode variance: [0.9887446339130401, 30.799721420288087, 1.1595577733516693], time: 76.024
Running avgs for agent 0: q_loss: 7.3892717361450195, p_loss: -4.054139614105225, mean_rew: -7.863806810936214, variance: 3.9549784660339355, lamda: 2.031254768371582
Running avgs for agent 1: q_loss: 1242327.25, p_loss: 144.61825561523438, mean_rew: -7.866074639186887, variance: 123.19888568115235, lamda: 3.9333622455596924
Running avgs for agent 2: q_loss: 6.463465690612793, p_loss: -4.0303144454956055, mean_rew: -7.869311144529296, variance: 4.638230800628662, lamda: 1.7434518337249756

steps: 2999975, episodes: 120000, mean episode reward: -597.9120193536698, agent episode reward: [-199.30400645122327, -199.30400645122327, -199.30400645122327], time: 77.22
steps: 2999975, episodes: 120000, mean episode variance: 33.7809624478817, agent episode variance: [0.9936499025821686, 31.630771881103517, 1.1565406641960143], time: 77.22
Running avgs for agent 0: q_loss: 7.480199337005615, p_loss: -4.054358005523682, mean_rew: -7.880841931839058, variance: 3.974599599838257, lamda: 2.0314416885375977
Running avgs for agent 1: q_loss: 1275530.25, p_loss: 145.02493286132812, mean_rew: -7.88085966247547, variance: 126.52308752441407, lamda: 3.958336353302002
Running avgs for agent 2: q_loss: 6.538214206695557, p_loss: -4.039762496948242, mean_rew: -7.87712747003309, variance: 4.626162528991699, lamda: 1.7434858083724976

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -595.5064279633744, agent episode reward: [-198.50214265445814, -198.50214265445814, -198.50214265445814], time: 55.722
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 55.723
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -603.4264532397091, agent episode reward: [-201.14215107990302, -201.14215107990302, -201.14215107990302], time: 66.896
steps: 49975, episodes: 2000, mean episode variance: 2.134413197040558, agent episode variance: [1.2256517639160156, 0.0, 0.9087614331245423], time: 66.897
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.976048862194002, variance: 5.023163318634033, lamda: 2.031493663787842
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.983080932301497, variance: 0.0, lamda: 3.970874309539795
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.972400621084532, variance: 3.7244319915771484, lamda: 1.7435263395309448

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567760855.1091589: line 9: --exp_var_alpha: command not found
