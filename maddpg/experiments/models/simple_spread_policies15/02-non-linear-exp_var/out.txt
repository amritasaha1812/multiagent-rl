# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 15.0 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies15/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies15/02-non-linear-exp_var/
Job <1092151> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc127>>
arglist.u_estimation True
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -534.4676409229834, agent episode reward: [-178.1558803076612, -178.1558803076612, -178.1558803076612], time: 144.054
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 144.054
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -596.2774472230493, agent episode reward: [-198.75914907434978, -198.75914907434978, -198.75914907434978], time: 172.288
steps: 49975, episodes: 2000, mean episode variance: 4.654365109443664, agent episode variance: [0.8498147847950459, 0.8217633771598339, 2.9827869474887847], time: 172.288
Running avgs for agent 0: q_loss: 310.0079040527344, p_loss: 11.277814865112305, mean_rew: -7.468764860130857, variance: 3.4828474786682206, lamda: 1.0118165016174316
Running avgs for agent 1: q_loss: 384.73583984375, p_loss: 12.145219802856445, mean_rew: -7.478684915335069, variance: 3.3678826932780077, lamda: 1.010532259941101
Running avgs for agent 2: q_loss: 114.35708618164062, p_loss: -5.3173627853393555, mean_rew: -7.47189010661975, variance: 12.224536670036004, lamda: 1.00948166847229

steps: 74975, episodes: 3000, mean episode reward: -533.1426041812163, agent episode reward: [-177.71420139373873, -177.71420139373873, -177.71420139373873], time: 165.647
steps: 74975, episodes: 3000, mean episode variance: 6.7585652538537975, agent episode variance: [1.9225978541374207, 2.943195648789406, 1.8927717509269715], time: 165.647
Running avgs for agent 0: q_loss: 2164.3818359375, p_loss: 25.646759033203125, mean_rew: -7.495935652338753, variance: 7.690391416549683, lamda: 1.0365086793899536
Running avgs for agent 1: q_loss: 3318.577880859375, p_loss: 26.464778900146484, mean_rew: -7.495295537920209, variance: 11.772782595157624, lamda: 1.0351879596710205
Running avgs for agent 2: q_loss: 18.639589309692383, p_loss: -4.172101974487305, mean_rew: -7.497974046203525, variance: 7.571086883544922, lamda: 1.0315510034561157

steps: 99975, episodes: 4000, mean episode reward: -495.6463033411249, agent episode reward: [-165.21543444704164, -165.21543444704164, -165.21543444704164], time: 169.5
steps: 99975, episodes: 4000, mean episode variance: 10.348027108669282, agent episode variance: [4.00278170967102, 4.594258929729461, 1.7509864692687989], time: 169.501
Running avgs for agent 0: q_loss: 7776.50927734375, p_loss: 38.311241149902344, mean_rew: -7.287780985757881, variance: 16.01112683868408, lamda: 1.06151282787323
Running avgs for agent 1: q_loss: 10485.087890625, p_loss: 39.127742767333984, mean_rew: -7.291277014959067, variance: 18.377035718917845, lamda: 1.0601922273635864
Running avgs for agent 2: q_loss: 13.33389663696289, p_loss: -3.7567005157470703, mean_rew: -7.298061631914759, variance: 7.003946304321289, lamda: 1.0557929277420044

steps: 124975, episodes: 5000, mean episode reward: -480.50727897340073, agent episode reward: [-160.16909299113357, -160.16909299113357, -160.16909299113357], time: 183.255
steps: 124975, episodes: 5000, mean episode variance: 14.154598920285702, agent episode variance: [6.353517462432385, 6.10229523396492, 1.6987862238883973], time: 183.256
Running avgs for agent 0: q_loss: 18186.0390625, p_loss: 48.7147331237793, mean_rew: -7.125903017214673, variance: 25.41406984972954, lamda: 1.086517095565796
Running avgs for agent 1: q_loss: 17874.640625, p_loss: 49.56707000732422, mean_rew: -7.133121954201965, variance: 24.40918093585968, lamda: 1.0851962566375732
Running avgs for agent 2: q_loss: 10.408744812011719, p_loss: -3.665541887283325, mean_rew: -7.130531147476567, variance: 6.795144557952881, lamda: 1.0777428150177002

steps: 149975, episodes: 6000, mean episode reward: -467.98713947046315, agent episode reward: [-155.99571315682107, -155.99571315682107, -155.99571315682107], time: 181.205
steps: 149975, episodes: 6000, mean episode variance: 17.0726893209219, agent episode variance: [7.432831703662872, 8.009367261290551, 1.6304903559684754], time: 181.206
Running avgs for agent 0: q_loss: 26340.423828125, p_loss: 57.5970344543457, mean_rew: -6.979637158392434, variance: 29.73132681465149, lamda: 1.1115211248397827
Running avgs for agent 1: q_loss: 30064.869140625, p_loss: 58.21979904174805, mean_rew: -6.97961448039302, variance: 32.037469045162204, lamda: 1.1102005243301392
Running avgs for agent 2: q_loss: 9.818937301635742, p_loss: -3.5989677906036377, mean_rew: -6.972434858190654, variance: 6.521961212158203, lamda: 1.0993082523345947

steps: 174975, episodes: 7000, mean episode reward: -467.61602161814204, agent episode reward: [-155.87200720604736, -155.87200720604736, -155.87200720604736], time: 167.955
steps: 174975, episodes: 7000, mean episode variance: 20.920688247442246, agent episode variance: [11.108085869073868, 8.250944498062134, 1.561657880306244], time: 167.956
Running avgs for agent 0: q_loss: 44741.42578125, p_loss: 64.90389251708984, mean_rew: -6.8671518826582885, variance: 44.43234347629547, lamda: 1.136525273323059
Running avgs for agent 1: q_loss: 44074.35546875, p_loss: 65.26219177246094, mean_rew: -6.868571332075848, variance: 33.00377799224854, lamda: 1.1352046728134155
Running avgs for agent 2: q_loss: 10.794285774230957, p_loss: -3.536156177520752, mean_rew: -6.863656041018543, variance: 6.246630668640137, lamda: 1.1219898462295532

steps: 199975, episodes: 8000, mean episode reward: -469.48158314914, agent episode reward: [-156.49386104971333, -156.49386104971333, -156.49386104971333], time: 168.263
steps: 199975, episodes: 8000, mean episode variance: 20.90589113354683, agent episode variance: [9.710321333169937, 9.675256462574005, 1.520313337802887], time: 168.263
Running avgs for agent 0: q_loss: 56235.45703125, p_loss: 70.12689971923828, mean_rew: -6.787807070215249, variance: 38.841285332679746, lamda: 1.161529541015625
Running avgs for agent 1: q_loss: 60976.76953125, p_loss: 70.236083984375, mean_rew: -6.781345815494228, variance: 38.70102585029602, lamda: 1.1602087020874023
Running avgs for agent 2: q_loss: 9.880354881286621, p_loss: -3.499795913696289, mean_rew: -6.780819360115698, variance: 6.081253528594971, lamda: 1.1441477537155151

steps: 224975, episodes: 9000, mean episode reward: -463.7637980265861, agent episode reward: [-154.5879326755287, -154.5879326755287, -154.5879326755287], time: 167.102
steps: 224975, episodes: 9000, mean episode variance: 35.405727762937545, agent episode variance: [11.939344369411469, 21.98724704837799, 1.4791363451480866], time: 167.103
Running avgs for agent 0: q_loss: 61045.2578125, p_loss: 74.35289001464844, mean_rew: -6.7222691398626555, variance: 47.757377477645875, lamda: 1.1865335702896118
Running avgs for agent 1: q_loss: 108884.40625, p_loss: 74.25517272949219, mean_rew: -6.71923069250622, variance: 87.94898819351197, lamda: 1.1852128505706787
Running avgs for agent 2: q_loss: 9.76742935180664, p_loss: -3.4499990940093994, mean_rew: -6.720602573899391, variance: 5.9165449142456055, lamda: 1.167276382446289

steps: 249975, episodes: 10000, mean episode reward: -461.19550090843575, agent episode reward: [-153.73183363614524, -153.73183363614524, -153.73183363614524], time: 169.923
steps: 249975, episodes: 10000, mean episode variance: 33.078111799955366, agent episode variance: [19.346489164590835, 12.29964078450203, 1.431981850862503], time: 169.924
Running avgs for agent 0: q_loss: 114142.578125, p_loss: 78.71562957763672, mean_rew: -6.658579846715044, variance: 77.38595665836334, lamda: 1.2115377187728882
Running avgs for agent 1: q_loss: 86297.7421875, p_loss: 78.64366149902344, mean_rew: -6.660024257354365, variance: 49.19856313800812, lamda: 1.2102171182632446
Running avgs for agent 2: q_loss: 9.033015251159668, p_loss: -3.4286153316497803, mean_rew: -6.661807793001589, variance: 5.727927207946777, lamda: 1.1872048377990723

steps: 274975, episodes: 11000, mean episode reward: -459.47201221627034, agent episode reward: [-153.1573374054234, -153.1573374054234, -153.1573374054234], time: 169.126
steps: 274975, episodes: 11000, mean episode variance: 49.26410102653504, agent episode variance: [26.942460689544678, 20.916105194568633, 1.4055351424217224], time: 169.126
Running avgs for agent 0: q_loss: 144429.65625, p_loss: 83.02159881591797, mean_rew: -6.608896668985908, variance: 107.76984275817871, lamda: 1.236541986465454
Running avgs for agent 1: q_loss: 135010.9375, p_loss: 83.12049865722656, mean_rew: -6.607212786507555, variance: 83.66442077827453, lamda: 1.2352211475372314
Running avgs for agent 2: q_loss: 8.032814979553223, p_loss: -3.4029643535614014, mean_rew: -6.609246372584072, variance: 5.622140407562256, lamda: 1.2064893245697021

steps: 299975, episodes: 12000, mean episode reward: -457.8080953820867, agent episode reward: [-152.60269846069556, -152.60269846069556, -152.60269846069556], time: 167.656
steps: 299975, episodes: 12000, mean episode variance: 48.64307152605057, agent episode variance: [18.959347005367277, 28.301029708862306, 1.3826948118209839], time: 167.657
Running avgs for agent 0: q_loss: 127824.9375, p_loss: 87.40814971923828, mean_rew: -6.5654823351671485, variance: 75.83738802146911, lamda: 1.261546015739441
Running avgs for agent 1: q_loss: 163117.546875, p_loss: 87.25543975830078, mean_rew: -6.565458968059263, variance: 113.20411883544922, lamda: 1.2602252960205078
Running avgs for agent 2: q_loss: 7.592741966247559, p_loss: -3.382904052734375, mean_rew: -6.56958050818734, variance: 5.5307793617248535, lamda: 1.2237311601638794

steps: 324975, episodes: 13000, mean episode reward: -461.4291665114131, agent episode reward: [-153.809722170471, -153.809722170471, -153.809722170471], time: 170.188
steps: 324975, episodes: 13000, mean episode variance: 55.939809130191804, agent episode variance: [23.733169040679932, 30.854848442077635, 1.3517916474342346], time: 170.189
Running avgs for agent 0: q_loss: 165986.171875, p_loss: 91.43579864501953, mean_rew: -6.529818715013922, variance: 94.93267616271973, lamda: 1.2865501642227173
Running avgs for agent 1: q_loss: 173384.453125, p_loss: 90.79066467285156, mean_rew: -6.531751346294365, variance: 123.41939376831054, lamda: 1.2852294445037842
Running avgs for agent 2: q_loss: 6.566298007965088, p_loss: -3.3445448875427246, mean_rew: -6.526594784256953, variance: 5.407166957855225, lamda: 1.2383266687393188

steps: 349975, episodes: 14000, mean episode reward: -456.2488600020346, agent episode reward: [-152.08295333401153, -152.08295333401153, -152.08295333401153], time: 169.204
steps: 349975, episodes: 14000, mean episode variance: 62.816162307262424, agent episode variance: [31.052692741394043, 30.42655047607422, 1.336919089794159], time: 169.205
Running avgs for agent 0: q_loss: 202198.34375, p_loss: 94.21513366699219, mean_rew: -6.5048203422668305, variance: 124.21077096557617, lamda: 1.3115544319152832
Running avgs for agent 1: q_loss: 194550.75, p_loss: 93.87464904785156, mean_rew: -6.50763163838067, variance: 121.70620190429688, lamda: 1.3102335929870605
Running avgs for agent 2: q_loss: 8.715036392211914, p_loss: -3.33872389793396, mean_rew: -6.502826580704779, variance: 5.3476762771606445, lamda: 1.2472786903381348

steps: 374975, episodes: 15000, mean episode reward: -454.1736334409853, agent episode reward: [-151.39121114699515, -151.39121114699515, -151.39121114699515], time: 168.315
steps: 374975, episodes: 15000, mean episode variance: 58.627089564085004, agent episode variance: [25.211869936943053, 32.09991235351563, 1.3153072736263276], time: 168.315
Running avgs for agent 0: q_loss: 203089.265625, p_loss: 96.7681884765625, mean_rew: -6.478204093112958, variance: 100.84747974777221, lamda: 1.33655846118927
Running avgs for agent 1: q_loss: 205190.453125, p_loss: 96.20291900634766, mean_rew: -6.474378536275012, variance: 128.3996494140625, lamda: 1.335237741470337
Running avgs for agent 2: q_loss: 7.0742878913879395, p_loss: -3.319925308227539, mean_rew: -6.47069042386144, variance: 5.261229038238525, lamda: 1.251294732093811

steps: 399975, episodes: 16000, mean episode reward: -455.2143496858455, agent episode reward: [-151.7381165619485, -151.7381165619485, -151.7381165619485], time: 166.437
steps: 399975, episodes: 16000, mean episode variance: 68.10266688656807, agent episode variance: [34.45811019897461, 32.3387234082222, 1.3058332793712617], time: 166.437
Running avgs for agent 0: q_loss: 233795.046875, p_loss: 98.26493072509766, mean_rew: -6.444200558782747, variance: 137.83244079589844, lamda: 1.3615626096725464
Running avgs for agent 1: q_loss: 234707.234375, p_loss: 98.0114974975586, mean_rew: -6.446430528494656, variance: 129.3548936328888, lamda: 1.3602418899536133
Running avgs for agent 2: q_loss: 6.216839790344238, p_loss: -3.2834038734436035, mean_rew: -6.449466517826277, variance: 5.22333288192749, lamda: 1.2637038230895996

steps: 424975, episodes: 17000, mean episode reward: -457.94997870330764, agent episode reward: [-152.64999290110254, -152.64999290110254, -152.64999290110254], time: 167.193
steps: 424975, episodes: 17000, mean episode variance: 59.40707882761955, agent episode variance: [29.86116889190674, 28.274840677261352, 1.2710692584514618], time: 167.194
Running avgs for agent 0: q_loss: 270325.65625, p_loss: 99.53959655761719, mean_rew: -6.4287532214300445, variance: 119.44467556762696, lamda: 1.3865668773651123
Running avgs for agent 1: q_loss: 215604.5625, p_loss: 99.54212188720703, mean_rew: -6.4195550319212344, variance: 113.09936270904541, lamda: 1.3852460384368896
Running avgs for agent 2: q_loss: 6.711073398590088, p_loss: -3.2913429737091064, mean_rew: -6.427579996222212, variance: 5.0842766761779785, lamda: 1.2769279479980469

steps: 449975, episodes: 18000, mean episode reward: -457.5124200225508, agent episode reward: [-152.50414000751695, -152.50414000751695, -152.50414000751695], time: 170.175
steps: 449975, episodes: 18000, mean episode variance: 43.48771038126945, agent episode variance: [24.686292669296265, 17.523009875297547, 1.278407836675644], time: 170.176
Running avgs for agent 0: q_loss: 241203.46875, p_loss: 101.06068420410156, mean_rew: -6.409422794444079, variance: 98.74517067718506, lamda: 1.4115709066390991
Running avgs for agent 1: q_loss: 159298.5625, p_loss: 101.23213958740234, mean_rew: -6.401694907190058, variance: 70.09203950119019, lamda: 1.410250186920166
Running avgs for agent 2: q_loss: 6.60173225402832, p_loss: -3.2773265838623047, mean_rew: -6.406831828279374, variance: 5.113630771636963, lamda: 1.2871646881103516

steps: 474975, episodes: 19000, mean episode reward: -458.8339509677217, agent episode reward: [-152.9446503225739, -152.9446503225739, -152.9446503225739], time: 163.854
steps: 474975, episodes: 19000, mean episode variance: 53.16353939557076, agent episode variance: [36.72418941497803, 15.170044306278228, 1.2693056743144988], time: 163.855
Running avgs for agent 0: q_loss: 284629.0625, p_loss: 102.23698425292969, mean_rew: -6.388587209875724, variance: 146.8967576599121, lamda: 1.4365750551223755
Running avgs for agent 1: q_loss: 153205.53125, p_loss: 102.7430191040039, mean_rew: -6.390652288139364, variance: 60.680177225112914, lamda: 1.4352545738220215
Running avgs for agent 2: q_loss: 6.08042573928833, p_loss: -3.277982711791992, mean_rew: -6.393328264916016, variance: 5.07722282409668, lamda: 1.2952345609664917

steps: 499975, episodes: 20000, mean episode reward: -460.543134630929, agent episode reward: [-153.51437821030967, -153.51437821030967, -153.51437821030967], time: 168.663
steps: 499975, episodes: 20000, mean episode variance: 71.3497009549141, agent episode variance: [35.08834661102295, 35.025360036849975, 1.2359943070411683], time: 168.664
Running avgs for agent 0: q_loss: 291250.8125, p_loss: 103.50934600830078, mean_rew: -6.374940390350582, variance: 140.3533864440918, lamda: 1.4615792036056519
Running avgs for agent 1: q_loss: 293587.5, p_loss: 104.11787414550781, mean_rew: -6.372583337433527, variance: 140.1014401473999, lamda: 1.4602586030960083
Running avgs for agent 2: q_loss: 6.463683605194092, p_loss: -3.2520556449890137, mean_rew: -6.37740702682166, variance: 4.943977355957031, lamda: 1.306938648223877

steps: 524975, episodes: 21000, mean episode reward: -456.40991931924333, agent episode reward: [-152.13663977308113, -152.13663977308113, -152.13663977308113], time: 170.73
steps: 524975, episodes: 21000, mean episode variance: 66.9340034160614, agent episode variance: [31.779939960956572, 33.90945615386963, 1.244607301235199], time: 170.731
Running avgs for agent 0: q_loss: 295483.0, p_loss: 104.76838684082031, mean_rew: -6.365917735763466, variance: 127.11975984382629, lamda: 1.4865833520889282
Running avgs for agent 1: q_loss: 284076.6875, p_loss: 105.33273315429688, mean_rew: -6.370454394973993, variance: 135.6378246154785, lamda: 1.4852627515792847
Running avgs for agent 2: q_loss: 6.185009002685547, p_loss: -3.2592499256134033, mean_rew: -6.364343126142896, variance: 4.978428840637207, lamda: 1.3108315467834473

steps: 549975, episodes: 22000, mean episode reward: -454.23703457092876, agent episode reward: [-151.41234485697623, -151.41234485697623, -151.41234485697623], time: 168.149
steps: 549975, episodes: 22000, mean episode variance: 60.06318235707283, agent episode variance: [30.548145146369933, 28.283312480926515, 1.2317247297763825], time: 168.149
Running avgs for agent 0: q_loss: 304944.6875, p_loss: 105.86817169189453, mean_rew: -6.353399208126284, variance: 122.19258058547973, lamda: 1.5115875005722046
Running avgs for agent 1: q_loss: 271318.75, p_loss: 106.43061065673828, mean_rew: -6.3549258872170675, variance: 113.13324992370606, lamda: 1.510266900062561
Running avgs for agent 2: q_loss: 6.783452033996582, p_loss: -3.259294271469116, mean_rew: -6.349071208659039, variance: 4.92689847946167, lamda: 1.3149263858795166

steps: 574975, episodes: 23000, mean episode reward: -450.4295365698768, agent episode reward: [-150.1431788566256, -150.1431788566256, -150.1431788566256], time: 167.573
steps: 574975, episodes: 23000, mean episode variance: 54.852303449869154, agent episode variance: [36.48748134613037, 17.12581747341156, 1.2390046303272246], time: 167.574
Running avgs for agent 0: q_loss: 337477.6875, p_loss: 106.71275329589844, mean_rew: -6.338565790330609, variance: 145.9499253845215, lamda: 1.5365917682647705
Running avgs for agent 1: q_loss: 165579.328125, p_loss: 107.28427124023438, mean_rew: -6.340437752196001, variance: 68.50326989364623, lamda: 1.5352710485458374
Running avgs for agent 2: q_loss: 7.0833516120910645, p_loss: -3.2460267543792725, mean_rew: -6.339408157496411, variance: 4.956018447875977, lamda: 1.3186768293380737

steps: 599975, episodes: 24000, mean episode reward: -456.34423742940606, agent episode reward: [-152.11474580980203, -152.11474580980203, -152.11474580980203], time: 168.174
steps: 599975, episodes: 24000, mean episode variance: 69.6223536221981, agent episode variance: [35.224270797729496, 33.171907962799075, 1.2261748616695405], time: 168.175
Running avgs for agent 0: q_loss: 329541.9375, p_loss: 107.52095794677734, mean_rew: -6.33203684952442, variance: 140.89708319091798, lamda: 1.5615957975387573
Running avgs for agent 1: q_loss: 323148.09375, p_loss: 107.81745147705078, mean_rew: -6.332899340543674, variance: 132.6876318511963, lamda: 1.5602751970291138
Running avgs for agent 2: q_loss: 6.788238048553467, p_loss: -3.2491698265075684, mean_rew: -6.318300815353319, variance: 4.904699325561523, lamda: 1.3200803995132446

steps: 624975, episodes: 25000, mean episode reward: -456.73578117243295, agent episode reward: [-152.245260390811, -152.245260390811, -152.245260390811], time: 164.402
steps: 624975, episodes: 25000, mean episode variance: 72.6927826461792, agent episode variance: [36.36790898132324, 35.106894248962405, 1.2179794158935546], time: 164.402
Running avgs for agent 0: q_loss: 359408.0, p_loss: 108.1899185180664, mean_rew: -6.32102816297808, variance: 145.47163592529296, lamda: 1.5865998268127441
Running avgs for agent 1: q_loss: 314309.25, p_loss: 108.39128112792969, mean_rew: -6.3157390085567835, variance: 140.42757699584962, lamda: 1.5852793455123901
Running avgs for agent 2: q_loss: 5.159512996673584, p_loss: -3.242554187774658, mean_rew: -6.315752025300747, variance: 4.871918201446533, lamda: 1.3239567279815674

steps: 649975, episodes: 26000, mean episode reward: -455.39771164064956, agent episode reward: [-151.79923721354984, -151.79923721354984, -151.79923721354984], time: 164.5
steps: 649975, episodes: 26000, mean episode variance: 70.42318466901779, agent episode variance: [34.92235386657715, 34.280157836914064, 1.2206729655265809], time: 164.501
Running avgs for agent 0: q_loss: 360420.8125, p_loss: 109.02178955078125, mean_rew: -6.307633032707519, variance: 139.6894154663086, lamda: 1.61160409450531
Running avgs for agent 1: q_loss: 335345.03125, p_loss: 109.12459564208984, mean_rew: -6.30914640861727, variance: 137.12063134765626, lamda: 1.6102834939956665
Running avgs for agent 2: q_loss: 5.639382839202881, p_loss: -3.228656053543091, mean_rew: -6.307145433930996, variance: 4.882691860198975, lamda: 1.3322875499725342

steps: 674975, episodes: 27000, mean episode reward: -461.2546722198068, agent episode reward: [-153.75155740660225, -153.75155740660225, -153.75155740660225], time: 165.236
steps: 674975, episodes: 27000, mean episode variance: 67.78167730522156, agent episode variance: [33.40866046142578, 33.15485933685303, 1.218157506942749], time: 165.237
Running avgs for agent 0: q_loss: 358578.125, p_loss: 109.77999114990234, mean_rew: -6.301970990911667, variance: 133.63464184570313, lamda: 1.636608362197876
Running avgs for agent 1: q_loss: 334554.6875, p_loss: 109.87557983398438, mean_rew: -6.306528359473421, variance: 132.61943734741212, lamda: 1.6352876424789429
Running avgs for agent 2: q_loss: 5.798498153686523, p_loss: -3.2318906784057617, mean_rew: -6.3032482489423485, variance: 4.8726301193237305, lamda: 1.333825945854187

steps: 699975, episodes: 28000, mean episode reward: -459.90417391503104, agent episode reward: [-153.30139130501036, -153.30139130501036, -153.30139130501036], time: 163.479
steps: 699975, episodes: 28000, mean episode variance: 58.33937319040299, agent episode variance: [23.00112673664093, 34.14411859130859, 1.1941278624534606], time: 163.48
Running avgs for agent 0: q_loss: 236041.15625, p_loss: 110.504638671875, mean_rew: -6.292876691898015, variance: 92.00450694656372, lamda: 1.6616123914718628
Running avgs for agent 1: q_loss: 339947.65625, p_loss: 110.6035385131836, mean_rew: -6.298289240201484, variance: 136.57647436523436, lamda: 1.6602917909622192
Running avgs for agent 2: q_loss: 5.454753875732422, p_loss: -3.2429888248443604, mean_rew: -6.295732903734619, variance: 4.776511192321777, lamda: 1.3428682088851929

steps: 724975, episodes: 29000, mean episode reward: -465.3685314789257, agent episode reward: [-155.12284382630855, -155.12284382630855, -155.12284382630855], time: 167.02
steps: 724975, episodes: 29000, mean episode variance: 68.80588011479378, agent episode variance: [34.20341258239746, 33.40603686523438, 1.1964306671619416], time: 167.02
Running avgs for agent 0: q_loss: 382633.03125, p_loss: 111.26881408691406, mean_rew: -6.298584913186777, variance: 136.81365032958985, lamda: 1.6866165399551392
Running avgs for agent 1: q_loss: 343775.875, p_loss: 111.34333801269531, mean_rew: -6.28861568731007, variance: 133.6241474609375, lamda: 1.6852959394454956
Running avgs for agent 2: q_loss: 5.6251702308654785, p_loss: -3.2136781215667725, mean_rew: -6.291266605752581, variance: 4.785722732543945, lamda: 1.350123405456543

steps: 749975, episodes: 30000, mean episode reward: -466.52306498572244, agent episode reward: [-155.50768832857415, -155.50768832857415, -155.50768832857415], time: 172.703
steps: 749975, episodes: 30000, mean episode variance: 67.9720635752678, agent episode variance: [34.22409980773926, 32.565887817382816, 1.1820759501457214], time: 172.703
Running avgs for agent 0: q_loss: 393375.15625, p_loss: 111.84612274169922, mean_rew: -6.289466199876926, variance: 136.89639923095703, lamda: 1.711620807647705
Running avgs for agent 1: q_loss: 358887.9375, p_loss: 112.02934265136719, mean_rew: -6.288413932039954, variance: 130.26355126953126, lamda: 1.710300087928772
Running avgs for agent 2: q_loss: 5.869246959686279, p_loss: -3.2319109439849854, mean_rew: -6.283678200642159, variance: 4.728303909301758, lamda: 1.352245569229126

steps: 774975, episodes: 31000, mean episode reward: -471.5459150820125, agent episode reward: [-157.1819716940042, -157.1819716940042, -157.1819716940042], time: 164.152
steps: 774975, episodes: 31000, mean episode variance: 69.16808298110962, agent episode variance: [34.665102684020994, 33.309908515930175, 1.1930717811584473], time: 164.152
Running avgs for agent 0: q_loss: 389336.71875, p_loss: 112.46412658691406, mean_rew: -6.290382603859173, variance: 138.66041073608397, lamda: 1.7366247177124023
Running avgs for agent 1: q_loss: 357965.5, p_loss: 112.5771713256836, mean_rew: -6.28242978226766, variance: 133.2396340637207, lamda: 1.7353042364120483
Running avgs for agent 2: q_loss: 5.751579284667969, p_loss: -3.2313132286071777, mean_rew: -6.289545962543392, variance: 4.772286891937256, lamda: 1.352752447128296

steps: 799975, episodes: 32000, mean episode reward: -467.67297799594166, agent episode reward: [-155.8909926653139, -155.8909926653139, -155.8909926653139], time: 161.125
steps: 799975, episodes: 32000, mean episode variance: 67.35759616971016, agent episode variance: [33.058724716186525, 33.11193304443359, 1.1869384090900421], time: 161.126
Running avgs for agent 0: q_loss: 407010.25, p_loss: 113.17057800292969, mean_rew: -6.285608972342828, variance: 132.2348988647461, lamda: 1.7616289854049683
Running avgs for agent 1: q_loss: 371100.4375, p_loss: 113.20145416259766, mean_rew: -6.2845139235795235, variance: 132.44773217773437, lamda: 1.7603083848953247
Running avgs for agent 2: q_loss: 9.034806251525879, p_loss: -3.231422185897827, mean_rew: -6.284869368180345, variance: 4.747754096984863, lamda: 1.3537126779556274

steps: 824975, episodes: 33000, mean episode reward: -467.3151588107302, agent episode reward: [-155.77171960357674, -155.77171960357674, -155.77171960357674], time: 164.394
steps: 824975, episodes: 33000, mean episode variance: 67.2763332517147, agent episode variance: [33.692765037536624, 32.39626459503174, 1.187303619146347], time: 164.395
Running avgs for agent 0: q_loss: 411840.15625, p_loss: 113.6389389038086, mean_rew: -6.286104383014392, variance: 134.7710601501465, lamda: 1.7866332530975342
Running avgs for agent 1: q_loss: 377345.21875, p_loss: 113.7426528930664, mean_rew: -6.282122826895858, variance: 129.58505838012695, lamda: 1.785312533378601
Running avgs for agent 2: q_loss: 7.286210536956787, p_loss: -3.2244350910186768, mean_rew: -6.285384264913945, variance: 4.7492146492004395, lamda: 1.3543574810028076

steps: 849975, episodes: 34000, mean episode reward: -464.037000111234, agent episode reward: [-154.67900003707803, -154.67900003707803, -154.67900003707803], time: 165.416
steps: 849975, episodes: 34000, mean episode variance: 68.12053595471382, agent episode variance: [34.2235022277832, 32.70270798492432, 1.1943257420063018], time: 165.416
Running avgs for agent 0: q_loss: 409524.34375, p_loss: 114.12556457519531, mean_rew: -6.284094530588946, variance: 136.8940089111328, lamda: 1.811637282371521
Running avgs for agent 1: q_loss: 376549.0625, p_loss: 114.38079833984375, mean_rew: -6.2883200017916066, variance: 130.81083193969727, lamda: 1.8103166818618774
Running avgs for agent 2: q_loss: 4.243815898895264, p_loss: -3.2199931144714355, mean_rew: -6.281574497293321, variance: 4.7773027420043945, lamda: 1.356252670288086

steps: 874975, episodes: 35000, mean episode reward: -466.07615154213846, agent episode reward: [-155.35871718071286, -155.35871718071286, -155.35871718071286], time: 168.096
steps: 874975, episodes: 35000, mean episode variance: 67.09958663654328, agent episode variance: [33.858785003662106, 32.06344580841064, 1.17735582447052], time: 168.097
Running avgs for agent 0: q_loss: 420274.46875, p_loss: 114.59871673583984, mean_rew: -6.281213408035202, variance: 135.43514001464843, lamda: 1.8366414308547974
Running avgs for agent 1: q_loss: 379163.3125, p_loss: 114.79328155517578, mean_rew: -6.274783225422805, variance: 128.25378323364257, lamda: 1.8353208303451538
Running avgs for agent 2: q_loss: 4.750331878662109, p_loss: -3.2286033630371094, mean_rew: -6.275086261157749, variance: 4.709423065185547, lamda: 1.3638899326324463

steps: 899975, episodes: 36000, mean episode reward: -462.9326525780283, agent episode reward: [-154.3108841926761, -154.3108841926761, -154.3108841926761], time: 164.715
steps: 899975, episodes: 36000, mean episode variance: 67.25770217037201, agent episode variance: [33.6020458984375, 32.487343292236325, 1.168312979698181], time: 164.716
Running avgs for agent 0: q_loss: 425682.625, p_loss: 115.0053939819336, mean_rew: -6.27749978042578, variance: 134.40818359375, lamda: 1.8616456985473633
Running avgs for agent 1: q_loss: 384763.1875, p_loss: 115.30607604980469, mean_rew: -6.277633535726098, variance: 129.9493731689453, lamda: 1.8603249788284302
Running avgs for agent 2: q_loss: 4.963316917419434, p_loss: -3.2261319160461426, mean_rew: -6.280000911940245, variance: 4.673252105712891, lamda: 1.36671781539917

steps: 924975, episodes: 37000, mean episode reward: -464.2449720265785, agent episode reward: [-154.74832400885953, -154.74832400885953, -154.74832400885953], time: 163.214
steps: 924975, episodes: 37000, mean episode variance: 67.45983745026588, agent episode variance: [33.89278424835205, 32.39067282104492, 1.1763803808689117], time: 163.214
Running avgs for agent 0: q_loss: 432108.53125, p_loss: 115.31393432617188, mean_rew: -6.275071011304402, variance: 135.5711369934082, lamda: 1.8866496086120605
Running avgs for agent 1: q_loss: 397677.125, p_loss: 115.7256851196289, mean_rew: -6.278846051297051, variance: 129.5626912841797, lamda: 1.8853291273117065
Running avgs for agent 2: q_loss: 4.712658405303955, p_loss: -3.2197675704956055, mean_rew: -6.278037506340413, variance: 4.705522060394287, lamda: 1.3683804273605347

steps: 949975, episodes: 38000, mean episode reward: -458.74150179718737, agent episode reward: [-152.9138339323958, -152.9138339323958, -152.9138339323958], time: 163.758
steps: 949975, episodes: 38000, mean episode variance: 66.85623937368393, agent episode variance: [33.23786328125, 32.439916091918946, 1.1784600005149841], time: 163.759
Running avgs for agent 0: q_loss: 431443.5625, p_loss: 115.51715850830078, mean_rew: -6.272102945379309, variance: 132.951453125, lamda: 1.9116538763046265
Running avgs for agent 1: q_loss: 393558.25, p_loss: 115.88251495361328, mean_rew: -6.270884786089147, variance: 129.75966436767578, lamda: 1.910333275794983
Running avgs for agent 2: q_loss: 4.535982608795166, p_loss: -3.2109456062316895, mean_rew: -6.277624641851206, variance: 4.713840007781982, lamda: 1.3745468854904175

steps: 974975, episodes: 39000, mean episode reward: -463.76649578194355, agent episode reward: [-154.58883192731452, -154.58883192731452, -154.58883192731452], time: 163.719
steps: 974975, episodes: 39000, mean episode variance: 66.28490025830268, agent episode variance: [32.96251201629639, 32.15866325378418, 1.1637249882221221], time: 163.72
Running avgs for agent 0: q_loss: 439672.96875, p_loss: 115.69188690185547, mean_rew: -6.271485794190994, variance: 131.85004806518555, lamda: 1.9366579055786133
Running avgs for agent 1: q_loss: 406799.5, p_loss: 116.1532211303711, mean_rew: -6.270896817302242, variance: 128.6346530151367, lamda: 1.9353374242782593
Running avgs for agent 2: q_loss: 4.671472549438477, p_loss: -3.2153360843658447, mean_rew: -6.269692073215588, variance: 4.654900074005127, lamda: 1.3823493719100952

steps: 999975, episodes: 40000, mean episode reward: -469.10155250402795, agent episode reward: [-156.36718416800932, -156.36718416800932, -156.36718416800932], time: 167.26
steps: 999975, episodes: 40000, mean episode variance: 66.72569023132324, agent episode variance: [33.31530961608887, 32.25514692687988, 1.1552336883544922], time: 167.26
Running avgs for agent 0: q_loss: 440302.59375, p_loss: 115.86396789550781, mean_rew: -6.269454863100856, variance: 133.26123846435547, lamda: 1.9616624116897583
Running avgs for agent 1: q_loss: 414902.53125, p_loss: 116.35184478759766, mean_rew: -6.269899760515953, variance: 129.02058770751952, lamda: 1.9603415727615356
Running avgs for agent 2: q_loss: 4.539611339569092, p_loss: -3.2309229373931885, mean_rew: -6.267027017905226, variance: 4.62093448638916, lamda: 1.3833953142166138

steps: 1024975, episodes: 41000, mean episode reward: -464.12980762340686, agent episode reward: [-154.70993587446898, -154.70993587446898, -154.70993587446898], time: 167.679
steps: 1024975, episodes: 41000, mean episode variance: 66.02043225359917, agent episode variance: [32.96450022125244, 31.899508949279785, 1.1564230830669404], time: 167.679
Running avgs for agent 0: q_loss: 441338.625, p_loss: 115.8758773803711, mean_rew: -6.253082058140845, variance: 131.85800088500977, lamda: 1.9866664409637451
Running avgs for agent 1: q_loss: 408109.03125, p_loss: 116.34744262695312, mean_rew: -6.258851001130962, variance: 127.59803579711914, lamda: 1.985345721244812
Running avgs for agent 2: q_loss: 4.57015323638916, p_loss: -3.2193846702575684, mean_rew: -6.255312469468372, variance: 4.625692367553711, lamda: 1.3836445808410645

steps: 1049975, episodes: 42000, mean episode reward: -466.6522398419493, agent episode reward: [-155.55074661398314, -155.55074661398314, -155.55074661398314], time: 166.039
steps: 1049975, episodes: 42000, mean episode variance: 65.04825484752655, agent episode variance: [32.4026997833252, 31.49400252532959, 1.1515525388717651], time: 166.039
Running avgs for agent 0: q_loss: 427897.34375, p_loss: 115.78501892089844, mean_rew: -6.221781294883672, variance: 129.6107991333008, lamda: 2.0116565227508545
Running avgs for agent 1: q_loss: 397684.46875, p_loss: 116.21043395996094, mean_rew: -6.221170933450768, variance: 125.97601010131837, lamda: 2.0103375911712646
Running avgs for agent 2: q_loss: 4.043814182281494, p_loss: -3.203725576400757, mean_rew: -6.216937981802242, variance: 4.606209754943848, lamda: 1.3841054439544678

steps: 1074975, episodes: 43000, mean episode reward: -472.0010977357508, agent episode reward: [-157.33369924525027, -157.33369924525027, -157.33369924525027], time: 168.262
steps: 1074975, episodes: 43000, mean episode variance: 65.17478254795074, agent episode variance: [32.55457769012451, 31.481066291809082, 1.139138566017151], time: 168.262
Running avgs for agent 0: q_loss: 416392.375, p_loss: 115.69669342041016, mean_rew: -6.1903806993558295, variance: 130.21831076049804, lamda: 2.036630868911743
Running avgs for agent 1: q_loss: 383371.75, p_loss: 115.9858169555664, mean_rew: -6.190689988279392, variance: 125.92426516723633, lamda: 2.0353119373321533
Running avgs for agent 2: q_loss: 3.7121164798736572, p_loss: -3.1928513050079346, mean_rew: -6.190866553367954, variance: 4.556554317474365, lamda: 1.3842030763626099

steps: 1099975, episodes: 44000, mean episode reward: -479.8546035754116, agent episode reward: [-159.95153452513722, -159.95153452513722, -159.95153452513722], time: 168.599
steps: 1099975, episodes: 44000, mean episode variance: 64.24611477184295, agent episode variance: [31.79356007385254, 31.312003646850584, 1.1405510511398316], time: 168.599
Running avgs for agent 0: q_loss: 414398.96875, p_loss: 115.70382690429688, mean_rew: -6.178889455878441, variance: 127.17424029541016, lamda: 2.061605215072632
Running avgs for agent 1: q_loss: 387434.78125, p_loss: 116.05306243896484, mean_rew: -6.180346546948167, variance: 125.24801458740234, lamda: 2.060286045074463
Running avgs for agent 2: q_loss: 3.6673524379730225, p_loss: -3.188453197479248, mean_rew: -6.177262627183399, variance: 4.562203884124756, lamda: 1.3842028379440308

steps: 1124975, episodes: 45000, mean episode reward: -480.8149649658703, agent episode reward: [-160.2716549886234, -160.2716549886234, -160.2716549886234], time: 166.094
steps: 1124975, episodes: 45000, mean episode variance: 63.49303620839119, agent episode variance: [31.57114730834961, 30.782206451416016, 1.1396824486255646], time: 166.095
Running avgs for agent 0: q_loss: 415133.28125, p_loss: 115.78588104248047, mean_rew: -6.174153239200889, variance: 126.28458923339844, lamda: 2.0865795612335205
Running avgs for agent 1: q_loss: 386269.875, p_loss: 115.95631408691406, mean_rew: -6.167283736484167, variance: 123.12882580566406, lamda: 2.0852606296539307
Running avgs for agent 2: q_loss: 3.723759412765503, p_loss: -3.188131332397461, mean_rew: -6.177221643793807, variance: 4.558729648590088, lamda: 1.3842030763626099

steps: 1149975, episodes: 46000, mean episode reward: -484.6744290472119, agent episode reward: [-161.5581430157373, -161.5581430157373, -161.5581430157373], time: 170.308
steps: 1149975, episodes: 46000, mean episode variance: 64.29838469696045, agent episode variance: [31.98336971282959, 31.180113639831543, 1.1349013442993163], time: 170.309
Running avgs for agent 0: q_loss: 419592.1875, p_loss: 115.90286254882812, mean_rew: -6.178505679720491, variance: 127.93347885131836, lamda: 2.11155366897583
Running avgs for agent 1: q_loss: 387397.34375, p_loss: 116.04010009765625, mean_rew: -6.173197842165119, variance: 124.72045455932617, lamda: 2.1102349758148193
Running avgs for agent 2: q_loss: 3.617762565612793, p_loss: -3.1940248012542725, mean_rew: -6.178323189066532, variance: 4.539605617523193, lamda: 1.3842030763626099

steps: 1174975, episodes: 47000, mean episode reward: -480.0740747145753, agent episode reward: [-160.0246915715251, -160.0246915715251, -160.0246915715251], time: 172.796
steps: 1174975, episodes: 47000, mean episode variance: 62.61293062281609, agent episode variance: [30.664475524902343, 30.803398178100586, 1.1450569198131562], time: 172.796
Running avgs for agent 0: q_loss: 422144.375, p_loss: 115.97499084472656, mean_rew: -6.183943468505603, variance: 122.65790209960937, lamda: 2.136528253555298
Running avgs for agent 1: q_loss: 397497.46875, p_loss: 115.99728393554688, mean_rew: -6.1853730474612405, variance: 123.21359271240235, lamda: 2.135209321975708
Running avgs for agent 2: q_loss: 3.705719232559204, p_loss: -3.196380376815796, mean_rew: -6.184773567811745, variance: 4.580227375030518, lamda: 1.3842029571533203

steps: 1199975, episodes: 48000, mean episode reward: -490.63223563558074, agent episode reward: [-163.54407854519354, -163.54407854519354, -163.54407854519354], time: 178.856
steps: 1199975, episodes: 48000, mean episode variance: 62.04158230829239, agent episode variance: [30.86678269958496, 30.030142959594727, 1.1446566491127015], time: 178.857
Running avgs for agent 0: q_loss: 420153.125, p_loss: 116.09160614013672, mean_rew: -6.188668941871177, variance: 123.46713079833984, lamda: 2.1615025997161865
Running avgs for agent 1: q_loss: 396791.875, p_loss: 116.00086975097656, mean_rew: -6.184977771136497, variance: 120.12057183837891, lamda: 2.1601836681365967
Running avgs for agent 2: q_loss: 3.70870304107666, p_loss: -3.1974151134490967, mean_rew: -6.192419618523255, variance: 4.57862663269043, lamda: 1.3842030763626099

steps: 1224975, episodes: 49000, mean episode reward: -492.9900858565282, agent episode reward: [-164.33002861884273, -164.33002861884273, -164.33002861884273], time: 180.588
steps: 1224975, episodes: 49000, mean episode variance: 62.38023868227005, agent episode variance: [30.667276023864748, 30.567726409912108, 1.1452362484931946], time: 180.589
Running avgs for agent 0: q_loss: 421608.3125, p_loss: 116.15574645996094, mean_rew: -6.197179863600646, variance: 122.66910409545899, lamda: 2.186476945877075
Running avgs for agent 1: q_loss: 399883.71875, p_loss: 116.0678939819336, mean_rew: -6.197724415428111, variance: 122.27090563964843, lamda: 2.1851580142974854
Running avgs for agent 2: q_loss: 3.7450859546661377, p_loss: -3.199953079223633, mean_rew: -6.1978853713202335, variance: 4.5809454917907715, lamda: 1.3842031955718994

steps: 1249975, episodes: 50000, mean episode reward: -495.94469037684223, agent episode reward: [-165.31489679228073, -165.31489679228073, -165.31489679228073], time: 169.541
steps: 1249975, episodes: 50000, mean episode variance: 61.462063424348834, agent episode variance: [30.24011498260498, 30.07305743408203, 1.1488910076618195], time: 169.541
Running avgs for agent 0: q_loss: 426177.0, p_loss: 116.3341064453125, mean_rew: -6.214046372478802, variance: 120.96045993041992, lamda: 2.211451530456543
Running avgs for agent 1: q_loss: 405296.25, p_loss: 116.17914581298828, mean_rew: -6.212882580949632, variance: 120.29222973632812, lamda: 2.210132360458374
Running avgs for agent 2: q_loss: 3.746556043624878, p_loss: -3.2038705348968506, mean_rew: -6.2082049139329945, variance: 4.595564365386963, lamda: 1.3842030763626099

steps: 1274975, episodes: 51000, mean episode reward: -491.40126812124464, agent episode reward: [-163.80042270708154, -163.80042270708154, -163.80042270708154], time: 164.951
steps: 1274975, episodes: 51000, mean episode variance: 60.81636317014694, agent episode variance: [30.09023109436035, 29.57636750793457, 1.1497645678520203], time: 164.952
Running avgs for agent 0: q_loss: 428785.03125, p_loss: 116.40840911865234, mean_rew: -6.220754791050023, variance: 120.3609243774414, lamda: 2.2364258766174316
Running avgs for agent 1: q_loss: 415945.4375, p_loss: 116.41458129882812, mean_rew: -6.226790522464268, variance: 118.30547003173828, lamda: 2.2351067066192627
Running avgs for agent 2: q_loss: 3.7873759269714355, p_loss: -3.2066688537597656, mean_rew: -6.2176361283958705, variance: 4.599058628082275, lamda: 1.3842030763626099

steps: 1299975, episodes: 52000, mean episode reward: -489.0808513136235, agent episode reward: [-163.02695043787452, -163.02695043787452, -163.02695043787452], time: 168.285
steps: 1299975, episodes: 52000, mean episode variance: 62.653832951307294, agent episode variance: [30.41125718688965, 31.09350553894043, 1.1490702254772187], time: 168.285
Running avgs for agent 0: q_loss: 433227.625, p_loss: 116.36573028564453, mean_rew: -6.228117584705524, variance: 121.6450287475586, lamda: 2.261399984359741
Running avgs for agent 1: q_loss: 410479.4375, p_loss: 116.20050811767578, mean_rew: -6.233634802806048, variance: 124.37402215576172, lamda: 2.2600810527801514
Running avgs for agent 2: q_loss: 3.762925863265991, p_loss: -3.211318254470825, mean_rew: -6.2265043832658105, variance: 4.596281051635742, lamda: 1.3842031955718994

steps: 1324975, episodes: 53000, mean episode reward: -494.3088798363851, agent episode reward: [-164.76962661212838, -164.76962661212838, -164.76962661212838], time: 168.962
steps: 1324975, episodes: 53000, mean episode variance: 60.549576792240146, agent episode variance: [29.927697471618654, 29.465084213256837, 1.1567951073646545], time: 168.963
Running avgs for agent 0: q_loss: 427530.46875, p_loss: 116.24317169189453, mean_rew: -6.236905862565764, variance: 119.71078988647461, lamda: 2.286374568939209
Running avgs for agent 1: q_loss: 419282.3125, p_loss: 116.16984558105469, mean_rew: -6.241947671092254, variance: 117.86033685302735, lamda: 2.285055160522461
Running avgs for agent 2: q_loss: 3.819350242614746, p_loss: -3.2179558277130127, mean_rew: -6.24205231726843, variance: 4.627180576324463, lamda: 1.3842030763626099

steps: 1349975, episodes: 54000, mean episode reward: -491.49689711279257, agent episode reward: [-163.83229903759755, -163.83229903759755, -163.83229903759755], time: 175.257
steps: 1349975, episodes: 54000, mean episode variance: 61.08546010780334, agent episode variance: [30.639936462402343, 29.29195803833008, 1.1535656070709228], time: 175.258
Running avgs for agent 0: q_loss: 431770.3125, p_loss: 116.09959411621094, mean_rew: -6.256190909230108, variance: 122.55974584960937, lamda: 2.3113486766815186
Running avgs for agent 1: q_loss: 423512.96875, p_loss: 116.16109466552734, mean_rew: -6.249729358847867, variance: 117.16783215332032, lamda: 2.3100297451019287
Running avgs for agent 2: q_loss: 3.8344194889068604, p_loss: -3.2206640243530273, mean_rew: -6.255181760000294, variance: 4.614262104034424, lamda: 1.3842030763626099

steps: 1374975, episodes: 55000, mean episode reward: -483.56020382027816, agent episode reward: [-161.1867346067594, -161.1867346067594, -161.1867346067594], time: 177.499
steps: 1374975, episodes: 55000, mean episode variance: 62.49516464281082, agent episode variance: [31.012986877441406, 30.323511642456054, 1.1586661229133606], time: 177.499
Running avgs for agent 0: q_loss: 432890.6875, p_loss: 116.0174560546875, mean_rew: -6.267002786270914, variance: 124.05194750976563, lamda: 2.3363232612609863
Running avgs for agent 1: q_loss: 427917.0625, p_loss: 116.22562408447266, mean_rew: -6.269075405518592, variance: 121.29404656982422, lamda: 2.3350038528442383
Running avgs for agent 2: q_loss: 3.847440004348755, p_loss: -3.230689287185669, mean_rew: -6.271067307738472, variance: 4.634664058685303, lamda: 1.3842031955718994

steps: 1399975, episodes: 56000, mean episode reward: -477.7522150141498, agent episode reward: [-159.2507383380499, -159.2507383380499, -159.2507383380499], time: 178.147
steps: 1399975, episodes: 56000, mean episode variance: 60.57731022930145, agent episode variance: [29.532887748718263, 29.88675883483887, 1.1576636457443237], time: 178.148
Running avgs for agent 0: q_loss: 436105.75, p_loss: 115.93826293945312, mean_rew: -6.277252243012808, variance: 118.13155099487305, lamda: 2.361297369003296
Running avgs for agent 1: q_loss: 428418.6875, p_loss: 116.15343475341797, mean_rew: -6.274074642180669, variance: 119.54703533935547, lamda: 2.359978437423706
Running avgs for agent 2: q_loss: 3.8371193408966064, p_loss: -3.225494623184204, mean_rew: -6.269277769218172, variance: 4.630654335021973, lamda: 1.3842030763626099

steps: 1424975, episodes: 57000, mean episode reward: -481.80468901268375, agent episode reward: [-160.6015630042279, -160.6015630042279, -160.6015630042279], time: 178.873
steps: 1424975, episodes: 57000, mean episode variance: 60.866174125671385, agent episode variance: [30.09743529510498, 29.60852247619629, 1.1602163543701172], time: 178.873
Running avgs for agent 0: q_loss: 437858.1875, p_loss: 115.87091827392578, mean_rew: -6.281476788485464, variance: 120.38974118041992, lamda: 2.3862717151641846
Running avgs for agent 1: q_loss: 435822.28125, p_loss: 116.09651184082031, mean_rew: -6.275901708073059, variance: 118.43408990478515, lamda: 2.3849525451660156
Running avgs for agent 2: q_loss: 3.8198299407958984, p_loss: -3.2308549880981445, mean_rew: -6.274512891199857, variance: 4.640865325927734, lamda: 1.384203314781189

steps: 1449975, episodes: 58000, mean episode reward: -484.45964700797794, agent episode reward: [-161.48654900265933, -161.48654900265933, -161.48654900265933], time: 178.979
steps: 1449975, episodes: 58000, mean episode variance: 60.955685856580736, agent episode variance: [29.714823944091798, 30.080353485107423, 1.1605084273815156], time: 178.979
Running avgs for agent 0: q_loss: 440807.625, p_loss: 115.79220581054688, mean_rew: -6.290441492981736, variance: 118.85929577636719, lamda: 2.4112462997436523
Running avgs for agent 1: q_loss: 439241.5, p_loss: 116.03657531738281, mean_rew: -6.286933836427607, variance: 120.3214139404297, lamda: 2.4099271297454834
Running avgs for agent 2: q_loss: 3.8033382892608643, p_loss: -3.2332656383514404, mean_rew: -6.283635081590343, variance: 4.642033576965332, lamda: 1.384203314781189

steps: 1474975, episodes: 59000, mean episode reward: -483.82502427593863, agent episode reward: [-161.27500809197954, -161.27500809197954, -161.27500809197954], time: 177.54
steps: 1474975, episodes: 59000, mean episode variance: 61.06852618288994, agent episode variance: [29.450817138671876, 30.445579803466796, 1.1721292407512665], time: 177.541
Running avgs for agent 0: q_loss: 444197.15625, p_loss: 115.70069885253906, mean_rew: -6.294522544347341, variance: 117.8032685546875, lamda: 2.436220407485962
Running avgs for agent 1: q_loss: 455937.90625, p_loss: 116.0936508178711, mean_rew: -6.298213608878519, variance: 121.78231921386718, lamda: 2.434901475906372
Running avgs for agent 2: q_loss: 3.8335824012756348, p_loss: -3.2334654331207275, mean_rew: -6.295886684946421, variance: 4.688516616821289, lamda: 1.3842030763626099

steps: 1499975, episodes: 60000, mean episode reward: -482.7219630145825, agent episode reward: [-160.90732100486082, -160.90732100486082, -160.90732100486082], time: 178.492
steps: 1499975, episodes: 60000, mean episode variance: 60.48232408618927, agent episode variance: [29.2511774520874, 30.063610412597658, 1.1675362215042113], time: 178.493
Running avgs for agent 0: q_loss: 449799.78125, p_loss: 115.65560150146484, mean_rew: -6.3011964411898, variance: 117.0047098083496, lamda: 2.4611947536468506
Running avgs for agent 1: q_loss: 449286.28125, p_loss: 116.13300323486328, mean_rew: -6.298801293425593, variance: 120.25444165039063, lamda: 2.4598758220672607
Running avgs for agent 2: q_loss: 3.8591697216033936, p_loss: -3.249034881591797, mean_rew: -6.306076094239667, variance: 4.670144557952881, lamda: 1.384203314781189

steps: 1524975, episodes: 61000, mean episode reward: -483.7827522492558, agent episode reward: [-161.2609174164186, -161.2609174164186, -161.2609174164186], time: 188.926
steps: 1524975, episodes: 61000, mean episode variance: 60.65414547586441, agent episode variance: [29.322025016784668, 30.163833755493165, 1.1682867035865783], time: 188.927
Running avgs for agent 0: q_loss: 458161.15625, p_loss: 115.52216339111328, mean_rew: -6.3117023105099745, variance: 117.28810006713867, lamda: 2.4861690998077393
Running avgs for agent 1: q_loss: 453593.40625, p_loss: 116.1627197265625, mean_rew: -6.312262514989333, variance: 120.65533502197266, lamda: 2.4848501682281494
Running avgs for agent 2: q_loss: 3.869271755218506, p_loss: -3.2429752349853516, mean_rew: -6.307194766922505, variance: 4.673147201538086, lamda: 1.384203314781189

steps: 1549975, episodes: 62000, mean episode reward: -490.55999679069413, agent episode reward: [-163.51999893023134, -163.51999893023134, -163.51999893023134], time: 187.757
steps: 1549975, episodes: 62000, mean episode variance: 59.24263166880608, agent episode variance: [28.470650398254396, 29.597720123291015, 1.1742611472606659], time: 187.757
Running avgs for agent 0: q_loss: 463766.21875, p_loss: 115.52120208740234, mean_rew: -6.315767772638502, variance: 113.88260159301758, lamda: 2.511143445968628
Running avgs for agent 1: q_loss: 458955.78125, p_loss: 116.2127685546875, mean_rew: -6.326208241554781, variance: 118.39088049316406, lamda: 2.509824514389038
Running avgs for agent 2: q_loss: 3.9065701961517334, p_loss: -3.2490437030792236, mean_rew: -6.322196643528361, variance: 4.697044849395752, lamda: 1.3842030763626099

steps: 1574975, episodes: 63000, mean episode reward: -487.9200707015319, agent episode reward: [-162.64002356717734, -162.64002356717734, -162.64002356717734], time: 186.112
steps: 1574975, episodes: 63000, mean episode variance: 59.22598818969727, agent episode variance: [29.39754079437256, 28.655585388183592, 1.1728620071411133], time: 186.112
Running avgs for agent 0: q_loss: 464852.75, p_loss: 115.43524932861328, mean_rew: -6.338216508024554, variance: 117.59016317749024, lamda: 2.5361180305480957
Running avgs for agent 1: q_loss: 461635.0, p_loss: 116.21332550048828, mean_rew: -6.334416769991458, variance: 114.62234155273437, lamda: 2.5347988605499268
Running avgs for agent 2: q_loss: 3.815896511077881, p_loss: -3.252960443496704, mean_rew: -6.332028193723645, variance: 4.691448211669922, lamda: 1.384203314781189

steps: 1599975, episodes: 64000, mean episode reward: -487.5365626831438, agent episode reward: [-162.51218756104797, -162.51218756104797, -162.51218756104797], time: 184.452
steps: 1599975, episodes: 64000, mean episode variance: 60.31879998612404, agent episode variance: [29.733886795043944, 29.411040061950683, 1.1738731291294098], time: 184.453
Running avgs for agent 0: q_loss: 471125.40625, p_loss: 115.45652770996094, mean_rew: -6.3442356289748405, variance: 118.93554718017577, lamda: 2.5610923767089844
Running avgs for agent 1: q_loss: 465753.9375, p_loss: 116.29183959960938, mean_rew: -6.348623970695156, variance: 117.64416024780273, lamda: 2.5597729682922363
Running avgs for agent 2: q_loss: 3.844187021255493, p_loss: -3.264237880706787, mean_rew: -6.34880330736208, variance: 4.695492267608643, lamda: 1.384203314781189

steps: 1624975, episodes: 65000, mean episode reward: -486.6231258966305, agent episode reward: [-162.20770863221014, -162.20770863221014, -162.20770863221014], time: 175.823
steps: 1624975, episodes: 65000, mean episode variance: 59.49069009470939, agent episode variance: [29.084601303100587, 29.225124130249025, 1.180964661359787], time: 175.824
Running avgs for agent 0: q_loss: 474182.03125, p_loss: 115.40843200683594, mean_rew: -6.359075709195698, variance: 116.33840521240235, lamda: 2.586066484451294
Running avgs for agent 1: q_loss: 467726.65625, p_loss: 116.28028869628906, mean_rew: -6.357117218604595, variance: 116.9004965209961, lamda: 2.584747552871704
Running avgs for agent 2: q_loss: 3.8525688648223877, p_loss: -3.2644526958465576, mean_rew: -6.356416491851921, variance: 4.72385835647583, lamda: 1.3842030763626099

steps: 1649975, episodes: 66000, mean episode reward: -489.00844174781264, agent episode reward: [-163.00281391593757, -163.00281391593757, -163.00281391593757], time: 181.433
steps: 1649975, episodes: 66000, mean episode variance: 59.41108398842812, agent episode variance: [28.84913900756836, 29.38127311706543, 1.1806718637943268], time: 181.434
Running avgs for agent 0: q_loss: 478242.25, p_loss: 115.40326690673828, mean_rew: -6.365234585027982, variance: 115.39655603027344, lamda: 2.6110410690307617
Running avgs for agent 1: q_loss: 472630.25, p_loss: 116.41381072998047, mean_rew: -6.366842751041349, variance: 117.52509246826172, lamda: 2.6097216606140137
Running avgs for agent 2: q_loss: 3.895561933517456, p_loss: -3.273827314376831, mean_rew: -6.3717412473051, variance: 4.722687721252441, lamda: 1.384203314781189

steps: 1674975, episodes: 67000, mean episode reward: -486.73669918591685, agent episode reward: [-162.24556639530562, -162.24556639530562, -162.24556639530562], time: 178.165
steps: 1674975, episodes: 67000, mean episode variance: 59.44035415053368, agent episode variance: [29.04280037689209, 29.21737339782715, 1.1801803758144378], time: 178.165
Running avgs for agent 0: q_loss: 487666.75, p_loss: 115.47785949707031, mean_rew: -6.375837997933781, variance: 116.17120150756836, lamda: 2.6360151767730713
Running avgs for agent 1: q_loss: 473462.84375, p_loss: 116.37995910644531, mean_rew: -6.376793407490892, variance: 116.8694935913086, lamda: 2.6346962451934814
Running avgs for agent 2: q_loss: 3.8925156593322754, p_loss: -3.274414539337158, mean_rew: -6.374281227740517, variance: 4.72072172164917, lamda: 1.384203314781189

steps: 1699975, episodes: 68000, mean episode reward: -481.83970548557386, agent episode reward: [-160.61323516185797, -160.61323516185797, -160.61323516185797], time: 178.646
steps: 1699975, episodes: 68000, mean episode variance: 58.82575975251198, agent episode variance: [28.78833903503418, 28.849073516845703, 1.1883472006320954], time: 178.647
Running avgs for agent 0: q_loss: 487135.03125, p_loss: 115.47529602050781, mean_rew: -6.385598560880769, variance: 115.15335614013672, lamda: 2.66098952293396
Running avgs for agent 1: q_loss: 479307.71875, p_loss: 116.45042419433594, mean_rew: -6.382856665681647, variance: 115.39629406738281, lamda: 2.659670352935791
Running avgs for agent 2: q_loss: 3.8857152462005615, p_loss: -3.2770419120788574, mean_rew: -6.384727302580351, variance: 4.75338888168335, lamda: 1.3842030763626099

steps: 1724975, episodes: 69000, mean episode reward: -488.4378347696918, agent episode reward: [-162.81261158989724, -162.81261158989724, -162.81261158989724], time: 176.993
steps: 1724975, episodes: 69000, mean episode variance: 57.19025743746757, agent episode variance: [28.118734550476074, 27.887238189697264, 1.1842846972942351], time: 176.994
Running avgs for agent 0: q_loss: 496746.0625, p_loss: 115.59553527832031, mean_rew: -6.398167457925379, variance: 112.4749382019043, lamda: 2.6859638690948486
Running avgs for agent 1: q_loss: 484568.9375, p_loss: 116.47833251953125, mean_rew: -6.394536083250057, variance: 111.54895275878906, lamda: 2.6846446990966797
Running avgs for agent 2: q_loss: 3.8629937171936035, p_loss: -3.2890405654907227, mean_rew: -6.394247007844108, variance: 4.737138748168945, lamda: 1.384203314781189

steps: 1749975, episodes: 70000, mean episode reward: -494.0903620501428, agent episode reward: [-164.6967873500476, -164.6967873500476, -164.6967873500476], time: 184.535
steps: 1749975, episodes: 70000, mean episode variance: 58.820668281793594, agent episode variance: [29.037098892211915, 28.59306755065918, 1.1905018389225006], time: 184.536
Running avgs for agent 0: q_loss: 497896.21875, p_loss: 115.64363861083984, mean_rew: -6.399940299902084, variance: 116.14839556884766, lamda: 2.7109382152557373
Running avgs for agent 1: q_loss: 484480.0, p_loss: 116.40401458740234, mean_rew: -6.39909555936074, variance: 114.37227020263671, lamda: 2.7096190452575684
Running avgs for agent 2: q_loss: 3.91886568069458, p_loss: -3.287405014038086, mean_rew: -6.404515662433152, variance: 4.762007236480713, lamda: 1.384203314781189

steps: 1774975, episodes: 71000, mean episode reward: -487.4126808232338, agent episode reward: [-162.47089360774459, -162.47089360774459, -162.47089360774459], time: 192.376
steps: 1774975, episodes: 71000, mean episode variance: 58.1422368016243, agent episode variance: [28.198866439819337, 28.749353614807127, 1.1940167469978333], time: 192.377
Running avgs for agent 0: q_loss: 505663.625, p_loss: 115.80328369140625, mean_rew: -6.412251153142006, variance: 112.79546575927735, lamda: 2.735912561416626
Running avgs for agent 1: q_loss: 490963.46875, p_loss: 116.4416275024414, mean_rew: -6.403956621505656, variance: 114.99741445922851, lamda: 2.734593391418457
Running avgs for agent 2: q_loss: 3.9593374729156494, p_loss: -3.2945668697357178, mean_rew: -6.412819326676108, variance: 4.776066780090332, lamda: 1.3842030763626099

steps: 1799975, episodes: 72000, mean episode reward: -490.9621225561957, agent episode reward: [-163.65404085206526, -163.65404085206526, -163.65404085206526], time: 198.59
steps: 1799975, episodes: 72000, mean episode variance: 57.38846971702576, agent episode variance: [28.5655599899292, 27.631175148010254, 1.1917345790863036], time: 198.591
Running avgs for agent 0: q_loss: 512554.84375, p_loss: 115.9134521484375, mean_rew: -6.4132044462319815, variance: 114.2622399597168, lamda: 2.7608869075775146
Running avgs for agent 1: q_loss: 496205.5625, p_loss: 116.51649475097656, mean_rew: -6.419210061225365, variance: 110.52470059204101, lamda: 2.759567975997925
Running avgs for agent 2: q_loss: 3.9123964309692383, p_loss: -3.294126510620117, mean_rew: -6.418195285721296, variance: 4.76693868637085, lamda: 1.384203314781189

steps: 1824975, episodes: 73000, mean episode reward: -484.5429433033859, agent episode reward: [-161.51431443446197, -161.51431443446197, -161.51431443446197], time: 190.44
steps: 1824975, episodes: 73000, mean episode variance: 59.17415349316597, agent episode variance: [29.36460562133789, 28.620296493530272, 1.1892513782978058], time: 190.441
Running avgs for agent 0: q_loss: 518830.875, p_loss: 116.07209014892578, mean_rew: -6.42633551363356, variance: 117.45842248535156, lamda: 2.7858612537384033
Running avgs for agent 1: q_loss: 498187.96875, p_loss: 116.53489685058594, mean_rew: -6.424405719928299, variance: 114.48118597412109, lamda: 2.7845423221588135
Running avgs for agent 2: q_loss: 3.983621597290039, p_loss: -3.29836368560791, mean_rew: -6.41747508638714, variance: 4.75700569152832, lamda: 1.384203314781189

steps: 1849975, episodes: 74000, mean episode reward: -492.920636542616, agent episode reward: [-164.30687884753866, -164.30687884753866, -164.30687884753866], time: 184.523
steps: 1849975, episodes: 74000, mean episode variance: 57.533197247982024, agent episode variance: [27.73909149169922, 28.594087814331054, 1.2000179419517516], time: 184.523
Running avgs for agent 0: q_loss: 523839.8125, p_loss: 116.22863006591797, mean_rew: -6.4285551499083455, variance: 110.95636596679688, lamda: 2.810835599899292
Running avgs for agent 1: q_loss: 502134.78125, p_loss: 116.63431549072266, mean_rew: -6.4301998709119, variance: 114.37635125732422, lamda: 2.809516668319702
Running avgs for agent 2: q_loss: 4.015191078186035, p_loss: -3.3031601905822754, mean_rew: -6.4329998155463475, variance: 4.8000712394714355, lamda: 1.3842030763626099

steps: 1874975, episodes: 75000, mean episode reward: -488.77748053590796, agent episode reward: [-162.92582684530265, -162.92582684530265, -162.92582684530265], time: 183.337
steps: 1874975, episodes: 75000, mean episode variance: 58.02316624331474, agent episode variance: [28.690200950622557, 28.14014128112793, 1.1928240115642548], time: 183.338
Running avgs for agent 0: q_loss: 528786.625, p_loss: 116.26192474365234, mean_rew: -6.435353456574642, variance: 114.76080380249023, lamda: 2.8358099460601807
Running avgs for agent 1: q_loss: 510708.53125, p_loss: 116.65863800048828, mean_rew: -6.445801623250278, variance: 112.56056512451173, lamda: 2.8344907760620117
Running avgs for agent 2: q_loss: 3.952687978744507, p_loss: -3.307013511657715, mean_rew: -6.439146138760903, variance: 4.77129602432251, lamda: 1.384203314781189

steps: 1899975, episodes: 76000, mean episode reward: -497.33005408006466, agent episode reward: [-165.77668469335487, -165.77668469335487, -165.77668469335487], time: 193.894
steps: 1899975, episodes: 76000, mean episode variance: 57.5334936876297, agent episode variance: [28.73667510986328, 27.600019203186037, 1.1967993745803833], time: 193.895
Running avgs for agent 0: q_loss: 539709.75, p_loss: 116.40666961669922, mean_rew: -6.444192664746549, variance: 114.94670043945312, lamda: 2.8607842922210693
Running avgs for agent 1: q_loss: 514222.09375, p_loss: 116.64618682861328, mean_rew: -6.448253971975872, variance: 110.40007681274415, lamda: 2.8594653606414795
Running avgs for agent 2: q_loss: 4.075507164001465, p_loss: -3.3096048831939697, mean_rew: -6.44927386442874, variance: 4.787197589874268, lamda: 1.384203314781189

steps: 1924975, episodes: 77000, mean episode reward: -493.5167532676585, agent episode reward: [-164.50558442255283, -164.50558442255283, -164.50558442255283], time: 197.437
steps: 1924975, episodes: 77000, mean episode variance: 58.03651332592964, agent episode variance: [28.435066680908204, 28.397291076660157, 1.2041555683612823], time: 197.438
Running avgs for agent 0: q_loss: 541724.1875, p_loss: 116.5443344116211, mean_rew: -6.457531404315469, variance: 113.74026672363281, lamda: 2.885758876800537
Running avgs for agent 1: q_loss: 515757.84375, p_loss: 116.60885620117188, mean_rew: -6.459261530242814, variance: 113.58916430664063, lamda: 2.884439706802368
Running avgs for agent 2: q_loss: 4.0909295082092285, p_loss: -3.3125524520874023, mean_rew: -6.457262235475755, variance: 4.816622257232666, lamda: 1.3842030763626099

steps: 1949975, episodes: 78000, mean episode reward: -495.6647017403905, agent episode reward: [-165.22156724679684, -165.22156724679684, -165.22156724679684], time: 185.712
steps: 1949975, episodes: 78000, mean episode variance: 56.50330510520935, agent episode variance: [27.936860580444336, 27.36800178527832, 1.1984427394866943], time: 185.712
Running avgs for agent 0: q_loss: 557966.4375, p_loss: 116.63412475585938, mean_rew: -6.477622365126393, variance: 111.74744232177734, lamda: 2.910733222961426
Running avgs for agent 1: q_loss: 522944.125, p_loss: 116.63013458251953, mean_rew: -6.466886684974103, variance: 109.47200714111328, lamda: 2.909414052963257
Running avgs for agent 2: q_loss: 4.107888698577881, p_loss: -3.322171449661255, mean_rew: -6.4675067513252085, variance: 4.793770790100098, lamda: 1.384203314781189

steps: 1974975, episodes: 79000, mean episode reward: -485.2915751819197, agent episode reward: [-161.7638583939732, -161.7638583939732, -161.7638583939732], time: 183.072
steps: 1974975, episodes: 79000, mean episode variance: 57.45638461661339, agent episode variance: [28.42746742248535, 27.820577476501466, 1.2083397176265716], time: 183.073
Running avgs for agent 0: q_loss: 560745.875, p_loss: 116.6853256225586, mean_rew: -6.47557059824932, variance: 113.7098696899414, lamda: 2.9357075691223145
Running avgs for agent 1: q_loss: 529343.0625, p_loss: 116.67755126953125, mean_rew: -6.471336212627981, variance: 111.28230990600586, lamda: 2.9343883991241455
Running avgs for agent 2: q_loss: 4.100520133972168, p_loss: -3.3224122524261475, mean_rew: -6.478445856453402, variance: 4.8333587646484375, lamda: 1.384203314781189

steps: 1999975, episodes: 80000, mean episode reward: -492.34426801994243, agent episode reward: [-164.1147560066475, -164.1147560066475, -164.1147560066475], time: 180.961
steps: 1999975, episodes: 80000, mean episode variance: 57.33710903596878, agent episode variance: [28.099582496643066, 28.029200401306152, 1.2083261380195618], time: 180.962
Running avgs for agent 0: q_loss: 568946.0625, p_loss: 116.80118560791016, mean_rew: -6.488287281245415, variance: 112.39832998657226, lamda: 2.960681676864624
Running avgs for agent 1: q_loss: 535307.0625, p_loss: 116.80754089355469, mean_rew: -6.490932291000174, variance: 112.11680160522461, lamda: 2.959362745285034
Running avgs for agent 2: q_loss: 4.14877462387085, p_loss: -3.3252527713775635, mean_rew: -6.483867492522038, variance: 4.8333048820495605, lamda: 1.3842030763626099

steps: 2024975, episodes: 81000, mean episode reward: -491.20629454130807, agent episode reward: [-163.73543151376936, -163.73543151376936, -163.73543151376936], time: 182.692
steps: 2024975, episodes: 81000, mean episode variance: 56.68275992918014, agent episode variance: [28.105374130249025, 27.366212875366212, 1.211172923564911], time: 182.693
Running avgs for agent 0: q_loss: 588662.125, p_loss: 116.97221374511719, mean_rew: -6.500570258375806, variance: 112.4214965209961, lamda: 2.9856560230255127
Running avgs for agent 1: q_loss: 546624.25, p_loss: 116.9652099609375, mean_rew: -6.501351515154285, variance: 109.46485150146485, lamda: 2.984337091445923
Running avgs for agent 2: q_loss: 4.156695365905762, p_loss: -3.332049608230591, mean_rew: -6.498184029465727, variance: 4.844691276550293, lamda: 1.384203314781189

steps: 2049975, episodes: 82000, mean episode reward: -489.77925822113764, agent episode reward: [-163.25975274037924, -163.25975274037924, -163.25975274037924], time: 183.258
steps: 2049975, episodes: 82000, mean episode variance: 56.86197894334793, agent episode variance: [28.307625343322755, 27.343812004089354, 1.2105415959358214], time: 183.259
Running avgs for agent 0: q_loss: 599344.0, p_loss: 117.17447662353516, mean_rew: -6.506780641165127, variance: 113.23050137329102, lamda: 3.0106303691864014
Running avgs for agent 1: q_loss: 557448.5, p_loss: 117.1300048828125, mean_rew: -6.506999751888763, variance: 109.37524801635742, lamda: 3.0093114376068115
Running avgs for agent 2: q_loss: 4.241695404052734, p_loss: -3.339479446411133, mean_rew: -6.502221795553341, variance: 4.842166423797607, lamda: 1.384203314781189

steps: 2074975, episodes: 83000, mean episode reward: -494.17352564988863, agent episode reward: [-164.72450854996285, -164.72450854996285, -164.72450854996285], time: 181.591
steps: 2074975, episodes: 83000, mean episode variance: 56.37138025546074, agent episode variance: [27.76036882019043, 27.393787254333496, 1.2172241809368134], time: 181.592
Running avgs for agent 0: q_loss: 607064.6875, p_loss: 117.34381103515625, mean_rew: -6.5111747998465255, variance: 111.04147528076172, lamda: 3.03560471534729
Running avgs for agent 1: q_loss: 562894.5625, p_loss: 117.28936004638672, mean_rew: -6.5139302509851635, variance: 109.57514901733398, lamda: 3.034285545349121
Running avgs for agent 2: q_loss: 4.200488567352295, p_loss: -3.3392646312713623, mean_rew: -6.510515285892296, variance: 4.868896484375, lamda: 1.3842030763626099

steps: 2099975, episodes: 84000, mean episode reward: -486.6391526847191, agent episode reward: [-162.21305089490633, -162.21305089490633, -162.21305089490633], time: 184.394
steps: 2099975, episodes: 84000, mean episode variance: 56.591729942798615, agent episode variance: [28.2706318359375, 27.108336235046387, 1.2127618718147277], time: 184.395
Running avgs for agent 0: q_loss: 616682.0, p_loss: 117.355712890625, mean_rew: -6.507283551244098, variance: 113.08252734375, lamda: 3.0605790615081787
Running avgs for agent 1: q_loss: 576142.0625, p_loss: 117.3123779296875, mean_rew: -6.5158146892747455, variance: 108.43334494018555, lamda: 3.059260368347168
Running avgs for agent 2: q_loss: 4.182481288909912, p_loss: -3.3371665477752686, mean_rew: -6.513299567676606, variance: 4.851047515869141, lamda: 1.384203314781189

steps: 2124975, episodes: 85000, mean episode reward: -487.66970248601086, agent episode reward: [-162.55656749533694, -162.55656749533694, -162.55656749533694], time: 183.126
steps: 2124975, episodes: 85000, mean episode variance: 57.79648171520233, agent episode variance: [28.397138206481934, 28.181444076538085, 1.217899432182312], time: 183.126
Running avgs for agent 0: q_loss: 631555.9375, p_loss: 117.44622802734375, mean_rew: -6.518789213795036, variance: 113.58855282592774, lamda: 3.0855534076690674
Running avgs for agent 1: q_loss: 581991.6875, p_loss: 117.4155502319336, mean_rew: -6.521270239016022, variance: 112.72577630615234, lamda: 3.0842344760894775
Running avgs for agent 2: q_loss: 4.239260196685791, p_loss: -3.3439853191375732, mean_rew: -6.518834837818625, variance: 4.871598243713379, lamda: 1.384203314781189

steps: 2149975, episodes: 86000, mean episode reward: -494.31874191214445, agent episode reward: [-164.77291397071485, -164.77291397071485, -164.77291397071485], time: 182.741
steps: 2149975, episodes: 86000, mean episode variance: 56.35484865260124, agent episode variance: [27.838520767211914, 27.302487747192384, 1.2138401381969453], time: 182.742
Running avgs for agent 0: q_loss: 641235.8125, p_loss: 117.53752136230469, mean_rew: -6.522604707324642, variance: 111.35408306884766, lamda: 3.110527753829956
Running avgs for agent 1: q_loss: 589551.0, p_loss: 117.43701934814453, mean_rew: -6.519501360549249, variance: 109.20995098876953, lamda: 3.1092090606689453
Running avgs for agent 2: q_loss: 4.175999164581299, p_loss: -3.347268581390381, mean_rew: -6.517143856381824, variance: 4.855360507965088, lamda: 1.3842030763626099

steps: 2174975, episodes: 87000, mean episode reward: -486.1742745664079, agent episode reward: [-162.05809152213595, -162.05809152213595, -162.05809152213595], time: 183.276
steps: 2174975, episodes: 87000, mean episode variance: 56.604058868646625, agent episode variance: [27.93088196563721, 27.461416458129882, 1.2117604448795318], time: 183.276
Running avgs for agent 0: q_loss: 651670.25, p_loss: 117.5741958618164, mean_rew: -6.5267860568165945, variance: 111.72352786254883, lamda: 3.1355020999908447
Running avgs for agent 1: q_loss: 598969.75, p_loss: 117.46208190917969, mean_rew: -6.520812535283628, variance: 109.84566583251953, lamda: 3.134183168411255
Running avgs for agent 2: q_loss: 4.18863582611084, p_loss: -3.3486058712005615, mean_rew: -6.526853657421283, variance: 4.847042083740234, lamda: 1.384203314781189

steps: 2199975, episodes: 88000, mean episode reward: -487.4951874589262, agent episode reward: [-162.49839581964207, -162.49839581964207, -162.49839581964207], time: 180.065
steps: 2199975, episodes: 88000, mean episode variance: 57.44362507796288, agent episode variance: [28.36084864807129, 27.862733489990234, 1.220042939901352], time: 180.066
Running avgs for agent 0: q_loss: 666470.625, p_loss: 117.53675842285156, mean_rew: -6.53035679742862, variance: 113.44339459228516, lamda: 3.1604764461517334
Running avgs for agent 1: q_loss: 605360.875, p_loss: 117.38330841064453, mean_rew: -6.520364324917875, variance: 111.45093395996093, lamda: 3.1591575145721436
Running avgs for agent 2: q_loss: 4.22016716003418, p_loss: -3.351208209991455, mean_rew: -6.525228714726831, variance: 4.880171775817871, lamda: 1.384203314781189

steps: 2224975, episodes: 89000, mean episode reward: -483.52866321278805, agent episode reward: [-161.17622107092936, -161.17622107092936, -161.17622107092936], time: 180.642
steps: 2224975, episodes: 89000, mean episode variance: 56.79754073548317, agent episode variance: [28.082747085571288, 27.49977375793457, 1.21501989197731], time: 180.642
Running avgs for agent 0: q_loss: 669856.625, p_loss: 117.40477752685547, mean_rew: -6.520864065988899, variance: 112.33098834228515, lamda: 3.185451030731201
Running avgs for agent 1: q_loss: 616123.8125, p_loss: 117.31970977783203, mean_rew: -6.525922917181029, variance: 109.99909503173828, lamda: 3.1841318607330322
Running avgs for agent 2: q_loss: 4.211640357971191, p_loss: -3.342613697052002, mean_rew: -6.517817006886233, variance: 4.860080242156982, lamda: 1.3842030763626099

steps: 2249975, episodes: 90000, mean episode reward: -481.49788858880623, agent episode reward: [-160.49929619626874, -160.49929619626874, -160.49929619626874], time: 184.23
steps: 2249975, episodes: 90000, mean episode variance: 56.370263649702075, agent episode variance: [27.8533516998291, 27.307239639282226, 1.209672310590744], time: 184.231
Running avgs for agent 0: q_loss: 679013.875, p_loss: 117.2860336303711, mean_rew: -6.521740437656537, variance: 111.4134067993164, lamda: 3.21042537689209
Running avgs for agent 1: q_loss: 622870.625, p_loss: 117.35265350341797, mean_rew: -6.5144227132803065, variance: 109.2289585571289, lamda: 3.209106206893921
Running avgs for agent 2: q_loss: 4.28185510635376, p_loss: -3.3524577617645264, mean_rew: -6.523307086825725, variance: 4.83868932723999, lamda: 1.384203314781189

steps: 2274975, episodes: 91000, mean episode reward: -483.67323852202543, agent episode reward: [-161.2244128406751, -161.2244128406751, -161.2244128406751], time: 166.265
steps: 2274975, episodes: 91000, mean episode variance: 57.554093399763104, agent episode variance: [28.560827545166017, 27.775946899414063, 1.2173189551830292], time: 166.265
Running avgs for agent 0: q_loss: 699196.6875, p_loss: 117.19051361083984, mean_rew: -6.514503653646932, variance: 114.24331018066407, lamda: 3.2353994846343994
Running avgs for agent 1: q_loss: 627972.5625, p_loss: 117.37276458740234, mean_rew: -6.511021996477871, variance: 111.10378759765625, lamda: 3.2340805530548096
Running avgs for agent 2: q_loss: 4.18400764465332, p_loss: -3.3418169021606445, mean_rew: -6.5159416618337875, variance: 4.86927604675293, lamda: 1.384203314781189

steps: 2299975, episodes: 92000, mean episode reward: -473.8976279552926, agent episode reward: [-157.9658759850975, -157.9658759850975, -157.9658759850975], time: 125.779
steps: 2299975, episodes: 92000, mean episode variance: 57.5171714053154, agent episode variance: [28.60715315246582, 27.69509220123291, 1.2149260516166687], time: 125.78
Running avgs for agent 0: q_loss: 701353.0625, p_loss: 117.09227752685547, mean_rew: -6.507408170865632, variance: 114.42861260986328, lamda: 3.260373830795288
Running avgs for agent 1: q_loss: 640673.6875, p_loss: 117.36563873291016, mean_rew: -6.511234828769011, variance: 110.78036880493164, lamda: 3.259054660797119
Running avgs for agent 2: q_loss: 4.249790191650391, p_loss: -3.3375136852264404, mean_rew: -6.506096025096927, variance: 4.85970401763916, lamda: 1.3842030763626099

steps: 2324975, episodes: 93000, mean episode reward: -475.658463646717, agent episode reward: [-158.55282121557235, -158.55282121557235, -158.55282121557235], time: 124.206
steps: 2324975, episodes: 93000, mean episode variance: 56.812540560722354, agent episode variance: [28.746794830322266, 26.85071536254883, 1.2150303678512573], time: 124.207
Running avgs for agent 0: q_loss: 710198.375, p_loss: 116.85552215576172, mean_rew: -6.503214696671735, variance: 114.98717932128906, lamda: 3.2853481769561768
Running avgs for agent 1: q_loss: 649732.25, p_loss: 117.33580017089844, mean_rew: -6.5043557964637655, variance: 107.40286145019532, lamda: 3.284029245376587
Running avgs for agent 2: q_loss: 4.191969871520996, p_loss: -3.333108901977539, mean_rew: -6.503653955550468, variance: 4.860121726989746, lamda: 1.384203314781189

steps: 2349975, episodes: 94000, mean episode reward: -481.6326766515296, agent episode reward: [-160.54422555050988, -160.54422555050988, -160.54422555050988], time: 126.301
steps: 2349975, episodes: 94000, mean episode variance: 57.563703010320665, agent episode variance: [28.574700134277343, 27.780868209838868, 1.2081346662044525], time: 126.302
Running avgs for agent 0: q_loss: 719648.5, p_loss: 116.6554183959961, mean_rew: -6.5008576575563035, variance: 114.29880053710937, lamda: 3.3103227615356445
Running avgs for agent 1: q_loss: 650184.4375, p_loss: 117.15394592285156, mean_rew: -6.503003964857378, variance: 111.12347283935547, lamda: 3.3090033531188965
Running avgs for agent 2: q_loss: 4.200909614562988, p_loss: -3.3390161991119385, mean_rew: -6.501850268406322, variance: 4.832538604736328, lamda: 1.384203314781189

steps: 2374975, episodes: 95000, mean episode reward: -474.965482365132, agent episode reward: [-158.32182745504403, -158.32182745504403, -158.32182745504403], time: 116.958
steps: 2374975, episodes: 95000, mean episode variance: 56.92709950637818, agent episode variance: [28.432157974243164, 27.28396830749512, 1.2109732246398925], time: 116.958
Running avgs for agent 0: q_loss: 726971.125, p_loss: 116.39698791503906, mean_rew: -6.492576399561871, variance: 113.72863189697266, lamda: 3.335296869277954
Running avgs for agent 1: q_loss: 660463.875, p_loss: 117.14451599121094, mean_rew: -6.49615557266671, variance: 109.13587322998048, lamda: 3.3339779376983643
Running avgs for agent 2: q_loss: 4.179584980010986, p_loss: -3.3317203521728516, mean_rew: -6.496571590162945, variance: 4.843893051147461, lamda: 1.3842030763626099

steps: 2399975, episodes: 96000, mean episode reward: -473.82822989601516, agent episode reward: [-157.9427432986717, -157.9427432986717, -157.9427432986717], time: 115.076
steps: 2399975, episodes: 96000, mean episode variance: 56.66322218036652, agent episode variance: [28.31735903930664, 27.137694427490235, 1.208168713569641], time: 115.076
Running avgs for agent 0: q_loss: 731473.5, p_loss: 116.15531158447266, mean_rew: -6.502244773739119, variance: 113.26943615722656, lamda: 3.3602712154388428
Running avgs for agent 1: q_loss: 671535.4375, p_loss: 116.97868347167969, mean_rew: -6.4946747169219865, variance: 108.55077770996094, lamda: 3.358952045440674
Running avgs for agent 2: q_loss: 4.191429138183594, p_loss: -3.3409852981567383, mean_rew: -6.501433978514807, variance: 4.832674980163574, lamda: 1.384203314781189

steps: 2424975, episodes: 97000, mean episode reward: -478.16182852031113, agent episode reward: [-159.38727617343704, -159.38727617343704, -159.38727617343704], time: 112.236
steps: 2424975, episodes: 97000, mean episode variance: 56.42714232993126, agent episode variance: [27.812466552734374, 27.407918762207032, 1.206757014989853], time: 112.237
Running avgs for agent 0: q_loss: 746224.5625, p_loss: 115.89785766601562, mean_rew: -6.495868080017034, variance: 111.2498662109375, lamda: 3.3852455615997314
Running avgs for agent 1: q_loss: 678269.0, p_loss: 116.82083129882812, mean_rew: -6.497882780362563, variance: 109.63167504882813, lamda: 3.3839268684387207
Running avgs for agent 2: q_loss: 4.174498081207275, p_loss: -3.3325107097625732, mean_rew: -6.490955972696422, variance: 4.827027797698975, lamda: 1.384203314781189

steps: 2449975, episodes: 98000, mean episode reward: -474.78883180604174, agent episode reward: [-158.26294393534727, -158.26294393534727, -158.26294393534727], time: 105.247
steps: 2449975, episodes: 98000, mean episode variance: 56.6260442841053, agent episode variance: [28.26319135284424, 27.147309982299806, 1.215542948961258], time: 105.248
Running avgs for agent 0: q_loss: 747574.375, p_loss: 115.47637176513672, mean_rew: -6.486965474374885, variance: 113.05276541137695, lamda: 3.41021990776062
Running avgs for agent 1: q_loss: 680245.9375, p_loss: 116.7691421508789, mean_rew: -6.492649303371149, variance: 108.58923992919922, lamda: 3.4089012145996094
Running avgs for agent 2: q_loss: 4.195789337158203, p_loss: -3.3335742950439453, mean_rew: -6.496877550512484, variance: 4.862171649932861, lamda: 1.3842030763626099

steps: 2474975, episodes: 99000, mean episode reward: -475.70426213541447, agent episode reward: [-158.5680873784715, -158.5680873784715, -158.5680873784715], time: 96.72
steps: 2474975, episodes: 99000, mean episode variance: 54.872089304685595, agent episode variance: [27.563488693237304, 26.100471893310548, 1.2081287181377411], time: 96.721
Running avgs for agent 0: q_loss: 757819.8125, p_loss: 115.25955200195312, mean_rew: -6.49143992546539, variance: 110.25395477294921, lamda: 3.435194253921509
Running avgs for agent 1: q_loss: 695062.0, p_loss: 116.6648941040039, mean_rew: -6.484684975379736, variance: 104.40188757324219, lamda: 3.433875322341919
Running avgs for agent 2: q_loss: 4.2055158615112305, p_loss: -3.333850145339966, mean_rew: -6.491374694968565, variance: 4.832514762878418, lamda: 1.384203314781189

steps: 2499975, episodes: 100000, mean episode reward: -473.83751901615705, agent episode reward: [-157.94583967205236, -157.94583967205236, -157.94583967205236], time: 98.83
steps: 2499975, episodes: 100000, mean episode variance: 55.8536222383976, agent episode variance: [27.76198811340332, 26.883502532958985, 1.2081315920352935], time: 98.831
Running avgs for agent 0: q_loss: 766336.375, p_loss: 114.8633804321289, mean_rew: -6.485252072658386, variance: 111.04795245361328, lamda: 3.4601686000823975
Running avgs for agent 1: q_loss: 697677.0, p_loss: 116.59986114501953, mean_rew: -6.482999336384423, variance: 107.53401013183594, lamda: 3.4588496685028076
Running avgs for agent 2: q_loss: 4.1445112228393555, p_loss: -3.329761266708374, mean_rew: -6.487783223544172, variance: 4.832526206970215, lamda: 1.384203314781189

steps: 2524975, episodes: 101000, mean episode reward: -476.7301882938897, agent episode reward: [-158.91006276462988, -158.91006276462988, -158.91006276462988], time: 99.525
steps: 2524975, episodes: 101000, mean episode variance: 55.251074311971664, agent episode variance: [27.08043865966797, 26.959237274169922, 1.2113983781337738], time: 99.526
Running avgs for agent 0: q_loss: 769776.125, p_loss: 114.52237701416016, mean_rew: -6.480302502771617, variance: 108.32175463867188, lamda: 3.4851431846618652
Running avgs for agent 1: q_loss: 702201.1875, p_loss: 116.55817413330078, mean_rew: -6.485483066194183, variance: 107.83694909667969, lamda: 3.4838240146636963
Running avgs for agent 2: q_loss: 4.25724983215332, p_loss: -3.332106351852417, mean_rew: -6.4880406329986835, variance: 4.8455939292907715, lamda: 1.3842030763626099

steps: 2549975, episodes: 102000, mean episode reward: -486.9380726888058, agent episode reward: [-162.31269089626858, -162.31269089626858, -162.31269089626858], time: 99.29
steps: 2549975, episodes: 102000, mean episode variance: 55.69043917822838, agent episode variance: [27.40418493652344, 27.081093688964845, 1.205160552740097], time: 99.29
Running avgs for agent 0: q_loss: 768806.0, p_loss: 114.13749694824219, mean_rew: -6.480720252048994, variance: 109.61673974609376, lamda: 3.510117292404175
Running avgs for agent 1: q_loss: 721891.3125, p_loss: 116.4495620727539, mean_rew: -6.48441369215215, variance: 108.32437475585938, lamda: 3.508798360824585
Running avgs for agent 2: q_loss: 4.21571159362793, p_loss: -3.3309895992279053, mean_rew: -6.484473614406583, variance: 4.820641994476318, lamda: 1.384203314781189

steps: 2574975, episodes: 103000, mean episode reward: -479.1167548417107, agent episode reward: [-159.7055849472369, -159.7055849472369, -159.7055849472369], time: 99.413
steps: 2574975, episodes: 103000, mean episode variance: 55.73556895756722, agent episode variance: [27.221820762634277, 27.305293487548827, 1.2084547073841094], time: 99.414
Running avgs for agent 0: q_loss: 785839.625, p_loss: 113.64071655273438, mean_rew: -6.476570148118814, variance: 108.8872830505371, lamda: 3.5350918769836426
Running avgs for agent 1: q_loss: 721761.625, p_loss: 116.35063934326172, mean_rew: -6.477440255056726, variance: 109.22117395019531, lamda: 3.5337724685668945
Running avgs for agent 2: q_loss: 4.1638970375061035, p_loss: -3.326059103012085, mean_rew: -6.48227971760599, variance: 4.8338189125061035, lamda: 1.384203314781189

steps: 2599975, episodes: 104000, mean episode reward: -482.9456742010807, agent episode reward: [-160.9818914003602, -160.9818914003602, -160.9818914003602], time: 97.823
steps: 2599975, episodes: 104000, mean episode variance: 54.50427758073807, agent episode variance: [27.03001326751709, 26.266667350769044, 1.2075969624519347], time: 97.823
Running avgs for agent 0: q_loss: 783393.25, p_loss: 113.16633605957031, mean_rew: -6.475050354757894, variance: 108.12005307006837, lamda: 3.560065984725952
Running avgs for agent 1: q_loss: 737230.5, p_loss: 116.27487182617188, mean_rew: -6.478317819848407, variance: 105.06666940307618, lamda: 3.5587470531463623
Running avgs for agent 2: q_loss: 4.180726528167725, p_loss: -3.3195860385894775, mean_rew: -6.4702172485732365, variance: 4.830387592315674, lamda: 1.3842030763626099

steps: 2624975, episodes: 105000, mean episode reward: -478.42863268970467, agent episode reward: [-159.4762108965682, -159.4762108965682, -159.4762108965682], time: 99.335
steps: 2624975, episodes: 105000, mean episode variance: 54.62519637417793, agent episode variance: [27.034155754089355, 26.38473034667969, 1.2063102734088897], time: 99.336
Running avgs for agent 0: q_loss: 784163.75, p_loss: 112.74989318847656, mean_rew: -6.4690652216480595, variance: 108.13662301635742, lamda: 3.58504056930542
Running avgs for agent 1: q_loss: 743304.9375, p_loss: 116.26313781738281, mean_rew: -6.478704105648371, variance: 105.53892138671876, lamda: 3.583721160888672
Running avgs for agent 2: q_loss: 4.149784564971924, p_loss: -3.322608470916748, mean_rew: -6.473068080770384, variance: 4.8252410888671875, lamda: 1.384203314781189

steps: 2649975, episodes: 106000, mean episode reward: -479.46393157454895, agent episode reward: [-159.82131052484962, -159.82131052484962, -159.82131052484962], time: 88.171
steps: 2649975, episodes: 106000, mean episode variance: 55.409200088500974, agent episode variance: [27.2448701171875, 26.96125399017334, 1.2030759811401368], time: 88.171
Running avgs for agent 0: q_loss: 790252.8125, p_loss: 112.4931640625, mean_rew: -6.47606546198461, variance: 108.97948046875, lamda: 3.6100149154663086
Running avgs for agent 1: q_loss: 754731.25, p_loss: 116.12145233154297, mean_rew: -6.4652631277081944, variance: 107.84501596069336, lamda: 3.6086957454681396
Running avgs for agent 2: q_loss: 4.2284746170043945, p_loss: -3.3164663314819336, mean_rew: -6.466032480721651, variance: 4.81230354309082, lamda: 1.384203314781189

steps: 2674975, episodes: 107000, mean episode reward: -483.9815237352631, agent episode reward: [-161.32717457842105, -161.32717457842105, -161.32717457842105], time: 84.157
steps: 2674975, episodes: 107000, mean episode variance: 53.88792819094658, agent episode variance: [26.0670411529541, 26.60970310974121, 1.2111839282512664], time: 84.157
Running avgs for agent 0: q_loss: 801362.3125, p_loss: 112.22831726074219, mean_rew: -6.475740283398754, variance: 104.2681646118164, lamda: 3.634989023208618
Running avgs for agent 1: q_loss: 767189.6875, p_loss: 116.26396179199219, mean_rew: -6.4707538836502065, variance: 106.43881243896485, lamda: 3.6336700916290283
Running avgs for agent 2: q_loss: 4.240356922149658, p_loss: -3.314760684967041, mean_rew: -6.4728739142365015, variance: 4.844735622406006, lamda: 1.3842030763626099

steps: 2699975, episodes: 108000, mean episode reward: -479.29671309768224, agent episode reward: [-159.76557103256076, -159.76557103256076, -159.76557103256076], time: 84.552
steps: 2699975, episodes: 108000, mean episode variance: 54.65816855978966, agent episode variance: [26.941122009277343, 26.510468254089357, 1.2065782964229583], time: 84.552
Running avgs for agent 0: q_loss: 804742.1875, p_loss: 112.01422119140625, mean_rew: -6.4742983415131965, variance: 107.76448803710937, lamda: 3.659963369369507
Running avgs for agent 1: q_loss: 776898.625, p_loss: 116.22613525390625, mean_rew: -6.470199299954169, variance: 106.04187301635743, lamda: 3.658644199371338
Running avgs for agent 2: q_loss: 4.290532112121582, p_loss: -3.324835777282715, mean_rew: -6.4761470580757985, variance: 4.826313495635986, lamda: 1.3842531442642212

steps: 2724975, episodes: 109000, mean episode reward: -482.7762070134242, agent episode reward: [-160.92540233780804, -160.92540233780804, -160.92540233780804], time: 86.347
steps: 2724975, episodes: 109000, mean episode variance: 53.90580919575691, agent episode variance: [26.503792457580566, 26.198056564331054, 1.2039601738452912], time: 86.347
Running avgs for agent 0: q_loss: 810408.5, p_loss: 111.65325927734375, mean_rew: -6.463605911790493, variance: 106.01516983032226, lamda: 3.6849377155303955
Running avgs for agent 1: q_loss: 778415.625, p_loss: 116.14443969726562, mean_rew: -6.462582719486849, variance: 104.79222625732422, lamda: 3.6836187839508057
Running avgs for agent 2: q_loss: 4.168587684631348, p_loss: -3.3169193267822266, mean_rew: -6.466054252855554, variance: 4.815840721130371, lamda: 1.3842946290969849

steps: 2749975, episodes: 110000, mean episode reward: -491.35327778878394, agent episode reward: [-163.78442592959465, -163.78442592959465, -163.78442592959465], time: 85.264
steps: 2749975, episodes: 110000, mean episode variance: 54.64669589138031, agent episode variance: [26.669526222229003, 26.774100036621093, 1.2030696325302124], time: 85.265
Running avgs for agent 0: q_loss: 806930.1875, p_loss: 111.3384780883789, mean_rew: -6.467576177784249, variance: 106.67810488891601, lamda: 3.709912061691284
Running avgs for agent 1: q_loss: 798436.8125, p_loss: 116.07891845703125, mean_rew: -6.472623990250682, variance: 107.09640014648437, lamda: 3.7085931301116943
Running avgs for agent 2: q_loss: 4.189599990844727, p_loss: -3.312668561935425, mean_rew: -6.458857014653426, variance: 4.8122782707214355, lamda: 1.3842943906784058

steps: 2774975, episodes: 111000, mean episode reward: -482.3067586708979, agent episode reward: [-160.76891955696593, -160.76891955696593, -160.76891955696593], time: 79.763
steps: 2774975, episodes: 111000, mean episode variance: 54.67299482393265, agent episode variance: [26.79498211669922, 26.677139793395995, 1.2008729138374328], time: 79.764
Running avgs for agent 0: q_loss: 816525.4375, p_loss: 111.04984283447266, mean_rew: -6.471468703405184, variance: 107.17992846679688, lamda: 3.734886646270752
Running avgs for agent 1: q_loss: 803304.0625, p_loss: 116.05058288574219, mean_rew: -6.466525583671733, variance: 106.70855917358398, lamda: 3.733567476272583
Running avgs for agent 2: q_loss: 4.185718059539795, p_loss: -3.317213296890259, mean_rew: -6.464209146669616, variance: 4.803491592407227, lamda: 1.3842946290969849

steps: 2799975, episodes: 112000, mean episode reward: -482.56127781906036, agent episode reward: [-160.85375927302007, -160.85375927302007, -160.85375927302007], time: 77.969
steps: 2799975, episodes: 112000, mean episode variance: 54.584522787570954, agent episode variance: [26.201004829406738, 27.17957685852051, 1.2039410996437072], time: 77.97
Running avgs for agent 0: q_loss: 827338.25, p_loss: 110.77044677734375, mean_rew: -6.464150363916209, variance: 104.80401931762695, lamda: 3.7598607540130615
Running avgs for agent 1: q_loss: 811556.9375, p_loss: 116.14933013916016, mean_rew: -6.462763371488509, variance: 108.71830743408204, lamda: 3.758542060852051
Running avgs for agent 2: q_loss: 4.135194301605225, p_loss: -3.3130218982696533, mean_rew: -6.466595342658396, variance: 4.8157639503479, lamda: 1.3842946290969849

steps: 2824975, episodes: 113000, mean episode reward: -479.9774022524406, agent episode reward: [-159.99246741748019, -159.99246741748019, -159.99246741748019], time: 75.695
steps: 2824975, episodes: 113000, mean episode variance: 53.41791799521446, agent episode variance: [25.485321754455565, 26.72734236907959, 1.205253871679306], time: 75.695
Running avgs for agent 0: q_loss: 833159.9375, p_loss: 110.85679626464844, mean_rew: -6.460284758075735, variance: 101.94128701782226, lamda: 3.78483510017395
Running avgs for agent 1: q_loss: 826133.75, p_loss: 116.15274047851562, mean_rew: -6.45651454026557, variance: 106.90936947631836, lamda: 3.7835161685943604
Running avgs for agent 2: q_loss: 4.153870105743408, p_loss: -3.3084194660186768, mean_rew: -6.458081581943636, variance: 4.82101583480835, lamda: 1.3842945098876953

steps: 2849975, episodes: 114000, mean episode reward: -481.9692592421805, agent episode reward: [-160.6564197473935, -160.6564197473935, -160.6564197473935], time: 73.056
steps: 2849975, episodes: 114000, mean episode variance: 54.04311944317818, agent episode variance: [26.25786050415039, 26.588594078063966, 1.1966648609638215], time: 73.056
Running avgs for agent 0: q_loss: 830583.0, p_loss: 110.89940643310547, mean_rew: -6.448687597330145, variance: 105.03144201660156, lamda: 3.809809684753418
Running avgs for agent 1: q_loss: 828785.0, p_loss: 116.18362426757812, mean_rew: -6.457307575518322, variance: 106.35437631225587, lamda: 3.808490514755249
Running avgs for agent 2: q_loss: 4.215540885925293, p_loss: -3.312919855117798, mean_rew: -6.455083576595209, variance: 4.786659240722656, lamda: 1.3842946290969849

steps: 2874975, episodes: 115000, mean episode reward: -485.30584322527716, agent episode reward: [-161.7686144084257, -161.7686144084257, -161.7686144084257], time: 69.677
steps: 2874975, episodes: 115000, mean episode variance: 54.51053458690643, agent episode variance: [26.495801094055174, 26.812308219909667, 1.2024252729415894], time: 69.677
Running avgs for agent 0: q_loss: 835948.5625, p_loss: 110.93952178955078, mean_rew: -6.45681027592067, variance: 105.9832043762207, lamda: 3.8347837924957275
Running avgs for agent 1: q_loss: 839409.5, p_loss: 116.21134185791016, mean_rew: -6.459572076277989, variance: 107.24923287963867, lamda: 3.8334648609161377
Running avgs for agent 2: q_loss: 4.184563636779785, p_loss: -3.304727077484131, mean_rew: -6.456629611311051, variance: 4.809700965881348, lamda: 1.3842947483062744

steps: 2899975, episodes: 116000, mean episode reward: -486.5751762325453, agent episode reward: [-162.19172541084842, -162.19172541084842, -162.19172541084842], time: 71.542
steps: 2899975, episodes: 116000, mean episode variance: 53.40584492182732, agent episode variance: [25.541232780456543, 26.66040296936035, 1.2042091720104218], time: 71.542
Running avgs for agent 0: q_loss: 848557.125, p_loss: 111.11770629882812, mean_rew: -6.453579705579093, variance: 102.16493112182617, lamda: 3.8597583770751953
Running avgs for agent 1: q_loss: 846583.6875, p_loss: 116.31077575683594, mean_rew: -6.453429410682814, variance: 106.6416118774414, lamda: 3.8584389686584473
Running avgs for agent 2: q_loss: 4.162424087524414, p_loss: -3.302199602127075, mean_rew: -6.45293354088568, variance: 4.816836357116699, lamda: 1.3842946290969849

steps: 2924975, episodes: 117000, mean episode reward: -483.3185208054865, agent episode reward: [-161.10617360182886, -161.10617360182886, -161.10617360182886], time: 71.249
steps: 2924975, episodes: 117000, mean episode variance: 53.71895336079597, agent episode variance: [25.781428565979002, 26.74017317199707, 1.1973516228199006], time: 71.249
Running avgs for agent 0: q_loss: 843277.6875, p_loss: 111.2572250366211, mean_rew: -6.448773724971672, variance: 103.12571426391601, lamda: 3.884732484817505
Running avgs for agent 1: q_loss: 864410.0, p_loss: 116.35276794433594, mean_rew: -6.446625738550281, variance: 106.96069268798828, lamda: 3.883413553237915
Running avgs for agent 2: q_loss: 4.153350353240967, p_loss: -3.3054392337799072, mean_rew: -6.447876646064644, variance: 4.7894062995910645, lamda: 1.3842946290969849

steps: 2949975, episodes: 118000, mean episode reward: -487.3901097972838, agent episode reward: [-162.46336993242792, -162.46336993242792, -162.46336993242792], time: 67.792
steps: 2949975, episodes: 118000, mean episode variance: 54.70388177490234, agent episode variance: [26.402950660705567, 27.09820623779297, 1.2027248764038085], time: 67.793
Running avgs for agent 0: q_loss: 853738.5625, p_loss: 111.43521881103516, mean_rew: -6.449270276851175, variance: 105.61180264282227, lamda: 3.9097068309783936
Running avgs for agent 1: q_loss: 862522.625, p_loss: 116.35929107666016, mean_rew: -6.449870232705008, variance: 108.39282495117187, lamda: 3.9083878993988037
Running avgs for agent 2: q_loss: 4.176920413970947, p_loss: -3.299483299255371, mean_rew: -6.4455652662882965, variance: 4.810899257659912, lamda: 1.3842947483062744

steps: 2974975, episodes: 119000, mean episode reward: -483.5271942662572, agent episode reward: [-161.17573142208573, -161.17573142208573, -161.17573142208573], time: 68.454
steps: 2974975, episodes: 119000, mean episode variance: 54.0848745405674, agent episode variance: [26.140669372558595, 26.746382125854492, 1.1978230421543121], time: 68.454
Running avgs for agent 0: q_loss: 857916.8125, p_loss: 111.69709777832031, mean_rew: -6.44643660621109, variance: 104.56267749023438, lamda: 3.9346811771392822
Running avgs for agent 1: q_loss: 874453.25, p_loss: 116.37370300292969, mean_rew: -6.440483270483909, variance: 106.98552850341797, lamda: 3.9333622455596924
Running avgs for agent 2: q_loss: 4.208030700683594, p_loss: -3.2979841232299805, mean_rew: -6.446149527006957, variance: 4.791292190551758, lamda: 1.3842946290969849

steps: 2999975, episodes: 120000, mean episode reward: -490.08472055508304, agent episode reward: [-163.36157351836098, -163.36157351836098, -163.36157351836098], time: 67.645
steps: 2999975, episodes: 120000, mean episode variance: 54.4093433508873, agent episode variance: [26.482812530517577, 26.72735954284668, 1.1991712775230408], time: 67.645
Running avgs for agent 0: q_loss: 863555.6875, p_loss: 111.93180847167969, mean_rew: -6.442110471993992, variance: 105.93125012207031, lamda: 3.959655523300171
Running avgs for agent 1: q_loss: 883224.0625, p_loss: 116.52554321289062, mean_rew: -6.443903787156906, variance: 106.90943817138673, lamda: 3.958336353302002
Running avgs for agent 2: q_loss: 4.1433916091918945, p_loss: -3.29728364944458, mean_rew: -6.446294388523245, variance: 4.796685695648193, lamda: 1.3842946290969849

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -491.41336809019765, agent episode reward: [-163.8044560300659, -163.8044560300659, -163.8044560300659], time: 45.713
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 45.714
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -484.7207678879197, agent episode reward: [-161.57358929597325, -161.57358929597325, -161.57358929597325], time: 56.593
steps: 49975, episodes: 2000, mean episode variance: 49.812816231250764, agent episode variance: [0.0, 48.34764289855957, 1.4651733326911927], time: 56.594
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -6.527525174277035, variance: 0.0, lamda: 3.9721920490264893
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -6.527164444581937, variance: 198.1460723876953, lamda: 3.970874309539795
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -6.526987434609893, variance: 6.0048089027404785, lamda: 1.3842881917953491

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567761481.1092151: line 9: --exp_var_alpha: command not found
