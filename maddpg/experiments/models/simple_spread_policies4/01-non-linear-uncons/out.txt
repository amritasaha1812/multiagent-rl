# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation False --constrained \
    False --exp-name MADDPG_simple_spread_uncons_01 --save-dir \
    models/simple_spread_policies4/01-non-linear-uncons/ --plots-dir \
    models/simple_spread_policies4/01-non-linear-uncons/
Job <1084034> is submitted to queue <x86_6h>.
arglist.u_estimation False
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -521.7859368570482, agent episode reward: [-173.9286456190161, -173.9286456190161, -173.9286456190161], time: 49.572
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 49.573
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -541.332382243341, agent episode reward: [-180.44412741444702, -180.44412741444702, -180.44412741444702], time: 73.204
steps: 49975, episodes: 2000, mean episode variance: 2.3393920931890606, agent episode variance: [0.8023498412072658, 0.7474098643288016, 0.7896323876529932], time: 73.205
Running avgs for agent 0: q_loss: 2.2737245559692383, p_loss: 10.913122177124023, mean_rew: -7.171034557146562, variance: 3.2883190213412536, mean_q: -10.998209953308105, std_q: 2.531554937362671
Running avgs for agent 1: q_loss: 2.1295053958892822, p_loss: 10.466767311096191, mean_rew: -7.178670358991565, variance: 3.0631551816754166, mean_q: -10.547762870788574, std_q: 2.3837149143218994
Running avgs for agent 2: q_loss: 2.5430150032043457, p_loss: 10.336731910705566, mean_rew: -7.176844835382391, variance: 3.236198310053251, mean_q: -10.432422637939453, std_q: 2.3247900009155273

steps: 74975, episodes: 3000, mean episode reward: -452.4937290591391, agent episode reward: [-150.83124301971307, -150.83124301971307, -150.83124301971307], time: 73.103
steps: 74975, episodes: 3000, mean episode variance: 6.20381465959549, agent episode variance: [1.5794497591257095, 2.1143770881295203, 2.5099878123402597], time: 73.103
Running avgs for agent 0: q_loss: 1.1235214471817017, p_loss: 21.789024353027344, mean_rew: -6.904451293806693, variance: 6.317799036502838, mean_q: -21.970741271972656, std_q: 5.205118179321289
Running avgs for agent 1: q_loss: 1.2738640308380127, p_loss: 21.157522201538086, mean_rew: -6.905843905813626, variance: 8.457508352518081, mean_q: -21.310598373413086, std_q: 5.118166923522949
Running avgs for agent 2: q_loss: 1.3035492897033691, p_loss: 21.058162689208984, mean_rew: -6.903174291609242, variance: 10.039951249361039, mean_q: -21.19132423400879, std_q: 4.930208206176758

steps: 99975, episodes: 4000, mean episode reward: -422.0331238866641, agent episode reward: [-140.67770796222138, -140.67770796222138, -140.67770796222138], time: 71.923
steps: 99975, episodes: 4000, mean episode variance: 17.183382900714875, agent episode variance: [4.9975041890144345, 6.0390952167510985, 6.146783494949341], time: 71.924
Running avgs for agent 0: q_loss: 1.533854603767395, p_loss: 31.753732681274414, mean_rew: -6.605937333892021, variance: 19.990016756057738, mean_q: -32.052452087402344, std_q: 7.488784313201904
Running avgs for agent 1: q_loss: 1.7244741916656494, p_loss: 31.065032958984375, mean_rew: -6.599610252963416, variance: 24.156380867004394, mean_q: -31.31911277770996, std_q: 7.426646709442139
Running avgs for agent 2: q_loss: 1.652614951133728, p_loss: 31.06117820739746, mean_rew: -6.592553440689109, variance: 24.587133979797365, mean_q: -31.27033042907715, std_q: 7.199104309082031

steps: 124975, episodes: 5000, mean episode reward: -392.71987459622216, agent episode reward: [-130.9066248654074, -130.9066248654074, -130.9066248654074], time: 73.304
steps: 124975, episodes: 5000, mean episode variance: 22.5279621925354, agent episode variance: [5.684147827148437, 8.57647932434082, 8.267335041046142], time: 73.305
Running avgs for agent 0: q_loss: 1.5314375162124634, p_loss: 39.67213439941406, mean_rew: -6.3403562409615315, variance: 22.73659130859375, mean_q: -40.088077545166016, std_q: 8.907740592956543
Running avgs for agent 1: q_loss: 1.8776538372039795, p_loss: 39.073875427246094, mean_rew: -6.332365785176545, variance: 34.30591729736328, mean_q: -39.420963287353516, std_q: 9.063867568969727
Running avgs for agent 2: q_loss: 1.8260046243667603, p_loss: 39.48029708862305, mean_rew: -6.340281431728749, variance: 33.06934016418457, mean_q: -39.768131256103516, std_q: 8.905652046203613

steps: 149975, episodes: 6000, mean episode reward: -382.8048947916163, agent episode reward: [-127.60163159720543, -127.60163159720543, -127.60163159720543], time: 81.342
steps: 149975, episodes: 6000, mean episode variance: 28.808070328354834, agent episode variance: [7.385840504288673, 10.809108772277831, 10.61312105178833], time: 81.343
Running avgs for agent 0: q_loss: 1.7612242698669434, p_loss: 45.999366760253906, mean_rew: -6.121896303403661, variance: 29.543362017154692, mean_q: -46.470279693603516, std_q: 9.95643138885498
Running avgs for agent 1: q_loss: 2.0123672485351562, p_loss: 45.659122467041016, mean_rew: -6.1261513220450485, variance: 43.236435089111325, mean_q: -46.03911209106445, std_q: 10.192849159240723
Running avgs for agent 2: q_loss: 1.9768249988555908, p_loss: 46.25811767578125, mean_rew: -6.122722665795979, variance: 42.45248420715332, mean_q: -46.57249069213867, std_q: 9.984068870544434

steps: 174975, episodes: 7000, mean episode reward: -374.6471689653203, agent episode reward: [-124.88238965510676, -124.88238965510676, -124.88238965510676], time: 76.678
steps: 174975, episodes: 7000, mean episode variance: 37.586889984130856, agent episode variance: [12.413495334625244, 12.612850437164306, 12.56054421234131], time: 76.679
Running avgs for agent 0: q_loss: 2.2821154594421387, p_loss: 51.22672653198242, mean_rew: -5.962908808233892, variance: 49.65398133850098, mean_q: -51.690616607666016, std_q: 10.672141075134277
Running avgs for agent 1: q_loss: 2.120069980621338, p_loss: 51.146270751953125, mean_rew: -5.955491790920052, variance: 50.451401748657226, mean_q: -51.54182052612305, std_q: 10.749217987060547
Running avgs for agent 2: q_loss: 2.12605619430542, p_loss: 51.79536819458008, mean_rew: -5.959958034455108, variance: 50.24217684936524, mean_q: -52.13138198852539, std_q: 10.620943069458008

steps: 199975, episodes: 8000, mean episode reward: -370.4324874212571, agent episode reward: [-123.47749580708572, -123.47749580708572, -123.47749580708572], time: 75.165
steps: 199975, episodes: 8000, mean episode variance: 41.8080961227417, agent episode variance: [13.889834228515625, 13.834272724151612, 14.083989170074464], time: 75.166
Running avgs for agent 0: q_loss: 2.3226888179779053, p_loss: 55.66924285888672, mean_rew: -5.827423262820792, variance: 55.5593369140625, mean_q: -56.111785888671875, std_q: 10.97762680053711
Running avgs for agent 1: q_loss: 2.190311908721924, p_loss: 55.88138198852539, mean_rew: -5.827559572629433, variance: 55.33709089660645, mean_q: -56.27229309082031, std_q: 11.072964668273926
Running avgs for agent 2: q_loss: 2.2298102378845215, p_loss: 56.41835021972656, mean_rew: -5.826352636502, variance: 56.335956680297855, mean_q: -56.77606201171875, std_q: 11.164795875549316

steps: 224975, episodes: 9000, mean episode reward: -369.2644116776653, agent episode reward: [-123.08813722588843, -123.08813722588843, -123.08813722588843], time: 72.023
steps: 224975, episodes: 9000, mean episode variance: 45.21296702575684, agent episode variance: [14.667108863830567, 15.33399393081665, 15.211864231109619], time: 72.024
Running avgs for agent 0: q_loss: 2.376420021057129, p_loss: 59.41857147216797, mean_rew: -5.71584605890206, variance: 58.66843545532227, mean_q: -59.82948303222656, std_q: 11.055047035217285
Running avgs for agent 1: q_loss: 2.272217035293579, p_loss: 59.85939407348633, mean_rew: -5.718471163594104, variance: 61.3359757232666, mean_q: -60.24396514892578, std_q: 11.259661674499512
Running avgs for agent 2: q_loss: 2.325584888458252, p_loss: 60.33711242675781, mean_rew: -5.7196814877623074, variance: 60.847456924438475, mean_q: -60.699710845947266, std_q: 11.535780906677246

steps: 249975, episodes: 10000, mean episode reward: -371.25380048513665, agent episode reward: [-123.75126682837886, -123.75126682837886, -123.75126682837886], time: 74.176
steps: 249975, episodes: 10000, mean episode variance: 48.98526037216187, agent episode variance: [15.971044593811035, 16.646459274291992, 16.36775650405884], time: 74.177
Running avgs for agent 0: q_loss: 2.3965322971343994, p_loss: 62.83143997192383, mean_rew: -5.638853369306932, variance: 63.88417837524414, mean_q: -63.216976165771484, std_q: 11.19587516784668
Running avgs for agent 1: q_loss: 2.3168227672576904, p_loss: 63.40501403808594, mean_rew: -5.6428845092457465, variance: 66.58583709716797, mean_q: -63.77549362182617, std_q: 11.507918357849121
Running avgs for agent 2: q_loss: 2.4191184043884277, p_loss: 63.727088928222656, mean_rew: -5.644262992027281, variance: 65.47102601623536, mean_q: -64.08815002441406, std_q: 11.64880084991455

steps: 274975, episodes: 11000, mean episode reward: -366.51517841078834, agent episode reward: [-122.17172613692945, -122.17172613692945, -122.17172613692945], time: 74.676
steps: 274975, episodes: 11000, mean episode variance: 50.872396354675296, agent episode variance: [16.060866672515868, 17.46508393096924, 17.346445751190185], time: 74.676
Running avgs for agent 0: q_loss: 2.39091420173645, p_loss: 65.86784362792969, mean_rew: -5.572063637177657, variance: 64.24346669006347, mean_q: -66.2295150756836, std_q: 11.231069564819336
Running avgs for agent 1: q_loss: 2.3344767093658447, p_loss: 66.36602783203125, mean_rew: -5.572086810069801, variance: 69.86033572387696, mean_q: -66.71884155273438, std_q: 11.434874534606934
Running avgs for agent 2: q_loss: 2.4397168159484863, p_loss: 66.6279525756836, mean_rew: -5.565997848331452, variance: 69.38578300476074, mean_q: -66.97957611083984, std_q: 11.602753639221191

steps: 299975, episodes: 12000, mean episode reward: -364.829491456469, agent episode reward: [-121.60983048548965, -121.60983048548965, -121.60983048548965], time: 74.833
steps: 299975, episodes: 12000, mean episode variance: 52.353880798339844, agent episode variance: [16.461356735229494, 17.90807225036621, 17.98445181274414], time: 74.834
Running avgs for agent 0: q_loss: 2.387986183166504, p_loss: 68.4469223022461, mean_rew: -5.512892059762938, variance: 65.84542694091797, mean_q: -68.79093933105469, std_q: 11.20139217376709
Running avgs for agent 1: q_loss: 2.3798301219940186, p_loss: 68.91873931884766, mean_rew: -5.508521739043772, variance: 71.63228900146484, mean_q: -69.26676940917969, std_q: 11.257950782775879
Running avgs for agent 2: q_loss: 2.451704502105713, p_loss: 69.11314392089844, mean_rew: -5.506321563578863, variance: 71.93780725097656, mean_q: -69.44993591308594, std_q: 11.436135292053223

steps: 324975, episodes: 13000, mean episode reward: -366.4191485873222, agent episode reward: [-122.13971619577406, -122.13971619577406, -122.13971619577406], time: 74.308
steps: 324975, episodes: 13000, mean episode variance: 55.7276526184082, agent episode variance: [17.40959073638916, 19.33781005859375, 18.98025182342529], time: 74.309
Running avgs for agent 0: q_loss: 2.4117202758789062, p_loss: 70.64159393310547, mean_rew: -5.460233013907182, variance: 69.63836294555664, mean_q: -70.96768951416016, std_q: 11.010977745056152
Running avgs for agent 1: q_loss: 2.367788076400757, p_loss: 71.19267272949219, mean_rew: -5.4618606059674715, variance: 77.351240234375, mean_q: -71.53221893310547, std_q: 11.288471221923828
Running avgs for agent 2: q_loss: 2.456986904144287, p_loss: 71.25615692138672, mean_rew: -5.454611013722061, variance: 75.92100729370117, mean_q: -71.58898162841797, std_q: 11.274908065795898

steps: 349975, episodes: 14000, mean episode reward: -366.52215431244525, agent episode reward: [-122.17405143748174, -122.17405143748174, -122.17405143748174], time: 74.522
steps: 349975, episodes: 14000, mean episode variance: 55.940497791290284, agent episode variance: [18.004447284698486, 19.062401863098145, 18.873648643493652], time: 74.523
Running avgs for agent 0: q_loss: 2.3847060203552246, p_loss: 72.65093994140625, mean_rew: -5.419852430677305, variance: 72.01778913879394, mean_q: -72.96135711669922, std_q: 10.866399765014648
Running avgs for agent 1: q_loss: 2.360177993774414, p_loss: 73.20686340332031, mean_rew: -5.417783510042472, variance: 76.24960745239258, mean_q: -73.54460906982422, std_q: 11.038671493530273
Running avgs for agent 2: q_loss: 2.450958728790283, p_loss: 73.24528503417969, mean_rew: -5.4177184807155445, variance: 75.49459457397461, mean_q: -73.5596923828125, std_q: 11.058662414550781

steps: 374975, episodes: 15000, mean episode reward: -363.19579928415607, agent episode reward: [-121.06526642805203, -121.06526642805203, -121.06526642805203], time: 74.269
steps: 374975, episodes: 15000, mean episode variance: 57.252654972076414, agent episode variance: [18.113836486816407, 20.108953674316407, 19.029864810943604], time: 74.27
Running avgs for agent 0: q_loss: 2.35676908493042, p_loss: 74.33181762695312, mean_rew: -5.3791728282181515, variance: 72.45534594726563, mean_q: -74.63147735595703, std_q: 10.679216384887695
Running avgs for agent 1: q_loss: 2.3574540615081787, p_loss: 74.84689331054688, mean_rew: -5.379582083575307, variance: 80.43581469726563, mean_q: -75.18051147460938, std_q: 10.829646110534668
Running avgs for agent 2: q_loss: 2.474517583847046, p_loss: 74.9362564086914, mean_rew: -5.3763841278736635, variance: 76.11945924377441, mean_q: -75.24613952636719, std_q: 10.980152130126953

steps: 399975, episodes: 16000, mean episode reward: -365.3367553140204, agent episode reward: [-121.77891843800681, -121.77891843800681, -121.77891843800681], time: 75.813
steps: 399975, episodes: 16000, mean episode variance: 59.52130569458008, agent episode variance: [19.2506644821167, 20.27631433105469, 19.994326881408693], time: 75.813
Running avgs for agent 0: q_loss: 2.3710618019104004, p_loss: 75.72966003417969, mean_rew: -5.348397623550432, variance: 77.0026579284668, mean_q: -76.0114517211914, std_q: 10.650504112243652
Running avgs for agent 1: q_loss: 2.3475229740142822, p_loss: 76.27545928955078, mean_rew: -5.349444325176309, variance: 81.10525732421875, mean_q: -76.6023941040039, std_q: 10.734638214111328
Running avgs for agent 2: q_loss: 2.4259462356567383, p_loss: 76.2783432006836, mean_rew: -5.3421052443731245, variance: 79.97730752563477, mean_q: -76.58343505859375, std_q: 10.761571884155273

steps: 424975, episodes: 17000, mean episode reward: -364.1881086293943, agent episode reward: [-121.3960362097981, -121.3960362097981, -121.3960362097981], time: 74.931
steps: 424975, episodes: 17000, mean episode variance: 59.06691764831543, agent episode variance: [18.79597576904297, 20.17058794403076, 20.1003539352417], time: 74.931
Running avgs for agent 0: q_loss: 2.3527841567993164, p_loss: 76.97847747802734, mean_rew: -5.31265631044479, variance: 75.18390307617187, mean_q: -77.25187683105469, std_q: 10.407142639160156
Running avgs for agent 1: q_loss: 2.347454071044922, p_loss: 77.55506896972656, mean_rew: -5.318134815911995, variance: 80.68235177612304, mean_q: -77.87699890136719, std_q: 10.60295581817627
Running avgs for agent 2: q_loss: 2.440965414047241, p_loss: 77.49134826660156, mean_rew: -5.322906283717019, variance: 80.4014157409668, mean_q: -77.7989273071289, std_q: 10.7572660446167

steps: 449975, episodes: 18000, mean episode reward: -363.59565777643724, agent episode reward: [-121.19855259214575, -121.19855259214575, -121.19855259214575], time: 75.795
steps: 449975, episodes: 18000, mean episode variance: 59.596045959472654, agent episode variance: [18.854429481506347, 20.789227935791015, 19.952388542175292], time: 75.796
Running avgs for agent 0: q_loss: 2.3481483459472656, p_loss: 78.11187744140625, mean_rew: -5.289655553342008, variance: 75.41771792602539, mean_q: -78.3781509399414, std_q: 10.379660606384277
Running avgs for agent 1: q_loss: 2.3757474422454834, p_loss: 78.49642944335938, mean_rew: -5.287938486184305, variance: 83.15691174316406, mean_q: -78.81826782226562, std_q: 10.413765907287598
Running avgs for agent 2: q_loss: 2.4627528190612793, p_loss: 78.4673843383789, mean_rew: -5.288279675955735, variance: 79.80955416870117, mean_q: -78.7629623413086, std_q: 10.620416641235352

steps: 474975, episodes: 19000, mean episode reward: -360.6230780594358, agent episode reward: [-120.2076926864786, -120.2076926864786, -120.2076926864786], time: 73.912
steps: 474975, episodes: 19000, mean episode variance: 60.858019073486325, agent episode variance: [19.622682754516603, 20.55956378173828, 20.675772537231445], time: 73.913
Running avgs for agent 0: q_loss: 2.368927478790283, p_loss: 78.98507690429688, mean_rew: -5.262893329420281, variance: 78.49073101806641, mean_q: -79.24632263183594, std_q: 10.11521053314209
Running avgs for agent 1: q_loss: 2.3546035289764404, p_loss: 79.43428039550781, mean_rew: -5.271230928637658, variance: 82.23825512695312, mean_q: -79.75951385498047, std_q: 10.399373054504395
Running avgs for agent 2: q_loss: 2.431929111480713, p_loss: 79.27134704589844, mean_rew: -5.2667130188648805, variance: 82.70309014892578, mean_q: -79.56681060791016, std_q: 10.483428955078125

steps: 499975, episodes: 20000, mean episode reward: -359.63916080229154, agent episode reward: [-119.8797202674305, -119.8797202674305, -119.8797202674305], time: 73.546
steps: 499975, episodes: 20000, mean episode variance: 61.06802988433838, agent episode variance: [19.81719049835205, 20.82638414001465, 20.42445524597168], time: 73.547
Running avgs for agent 0: q_loss: 2.3451502323150635, p_loss: 79.80323791503906, mean_rew: -5.244748549128598, variance: 79.2687619934082, mean_q: -80.0600814819336, std_q: 10.088397979736328
Running avgs for agent 1: q_loss: 2.345937490463257, p_loss: 80.15959930419922, mean_rew: -5.245801779539213, variance: 83.3055365600586, mean_q: -80.48090362548828, std_q: 10.150992393493652
Running avgs for agent 2: q_loss: 2.452232837677002, p_loss: 80.00180053710938, mean_rew: -5.240663472966458, variance: 81.69782098388671, mean_q: -80.29154205322266, std_q: 10.232050895690918

steps: 524975, episodes: 21000, mean episode reward: -359.4241288253474, agent episode reward: [-119.80804294178245, -119.80804294178245, -119.80804294178245], time: 79.353
steps: 524975, episodes: 21000, mean episode variance: 60.635491539001464, agent episode variance: [19.455372085571287, 20.743196014404298, 20.43692343902588], time: 79.354
Running avgs for agent 0: q_loss: 2.328857421875, p_loss: 80.45406341552734, mean_rew: -5.222518998930699, variance: 77.82148834228515, mean_q: -80.71045684814453, std_q: 9.9439697265625
Running avgs for agent 1: q_loss: 2.339597702026367, p_loss: 80.71514129638672, mean_rew: -5.216477654312522, variance: 82.97278405761719, mean_q: -81.03306579589844, std_q: 9.868712425231934
Running avgs for agent 2: q_loss: 2.440556526184082, p_loss: 80.61323547363281, mean_rew: -5.225107316794607, variance: 81.74769375610352, mean_q: -80.89776611328125, std_q: 10.271987915039062

steps: 549975, episodes: 22000, mean episode reward: -355.56024408945245, agent episode reward: [-118.5200813631508, -118.5200813631508, -118.5200813631508], time: 91.175
steps: 549975, episodes: 22000, mean episode variance: 61.77835099029541, agent episode variance: [19.637441719055175, 21.492031204223633, 20.6488780670166], time: 91.176
Running avgs for agent 0: q_loss: 2.3306596279144287, p_loss: 81.00548553466797, mean_rew: -5.201860880638678, variance: 78.5497668762207, mean_q: -81.2554702758789, std_q: 9.787896156311035
Running avgs for agent 1: q_loss: 2.317505359649658, p_loss: 81.1051254272461, mean_rew: -5.196874439110084, variance: 85.96812481689453, mean_q: -81.42261505126953, std_q: 9.754673957824707
Running avgs for agent 2: q_loss: 2.411849021911621, p_loss: 81.03768157958984, mean_rew: -5.196416278966148, variance: 82.5955122680664, mean_q: -81.32404327392578, std_q: 9.999869346618652

steps: 574975, episodes: 23000, mean episode reward: -356.9435841262612, agent episode reward: [-118.98119470875373, -118.98119470875373, -118.98119470875373], time: 91.795
steps: 574975, episodes: 23000, mean episode variance: 60.67954550933838, agent episode variance: [19.577513130187988, 20.598901718139647, 20.503130661010744], time: 91.795
Running avgs for agent 0: q_loss: 2.2914836406707764, p_loss: 81.39989471435547, mean_rew: -5.180479293416137, variance: 78.31005252075195, mean_q: -81.64887237548828, std_q: 9.648263931274414
Running avgs for agent 1: q_loss: 2.3067684173583984, p_loss: 81.59516906738281, mean_rew: -5.181802301711201, variance: 82.39560687255859, mean_q: -81.91358184814453, std_q: 9.649619102478027
Running avgs for agent 2: q_loss: 2.366166114807129, p_loss: 81.4496841430664, mean_rew: -5.181946439308841, variance: 82.01252264404297, mean_q: -81.73793029785156, std_q: 10.02876091003418

steps: 599975, episodes: 24000, mean episode reward: -357.20721256944165, agent episode reward: [-119.06907085648055, -119.06907085648055, -119.06907085648055], time: 93.042
steps: 599975, episodes: 24000, mean episode variance: 60.27166357421875, agent episode variance: [19.405660469055174, 20.3694214553833, 20.496581649780275], time: 93.042
Running avgs for agent 0: q_loss: 2.30787992477417, p_loss: 81.78121948242188, mean_rew: -5.165017622632841, variance: 77.6226418762207, mean_q: -82.02755737304688, std_q: 9.612018585205078
Running avgs for agent 1: q_loss: 2.257427215576172, p_loss: 81.9795913696289, mean_rew: -5.165619048744646, variance: 81.4776858215332, mean_q: -82.29170227050781, std_q: 9.53455924987793
Running avgs for agent 2: q_loss: 2.359908103942871, p_loss: 81.7101821899414, mean_rew: -5.1611511163212365, variance: 81.9863265991211, mean_q: -81.9964828491211, std_q: 9.735208511352539

steps: 624975, episodes: 25000, mean episode reward: -352.54007647985753, agent episode reward: [-117.51335882661918, -117.51335882661918, -117.51335882661918], time: 90.032
steps: 624975, episodes: 25000, mean episode variance: 60.038340377807614, agent episode variance: [19.334085136413574, 20.007749504089354, 20.696505737304687], time: 90.033
Running avgs for agent 0: q_loss: 2.2701315879821777, p_loss: 82.05807495117188, mean_rew: -5.1448087743094835, variance: 77.3363405456543, mean_q: -82.2994613647461, std_q: 9.366564750671387
Running avgs for agent 1: q_loss: 2.233067274093628, p_loss: 82.24345397949219, mean_rew: -5.143334195190731, variance: 80.03099801635742, mean_q: -82.5498275756836, std_q: 9.400092124938965
Running avgs for agent 2: q_loss: 2.3273730278015137, p_loss: 81.95987701416016, mean_rew: -5.143728443832278, variance: 82.78602294921875, mean_q: -82.24351501464844, std_q: 9.661624908447266

steps: 649975, episodes: 26000, mean episode reward: -355.6569060687696, agent episode reward: [-118.5523020229232, -118.5523020229232, -118.5523020229232], time: 90.813
steps: 649975, episodes: 26000, mean episode variance: 60.66471788787842, agent episode variance: [19.55130750274658, 20.594118194580076, 20.51929219055176], time: 90.814
Running avgs for agent 0: q_loss: 2.224788188934326, p_loss: 82.26872253417969, mean_rew: -5.127534922820922, variance: 78.20523001098633, mean_q: -82.50656127929688, std_q: 9.183340072631836
Running avgs for agent 1: q_loss: 2.2094295024871826, p_loss: 82.44717407226562, mean_rew: -5.129282790597685, variance: 82.3764727783203, mean_q: -82.7445297241211, std_q: 9.380013465881348
Running avgs for agent 2: q_loss: 2.2931816577911377, p_loss: 82.13410949707031, mean_rew: -5.126371662277937, variance: 82.07716876220704, mean_q: -82.41392517089844, std_q: 9.404742240905762

steps: 674975, episodes: 27000, mean episode reward: -355.070866374881, agent episode reward: [-118.35695545829367, -118.35695545829367, -118.35695545829367], time: 90.418
steps: 674975, episodes: 27000, mean episode variance: 58.95483731842041, agent episode variance: [19.16997629547119, 19.40476678466797, 20.38009423828125], time: 90.418
Running avgs for agent 0: q_loss: 2.2250115871429443, p_loss: 82.47168731689453, mean_rew: -5.114279945464085, variance: 76.67990518188476, mean_q: -82.70773315429688, std_q: 9.126546859741211
Running avgs for agent 1: q_loss: 2.1579530239105225, p_loss: 82.67304992675781, mean_rew: -5.109984136416022, variance: 77.61906713867188, mean_q: -82.96488952636719, std_q: 9.284564018249512
Running avgs for agent 2: q_loss: 2.2541863918304443, p_loss: 82.36685943603516, mean_rew: -5.120514183390996, variance: 81.520376953125, mean_q: -82.64389038085938, std_q: 9.374820709228516

steps: 699975, episodes: 28000, mean episode reward: -351.54666815765404, agent episode reward: [-117.18222271921803, -117.18222271921803, -117.18222271921803], time: 91.488
steps: 699975, episodes: 28000, mean episode variance: 59.3368796081543, agent episode variance: [19.288027305603027, 19.8445743560791, 20.204277946472168], time: 91.489
Running avgs for agent 0: q_loss: 2.207085609436035, p_loss: 82.52568817138672, mean_rew: -5.095791376878606, variance: 77.15210922241211, mean_q: -82.76821899414062, std_q: 8.933972358703613
Running avgs for agent 1: q_loss: 2.1165921688079834, p_loss: 82.7749252319336, mean_rew: -5.097119492740855, variance: 79.3782974243164, mean_q: -83.06099700927734, std_q: 9.145336151123047
Running avgs for agent 2: q_loss: 2.2512705326080322, p_loss: 82.46101379394531, mean_rew: -5.099486289919475, variance: 80.81711178588867, mean_q: -82.73175811767578, std_q: 9.280292510986328

steps: 724975, episodes: 29000, mean episode reward: -353.7214462866729, agent episode reward: [-117.9071487622243, -117.9071487622243, -117.9071487622243], time: 91.551
steps: 724975, episodes: 29000, mean episode variance: 57.86783731842041, agent episode variance: [18.884359619140625, 18.654127723693847, 20.329349975585938], time: 91.551
Running avgs for agent 0: q_loss: 2.1873292922973633, p_loss: 82.6921615600586, mean_rew: -5.089194131257582, variance: 75.5374384765625, mean_q: -82.93150329589844, std_q: 8.868278503417969
Running avgs for agent 1: q_loss: 2.0988516807556152, p_loss: 83.00270080566406, mean_rew: -5.088658864490334, variance: 74.61651089477539, mean_q: -83.27725219726562, std_q: 9.08436107635498
Running avgs for agent 2: q_loss: 2.2034873962402344, p_loss: 82.56504821777344, mean_rew: -5.08816261565514, variance: 81.31739990234375, mean_q: -82.83596801757812, std_q: 9.13223648071289

steps: 749975, episodes: 30000, mean episode reward: -351.78287543437364, agent episode reward: [-117.26095847812455, -117.26095847812455, -117.26095847812455], time: 92.089
steps: 749975, episodes: 30000, mean episode variance: 56.596095024108884, agent episode variance: [18.326409729003906, 18.652002128601076, 19.617683166503905], time: 92.09
Running avgs for agent 0: q_loss: 2.1470589637756348, p_loss: 82.79923248291016, mean_rew: -5.072282262467723, variance: 73.30563891601562, mean_q: -83.04045867919922, std_q: 8.721320152282715
Running avgs for agent 1: q_loss: 2.059762477874756, p_loss: 83.13159942626953, mean_rew: -5.06819104044421, variance: 74.6080085144043, mean_q: -83.39739227294922, std_q: 8.86327838897705
Running avgs for agent 2: q_loss: 2.1966986656188965, p_loss: 82.65996551513672, mean_rew: -5.071020660271348, variance: 78.47073266601562, mean_q: -82.92102813720703, std_q: 8.911138534545898

steps: 774975, episodes: 31000, mean episode reward: -354.9104579563898, agent episode reward: [-118.30348598546328, -118.30348598546328, -118.30348598546328], time: 91.18
steps: 774975, episodes: 31000, mean episode variance: 57.33206777954101, agent episode variance: [18.56687725830078, 19.03324658203125, 19.731943939208985], time: 91.181
Running avgs for agent 0: q_loss: 2.160245180130005, p_loss: 82.84709930419922, mean_rew: -5.060121137688418, variance: 74.26750903320313, mean_q: -83.08247375488281, std_q: 8.756515502929688
Running avgs for agent 1: q_loss: 2.002194404602051, p_loss: 83.18229675292969, mean_rew: -5.061088050710106, variance: 76.132986328125, mean_q: -83.44763946533203, std_q: 8.743022918701172
Running avgs for agent 2: q_loss: 2.130727529525757, p_loss: 82.7310791015625, mean_rew: -5.057080371836901, variance: 78.92777575683594, mean_q: -82.9878921508789, std_q: 8.608674049377441

steps: 799975, episodes: 32000, mean episode reward: -351.1205026014141, agent episode reward: [-117.04016753380469, -117.04016753380469, -117.04016753380469], time: 97.887
steps: 799975, episodes: 32000, mean episode variance: 56.812784774780276, agent episode variance: [18.704894416809083, 18.556409034729004, 19.551481323242186], time: 97.888
Running avgs for agent 0: q_loss: 2.139017105102539, p_loss: 82.88713836669922, mean_rew: -5.049988945565305, variance: 74.81957766723633, mean_q: -83.11979675292969, std_q: 8.522359848022461
Running avgs for agent 1: q_loss: 1.9938117265701294, p_loss: 83.23584747314453, mean_rew: -5.05137009928234, variance: 74.22563613891602, mean_q: -83.49978637695312, std_q: 8.655138969421387
Running avgs for agent 2: q_loss: 2.147284746170044, p_loss: 82.84376525878906, mean_rew: -5.053555966884823, variance: 78.20592529296874, mean_q: -83.09312438964844, std_q: 8.632966041564941

steps: 824975, episodes: 33000, mean episode reward: -353.0715590351949, agent episode reward: [-117.69051967839832, -117.69051967839832, -117.69051967839832], time: 93.109
steps: 824975, episodes: 33000, mean episode variance: 56.10397607040405, agent episode variance: [18.4059239654541, 18.05335467147827, 19.64469743347168], time: 93.11
Running avgs for agent 0: q_loss: 2.127872943878174, p_loss: 82.90486145019531, mean_rew: -5.039707425705043, variance: 73.6236958618164, mean_q: -83.1347427368164, std_q: 8.462517738342285
Running avgs for agent 1: q_loss: 1.964435338973999, p_loss: 83.22978210449219, mean_rew: -5.0412822739123975, variance: 72.21341868591308, mean_q: -83.49409484863281, std_q: 8.619770050048828
Running avgs for agent 2: q_loss: 2.095853090286255, p_loss: 82.8746109008789, mean_rew: -5.040883230536001, variance: 78.57878973388672, mean_q: -83.11407470703125, std_q: 8.446166038513184

steps: 849975, episodes: 34000, mean episode reward: -350.19637460024114, agent episode reward: [-116.73212486674703, -116.73212486674703, -116.73212486674703], time: 90.682
steps: 849975, episodes: 34000, mean episode variance: 56.20563256835938, agent episode variance: [18.377584114074708, 17.984493019104004, 19.843555435180665], time: 90.683
Running avgs for agent 0: q_loss: 2.082026481628418, p_loss: 82.88473510742188, mean_rew: -5.026352081461318, variance: 73.51033645629883, mean_q: -83.11517333984375, std_q: 8.432328224182129
Running avgs for agent 1: q_loss: 1.9301170110702515, p_loss: 83.18588256835938, mean_rew: -5.027343590307174, variance: 71.93797207641602, mean_q: -83.44541931152344, std_q: 8.455018997192383
Running avgs for agent 2: q_loss: 2.0428576469421387, p_loss: 82.78509521484375, mean_rew: -5.027399787411962, variance: 79.37422174072266, mean_q: -83.02104949951172, std_q: 8.275226593017578

steps: 874975, episodes: 35000, mean episode reward: -352.01291734471323, agent episode reward: [-117.3376391149044, -117.3376391149044, -117.3376391149044], time: 90.947
steps: 874975, episodes: 35000, mean episode variance: 55.76263028335571, agent episode variance: [18.28899182510376, 18.07106185913086, 19.402576599121094], time: 90.947
Running avgs for agent 0: q_loss: 2.063182830810547, p_loss: 82.90593719482422, mean_rew: -5.022234176167839, variance: 73.15596730041504, mean_q: -83.1356430053711, std_q: 8.331110954284668
Running avgs for agent 1: q_loss: 1.8786683082580566, p_loss: 83.09960174560547, mean_rew: -5.0140478641357085, variance: 72.28424743652344, mean_q: -83.35834503173828, std_q: 8.288861274719238
Running avgs for agent 2: q_loss: 2.011793613433838, p_loss: 82.74309539794922, mean_rew: -5.018383372138563, variance: 77.61030639648438, mean_q: -82.98039245605469, std_q: 8.333542823791504

steps: 899975, episodes: 36000, mean episode reward: -350.0825126300494, agent episode reward: [-116.69417087668312, -116.69417087668312, -116.69417087668312], time: 89.952
steps: 899975, episodes: 36000, mean episode variance: 53.58769983291626, agent episode variance: [18.16485932159424, 16.8306376953125, 18.59220281600952], time: 89.953
Running avgs for agent 0: q_loss: 2.0850093364715576, p_loss: 82.7806625366211, mean_rew: -5.006063087031265, variance: 72.65943728637696, mean_q: -83.00582885742188, std_q: 8.281351089477539
Running avgs for agent 1: q_loss: 1.856426477432251, p_loss: 83.10453033447266, mean_rew: -5.009617987165724, variance: 67.32255078125, mean_q: -83.36302947998047, std_q: 8.332169532775879
Running avgs for agent 2: q_loss: 2.029263734817505, p_loss: 82.70420837402344, mean_rew: -5.009963516602707, variance: 74.36881126403809, mean_q: -82.9409408569336, std_q: 8.285144805908203

steps: 924975, episodes: 37000, mean episode reward: -352.01796024525913, agent episode reward: [-117.33932008175304, -117.33932008175304, -117.33932008175304], time: 136.098
steps: 924975, episodes: 37000, mean episode variance: 54.568423587799074, agent episode variance: [18.529829376220704, 16.935533191680907, 19.10306101989746], time: 136.098
Running avgs for agent 0: q_loss: 2.0482494831085205, p_loss: 82.69786071777344, mean_rew: -5.000582518206014, variance: 74.11931750488282, mean_q: -82.91995239257812, std_q: 8.255361557006836
Running avgs for agent 1: q_loss: 1.852769374847412, p_loss: 83.04364013671875, mean_rew: -4.997695782049156, variance: 67.74213276672363, mean_q: -83.29618072509766, std_q: 8.216779708862305
Running avgs for agent 2: q_loss: 1.969661831855774, p_loss: 82.59934997558594, mean_rew: -5.0022157021629665, variance: 76.41224407958984, mean_q: -82.83641815185547, std_q: 8.26235580444336

steps: 949975, episodes: 38000, mean episode reward: -348.81096962019694, agent episode reward: [-116.27032320673231, -116.27032320673231, -116.27032320673231], time: 125.874
steps: 949975, episodes: 38000, mean episode variance: 53.49686131286621, agent episode variance: [18.06820426940918, 16.886433265686033, 18.542223777770996], time: 125.875
Running avgs for agent 0: q_loss: 2.0297763347625732, p_loss: 82.63197326660156, mean_rew: -4.991889432226348, variance: 72.27281707763672, mean_q: -82.8554916381836, std_q: 8.145893096923828
Running avgs for agent 1: q_loss: 1.8237448930740356, p_loss: 83.00296783447266, mean_rew: -4.99113119355768, variance: 67.54573306274413, mean_q: -83.25344848632812, std_q: 8.158677101135254
Running avgs for agent 2: q_loss: 1.9462273120880127, p_loss: 82.48121643066406, mean_rew: -4.9929849243982884, variance: 74.16889511108398, mean_q: -82.718505859375, std_q: 8.093920707702637

steps: 974975, episodes: 39000, mean episode reward: -350.3599124386109, agent episode reward: [-116.78663747953696, -116.78663747953696, -116.78663747953696], time: 91.981
steps: 974975, episodes: 39000, mean episode variance: 52.821770690917965, agent episode variance: [17.858917137145998, 16.688069858551025, 18.274783695220947], time: 91.981
Running avgs for agent 0: q_loss: 2.0041728019714355, p_loss: 82.56890869140625, mean_rew: -4.978809334301966, variance: 71.43566854858399, mean_q: -82.78885650634766, std_q: 8.181571006774902
Running avgs for agent 1: q_loss: 1.8249174356460571, p_loss: 82.93907928466797, mean_rew: -4.9831063957612045, variance: 66.7522794342041, mean_q: -83.1891098022461, std_q: 8.019731521606445
Running avgs for agent 2: q_loss: 1.9477949142456055, p_loss: 82.41326904296875, mean_rew: -4.984476876078734, variance: 73.09913478088379, mean_q: -82.64238739013672, std_q: 8.005841255187988

steps: 999975, episodes: 40000, mean episode reward: -351.4580667045667, agent episode reward: [-117.15268890152225, -117.15268890152225, -117.15268890152225], time: 91.11
steps: 999975, episodes: 40000, mean episode variance: 52.72477978134155, agent episode variance: [17.91201025390625, 16.526150966644288, 18.286618560791016], time: 91.111
Running avgs for agent 0: q_loss: 1.986255168914795, p_loss: 82.51143646240234, mean_rew: -4.975973217421346, variance: 71.648041015625, mean_q: -82.72715759277344, std_q: 8.0678129196167
Running avgs for agent 1: q_loss: 1.7762240171432495, p_loss: 82.90396881103516, mean_rew: -4.978965801155119, variance: 66.10460386657715, mean_q: -83.15277862548828, std_q: 8.01219654083252
Running avgs for agent 2: q_loss: 1.9092929363250732, p_loss: 82.27557373046875, mean_rew: -4.9755772681035415, variance: 73.14647424316406, mean_q: -82.505126953125, std_q: 7.870341777801514

steps: 1024975, episodes: 41000, mean episode reward: -346.69554438534806, agent episode reward: [-115.56518146178269, -115.56518146178269, -115.56518146178269], time: 91.237
steps: 1024975, episodes: 41000, mean episode variance: 51.699055629730225, agent episode variance: [17.46988341140747, 16.034414802551268, 18.194757415771484], time: 91.238
Running avgs for agent 0: q_loss: 1.9936407804489136, p_loss: 82.3454360961914, mean_rew: -4.940625102734452, variance: 69.87953364562988, mean_q: -82.54740905761719, std_q: 7.783964157104492
Running avgs for agent 1: q_loss: 1.7877427339553833, p_loss: 82.7049331665039, mean_rew: -4.9431558063135785, variance: 64.13765921020507, mean_q: -82.94551086425781, std_q: 7.832749843597412
Running avgs for agent 2: q_loss: 1.8844646215438843, p_loss: 81.97638702392578, mean_rew: -4.941258947117192, variance: 72.77902966308594, mean_q: -82.18775177001953, std_q: 7.835031032562256

steps: 1049975, episodes: 42000, mean episode reward: -349.82631504040955, agent episode reward: [-116.6087716801365, -116.6087716801365, -116.6087716801365], time: 92.294
steps: 1049975, episodes: 42000, mean episode variance: 50.52949283599853, agent episode variance: [17.40346304321289, 15.721502906799316, 17.40452688598633], time: 92.295
Running avgs for agent 0: q_loss: 1.8560527563095093, p_loss: 81.86880493164062, mean_rew: -4.874336414444273, variance: 69.61385217285157, mean_q: -82.04815673828125, std_q: 6.650763988494873
Running avgs for agent 1: q_loss: 1.693994402885437, p_loss: 82.2110595703125, mean_rew: -4.878518677026337, variance: 62.88601162719726, mean_q: -82.42778015136719, std_q: 6.822887897491455
Running avgs for agent 2: q_loss: 1.7819362878799438, p_loss: 81.48571014404297, mean_rew: -4.879009694920817, variance: 69.61810754394531, mean_q: -81.66796875, std_q: 6.7257890701293945

steps: 1074975, episodes: 43000, mean episode reward: -348.7105161045283, agent episode reward: [-116.23683870150943, -116.23683870150943, -116.23683870150943], time: 91.381
steps: 1074975, episodes: 43000, mean episode variance: 50.03257249450684, agent episode variance: [16.969643981933594, 15.836308403015137, 17.226620109558105], time: 91.382
Running avgs for agent 0: q_loss: 1.8078618049621582, p_loss: 81.6142807006836, mean_rew: -4.831336336465668, variance: 67.87857592773437, mean_q: -81.7724838256836, std_q: 6.161721706390381
Running avgs for agent 1: q_loss: 1.6255769729614258, p_loss: 81.90535736083984, mean_rew: -4.835764648780805, variance: 63.34523361206055, mean_q: -82.10323333740234, std_q: 6.390900611877441
Running avgs for agent 2: q_loss: 1.7109826803207397, p_loss: 81.21284484863281, mean_rew: -4.831202630057123, variance: 68.90648043823242, mean_q: -81.37372589111328, std_q: 6.298328399658203

steps: 1099975, episodes: 44000, mean episode reward: -350.19024127126806, agent episode reward: [-116.73008042375604, -116.73008042375604, -116.73008042375604], time: 91.731
steps: 1099975, episodes: 44000, mean episode variance: 49.89118748092651, agent episode variance: [17.053983451843262, 15.586745693206787, 17.250458335876466], time: 91.732
Running avgs for agent 0: q_loss: 1.7544547319412231, p_loss: 81.49834442138672, mean_rew: -4.802578438569884, variance: 68.21593380737305, mean_q: -81.64644622802734, std_q: 6.11799430847168
Running avgs for agent 1: q_loss: 1.5942885875701904, p_loss: 81.70006561279297, mean_rew: -4.805030548425503, variance: 62.34698277282715, mean_q: -81.8846435546875, std_q: 6.3353400230407715
Running avgs for agent 2: q_loss: 1.679222822189331, p_loss: 81.07476806640625, mean_rew: -4.804916235157282, variance: 69.00183334350587, mean_q: -81.22636413574219, std_q: 6.289964199066162

steps: 1124975, episodes: 45000, mean episode reward: -345.8675034636242, agent episode reward: [-115.28916782120804, -115.28916782120804, -115.28916782120804], time: 97.375
steps: 1124975, episodes: 45000, mean episode variance: 48.31210619735718, agent episode variance: [16.57312668609619, 15.137793895721435, 16.60118561553955], time: 97.375
Running avgs for agent 0: q_loss: 1.7363474369049072, p_loss: 81.38404846191406, mean_rew: -4.78075568713016, variance: 66.29250674438477, mean_q: -81.53067016601562, std_q: 6.0819091796875
Running avgs for agent 1: q_loss: 1.5850187540054321, p_loss: 81.55831146240234, mean_rew: -4.785978830396866, variance: 60.55117558288574, mean_q: -81.7315902709961, std_q: 6.333953380584717
Running avgs for agent 2: q_loss: 1.6605281829833984, p_loss: 80.96625518798828, mean_rew: -4.783946596989353, variance: 66.4047424621582, mean_q: -81.1128158569336, std_q: 6.27230978012085

steps: 1149975, episodes: 46000, mean episode reward: -349.9741295312629, agent episode reward: [-116.65804317708765, -116.65804317708765, -116.65804317708765], time: 91.81
steps: 1149975, episodes: 46000, mean episode variance: 49.15463385009765, agent episode variance: [17.010503295898438, 15.222417442321778, 16.92171311187744], time: 91.81
Running avgs for agent 0: q_loss: 1.7022008895874023, p_loss: 81.27879333496094, mean_rew: -4.771063585143402, variance: 68.04201318359375, mean_q: -81.41939544677734, std_q: 6.079653263092041
Running avgs for agent 1: q_loss: 1.5448428392410278, p_loss: 81.36675262451172, mean_rew: -4.765661714530696, variance: 60.88966976928711, mean_q: -81.53019714355469, std_q: 6.305445671081543
Running avgs for agent 2: q_loss: 1.6370465755462646, p_loss: 80.84695434570312, mean_rew: -4.770381860631043, variance: 67.68685244750976, mean_q: -80.98884582519531, std_q: 6.211942672729492

steps: 1174975, episodes: 47000, mean episode reward: -348.1198817509249, agent episode reward: [-116.03996058364164, -116.03996058364164, -116.03996058364164], time: 92.007
steps: 1174975, episodes: 47000, mean episode variance: 47.92110153961182, agent episode variance: [16.631112258911134, 14.68712117767334, 16.602868103027344], time: 92.008
Running avgs for agent 0: q_loss: 1.701194405555725, p_loss: 81.18084716796875, mean_rew: -4.758840302075558, variance: 66.52444903564454, mean_q: -81.3194808959961, std_q: 6.055551052093506
Running avgs for agent 1: q_loss: 1.5301697254180908, p_loss: 81.2677993774414, mean_rew: -4.757492660572035, variance: 58.74848471069336, mean_q: -81.4275131225586, std_q: 6.292752742767334
Running avgs for agent 2: q_loss: 1.6170628070831299, p_loss: 80.76362609863281, mean_rew: -4.759129178703857, variance: 66.41147241210938, mean_q: -80.89974975585938, std_q: 6.191146373748779

steps: 1199975, episodes: 48000, mean episode reward: -347.2068583867219, agent episode reward: [-115.73561946224063, -115.73561946224063, -115.73561946224063], time: 94.169
steps: 1199975, episodes: 48000, mean episode variance: 46.8258816986084, agent episode variance: [16.309189521789552, 14.370274215698242, 16.146417961120605], time: 94.17
Running avgs for agent 0: q_loss: 1.6982659101486206, p_loss: 81.14192199707031, mean_rew: -4.755695664053899, variance: 65.23675808715821, mean_q: -81.27427673339844, std_q: 6.065208435058594
Running avgs for agent 1: q_loss: 1.5371333360671997, p_loss: 81.19325256347656, mean_rew: -4.7519391153001465, variance: 57.48109686279297, mean_q: -81.35012817382812, std_q: 6.318505764007568
Running avgs for agent 2: q_loss: 1.5963523387908936, p_loss: 80.71363830566406, mean_rew: -4.752679105246349, variance: 64.58567184448242, mean_q: -80.84767150878906, std_q: 6.145235538482666

steps: 1224975, episodes: 49000, mean episode reward: -345.6226259904285, agent episode reward: [-115.20754199680951, -115.20754199680951, -115.20754199680951], time: 93.814
steps: 1224975, episodes: 49000, mean episode variance: 48.25151243209839, agent episode variance: [16.763144073486327, 14.838690937042236, 16.649677421569823], time: 93.815
Running avgs for agent 0: q_loss: 1.6945629119873047, p_loss: 81.01256561279297, mean_rew: -4.748892762564421, variance: 67.05257629394531, mean_q: -81.14086151123047, std_q: 6.0517048835754395
Running avgs for agent 1: q_loss: 1.5175902843475342, p_loss: 81.09259033203125, mean_rew: -4.745862984623736, variance: 59.35476374816894, mean_q: -81.24503326416016, std_q: 6.2996954917907715
Running avgs for agent 2: q_loss: 1.5986582040786743, p_loss: 80.6341323852539, mean_rew: -4.745825974607172, variance: 66.59870968627929, mean_q: -80.76828002929688, std_q: 6.1660847663879395

steps: 1249975, episodes: 50000, mean episode reward: -347.26181433124145, agent episode reward: [-115.75393811041381, -115.75393811041381, -115.75393811041381], time: 93.337
steps: 1249975, episodes: 50000, mean episode variance: 46.94212116622925, agent episode variance: [16.348150386810303, 14.55013262939453, 16.043838150024413], time: 93.337
Running avgs for agent 0: q_loss: 1.6929142475128174, p_loss: 80.94400024414062, mean_rew: -4.739341653471654, variance: 65.39260154724121, mean_q: -81.0744400024414, std_q: 6.078524112701416
Running avgs for agent 1: q_loss: 1.5082812309265137, p_loss: 81.04056549072266, mean_rew: -4.737014501943694, variance: 58.20053051757812, mean_q: -81.19322967529297, std_q: 6.260072231292725
Running avgs for agent 2: q_loss: 1.5892277956008911, p_loss: 80.6010971069336, mean_rew: -4.7400312541364, variance: 64.17535260009765, mean_q: -80.73686981201172, std_q: 6.124444484710693

steps: 1274975, episodes: 51000, mean episode reward: -343.77399633260967, agent episode reward: [-114.5913321108699, -114.5913321108699, -114.5913321108699], time: 93.395
steps: 1274975, episodes: 51000, mean episode variance: 46.67289047241211, agent episode variance: [15.98761054611206, 14.695953182220459, 15.98932674407959], time: 93.396
Running avgs for agent 0: q_loss: 1.6824204921722412, p_loss: 80.88172149658203, mean_rew: -4.731956351562799, variance: 63.95044218444824, mean_q: -81.00791931152344, std_q: 6.079600811004639
Running avgs for agent 1: q_loss: 1.5087029933929443, p_loss: 80.99541473388672, mean_rew: -4.7306905377708155, variance: 58.783812728881834, mean_q: -81.146240234375, std_q: 6.258849143981934
Running avgs for agent 2: q_loss: 1.586612343788147, p_loss: 80.50143432617188, mean_rew: -4.725736364085432, variance: 63.95730697631836, mean_q: -80.63558959960938, std_q: 6.12354850769043

steps: 1299975, episodes: 52000, mean episode reward: -345.03973616154383, agent episode reward: [-115.01324538718126, -115.01324538718126, -115.01324538718126], time: 92.462
steps: 1299975, episodes: 52000, mean episode variance: 46.82174454498291, agent episode variance: [16.218936225891113, 14.370347553253174, 16.232460765838624], time: 92.462
Running avgs for agent 0: q_loss: 1.6708080768585205, p_loss: 80.74901580810547, mean_rew: -4.721674384976586, variance: 64.87574490356445, mean_q: -80.87123107910156, std_q: 6.043404579162598
Running avgs for agent 1: q_loss: 1.4966374635696411, p_loss: 80.91993713378906, mean_rew: -4.720430104728186, variance: 57.481390213012695, mean_q: -81.06759643554688, std_q: 6.188689231872559
Running avgs for agent 2: q_loss: 1.589032769203186, p_loss: 80.36006164550781, mean_rew: -4.722252199541048, variance: 64.9298430633545, mean_q: -80.49292755126953, std_q: 6.157203197479248

steps: 1324975, episodes: 53000, mean episode reward: -345.00613004614416, agent episode reward: [-115.00204334871474, -115.00204334871474, -115.00204334871474], time: 93.207
steps: 1324975, episodes: 53000, mean episode variance: 46.849624114990235, agent episode variance: [15.905851081848144, 14.636549598693847, 16.307223434448243], time: 93.208
Running avgs for agent 0: q_loss: 1.6587623357772827, p_loss: 80.70768737792969, mean_rew: -4.714361997367198, variance: 63.62340432739258, mean_q: -80.82587432861328, std_q: 6.044238090515137
Running avgs for agent 1: q_loss: 1.4673510789871216, p_loss: 80.87195587158203, mean_rew: -4.712770688506874, variance: 58.54619839477539, mean_q: -81.01912689208984, std_q: 6.2086896896362305
Running avgs for agent 2: q_loss: 1.5875550508499146, p_loss: 80.21295928955078, mean_rew: -4.714095338397058, variance: 65.22889373779297, mean_q: -80.34857177734375, std_q: 6.116754055023193

steps: 1349975, episodes: 54000, mean episode reward: -341.3318030311961, agent episode reward: [-113.77726767706537, -113.77726767706537, -113.77726767706537], time: 92.283
steps: 1349975, episodes: 54000, mean episode variance: 45.63270602035522, agent episode variance: [15.664040622711182, 14.117311237335205, 15.851354160308837], time: 92.284
Running avgs for agent 0: q_loss: 1.6444368362426758, p_loss: 80.64014434814453, mean_rew: -4.706959423549079, variance: 62.65616249084473, mean_q: -80.7548599243164, std_q: 6.035447120666504
Running avgs for agent 1: q_loss: 1.4741321802139282, p_loss: 80.82672882080078, mean_rew: -4.703149225608899, variance: 56.46924494934082, mean_q: -80.97090911865234, std_q: 6.178660869598389
Running avgs for agent 2: q_loss: 1.5965358018875122, p_loss: 80.10012817382812, mean_rew: -4.708836118906594, variance: 63.40541664123535, mean_q: -80.23832702636719, std_q: 6.136104583740234

steps: 1374975, episodes: 55000, mean episode reward: -347.97529892434096, agent episode reward: [-115.99176630811367, -115.99176630811367, -115.99176630811367], time: 81.048
steps: 1374975, episodes: 55000, mean episode variance: 46.19627855300903, agent episode variance: [15.574268695831298, 14.333305030822753, 16.28870482635498], time: 81.049
Running avgs for agent 0: q_loss: 1.6495722532272339, p_loss: 80.53048706054688, mean_rew: -4.697289230642814, variance: 62.29707478332519, mean_q: -80.64251708984375, std_q: 6.033602714538574
Running avgs for agent 1: q_loss: 1.4737900495529175, p_loss: 80.79977416992188, mean_rew: -4.703628257831304, variance: 57.333220123291014, mean_q: -80.94088745117188, std_q: 6.152693271636963
Running avgs for agent 2: q_loss: 1.5924428701400757, p_loss: 79.90904998779297, mean_rew: -4.698136084935886, variance: 65.15481930541992, mean_q: -80.05007934570312, std_q: 6.10142707824707

steps: 1399975, episodes: 56000, mean episode reward: -343.41768219077784, agent episode reward: [-114.47256073025927, -114.47256073025927, -114.47256073025927], time: 80.234
steps: 1399975, episodes: 56000, mean episode variance: 45.40029797363281, agent episode variance: [15.211436599731446, 14.143094272613526, 16.045767101287844], time: 80.234
Running avgs for agent 0: q_loss: 1.6225682497024536, p_loss: 80.51551818847656, mean_rew: -4.694340177101346, variance: 60.84574639892578, mean_q: -80.62482452392578, std_q: 6.050267219543457
Running avgs for agent 1: q_loss: 1.449221134185791, p_loss: 80.77889251708984, mean_rew: -4.696091893251798, variance: 56.572377090454104, mean_q: -80.91895294189453, std_q: 6.181211471557617
Running avgs for agent 2: q_loss: 1.5886263847351074, p_loss: 79.80003356933594, mean_rew: -4.692238632376779, variance: 64.18306840515137, mean_q: -79.94103240966797, std_q: 6.126595497131348

steps: 1424975, episodes: 57000, mean episode reward: -340.8689723946019, agent episode reward: [-113.62299079820065, -113.62299079820065, -113.62299079820065], time: 81.379
steps: 1424975, episodes: 57000, mean episode variance: 44.58927645492554, agent episode variance: [14.887205619812011, 14.002571414947509, 15.699499420166015], time: 81.38
Running avgs for agent 0: q_loss: 1.6166046857833862, p_loss: 80.45021057128906, mean_rew: -4.686161914632492, variance: 59.548822479248045, mean_q: -80.55986785888672, std_q: 6.030152797698975
Running avgs for agent 1: q_loss: 1.4562709331512451, p_loss: 80.71744537353516, mean_rew: -4.683528505839864, variance: 56.010285659790036, mean_q: -80.85801696777344, std_q: 6.153565406799316
Running avgs for agent 2: q_loss: 1.589895248413086, p_loss: 79.66445922851562, mean_rew: -4.690006519993254, variance: 62.79799768066406, mean_q: -79.8070297241211, std_q: 6.119431495666504

steps: 1449975, episodes: 58000, mean episode reward: -340.83491010590586, agent episode reward: [-113.6116367019686, -113.6116367019686, -113.6116367019686], time: 79.971
steps: 1449975, episodes: 58000, mean episode variance: 45.31358394622803, agent episode variance: [15.46887883758545, 14.130654457092286, 15.714050651550293], time: 79.971
Running avgs for agent 0: q_loss: 1.5933228731155396, p_loss: 80.30305480957031, mean_rew: -4.681085414204083, variance: 61.8755153503418, mean_q: -80.40898132324219, std_q: 6.017552852630615
Running avgs for agent 1: q_loss: 1.4487272500991821, p_loss: 80.62349700927734, mean_rew: -4.680501736215611, variance: 56.52261782836914, mean_q: -80.76626586914062, std_q: 6.148308277130127
Running avgs for agent 2: q_loss: 1.58659029006958, p_loss: 79.45663452148438, mean_rew: -4.677559088163408, variance: 62.85620260620117, mean_q: -79.59606170654297, std_q: 6.078805446624756

steps: 1474975, episodes: 59000, mean episode reward: -336.9794903835752, agent episode reward: [-112.32649679452507, -112.32649679452507, -112.32649679452507], time: 81.017
steps: 1474975, episodes: 59000, mean episode variance: 44.40486245727539, agent episode variance: [14.87937403869629, 13.878952709197998, 15.646535709381103], time: 81.018
Running avgs for agent 0: q_loss: 1.5877795219421387, p_loss: 80.21996307373047, mean_rew: -4.675174674769281, variance: 59.51749615478516, mean_q: -80.3292007446289, std_q: 6.034423351287842
Running avgs for agent 1: q_loss: 1.4274896383285522, p_loss: 80.52430725097656, mean_rew: -4.669784660158524, variance: 55.51581083679199, mean_q: -80.66334533691406, std_q: 6.159485816955566
Running avgs for agent 2: q_loss: 1.5724719762802124, p_loss: 79.33531188964844, mean_rew: -4.670206065125734, variance: 62.58614283752441, mean_q: -79.47693634033203, std_q: 6.089776515960693

steps: 1499975, episodes: 60000, mean episode reward: -339.558691433115, agent episode reward: [-113.18623047770498, -113.18623047770498, -113.18623047770498], time: 80.669
steps: 1499975, episodes: 60000, mean episode variance: 43.63759233474732, agent episode variance: [14.676239910125732, 13.4692395362854, 15.492112888336182], time: 80.67
Running avgs for agent 0: q_loss: 1.5582973957061768, p_loss: 80.0707015991211, mean_rew: -4.662769723139101, variance: 58.70495964050293, mean_q: -80.17949676513672, std_q: 5.984502792358398
Running avgs for agent 1: q_loss: 1.4249732494354248, p_loss: 80.44310760498047, mean_rew: -4.662203129503587, variance: 53.8769581451416, mean_q: -80.57707977294922, std_q: 6.146988391876221
Running avgs for agent 2: q_loss: 1.5804990530014038, p_loss: 79.16681671142578, mean_rew: -4.660822505973548, variance: 61.96845155334473, mean_q: -79.3080062866211, std_q: 6.095752716064453

...Finished total of 60001 episodes... Now freezing policy and running for 10000 more episodes to get cvar estimation
steps: 24975, episodes: 1000, mean episode reward: -339.01107224498213, agent episode reward: [-113.00369074832737, -113.00369074832737, -113.00369074832737], time: 55.27
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 55.271
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -337.9752453973997, agent episode reward: [-112.65841513246656, -112.65841513246656, -112.65841513246656], time: 71.514
steps: 49975, episodes: 2000, mean episode variance: 87.34434034729004, agent episode variance: [27.565951263427735, 30.203785850524902, 29.574603233337402], time: 71.515
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.523340115539657, variance: 112.97520446777344, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -78.59375, std_q: 6.126941204071045
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.521741107294045, variance: 123.78600311279297, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -79.27948760986328, std_q: 6.0022101402282715
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.517339879712679, variance: 121.20738983154297, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -77.94728088378906, std_q: 6.125387191772461

steps: 74975, episodes: 3000, mean episode reward: -339.95632058369296, agent episode reward: [-113.31877352789766, -113.31877352789766, -113.31877352789766], time: 68.431
steps: 74975, episodes: 3000, mean episode variance: 89.8768602218628, agent episode variance: [28.51762343597412, 30.97525132751465, 30.383985458374024], time: 68.431
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5228413214608265, variance: 114.07049560546875, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -78.52473449707031, std_q: 6.083796977996826
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.52012638407384, variance: 123.90100860595703, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -79.3109359741211, std_q: 5.990451812744141
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.520479707227605, variance: 121.53593444824219, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -77.97110748291016, std_q: 6.1617608070373535

steps: 99975, episodes: 4000, mean episode reward: -336.0890530705528, agent episode reward: [-112.02968435685092, -112.02968435685092, -112.02968435685092], time: 70.265
steps: 99975, episodes: 4000, mean episode variance: 89.94971290588379, agent episode variance: [28.39038572692871, 31.040281532287597, 30.51904564666748], time: 70.266
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.514850618693017, variance: 113.5615463256836, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -78.56517791748047, std_q: 6.11244535446167
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.518704365414565, variance: 124.16112518310547, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -79.32469177246094, std_q: 6.016216278076172
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.524631886313724, variance: 122.07618713378906, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -78.01052856445312, std_q: 6.197018623352051

steps: 124975, episodes: 5000, mean episode reward: -339.71836180610524, agent episode reward: [-113.2394539353684, -113.2394539353684, -113.2394539353684], time: 70.162
steps: 124975, episodes: 5000, mean episode variance: 89.99071280670167, agent episode variance: [28.34980394744873, 31.054890121459962, 30.58601873779297], time: 70.163
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.51862498322232, variance: 113.39921569824219, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -78.55435943603516, std_q: 6.115804672241211
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.520632416256385, variance: 124.21955871582031, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -79.3016586303711, std_q: 6.001303672790527
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.519230605314899, variance: 122.34407806396484, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -77.97724914550781, std_q: 6.159539222717285

steps: 149975, episodes: 6000, mean episode reward: -338.3614226396771, agent episode reward: [-112.78714087989235, -112.78714087989235, -112.78714087989235], time: 69.483
steps: 149975, episodes: 6000, mean episode variance: 90.20099050140381, agent episode variance: [28.518494216918945, 30.987103279113768, 30.695393005371095], time: 69.484
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.519787862263371, variance: 114.073974609375, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -78.5523910522461, std_q: 6.118448257446289
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.518057875735903, variance: 123.94841003417969, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -79.278076171875, std_q: 6.0167341232299805
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.517856813750279, variance: 122.78157043457031, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -77.97077941894531, std_q: 6.1430983543396

steps: 174975, episodes: 7000, mean episode reward: -337.1614795822517, agent episode reward: [-112.38715986075056, -112.38715986075056, -112.38715986075056], time: 73.481
steps: 174975, episodes: 7000, mean episode variance: 90.38913521575928, agent episode variance: [28.54313133239746, 30.946283905029297, 30.89971997833252], time: 73.482
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5185005521566985, variance: 114.17253112792969, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -78.55326843261719, std_q: 6.106508255004883
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.519189931705142, variance: 123.78514099121094, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -79.30419158935547, std_q: 6.0093278884887695
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.518809592448906, variance: 123.59888458251953, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -77.9645004272461, std_q: 6.1414337158203125

steps: 199975, episodes: 8000, mean episode reward: -337.2294399753178, agent episode reward: [-112.40981332510592, -112.40981332510592, -112.40981332510592], time: 69.482
steps: 199975, episodes: 8000, mean episode variance: 89.80026099395752, agent episode variance: [28.368766510009767, 30.776985061645508, 30.654509422302247], time: 69.483
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.513014973445477, variance: 113.47505950927734, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -78.55697631835938, std_q: 6.0507001876831055
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.51529592360854, variance: 123.10794067382812, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -79.30022430419922, std_q: 5.964116096496582
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.518533978088286, variance: 122.6180419921875, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -77.96324157714844, std_q: 6.10890531539917

steps: 224975, episodes: 9000, mean episode reward: -339.2488092411253, agent episode reward: [-113.08293641370842, -113.08293641370842, -113.08293641370842], time: 70.033
steps: 224975, episodes: 9000, mean episode variance: 89.42064730834961, agent episode variance: [28.30257048034668, 30.635550910949707, 30.482525917053223], time: 70.034
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.51920640201228, variance: 113.21028137207031, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -78.60328674316406, std_q: 6.129136085510254
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.509451190581985, variance: 122.54220581054688, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -79.29776000976562, std_q: 5.960290908813477
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.510934481397864, variance: 121.93009948730469, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -77.94298553466797, std_q: 6.107127666473389

steps: 249975, episodes: 10000, mean episode reward: -339.7037298502394, agent episode reward: [-113.23457661674647, -113.23457661674647, -113.23457661674647], time: 70.268
steps: 249975, episodes: 10000, mean episode variance: 89.55193034362793, agent episode variance: [28.290182327270507, 30.699705673217775, 30.562042343139648], time: 70.268
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.515741078789077, variance: 113.16072845458984, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -78.59707641601562, std_q: 6.097099781036377
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.517610844825107, variance: 122.798828125, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -79.34364318847656, std_q: 6.002431869506836
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.522839961386722, variance: 122.2481689453125, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -78.00790405273438, std_q: 6.162476539611816

steps: 274975, episodes: 11000, mean episode reward: -338.6814888516864, agent episode reward: [-112.8938296172288, -112.8938296172288, -112.8938296172288], time: 75.719
steps: 274975, episodes: 11000, mean episode variance: 89.69363817596435, agent episode variance: [28.284905044555664, 30.739572807312012, 30.66916032409668], time: 75.72
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.512753360280509, variance: 113.13961791992188, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -78.61676025390625, std_q: 6.098775863647461
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.516677058025293, variance: 122.95829772949219, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -79.3485107421875, std_q: 5.97653865814209
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.521607270682835, variance: 122.67664337158203, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -78.00975036621094, std_q: 6.153743743896484

steps: 299975, episodes: 12000, mean episode reward: -338.31690375217704, agent episode reward: [-112.77230125072566, -112.77230125072566, -112.77230125072566], time: 70.746
steps: 299975, episodes: 12000, mean episode variance: 89.60524118041992, agent episode variance: [28.363938873291016, 30.626922203063966, 30.614380104064942], time: 70.746
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5172649899346435, variance: 113.45575714111328, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -78.61323547363281, std_q: 6.108850955963135
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.521226534774781, variance: 122.5076904296875, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -79.40199279785156, std_q: 6.019580841064453
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.519230352623977, variance: 122.45752716064453, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -78.01078033447266, std_q: 6.141237258911133

some error in feed_dict
Tensor("observation0:0", shape=(?, 16), dtype=float32) :: [[-0.16756065 -0.10958507  0.05704699 -0.0761008   0.45637715  0.22158063
  -0.76480798 -0.47532587  0.52410265  0.30869986 -0.77101353 -0.4443738
   0.          0.          0.          0.        ]]
Traceback (most recent call last):
  File "train.py", line 493, in <module>
    train(arglist)
  File "train.py", line 192, in train
    action_n = [agent.action(obs) for agent, obs in zip(trainers,obs_n)]
  File "train.py", line 192, in <listcomp>
    action_n = [agent.action(obs) for agent, obs in zip(trainers,obs_n)]
  File "../maddpg/trainer/maddpg.py", line 259, in action
    return self.act(obs[None])[0]
  File "../maddpg/common/tf_util.py", line 289, in <lambda>
    return lambda *args, **kwargs: f(*args, **kwargs)[0]
  File "../maddpg/common/tf_util.py", line 346, in __call__
    return results
UnboundLocalError: local variable 'results' referenced before assignment
