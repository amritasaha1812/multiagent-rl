# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 23.72 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies4/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies4/02-non-linear-exp_var/
Job <1091275> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc253>>
arglist.u_estimation True
2019-09-06 05:00:36.024944: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -516.8929587735809, agent episode reward: [-172.29765292452694, -172.29765292452694, -172.29765292452694], time: 51.797
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 51.797
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -697.7934897679519, agent episode reward: [-232.59782992265062, -232.59782992265062, -232.59782992265062], time: 78.558
steps: 49975, episodes: 2000, mean episode variance: 8.754670333623887, agent episode variance: [2.856479988336563, 2.8695235786437987, 3.028666766643524], time: 78.559
Running avgs for agent 0: q_loss: 105.75739288330078, p_loss: -5.413417339324951, mean_rew: -7.680009120987298, variance: 11.706885198100668, lamda: 1.0108860731124878
Running avgs for agent 1: q_loss: 109.7889404296875, p_loss: -5.4081902503967285, mean_rew: -7.679437700460303, variance: 11.760342535425405, lamda: 1.011136531829834
Running avgs for agent 2: q_loss: 148.40835571289062, p_loss: -5.686866760253906, mean_rew: -7.6916906335928354, variance: 12.412568715752148, lamda: 1.0111236572265625

steps: 74975, episodes: 3000, mean episode reward: -761.3177347543909, agent episode reward: [-253.77257825146359, -253.77257825146359, -253.77257825146359], time: 75.71
steps: 74975, episodes: 3000, mean episode variance: 6.214516823768616, agent episode variance: [2.0810070729255674, 2.06571066904068, 2.0677990818023684], time: 75.711
Running avgs for agent 0: q_loss: 38.33378982543945, p_loss: -4.602885723114014, mean_rew: -8.47423017128546, variance: 8.324028015136719, lamda: 1.0351876020431519
Running avgs for agent 1: q_loss: 38.31587600708008, p_loss: -4.610743045806885, mean_rew: -8.486065680179061, variance: 8.262843132019043, lamda: 1.0355290174484253
Running avgs for agent 2: q_loss: 35.91470718383789, p_loss: -4.604881763458252, mean_rew: -8.488701292359362, variance: 8.271196365356445, lamda: 1.0357898473739624

steps: 99975, episodes: 4000, mean episode reward: -835.3332329539891, agent episode reward: [-278.444410984663, -278.444410984663, -278.444410984663], time: 77.947
steps: 99975, episodes: 4000, mean episode variance: 6.397346893072128, agent episode variance: [2.1059875721931456, 2.1410948934555054, 2.150264427423477], time: 77.947
Running avgs for agent 0: q_loss: 47.92735290527344, p_loss: -4.623629093170166, mean_rew: -9.089962117639352, variance: 8.4239501953125, lamda: 1.0601916313171387
Running avgs for agent 1: q_loss: 61.22966384887695, p_loss: -4.6961283683776855, mean_rew: -9.090285153571344, variance: 8.56437873840332, lamda: 1.0605331659317017
Running avgs for agent 2: q_loss: 50.081336975097656, p_loss: -4.6355767250061035, mean_rew: -9.091315970624494, variance: 8.601057052612305, lamda: 1.0607941150665283

steps: 124975, episodes: 5000, mean episode reward: -840.0257171060846, agent episode reward: [-280.0085723686949, -280.0085723686949, -280.0085723686949], time: 74.345
steps: 124975, episodes: 5000, mean episode variance: 6.746543358802795, agent episode variance: [2.254387801885605, 2.2635438685417175, 2.228611688375473], time: 74.346
Running avgs for agent 0: q_loss: 60.011756896972656, p_loss: -4.836469650268555, mean_rew: -9.594799058836148, variance: 9.017550468444824, lamda: 1.0841279029846191
Running avgs for agent 1: q_loss: 68.58484649658203, p_loss: -4.878846645355225, mean_rew: -9.587054688318004, variance: 9.05417537689209, lamda: 1.085537314414978
Running avgs for agent 2: q_loss: 70.60496520996094, p_loss: -4.897487163543701, mean_rew: -9.582788209478839, variance: 8.914446830749512, lamda: 1.0857373476028442

steps: 149975, episodes: 6000, mean episode reward: -782.2105319254072, agent episode reward: [-260.73684397513574, -260.73684397513574, -260.73684397513574], time: 75.407
steps: 149975, episodes: 6000, mean episode variance: 6.817155347585678, agent episode variance: [2.286218569755554, 2.2752663555145265, 2.2556704223155974], time: 75.408
Running avgs for agent 0: q_loss: 54.69119644165039, p_loss: -5.011061668395996, mean_rew: -9.858023121931096, variance: 9.14487361907959, lamda: 1.1016623973846436
Running avgs for agent 1: q_loss: 65.76393127441406, p_loss: -5.003607273101807, mean_rew: -9.848689843330293, variance: 9.101064682006836, lamda: 1.1103425025939941
Running avgs for agent 2: q_loss: 64.13064575195312, p_loss: -4.987093925476074, mean_rew: -9.851651757384712, variance: 9.02268123626709, lamda: 1.109872579574585

steps: 174975, episodes: 7000, mean episode reward: -725.4722682773603, agent episode reward: [-241.82408942578672, -241.82408942578672, -241.82408942578672], time: 105.366
steps: 174975, episodes: 7000, mean episode variance: 6.675626668453217, agent episode variance: [2.210173515558243, 2.262298259973526, 2.203154892921448], time: 105.366
Running avgs for agent 0: q_loss: 54.3355827331543, p_loss: -5.018537998199463, mean_rew: -9.854201419378848, variance: 8.840694427490234, lamda: 1.114614486694336
Running avgs for agent 1: q_loss: 60.97270202636719, p_loss: -4.990968227386475, mean_rew: -9.856101334690251, variance: 9.049193382263184, lamda: 1.1339222192764282
Running avgs for agent 2: q_loss: 66.17637634277344, p_loss: -5.031822204589844, mean_rew: -9.858032383894631, variance: 8.81261920928955, lamda: 1.1334447860717773

steps: 199975, episodes: 8000, mean episode reward: -761.1532690485278, agent episode reward: [-253.7177563495093, -253.7177563495093, -253.7177563495093], time: 104.104
steps: 199975, episodes: 8000, mean episode variance: 6.535284167051315, agent episode variance: [2.2027089252471925, 2.18025550532341, 2.1523197364807127], time: 104.105
Running avgs for agent 0: q_loss: 56.31239318847656, p_loss: -5.0288896560668945, mean_rew: -9.872203381810545, variance: 8.810835838317871, lamda: 1.1286265850067139
Running avgs for agent 1: q_loss: 61.2722282409668, p_loss: -4.980903148651123, mean_rew: -9.8504172689066, variance: 8.72102165222168, lamda: 1.1580969095230103
Running avgs for agent 2: q_loss: 59.67695617675781, p_loss: -5.041955947875977, mean_rew: -9.846100299514532, variance: 8.60927963256836, lamda: 1.1580833196640015

steps: 224975, episodes: 9000, mean episode reward: -726.4022307863482, agent episode reward: [-242.13407692878275, -242.13407692878275, -242.13407692878275], time: 102.866
steps: 224975, episodes: 9000, mean episode variance: 6.343053509712219, agent episode variance: [2.1375302624702455, 2.113701595544815, 2.091821651697159], time: 102.866
Running avgs for agent 0: q_loss: 54.52082824707031, p_loss: -4.976395606994629, mean_rew: -9.852213866374983, variance: 8.550121307373047, lamda: 1.147306203842163
Running avgs for agent 1: q_loss: 59.721012115478516, p_loss: -4.990119934082031, mean_rew: -9.882083924779527, variance: 8.454806327819824, lamda: 1.1827365159988403
Running avgs for agent 2: q_loss: 62.151641845703125, p_loss: -5.035782814025879, mean_rew: -9.862885638462085, variance: 8.367286682128906, lamda: 1.182267189025879

steps: 249975, episodes: 10000, mean episode reward: -703.848977338393, agent episode reward: [-234.61632577946432, -234.61632577946432, -234.61632577946432], time: 103.479
steps: 249975, episodes: 10000, mean episode variance: 6.226974038362503, agent episode variance: [2.079072182416916, 2.07584126329422, 2.072060592651367], time: 103.479
Running avgs for agent 0: q_loss: 51.9588623046875, p_loss: -4.988915920257568, mean_rew: -9.827729938146614, variance: 8.316288948059082, lamda: 1.17128586769104
Running avgs for agent 1: q_loss: 58.33015441894531, p_loss: -4.957390785217285, mean_rew: -9.825648813473231, variance: 8.303365707397461, lamda: 1.2058892250061035
Running avgs for agent 2: q_loss: 45.18962478637695, p_loss: -4.989848613739014, mean_rew: -9.840139185237607, variance: 8.28824234008789, lamda: 1.2070105075836182

steps: 274975, episodes: 11000, mean episode reward: -676.6803033312555, agent episode reward: [-225.56010111041851, -225.56010111041851, -225.56010111041851], time: 103.791
steps: 274975, episodes: 11000, mean episode variance: 6.065059161663055, agent episode variance: [2.040681547164917, 2.0007752780914307, 2.0236023364067077], time: 103.792
Running avgs for agent 0: q_loss: 41.12530517578125, p_loss: -4.944343566894531, mean_rew: -9.781712385705678, variance: 8.162726402282715, lamda: 1.1960557699203491
Running avgs for agent 1: q_loss: 48.35854721069336, p_loss: -4.933191299438477, mean_rew: -9.771391385891976, variance: 8.003101348876953, lamda: 1.2259482145309448
Running avgs for agent 2: q_loss: 51.027183532714844, p_loss: -4.967916965484619, mean_rew: -9.768739451504144, variance: 8.094408988952637, lamda: 1.2302559614181519

steps: 299975, episodes: 12000, mean episode reward: -704.4094789630151, agent episode reward: [-234.80315965433837, -234.80315965433837, -234.80315965433837], time: 114.853
steps: 299975, episodes: 12000, mean episode variance: 5.948250119209289, agent episode variance: [2.0258622028827666, 1.9399001624584198, 1.982487753868103], time: 114.854
Running avgs for agent 0: q_loss: 47.651336669921875, p_loss: -4.901699066162109, mean_rew: -9.707346472873327, variance: 8.103448867797852, lamda: 1.2198843955993652
Running avgs for agent 1: q_loss: 44.587608337402344, p_loss: -4.917304515838623, mean_rew: -9.717103129941282, variance: 7.759600639343262, lamda: 1.247836709022522
Running avgs for agent 2: q_loss: 48.94548416137695, p_loss: -4.93266487121582, mean_rew: -9.717466874514761, variance: 7.929951190948486, lamda: 1.2451242208480835

steps: 324975, episodes: 13000, mean episode reward: -735.4862723320446, agent episode reward: [-245.16209077734814, -245.16209077734814, -245.16209077734814], time: 175.704
steps: 324975, episodes: 13000, mean episode variance: 5.854683590650558, agent episode variance: [1.9565752091407775, 1.9533359367847443, 1.9447724447250365], time: 175.704
Running avgs for agent 0: q_loss: 39.04353332519531, p_loss: -4.935951232910156, mean_rew: -9.707139699579823, variance: 7.826300621032715, lamda: 1.2427202463150024
Running avgs for agent 1: q_loss: 42.523704528808594, p_loss: -4.902985095977783, mean_rew: -9.710554387008031, variance: 7.813343524932861, lamda: 1.2620965242385864
Running avgs for agent 2: q_loss: 48.56414794921875, p_loss: -4.937839031219482, mean_rew: -9.711696044401913, variance: 7.77908992767334, lamda: 1.2587621212005615

steps: 349975, episodes: 14000, mean episode reward: -752.1922188680035, agent episode reward: [-250.73073962266784, -250.73073962266784, -250.73073962266784], time: 165.95
steps: 349975, episodes: 14000, mean episode variance: 5.773591831684112, agent episode variance: [1.9285221602916718, 1.9119487986564636, 1.9331208727359772], time: 165.951
Running avgs for agent 0: q_loss: 50.2623291015625, p_loss: -4.937188148498535, mean_rew: -9.732974671321903, variance: 7.714088439941406, lamda: 1.2614350318908691
Running avgs for agent 1: q_loss: 29.760896682739258, p_loss: -4.902530193328857, mean_rew: -9.727544603607507, variance: 7.647794723510742, lamda: 1.2830737829208374
Running avgs for agent 2: q_loss: 35.380455017089844, p_loss: -4.931400299072266, mean_rew: -9.725851301529408, variance: 7.732483386993408, lamda: 1.2684426307678223

steps: 374975, episodes: 15000, mean episode reward: -804.3448143011638, agent episode reward: [-268.1149381003879, -268.1149381003879, -268.1149381003879], time: 164.087
steps: 374975, episodes: 15000, mean episode variance: 5.73363739323616, agent episode variance: [1.9187895276546478, 1.897319049835205, 1.9175288157463073], time: 164.088
Running avgs for agent 0: q_loss: 39.221649169921875, p_loss: -4.974154472351074, mean_rew: -9.778905828914647, variance: 7.6751580238342285, lamda: 1.2720425128936768
Running avgs for agent 1: q_loss: 31.679031372070312, p_loss: -4.925588607788086, mean_rew: -9.77665579092281, variance: 7.589276313781738, lamda: 1.303846001625061
Running avgs for agent 2: q_loss: 31.170942306518555, p_loss: -4.945633888244629, mean_rew: -9.766193118377696, variance: 7.6701154708862305, lamda: 1.289743423461914

steps: 399975, episodes: 16000, mean episode reward: -847.1255445985289, agent episode reward: [-282.37518153284293, -282.37518153284293, -282.37518153284293], time: 165.891
steps: 399975, episodes: 16000, mean episode variance: 5.670256717681885, agent episode variance: [1.9009002952575684, 1.8677669076919556, 1.901589514732361], time: 165.892
Running avgs for agent 0: q_loss: 32.15752029418945, p_loss: -5.023231029510498, mean_rew: -9.84969268611378, variance: 7.603600978851318, lamda: 1.2922639846801758
Running avgs for agent 1: q_loss: 45.00421142578125, p_loss: -4.969249725341797, mean_rew: -9.843757150460712, variance: 7.471067428588867, lamda: 1.3215168714523315
Running avgs for agent 2: q_loss: 43.7380485534668, p_loss: -4.986652374267578, mean_rew: -9.860730885136297, variance: 7.606358528137207, lamda: 1.304029107093811

steps: 424975, episodes: 17000, mean episode reward: -861.303409945886, agent episode reward: [-287.1011366486287, -287.1011366486287, -287.1011366486287], time: 162.776
steps: 424975, episodes: 17000, mean episode variance: 5.699516993999481, agent episode variance: [1.9186231787204742, 1.8819213755130768, 1.8989724397659302], time: 162.777
Running avgs for agent 0: q_loss: 53.4221076965332, p_loss: -5.057007789611816, mean_rew: -9.959330985985316, variance: 7.674493312835693, lamda: 1.3050920963287354
Running avgs for agent 1: q_loss: 37.228782653808594, p_loss: -5.020072937011719, mean_rew: -9.945066981204809, variance: 7.527685642242432, lamda: 1.3308717012405396
Running avgs for agent 2: q_loss: 44.10500717163086, p_loss: -5.022984504699707, mean_rew: -9.94720862667757, variance: 7.595889568328857, lamda: 1.3102554082870483

steps: 449975, episodes: 18000, mean episode reward: -847.2935063739692, agent episode reward: [-282.4311687913231, -282.4311687913231, -282.4311687913231], time: 162.03
steps: 449975, episodes: 18000, mean episode variance: 5.691189728975296, agent episode variance: [1.92282581114769, 1.8572490835189819, 1.9111148343086242], time: 162.031
Running avgs for agent 0: q_loss: 48.0282096862793, p_loss: -5.089890480041504, mean_rew: -10.03196465835047, variance: 7.691303253173828, lamda: 1.3143327236175537
Running avgs for agent 1: q_loss: 32.2659797668457, p_loss: -5.06826639175415, mean_rew: -10.036418532171924, variance: 7.428996562957764, lamda: 1.3501631021499634
Running avgs for agent 2: q_loss: 32.27423858642578, p_loss: -5.061232566833496, mean_rew: -10.036562780208582, variance: 7.644459247589111, lamda: 1.3184837102890015

steps: 474975, episodes: 19000, mean episode reward: -860.6116751112277, agent episode reward: [-286.8705583704092, -286.8705583704092, -286.8705583704092], time: 165.44
steps: 474975, episodes: 19000, mean episode variance: 5.659595332145691, agent episode variance: [1.9042203359603882, 1.8586671991348267, 1.896707797050476], time: 165.441
Running avgs for agent 0: q_loss: 32.84910583496094, p_loss: -5.129800319671631, mean_rew: -10.109742802571377, variance: 7.616881847381592, lamda: 1.3290141820907593
Running avgs for agent 1: q_loss: 31.364919662475586, p_loss: -5.072918891906738, mean_rew: -10.095813407957353, variance: 7.434669017791748, lamda: 1.3713557720184326
Running avgs for agent 2: q_loss: 29.281105041503906, p_loss: -5.091212272644043, mean_rew: -10.097273477750287, variance: 7.586831092834473, lamda: 1.3348016738891602

steps: 499975, episodes: 20000, mean episode reward: -862.9039304523166, agent episode reward: [-287.63464348410554, -287.63464348410554, -287.63464348410554], time: 165.069
steps: 499975, episodes: 20000, mean episode variance: 5.65354132270813, agent episode variance: [1.9186588871479033, 1.8440239119529724, 1.890858523607254], time: 165.069
Running avgs for agent 0: q_loss: 41.0868034362793, p_loss: -5.161188125610352, mean_rew: -10.183550979493782, variance: 7.674635887145996, lamda: 1.345520257949829
Running avgs for agent 1: q_loss: 31.419452667236328, p_loss: -5.1249189376831055, mean_rew: -10.163021099388475, variance: 7.376095771789551, lamda: 1.3923519849777222
Running avgs for agent 2: q_loss: 38.84687805175781, p_loss: -5.119387149810791, mean_rew: -10.1682662591461, variance: 7.563433647155762, lamda: 1.351296305656433

steps: 524975, episodes: 21000, mean episode reward: -861.3870580122233, agent episode reward: [-287.1290193374078, -287.1290193374078, -287.1290193374078], time: 163.153
steps: 524975, episodes: 21000, mean episode variance: 5.63639291357994, agent episode variance: [1.9081928904056549, 1.8208589067459107, 1.9073411164283753], time: 163.153
Running avgs for agent 0: q_loss: 38.101497650146484, p_loss: -5.202887058258057, mean_rew: -10.24927439064492, variance: 7.6327714920043945, lamda: 1.3626948595046997
Running avgs for agent 1: q_loss: 31.898052215576172, p_loss: -5.174226760864258, mean_rew: -10.267356293868836, variance: 7.283435344696045, lamda: 1.410901665687561
Running avgs for agent 2: q_loss: 46.83170700073242, p_loss: -5.164975643157959, mean_rew: -10.236890609749292, variance: 7.629364490509033, lamda: 1.3559805154800415

steps: 549975, episodes: 22000, mean episode reward: -866.1912842760364, agent episode reward: [-288.7304280920121, -288.7304280920121, -288.7304280920121], time: 162.727
steps: 549975, episodes: 22000, mean episode variance: 5.590496857881546, agent episode variance: [1.8772496647834778, 1.8150821900367737, 1.8981650030612947], time: 162.727
Running avgs for agent 0: q_loss: 35.454498291015625, p_loss: -5.22490930557251, mean_rew: -10.301456776804766, variance: 7.508998870849609, lamda: 1.3817981481552124
Running avgs for agent 1: q_loss: 31.47532844543457, p_loss: -5.197360992431641, mean_rew: -10.296325554762046, variance: 7.260328769683838, lamda: 1.4268109798431396
Running avgs for agent 2: q_loss: 40.134944915771484, p_loss: -5.195458889007568, mean_rew: -10.295297045357275, variance: 7.592659950256348, lamda: 1.359827995300293

steps: 574975, episodes: 23000, mean episode reward: -881.5985131449162, agent episode reward: [-293.86617104830543, -293.86617104830543, -293.86617104830543], time: 165.809
steps: 574975, episodes: 23000, mean episode variance: 5.549372233390808, agent episode variance: [1.8609047422409057, 1.8021097273826598, 1.8863577637672424], time: 165.809
Running avgs for agent 0: q_loss: 32.35663604736328, p_loss: -5.258764743804932, mean_rew: -10.366565939910366, variance: 7.443619251251221, lamda: 1.399352788925171
Running avgs for agent 1: q_loss: 33.699256896972656, p_loss: -5.237682819366455, mean_rew: -10.377949442652843, variance: 7.208438873291016, lamda: 1.443877935409546
Running avgs for agent 2: q_loss: 30.585115432739258, p_loss: -5.217097759246826, mean_rew: -10.35831650710123, variance: 7.545431613922119, lamda: 1.3757750988006592

steps: 599975, episodes: 24000, mean episode reward: -883.7536131505464, agent episode reward: [-294.5845377168488, -294.5845377168488, -294.5845377168488], time: 163.252
steps: 599975, episodes: 24000, mean episode variance: 5.517377188682556, agent episode variance: [1.851292584657669, 1.7882852365970612, 1.877799367427826], time: 163.252
Running avgs for agent 0: q_loss: 45.30522918701172, p_loss: -5.279468536376953, mean_rew: -10.414117767985234, variance: 7.405170440673828, lamda: 1.409878134727478
Running avgs for agent 1: q_loss: 34.16109848022461, p_loss: -5.257963180541992, mean_rew: -10.41995876511298, variance: 7.153140544891357, lamda: 1.4604123830795288
Running avgs for agent 2: q_loss: 28.17046356201172, p_loss: -5.257983207702637, mean_rew: -10.417967424779862, variance: 7.511197566986084, lamda: 1.391906499862671

steps: 624975, episodes: 25000, mean episode reward: -910.7277077741646, agent episode reward: [-303.5759025913882, -303.5759025913882, -303.5759025913882], time: 161.656
steps: 624975, episodes: 25000, mean episode variance: 5.533362327337265, agent episode variance: [1.8762147948741912, 1.7755815572738647, 1.881565975189209], time: 161.656
Running avgs for agent 0: q_loss: 53.01276397705078, p_loss: -5.306238174438477, mean_rew: -10.479970901972342, variance: 7.504859447479248, lamda: 1.4143998622894287
Running avgs for agent 1: q_loss: 32.534725189208984, p_loss: -5.297005653381348, mean_rew: -10.488479698521173, variance: 7.102326393127441, lamda: 1.4733630418777466
Running avgs for agent 2: q_loss: 30.391218185424805, p_loss: -5.287313938140869, mean_rew: -10.489385724619261, variance: 7.52626371383667, lamda: 1.4061211347579956

steps: 649975, episodes: 26000, mean episode reward: -876.1070529578742, agent episode reward: [-292.03568431929136, -292.03568431929136, -292.03568431929136], time: 163.648
steps: 649975, episodes: 26000, mean episode variance: 5.550299471616745, agent episode variance: [1.8777552711963654, 1.7979001655578613, 1.8746440348625184], time: 163.648
Running avgs for agent 0: q_loss: 53.53864669799805, p_loss: -5.354854106903076, mean_rew: -10.555127411906273, variance: 7.511020660400391, lamda: 1.4170308113098145
Running avgs for agent 1: q_loss: 32.821563720703125, p_loss: -5.316995620727539, mean_rew: -10.537044030690126, variance: 7.191600799560547, lamda: 1.4854695796966553
Running avgs for agent 2: q_loss: 29.071765899658203, p_loss: -5.326318264007568, mean_rew: -10.55994796967233, variance: 7.4985761642456055, lamda: 1.4198659658432007

steps: 674975, episodes: 27000, mean episode reward: -901.1919958997107, agent episode reward: [-300.3973319665702, -300.3973319665702, -300.3973319665702], time: 165.155
steps: 674975, episodes: 27000, mean episode variance: 5.473769630670548, agent episode variance: [1.8514873797893525, 1.769269977092743, 1.8530122737884522], time: 165.155
Running avgs for agent 0: q_loss: 52.766117095947266, p_loss: -5.367978572845459, mean_rew: -10.58674470313887, variance: 7.405949115753174, lamda: 1.4201825857162476
Running avgs for agent 1: q_loss: 34.28535079956055, p_loss: -5.347470760345459, mean_rew: -10.587700405408405, variance: 7.077080249786377, lamda: 1.4997209310531616
Running avgs for agent 2: q_loss: 30.489734649658203, p_loss: -5.335470676422119, mean_rew: -10.592283503543547, variance: 7.412048816680908, lamda: 1.4339282512664795

steps: 699975, episodes: 28000, mean episode reward: -883.6074767551473, agent episode reward: [-294.53582558504905, -294.53582558504905, -294.53582558504905], time: 169.577
steps: 699975, episodes: 28000, mean episode variance: 5.528478348970413, agent episode variance: [1.8958153655529022, 1.7759534258842469, 1.8567095575332642], time: 169.577
Running avgs for agent 0: q_loss: 49.67028045654297, p_loss: -5.363775253295898, mean_rew: -10.627653700302618, variance: 7.583261489868164, lamda: 1.4255943298339844
Running avgs for agent 1: q_loss: 37.413570404052734, p_loss: -5.37167501449585, mean_rew: -10.637725122396187, variance: 7.103814125061035, lamda: 1.5129828453063965
Running avgs for agent 2: q_loss: 44.95956039428711, p_loss: -5.363202095031738, mean_rew: -10.649189308752222, variance: 7.426838397979736, lamda: 1.4465954303741455

steps: 724975, episodes: 29000, mean episode reward: -862.5008232345076, agent episode reward: [-287.50027441150246, -287.50027441150246, -287.50027441150246], time: 164.649
steps: 724975, episodes: 29000, mean episode variance: 5.507844303846359, agent episode variance: [1.8923564455509185, 1.7533063864707947, 1.862181471824646], time: 164.65
Running avgs for agent 0: q_loss: 53.17034912109375, p_loss: -5.404293060302734, mean_rew: -10.693034605639744, variance: 7.569425582885742, lamda: 1.42934250831604
Running avgs for agent 1: q_loss: 34.71181106567383, p_loss: -5.386867523193359, mean_rew: -10.67672731712066, variance: 7.013225078582764, lamda: 1.5289504528045654
Running avgs for agent 2: q_loss: 47.913639068603516, p_loss: -5.373805522918701, mean_rew: -10.672147576850598, variance: 7.448725700378418, lamda: 1.4503765106201172

steps: 749975, episodes: 30000, mean episode reward: -855.6121357311237, agent episode reward: [-285.2040452437078, -285.2040452437078, -285.2040452437078], time: 166.036
steps: 749975, episodes: 30000, mean episode variance: 5.44970498919487, agent episode variance: [1.8567449150085449, 1.730819640159607, 1.8621404340267182], time: 166.036
Running avgs for agent 0: q_loss: 52.398990631103516, p_loss: -5.42096471786499, mean_rew: -10.708541290293564, variance: 7.4269795417785645, lamda: 1.435160756111145
Running avgs for agent 1: q_loss: 46.4743537902832, p_loss: -5.388937473297119, mean_rew: -10.693785685033278, variance: 6.92327880859375, lamda: 1.540818691253662
Running avgs for agent 2: q_loss: 47.173744201660156, p_loss: -5.401206970214844, mean_rew: -10.70755153119643, variance: 7.448561668395996, lamda: 1.4523204565048218

steps: 774975, episodes: 31000, mean episode reward: -870.7938579033306, agent episode reward: [-290.26461930111014, -290.26461930111014, -290.26461930111014], time: 167.056
steps: 774975, episodes: 31000, mean episode variance: 5.447089290857315, agent episode variance: [1.8463198585510254, 1.7452620289325713, 1.8555074033737182], time: 167.057
Running avgs for agent 0: q_loss: 50.80171585083008, p_loss: -5.423256874084473, mean_rew: -10.725851106512044, variance: 7.385279178619385, lamda: 1.43869948387146
Running avgs for agent 1: q_loss: 56.0230827331543, p_loss: -5.398705959320068, mean_rew: -10.718897263580821, variance: 6.981048107147217, lamda: 1.5455336570739746
Running avgs for agent 2: q_loss: 47.456146240234375, p_loss: -5.401481628417969, mean_rew: -10.726591028369416, variance: 7.422029972076416, lamda: 1.455072283744812

steps: 799975, episodes: 32000, mean episode reward: -856.8837192385157, agent episode reward: [-285.6279064128386, -285.6279064128386, -285.6279064128386], time: 160.64
steps: 799975, episodes: 32000, mean episode variance: 5.480711388111114, agent episode variance: [1.8794400007724763, 1.738142275094986, 1.8631291122436524], time: 160.641
Running avgs for agent 0: q_loss: 35.935699462890625, p_loss: -5.434052467346191, mean_rew: -10.76046094677536, variance: 7.517759799957275, lamda: 1.4469244480133057
Running avgs for agent 1: q_loss: 55.53147888183594, p_loss: -5.4188714027404785, mean_rew: -10.743255038062307, variance: 6.952569484710693, lamda: 1.5489546060562134
Running avgs for agent 2: q_loss: 47.107078552246094, p_loss: -5.413226127624512, mean_rew: -10.757110714937044, variance: 7.452516555786133, lamda: 1.4571706056594849

steps: 824975, episodes: 33000, mean episode reward: -877.8168542860242, agent episode reward: [-292.60561809534136, -292.60561809534136, -292.60561809534136], time: 163.268
steps: 824975, episodes: 33000, mean episode variance: 5.481975391387939, agent episode variance: [1.8560437698364258, 1.7569712941646576, 1.868960327386856], time: 163.268
Running avgs for agent 0: q_loss: 49.95697784423828, p_loss: -5.436354160308838, mean_rew: -10.770267757816491, variance: 7.424175262451172, lamda: 1.4585403203964233
Running avgs for agent 1: q_loss: 55.38398361206055, p_loss: -5.428257942199707, mean_rew: -10.77642090064031, variance: 7.0278849601745605, lamda: 1.5527770519256592
Running avgs for agent 2: q_loss: 47.20009231567383, p_loss: -5.430468559265137, mean_rew: -10.780765835474389, variance: 7.475841045379639, lamda: 1.4598723649978638

steps: 849975, episodes: 34000, mean episode reward: -884.9170831293301, agent episode reward: [-294.97236104311, -294.97236104311, -294.97236104311], time: 169.945
steps: 849975, episodes: 34000, mean episode variance: 5.475106051445008, agent episode variance: [1.8680835754871368, 1.7533699119091033, 1.853652564048767], time: 169.946
Running avgs for agent 0: q_loss: 50.53224563598633, p_loss: -5.458173751831055, mean_rew: -10.817040470471749, variance: 7.472334861755371, lamda: 1.4602274894714355
Running avgs for agent 1: q_loss: 54.57535171508789, p_loss: -5.4491095542907715, mean_rew: -10.813581555518278, variance: 7.013479709625244, lamda: 1.5579179525375366
Running avgs for agent 2: q_loss: 46.36613464355469, p_loss: -5.442059516906738, mean_rew: -10.784476653456847, variance: 7.414610385894775, lamda: 1.4624205827713013

steps: 874975, episodes: 35000, mean episode reward: -859.9246718262254, agent episode reward: [-286.64155727540845, -286.64155727540845, -286.64155727540845], time: 170.582
steps: 874975, episodes: 35000, mean episode variance: 5.529180239915847, agent episode variance: [1.8886131863594056, 1.7673923568725587, 1.8731746966838836], time: 170.583
Running avgs for agent 0: q_loss: 49.840763092041016, p_loss: -5.473971366882324, mean_rew: -10.844662667865276, variance: 7.554452419281006, lamda: 1.4622150659561157
Running avgs for agent 1: q_loss: 54.3162956237793, p_loss: -5.460180759429932, mean_rew: -10.82866620642224, variance: 7.069569110870361, lamda: 1.5632450580596924
Running avgs for agent 2: q_loss: 46.769596099853516, p_loss: -5.448965549468994, mean_rew: -10.831799093550135, variance: 7.492698669433594, lamda: 1.465822696685791

steps: 899975, episodes: 36000, mean episode reward: -875.7491929390892, agent episode reward: [-291.916397646363, -291.916397646363, -291.916397646363], time: 172.164
steps: 899975, episodes: 36000, mean episode variance: 5.4827141780853275, agent episode variance: [1.8722607178688049, 1.748576159954071, 1.8618773002624511], time: 172.165
Running avgs for agent 0: q_loss: 49.71783447265625, p_loss: -5.487382411956787, mean_rew: -10.850229963789985, variance: 7.48904275894165, lamda: 1.4643415212631226
Running avgs for agent 1: q_loss: 53.50226593017578, p_loss: -5.479976177215576, mean_rew: -10.866516156866673, variance: 6.994304656982422, lamda: 1.5680246353149414
Running avgs for agent 2: q_loss: 41.88451385498047, p_loss: -5.484030723571777, mean_rew: -10.871634156119653, variance: 7.447509288787842, lamda: 1.4696128368377686

steps: 924975, episodes: 37000, mean episode reward: -859.79117993393, agent episode reward: [-286.59705997797664, -286.59705997797664, -286.59705997797664], time: 171.745
steps: 924975, episodes: 37000, mean episode variance: 5.454921866178513, agent episode variance: [1.8768704166412353, 1.7264435133934022, 1.851607936143875], time: 171.745
Running avgs for agent 0: q_loss: 43.685699462890625, p_loss: -5.483152389526367, mean_rew: -10.873390727615698, variance: 7.507481575012207, lamda: 1.468489408493042
Running avgs for agent 1: q_loss: 53.90172576904297, p_loss: -5.484661102294922, mean_rew: -10.884202872848295, variance: 6.905774116516113, lamda: 1.5723552703857422
Running avgs for agent 2: q_loss: 43.11222839355469, p_loss: -5.472967147827148, mean_rew: -10.873484676259768, variance: 7.406431674957275, lamda: 1.4814317226409912

steps: 949975, episodes: 38000, mean episode reward: -847.9870747433006, agent episode reward: [-282.66235824776686, -282.66235824776686, -282.66235824776686], time: 163.727
steps: 949975, episodes: 38000, mean episode variance: 5.454435660362243, agent episode variance: [1.8597473778724671, 1.740452907562256, 1.8542353749275207], time: 163.727
Running avgs for agent 0: q_loss: 31.769643783569336, p_loss: -5.489621162414551, mean_rew: -10.879737519368607, variance: 7.438989162445068, lamda: 1.4850711822509766
Running avgs for agent 1: q_loss: 53.178253173828125, p_loss: -5.482962131500244, mean_rew: -10.879491600585158, variance: 6.961811542510986, lamda: 1.5773252248764038
Running avgs for agent 2: q_loss: 48.14205551147461, p_loss: -5.486453056335449, mean_rew: -10.88259857183495, variance: 7.4169416427612305, lamda: 1.4863364696502686

steps: 974975, episodes: 39000, mean episode reward: -873.5759314228496, agent episode reward: [-291.19197714094986, -291.19197714094986, -291.19197714094986], time: 161.529
steps: 974975, episodes: 39000, mean episode variance: 5.425544204235077, agent episode variance: [1.828387300491333, 1.733207543373108, 1.863949360370636], time: 161.53
Running avgs for agent 0: q_loss: 32.50630187988281, p_loss: -5.512058734893799, mean_rew: -10.917967895133438, variance: 7.313549518585205, lamda: 1.498374342918396
Running avgs for agent 1: q_loss: 52.21452713012695, p_loss: -5.506340503692627, mean_rew: -10.905203828526355, variance: 6.932829856872559, lamda: 1.580866813659668
Running avgs for agent 2: q_loss: 46.55665588378906, p_loss: -5.494501113891602, mean_rew: -10.910170034193245, variance: 7.45579719543457, lamda: 1.488802194595337

steps: 999975, episodes: 40000, mean episode reward: -884.4067646961436, agent episode reward: [-294.80225489871447, -294.80225489871447, -294.80225489871447], time: 169.227
steps: 999975, episodes: 40000, mean episode variance: 5.4098993973731995, agent episode variance: [1.8302268061637879, 1.7411796452999115, 1.8384929459095], time: 169.228
Running avgs for agent 0: q_loss: 42.9112548828125, p_loss: -5.505550861358643, mean_rew: -10.9279175490143, variance: 7.320907115936279, lamda: 1.5134332180023193
Running avgs for agent 1: q_loss: 53.07655715942383, p_loss: -5.515375137329102, mean_rew: -10.937028577763995, variance: 6.964718341827393, lamda: 1.5856544971466064
Running avgs for agent 2: q_loss: 47.410552978515625, p_loss: -5.5099101066589355, mean_rew: -10.919568057905952, variance: 7.353971481323242, lamda: 1.4910857677459717

steps: 1024975, episodes: 41000, mean episode reward: -879.3367546717243, agent episode reward: [-293.11225155724145, -293.11225155724145, -293.11225155724145], time: 169.526
steps: 1024975, episodes: 41000, mean episode variance: 5.432136643171311, agent episode variance: [1.8353693144321441, 1.7366219942569732, 1.860145334482193], time: 169.526
Running avgs for agent 0: q_loss: 48.079437255859375, p_loss: -5.542298793792725, mean_rew: -10.999894013128268, variance: 7.341476917266846, lamda: 1.5164192914962769
Running avgs for agent 1: q_loss: 52.11760711669922, p_loss: -5.547491073608398, mean_rew: -10.991652349545014, variance: 6.946487903594971, lamda: 1.5898805856704712
Running avgs for agent 2: q_loss: 48.05856704711914, p_loss: -5.5427374839782715, mean_rew: -10.994630907295875, variance: 7.44058084487915, lamda: 1.4941996335983276

steps: 1049975, episodes: 42000, mean episode reward: -865.7581500746927, agent episode reward: [-288.58605002489753, -288.58605002489753, -288.58605002489753], time: 175.237
steps: 1049975, episodes: 42000, mean episode variance: 5.441191577911377, agent episode variance: [1.8487794296741487, 1.7296173210144044, 1.862794827222824], time: 175.237
Running avgs for agent 0: q_loss: 48.648494720458984, p_loss: -5.59292459487915, mean_rew: -11.098570305394016, variance: 7.39511775970459, lamda: 1.5169358253479004
Running avgs for agent 1: q_loss: 37.67735290527344, p_loss: -5.593785285949707, mean_rew: -11.087634438242564, variance: 6.918468952178955, lamda: 1.603797435760498
Running avgs for agent 2: q_loss: 48.34526443481445, p_loss: -5.583989143371582, mean_rew: -11.08202727212854, variance: 7.451179027557373, lamda: 1.4977363348007202

steps: 1074975, episodes: 43000, mean episode reward: -871.5691852621478, agent episode reward: [-290.52306175404925, -290.52306175404925, -290.52306175404925], time: 169.081
steps: 1074975, episodes: 43000, mean episode variance: 5.4682206466197965, agent episode variance: [1.872589406490326, 1.7210412769317627, 1.874589963197708], time: 169.082
Running avgs for agent 0: q_loss: 48.31371307373047, p_loss: -5.601368427276611, mean_rew: -11.131513219215767, variance: 7.490357398986816, lamda: 1.5186058282852173
Running avgs for agent 1: q_loss: 33.75970458984375, p_loss: -5.615408420562744, mean_rew: -11.13345111810992, variance: 6.884164810180664, lamda: 1.6220158338546753
Running avgs for agent 2: q_loss: 46.96672058105469, p_loss: -5.6021952629089355, mean_rew: -11.124857247305933, variance: 7.4983601570129395, lamda: 1.5008476972579956

steps: 1099975, episodes: 44000, mean episode reward: -875.84567416716, agent episode reward: [-291.94855805572007, -291.94855805572007, -291.94855805572007], time: 161.016
steps: 1099975, episodes: 44000, mean episode variance: 5.432867963552475, agent episode variance: [1.8302616746425628, 1.710051016330719, 1.8925552725791932], time: 161.016
Running avgs for agent 0: q_loss: 47.47929000854492, p_loss: -5.627591133117676, mean_rew: -11.164842171728072, variance: 7.321046829223633, lamda: 1.519715666770935
Running avgs for agent 1: q_loss: 34.509490966796875, p_loss: -5.6271257400512695, mean_rew: -11.15249937657025, variance: 6.840204238891602, lamda: 1.6354023218154907
Running avgs for agent 2: q_loss: 46.600074768066406, p_loss: -5.618982315063477, mean_rew: -11.151478581452611, variance: 7.570221424102783, lamda: 1.5033129453659058

steps: 1124975, episodes: 45000, mean episode reward: -877.9585006759631, agent episode reward: [-292.6528335586544, -292.6528335586544, -292.6528335586544], time: 161.071
steps: 1124975, episodes: 45000, mean episode variance: 5.417774375915528, agent episode variance: [1.8504976279735565, 1.6930483152866365, 1.8742284326553345], time: 161.071
Running avgs for agent 0: q_loss: 46.94855499267578, p_loss: -5.639587879180908, mean_rew: -11.187693634653622, variance: 7.4019904136657715, lamda: 1.5204284191131592
Running avgs for agent 1: q_loss: 34.60673904418945, p_loss: -5.6387224197387695, mean_rew: -11.178670535716407, variance: 6.772193431854248, lamda: 1.6528738737106323
Running avgs for agent 2: q_loss: 46.130001068115234, p_loss: -5.627810955047607, mean_rew: -11.156244707368952, variance: 7.496913433074951, lamda: 1.504906177520752

steps: 1149975, episodes: 46000, mean episode reward: -856.1654135966752, agent episode reward: [-285.3884711988917, -285.3884711988917, -285.3884711988917], time: 175.444
steps: 1149975, episodes: 46000, mean episode variance: 5.4175159900188445, agent episode variance: [1.8626467518806458, 1.6822897381782531, 1.8725794999599457], time: 175.444
Running avgs for agent 0: q_loss: 46.227970123291016, p_loss: -5.633624076843262, mean_rew: -11.194218203066251, variance: 7.450586795806885, lamda: 1.5208712816238403
Running avgs for agent 1: q_loss: 32.93489074707031, p_loss: -5.652695178985596, mean_rew: -11.18817040259163, variance: 6.729158878326416, lamda: 1.6684662103652954
Running avgs for agent 2: q_loss: 45.98604202270508, p_loss: -5.635087966918945, mean_rew: -11.177709200801392, variance: 7.490317344665527, lamda: 1.507804274559021

steps: 1174975, episodes: 47000, mean episode reward: -853.2519596204039, agent episode reward: [-284.41731987346805, -284.41731987346805, -284.41731987346805], time: 176.403
steps: 1174975, episodes: 47000, mean episode variance: 5.405190656423569, agent episode variance: [1.856609181880951, 1.6714603285789489, 1.8771211459636687], time: 176.404
Running avgs for agent 0: q_loss: 46.05371856689453, p_loss: -5.652334213256836, mean_rew: -11.223647533590336, variance: 7.426436424255371, lamda: 1.5218815803527832
Running avgs for agent 1: q_loss: 30.975059509277344, p_loss: -5.661356449127197, mean_rew: -11.220810881551097, variance: 6.685840606689453, lamda: 1.6794519424438477
Running avgs for agent 2: q_loss: 45.27642059326172, p_loss: -5.660831928253174, mean_rew: -11.229627190770826, variance: 7.508484363555908, lamda: 1.5094348192214966

steps: 1199975, episodes: 48000, mean episode reward: -861.3764585861823, agent episode reward: [-287.12548619539405, -287.12548619539405, -287.12548619539405], time: 166.513
steps: 1199975, episodes: 48000, mean episode variance: 5.425322165966034, agent episode variance: [1.8637222561836242, 1.6787979211807251, 1.8828019886016845], time: 166.513
Running avgs for agent 0: q_loss: 46.11328887939453, p_loss: -5.676876544952393, mean_rew: -11.271227811889043, variance: 7.454889297485352, lamda: 1.5232309103012085
Running avgs for agent 1: q_loss: 30.904796600341797, p_loss: -5.684755325317383, mean_rew: -11.257635793891357, variance: 6.71519136428833, lamda: 1.6861720085144043
Running avgs for agent 2: q_loss: 45.69318771362305, p_loss: -5.678110122680664, mean_rew: -11.25353925916434, variance: 7.531208038330078, lamda: 1.510582447052002

steps: 1224975, episodes: 49000, mean episode reward: -833.4100418924407, agent episode reward: [-277.8033472974803, -277.8033472974803, -277.8033472974803], time: 168.22
steps: 1224975, episodes: 49000, mean episode variance: 5.433315215826035, agent episode variance: [1.8661237597465514, 1.676023157596588, 1.8911682984828948], time: 168.221
Running avgs for agent 0: q_loss: 45.520469665527344, p_loss: -5.685419082641602, mean_rew: -11.293363810710519, variance: 7.4644951820373535, lamda: 1.5250991582870483
Running avgs for agent 1: q_loss: 31.71137809753418, p_loss: -5.711226940155029, mean_rew: -11.3120904077401, variance: 6.704092502593994, lamda: 1.6966406106948853
Running avgs for agent 2: q_loss: 44.68475341796875, p_loss: -5.685684680938721, mean_rew: -11.285961556706296, variance: 7.56467342376709, lamda: 1.5127077102661133

steps: 1249975, episodes: 50000, mean episode reward: -851.8009782225193, agent episode reward: [-283.93365940750647, -283.93365940750647, -283.93365940750647], time: 172.405
steps: 1249975, episodes: 50000, mean episode variance: 5.449812121391297, agent episode variance: [1.876318342447281, 1.6721192231178283, 1.9013745558261872], time: 172.405
Running avgs for agent 0: q_loss: 45.373207092285156, p_loss: -5.696343898773193, mean_rew: -11.323907853632983, variance: 7.505273342132568, lamda: 1.5255937576293945
Running avgs for agent 1: q_loss: 30.87989044189453, p_loss: -5.720806121826172, mean_rew: -11.337713415087027, variance: 6.6884765625, lamda: 1.7036489248275757
Running avgs for agent 2: q_loss: 44.38656234741211, p_loss: -5.734109401702881, mean_rew: -11.360813683441917, variance: 7.60549783706665, lamda: 1.5137040615081787

steps: 1274975, episodes: 51000, mean episode reward: -857.9323271345468, agent episode reward: [-285.97744237818233, -285.97744237818233, -285.97744237818233], time: 171.555
steps: 1274975, episodes: 51000, mean episode variance: 5.46569847536087, agent episode variance: [1.878603394985199, 1.6739903099536895, 1.913104770421982], time: 171.555
Running avgs for agent 0: q_loss: 45.59560775756836, p_loss: -5.7318925857543945, mean_rew: -11.387937053462007, variance: 7.514413356781006, lamda: 1.5267584323883057
Running avgs for agent 1: q_loss: 43.553585052490234, p_loss: -5.747688293457031, mean_rew: -11.400272418755536, variance: 6.6959614753723145, lamda: 1.713937759399414
Running avgs for agent 2: q_loss: 44.963314056396484, p_loss: -5.746547698974609, mean_rew: -11.392533208955669, variance: 7.652419090270996, lamda: 1.5152298212051392

steps: 1299975, episodes: 52000, mean episode reward: -838.0639242920463, agent episode reward: [-279.35464143068214, -279.35464143068214, -279.35464143068214], time: 173.528
steps: 1299975, episodes: 52000, mean episode variance: 5.444385235309601, agent episode variance: [1.8844169352054596, 1.6714814624786376, 1.8884868376255035], time: 173.529
Running avgs for agent 0: q_loss: 45.63542175292969, p_loss: -5.741856575012207, mean_rew: -11.434124066282537, variance: 7.537667751312256, lamda: 1.5278819799423218
Running avgs for agent 1: q_loss: 52.30043411254883, p_loss: -5.762808799743652, mean_rew: -11.44473554023388, variance: 6.6859259605407715, lamda: 1.7180445194244385
Running avgs for agent 2: q_loss: 44.34986877441406, p_loss: -5.769095420837402, mean_rew: -11.438748255681421, variance: 7.553947448730469, lamda: 1.517013430595398

steps: 1324975, episodes: 53000, mean episode reward: -820.2475543978708, agent episode reward: [-273.4158514659569, -273.4158514659569, -273.4158514659569], time: 174.482
steps: 1324975, episodes: 53000, mean episode variance: 5.507636165380478, agent episode variance: [1.9017911999225616, 1.6806775605678559, 1.9251674048900604], time: 174.482
Running avgs for agent 0: q_loss: 46.76142883300781, p_loss: -5.768184661865234, mean_rew: -11.470756809016637, variance: 7.6071648597717285, lamda: 1.5304509401321411
Running avgs for agent 1: q_loss: 51.950931549072266, p_loss: -5.781543731689453, mean_rew: -11.466495445803169, variance: 6.722710609436035, lamda: 1.719238519668579
Running avgs for agent 2: q_loss: 43.39557647705078, p_loss: -5.7857489585876465, mean_rew: -11.494488850615635, variance: 7.700669765472412, lamda: 1.5179804563522339

steps: 1349975, episodes: 54000, mean episode reward: -824.5277588027245, agent episode reward: [-274.8425862675748, -274.8425862675748, -274.8425862675748], time: 175.005
steps: 1349975, episodes: 54000, mean episode variance: 5.478682883262635, agent episode variance: [1.8828582589626313, 1.685491533279419, 1.910333091020584], time: 175.005
Running avgs for agent 0: q_loss: 45.945335388183594, p_loss: -5.785017967224121, mean_rew: -11.510111780693876, variance: 7.53143310546875, lamda: 1.5315932035446167
Running avgs for agent 1: q_loss: 51.26536560058594, p_loss: -5.804278373718262, mean_rew: -11.506915839240666, variance: 6.7419657707214355, lamda: 1.721111536026001
Running avgs for agent 2: q_loss: 40.161685943603516, p_loss: -5.801267623901367, mean_rew: -11.507578901403683, variance: 7.641332149505615, lamda: 1.5194329023361206

steps: 1374975, episodes: 55000, mean episode reward: -811.6230179757711, agent episode reward: [-270.5410059919237, -270.5410059919237, -270.5410059919237], time: 176.477
steps: 1374975, episodes: 55000, mean episode variance: 5.458567184686661, agent episode variance: [1.8876213753223419, 1.6915773558616638, 1.8793684535026551], time: 176.477
Running avgs for agent 0: q_loss: 45.72776794433594, p_loss: -5.7977824211120605, mean_rew: -11.52147043479648, variance: 7.550485134124756, lamda: 1.5322389602661133
Running avgs for agent 1: q_loss: 52.049015045166016, p_loss: -5.81474494934082, mean_rew: -11.525146946506181, variance: 6.76630973815918, lamda: 1.7229909896850586
Running avgs for agent 2: q_loss: 30.04514503479004, p_loss: -5.805968761444092, mean_rew: -11.510284457867574, variance: 7.517474174499512, lamda: 1.5312387943267822

steps: 1399975, episodes: 56000, mean episode reward: -831.8068479709705, agent episode reward: [-277.2689493236568, -277.2689493236568, -277.2689493236568], time: 174.998
steps: 1399975, episodes: 56000, mean episode variance: 5.451891373157501, agent episode variance: [1.8941460516452788, 1.688843422651291, 1.8689018988609314], time: 174.998
Running avgs for agent 0: q_loss: 44.878578186035156, p_loss: -5.7837815284729, mean_rew: -11.518882366660614, variance: 7.576584339141846, lamda: 1.5327397584915161
Running avgs for agent 1: q_loss: 51.80253982543945, p_loss: -5.822644233703613, mean_rew: -11.539309466217881, variance: 6.755373954772949, lamda: 1.7257899045944214
Running avgs for agent 2: q_loss: 28.87896728515625, p_loss: -5.817735195159912, mean_rew: -11.536605382271809, variance: 7.475607395172119, lamda: 1.5443423986434937

steps: 1424975, episodes: 57000, mean episode reward: -799.8106868491953, agent episode reward: [-266.6035622830652, -266.6035622830652, -266.6035622830652], time: 176.787
steps: 1424975, episodes: 57000, mean episode variance: 5.40146206998825, agent episode variance: [1.8618279941082, 1.6751003544330596, 1.864533721446991], time: 176.787
Running avgs for agent 0: q_loss: 45.74462127685547, p_loss: -5.7895355224609375, mean_rew: -11.503454487674302, variance: 7.447311878204346, lamda: 1.533410906791687
Running avgs for agent 1: q_loss: 51.5699462890625, p_loss: -5.810581207275391, mean_rew: -11.505673237626533, variance: 6.700401306152344, lamda: 1.728261947631836
Running avgs for agent 2: q_loss: 27.77644157409668, p_loss: -5.806064605712891, mean_rew: -11.513797885507325, variance: 7.458134651184082, lamda: 1.555860996246338

steps: 1449975, episodes: 58000, mean episode reward: -798.5280086569563, agent episode reward: [-266.1760028856521, -266.1760028856521, -266.1760028856521], time: 177.946
steps: 1449975, episodes: 58000, mean episode variance: 5.399283634662628, agent episode variance: [1.8808333799839019, 1.6741868040561676, 1.8442634506225586], time: 177.946
Running avgs for agent 0: q_loss: 36.88409423828125, p_loss: -5.77225399017334, mean_rew: -11.482443156549206, variance: 7.523333549499512, lamda: 1.5350795984268188
Running avgs for agent 1: q_loss: 42.272701263427734, p_loss: -5.8201446533203125, mean_rew: -11.501896749007742, variance: 6.696747303009033, lamda: 1.7338860034942627
Running avgs for agent 2: q_loss: 29.021240234375, p_loss: -5.80006217956543, mean_rew: -11.487019454756634, variance: 7.377053737640381, lamda: 1.5661524534225464

steps: 1474975, episodes: 59000, mean episode reward: -776.0339908171667, agent episode reward: [-258.67799693905556, -258.67799693905556, -258.67799693905556], time: 169.24
steps: 1474975, episodes: 59000, mean episode variance: 5.349243405818939, agent episode variance: [1.88123362159729, 1.6476198205947876, 1.8203899636268617], time: 169.24
Running avgs for agent 0: q_loss: 44.638553619384766, p_loss: -5.771139621734619, mean_rew: -11.499921790055064, variance: 7.524934768676758, lamda: 1.540601372718811
Running avgs for agent 1: q_loss: 47.9119873046875, p_loss: -5.7922210693359375, mean_rew: -11.45126293337395, variance: 6.590479373931885, lamda: 1.7465059757232666
Running avgs for agent 2: q_loss: 29.2985897064209, p_loss: -5.791149616241455, mean_rew: -11.47501028806211, variance: 7.281559944152832, lamda: 1.5780869722366333

steps: 1499975, episodes: 60000, mean episode reward: -771.0844756834554, agent episode reward: [-257.02815856115177, -257.02815856115177, -257.02815856115177], time: 172.468
steps: 1499975, episodes: 60000, mean episode variance: 5.304757294416428, agent episode variance: [1.865578825712204, 1.6263699193000793, 1.8128085494041444], time: 172.468
Running avgs for agent 0: q_loss: 43.338016510009766, p_loss: -5.750759124755859, mean_rew: -11.436858570810669, variance: 7.462315559387207, lamda: 1.54166841506958
Running avgs for agent 1: q_loss: 34.42369842529297, p_loss: -5.803873062133789, mean_rew: -11.441662472771158, variance: 6.505479335784912, lamda: 1.756806492805481
Running avgs for agent 2: q_loss: 27.187700271606445, p_loss: -5.771373271942139, mean_rew: -11.435555980982056, variance: 7.251234531402588, lamda: 1.5889428853988647

steps: 1524975, episodes: 61000, mean episode reward: -773.1469410618611, agent episode reward: [-257.7156470206204, -257.7156470206204, -257.7156470206204], time: 170.927
steps: 1524975, episodes: 61000, mean episode variance: 5.254793460607528, agent episode variance: [1.8565620667934417, 1.6097655429840088, 1.7884658508300781], time: 170.928
Running avgs for agent 0: q_loss: 42.84136199951172, p_loss: -5.733992576599121, mean_rew: -11.41234930875036, variance: 7.426248073577881, lamda: 1.541969895362854
Running avgs for agent 1: q_loss: 34.15736770629883, p_loss: -5.785546779632568, mean_rew: -11.415422461681008, variance: 6.439062118530273, lamda: 1.7697668075561523
Running avgs for agent 2: q_loss: 26.279672622680664, p_loss: -5.75453519821167, mean_rew: -11.39724773532903, variance: 7.153863906860352, lamda: 1.5990386009216309

steps: 1549975, episodes: 62000, mean episode reward: -766.2499975447829, agent episode reward: [-255.41666584826098, -255.41666584826098, -255.41666584826098], time: 174.974
steps: 1549975, episodes: 62000, mean episode variance: 5.231026646137238, agent episode variance: [1.845809736251831, 1.605068027973175, 1.7801488819122315], time: 174.975
Running avgs for agent 0: q_loss: 42.98242950439453, p_loss: -5.710312843322754, mean_rew: -11.382714150963658, variance: 7.383238792419434, lamda: 1.5426404476165771
Running avgs for agent 1: q_loss: 38.25422286987305, p_loss: -5.771562576293945, mean_rew: -11.384020141022104, variance: 6.420271873474121, lamda: 1.7811594009399414
Running avgs for agent 2: q_loss: 24.91196632385254, p_loss: -5.7361297607421875, mean_rew: -11.373758233851444, variance: 7.120595932006836, lamda: 1.6041114330291748

steps: 1574975, episodes: 63000, mean episode reward: -723.7270581507819, agent episode reward: [-241.24235271692731, -241.24235271692731, -241.24235271692731], time: 168.91
steps: 1574975, episodes: 63000, mean episode variance: 5.199822548866272, agent episode variance: [1.8521182808876038, 1.5658694245815277, 1.7818348433971405], time: 168.91
Running avgs for agent 0: q_loss: 42.707008361816406, p_loss: -5.694055557250977, mean_rew: -11.337237155383749, variance: 7.408473014831543, lamda: 1.5434490442276
Running avgs for agent 1: q_loss: 42.40354537963867, p_loss: -5.7529120445251465, mean_rew: -11.343846342285488, variance: 6.263477325439453, lamda: 1.79727303981781
Running avgs for agent 2: q_loss: 32.61086654663086, p_loss: -5.712619304656982, mean_rew: -11.338848497250638, variance: 7.1273393630981445, lamda: 1.6143630743026733

steps: 1599975, episodes: 64000, mean episode reward: -757.658332951714, agent episode reward: [-252.5527776505713, -252.5527776505713, -252.5527776505713], time: 173.942
steps: 1599975, episodes: 64000, mean episode variance: 5.138860519886017, agent episode variance: [1.8231353595256805, 1.568488677740097, 1.7472364826202393], time: 173.942
Running avgs for agent 0: q_loss: 42.417415618896484, p_loss: -5.6656575202941895, mean_rew: -11.280684260658854, variance: 7.29254150390625, lamda: 1.5450416803359985
Running avgs for agent 1: q_loss: 48.328636169433594, p_loss: -5.727095603942871, mean_rew: -11.300702051010028, variance: 6.27395486831665, lamda: 1.8021069765090942
Running avgs for agent 2: q_loss: 28.819473266601562, p_loss: -5.688223361968994, mean_rew: -11.286666801720752, variance: 6.988945960998535, lamda: 1.628385066986084

steps: 1624975, episodes: 65000, mean episode reward: -767.2202825177349, agent episode reward: [-255.7400941725783, -255.7400941725783, -255.7400941725783], time: 176.548
steps: 1624975, episodes: 65000, mean episode variance: 5.096239654064179, agent episode variance: [1.8296070702075957, 1.547355009317398, 1.7192775745391846], time: 176.548
Running avgs for agent 0: q_loss: 42.206687927246094, p_loss: -5.637106895446777, mean_rew: -11.229007234382294, variance: 7.318428039550781, lamda: 1.5458446741104126
Running avgs for agent 1: q_loss: 38.29671096801758, p_loss: -5.703553199768066, mean_rew: -11.253611657697697, variance: 6.189419746398926, lamda: 1.8077692985534668
Running avgs for agent 2: q_loss: 26.001277923583984, p_loss: -5.670655250549316, mean_rew: -11.248811418821614, variance: 6.877110481262207, lamda: 1.639620065689087

steps: 1649975, episodes: 66000, mean episode reward: -758.6260901791379, agent episode reward: [-252.87536339304597, -252.87536339304597, -252.87536339304597], time: 179.127
steps: 1649975, episodes: 66000, mean episode variance: 5.065130121231079, agent episode variance: [1.820582721710205, 1.5401003456115723, 1.7044470539093017], time: 179.127
Running avgs for agent 0: q_loss: 42.2590217590332, p_loss: -5.615485191345215, mean_rew: -11.197337826858222, variance: 7.2823309898376465, lamda: 1.5477222204208374
Running avgs for agent 1: q_loss: 31.057960510253906, p_loss: -5.678586483001709, mean_rew: -11.202049952996786, variance: 6.160401344299316, lamda: 1.8225029706954956
Running avgs for agent 2: q_loss: 27.863452911376953, p_loss: -5.649407386779785, mean_rew: -11.203916259040728, variance: 6.817788124084473, lamda: 1.6501755714416504

steps: 1674975, episodes: 67000, mean episode reward: -752.6570889516212, agent episode reward: [-250.88569631720708, -250.88569631720708, -250.88569631720708], time: 183.741
steps: 1674975, episodes: 67000, mean episode variance: 5.019538774251938, agent episode variance: [1.8108899915218353, 1.5222434358596801, 1.6864053468704223], time: 183.742
Running avgs for agent 0: q_loss: 41.4178352355957, p_loss: -5.598194122314453, mean_rew: -11.151158742912276, variance: 7.24355936050415, lamda: 1.5490562915802002
Running avgs for agent 1: q_loss: 32.6419677734375, p_loss: -5.648355960845947, mean_rew: -11.144036218432362, variance: 6.088973522186279, lamda: 1.8353383541107178
Running avgs for agent 2: q_loss: 38.1193733215332, p_loss: -5.617769718170166, mean_rew: -11.144737873344988, variance: 6.745621204376221, lamda: 1.658739447593689

steps: 1699975, episodes: 68000, mean episode reward: -760.3485854707038, agent episode reward: [-253.44952849023457, -253.44952849023457, -253.44952849023457], time: 182.85
steps: 1699975, episodes: 68000, mean episode variance: 5.001986521244049, agent episode variance: [1.8038611309528352, 1.4997115213871002, 1.6984138689041137], time: 182.851
Running avgs for agent 0: q_loss: 41.384368896484375, p_loss: -5.583059787750244, mean_rew: -11.108262630176538, variance: 7.215445041656494, lamda: 1.5503569841384888
Running avgs for agent 1: q_loss: 36.55766296386719, p_loss: -5.635838985443115, mean_rew: -11.112167948822288, variance: 5.99884557723999, lamda: 1.8524878025054932
Running avgs for agent 2: q_loss: 41.50380325317383, p_loss: -5.604764461517334, mean_rew: -11.104508947518273, variance: 6.7936553955078125, lamda: 1.6608284711837769

steps: 1724975, episodes: 69000, mean episode reward: -771.4265674927991, agent episode reward: [-257.14218916426637, -257.14218916426637, -257.14218916426637], time: 176.905
steps: 1724975, episodes: 69000, mean episode variance: 4.9435667722225185, agent episode variance: [1.7815898928642273, 1.4842318108081818, 1.6777450685501099], time: 176.906
Running avgs for agent 0: q_loss: 41.533058166503906, p_loss: -5.558993339538574, mean_rew: -11.074520126017447, variance: 7.126359939575195, lamda: 1.553702712059021
Running avgs for agent 1: q_loss: 48.27076721191406, p_loss: -5.616269111633301, mean_rew: -11.074261006327099, variance: 5.936927318572998, lamda: 1.861480712890625
Running avgs for agent 2: q_loss: 40.46470260620117, p_loss: -5.595816612243652, mean_rew: -11.086805551130865, variance: 6.710980415344238, lamda: 1.6623671054840088

steps: 1749975, episodes: 70000, mean episode reward: -762.0723212178265, agent episode reward: [-254.0241070726088, -254.0241070726088, -254.0241070726088], time: 172.666
steps: 1749975, episodes: 70000, mean episode variance: 4.9340595967769625, agent episode variance: [1.7796762688159942, 1.4847531838417054, 1.6696301441192627], time: 172.667
Running avgs for agent 0: q_loss: 41.31159973144531, p_loss: -5.539865016937256, mean_rew: -11.03081548534692, variance: 7.1187052726745605, lamda: 1.5569236278533936
Running avgs for agent 1: q_loss: 47.62876892089844, p_loss: -5.591556549072266, mean_rew: -11.025635158990728, variance: 5.93901252746582, lamda: 1.863383412361145
Running avgs for agent 2: q_loss: 40.851890563964844, p_loss: -5.574840545654297, mean_rew: -11.052765982816464, variance: 6.678520679473877, lamda: 1.6641507148742676

steps: 1774975, episodes: 71000, mean episode reward: -781.4971218993727, agent episode reward: [-260.49904063312425, -260.49904063312425, -260.49904063312425], time: 172.653
steps: 1774975, episodes: 71000, mean episode variance: 4.922265885591507, agent episode variance: [1.7848812894821167, 1.4738812994956971, 1.6635032966136933], time: 172.653
Running avgs for agent 0: q_loss: 40.72751998901367, p_loss: -5.530420780181885, mean_rew: -11.025351870048999, variance: 7.139525413513184, lamda: 1.5589553117752075
Running avgs for agent 1: q_loss: 48.257171630859375, p_loss: -5.575765609741211, mean_rew: -11.007406703731425, variance: 5.8955254554748535, lamda: 1.866125226020813
Running avgs for agent 2: q_loss: 40.09260940551758, p_loss: -5.549278736114502, mean_rew: -11.010915274511783, variance: 6.654013633728027, lamda: 1.6659022569656372

steps: 1799975, episodes: 72000, mean episode reward: -787.676937319844, agent episode reward: [-262.55897910661463, -262.55897910661463, -262.55897910661463], time: 177.954
steps: 1799975, episodes: 72000, mean episode variance: 4.900786525011062, agent episode variance: [1.780093538761139, 1.4681139674186707, 1.652579018831253], time: 177.954
Running avgs for agent 0: q_loss: 40.878265380859375, p_loss: -5.520849704742432, mean_rew: -10.998557077893034, variance: 7.1203742027282715, lamda: 1.5603290796279907
Running avgs for agent 1: q_loss: 49.06446075439453, p_loss: -5.583645343780518, mean_rew: -11.006608366173703, variance: 5.872456073760986, lamda: 1.8702173233032227
Running avgs for agent 2: q_loss: 40.63700866699219, p_loss: -5.537896633148193, mean_rew: -10.978843360426684, variance: 6.610316276550293, lamda: 1.6679307222366333

steps: 1824975, episodes: 73000, mean episode reward: -787.7932024793704, agent episode reward: [-262.59773415979015, -262.59773415979015, -262.59773415979015], time: 174.1
steps: 1824975, episodes: 73000, mean episode variance: 4.892992535352707, agent episode variance: [1.76737108874321, 1.4746295166015626, 1.6509919300079345], time: 174.101
Running avgs for agent 0: q_loss: 40.226890563964844, p_loss: -5.513522148132324, mean_rew: -10.971186196806539, variance: 7.069484233856201, lamda: 1.5627292394638062
Running avgs for agent 1: q_loss: 45.21463394165039, p_loss: -5.5605788230896, mean_rew: -10.95669101190018, variance: 5.898518085479736, lamda: 1.8748811483383179
Running avgs for agent 2: q_loss: 37.24446487426758, p_loss: -5.525666236877441, mean_rew: -10.962357567587873, variance: 6.603967666625977, lamda: 1.673065185546875

steps: 1849975, episodes: 74000, mean episode reward: -789.9776308922333, agent episode reward: [-263.3258769640777, -263.3258769640777, -263.3258769640777], time: 175.787
steps: 1849975, episodes: 74000, mean episode variance: 4.843950519323349, agent episode variance: [1.7574317669868469, 1.4551753294467926, 1.6313434228897095], time: 175.787
Running avgs for agent 0: q_loss: 28.09324073791504, p_loss: -5.505859851837158, mean_rew: -10.944322145998209, variance: 7.029726982116699, lamda: 1.5679185390472412
Running avgs for agent 1: q_loss: 42.54008483886719, p_loss: -5.551201820373535, mean_rew: -10.936068265977545, variance: 5.8207011222839355, lamda: 1.885227084159851
Running avgs for agent 2: q_loss: 27.338062286376953, p_loss: -5.516745567321777, mean_rew: -10.941217733028127, variance: 6.525373935699463, lamda: 1.6846479177474976

steps: 1874975, episodes: 75000, mean episode reward: -794.4503092874847, agent episode reward: [-264.81676976249497, -264.81676976249497, -264.81676976249497], time: 172.547
steps: 1874975, episodes: 75000, mean episode variance: 4.801155406236648, agent episode variance: [1.7331578178405762, 1.4602303507328034, 1.607767237663269], time: 172.548
Running avgs for agent 0: q_loss: 26.614835739135742, p_loss: -5.465238094329834, mean_rew: -10.880845892848455, variance: 6.932631492614746, lamda: 1.580289602279663
Running avgs for agent 1: q_loss: 39.2137336730957, p_loss: -5.5223164558410645, mean_rew: -10.905052320307076, variance: 5.840920925140381, lamda: 1.8928176164627075
Running avgs for agent 2: q_loss: 27.621713638305664, p_loss: -5.502694129943848, mean_rew: -10.906383278930505, variance: 6.431069374084473, lamda: 1.6976994276046753

steps: 1899975, episodes: 76000, mean episode reward: -796.6543988918016, agent episode reward: [-265.5514662972672, -265.5514662972672, -265.5514662972672], time: 175.914
steps: 1899975, episodes: 76000, mean episode variance: 4.7547467634677885, agent episode variance: [1.7188575885295867, 1.4326504530906676, 1.6032387218475341], time: 175.915
Running avgs for agent 0: q_loss: 32.55194091796875, p_loss: -5.461216449737549, mean_rew: -10.868303711775301, variance: 6.875430583953857, lamda: 1.5936883687973022
Running avgs for agent 1: q_loss: 32.52974319458008, p_loss: -5.5225443840026855, mean_rew: -10.862168013818588, variance: 5.730601787567139, lamda: 1.9122421741485596
Running avgs for agent 2: q_loss: 27.824865341186523, p_loss: -5.467582702636719, mean_rew: -10.858519214390933, variance: 6.412955284118652, lamda: 1.7121325731277466

steps: 1924975, episodes: 77000, mean episode reward: -811.0669828739116, agent episode reward: [-270.35566095797054, -270.35566095797054, -270.35566095797054], time: 178.169
steps: 1924975, episodes: 77000, mean episode variance: 4.688343872785568, agent episode variance: [1.7011148941516876, 1.4074467453956605, 1.5797822332382203], time: 178.169
Running avgs for agent 0: q_loss: 39.39375686645508, p_loss: -5.451982498168945, mean_rew: -10.843427664259954, variance: 6.804460048675537, lamda: 1.598077654838562
Running avgs for agent 1: q_loss: 36.45049285888672, p_loss: -5.506451606750488, mean_rew: -10.851631751747172, variance: 5.629786968231201, lamda: 1.928195595741272
Running avgs for agent 2: q_loss: 26.565933227539062, p_loss: -5.4728217124938965, mean_rew: -10.871641424110313, variance: 6.31912899017334, lamda: 1.7296557426452637

steps: 1949975, episodes: 78000, mean episode reward: -812.0614326848784, agent episode reward: [-270.68714422829277, -270.68714422829277, -270.68714422829277], time: 177.359
steps: 1949975, episodes: 78000, mean episode variance: 4.6967483940124515, agent episode variance: [1.7165849900245667, 1.4100906825065613, 1.5700727214813233], time: 177.359
Running avgs for agent 0: q_loss: 40.05859375, p_loss: -5.458613872528076, mean_rew: -10.850251542669898, variance: 6.866339683532715, lamda: 1.6005444526672363
Running avgs for agent 1: q_loss: 32.81978225708008, p_loss: -5.496683597564697, mean_rew: -10.842637010555954, variance: 5.640362739562988, lamda: 1.9415574073791504
Running avgs for agent 2: q_loss: 27.707813262939453, p_loss: -5.462368011474609, mean_rew: -10.83856523537682, variance: 6.2802910804748535, lamda: 1.7430025339126587

steps: 1974975, episodes: 79000, mean episode reward: -820.028795224821, agent episode reward: [-273.342931741607, -273.342931741607, -273.342931741607], time: 176.163
steps: 1974975, episodes: 79000, mean episode variance: 4.647623915314674, agent episode variance: [1.6995146462917328, 1.3893063362836837, 1.5588029327392579], time: 176.164
Running avgs for agent 0: q_loss: 39.998390197753906, p_loss: -5.451278209686279, mean_rew: -10.820813413613639, variance: 6.79805850982666, lamda: 1.603184461593628
Running avgs for agent 1: q_loss: 35.16355895996094, p_loss: -5.486745834350586, mean_rew: -10.80965253323815, variance: 5.557225227355957, lamda: 1.9590402841567993
Running avgs for agent 2: q_loss: 27.314788818359375, p_loss: -5.4547648429870605, mean_rew: -10.82098699319473, variance: 6.2352118492126465, lamda: 1.7549245357513428

steps: 1999975, episodes: 80000, mean episode reward: -832.9381156269917, agent episode reward: [-277.6460385423306, -277.6460385423306, -277.6460385423306], time: 174.947
steps: 1999975, episodes: 80000, mean episode variance: 4.611296186447143, agent episode variance: [1.6946606185436248, 1.3791960196495057, 1.537439548254013], time: 174.948
Running avgs for agent 0: q_loss: 39.230445861816406, p_loss: -5.450023651123047, mean_rew: -10.824686366276008, variance: 6.778642654418945, lamda: 1.6061633825302124
Running avgs for agent 1: q_loss: 29.415578842163086, p_loss: -5.482361793518066, mean_rew: -10.802504810790888, variance: 5.516784191131592, lamda: 1.9751073122024536
Running avgs for agent 2: q_loss: 41.941226959228516, p_loss: -5.45241117477417, mean_rew: -10.81554159865936, variance: 6.1497578620910645, lamda: 1.7651243209838867

steps: 2024975, episodes: 81000, mean episode reward: -838.9380866430479, agent episode reward: [-279.64602888101604, -279.64602888101604, -279.64602888101604], time: 178.999
steps: 2024975, episodes: 81000, mean episode variance: 4.614033200740814, agent episode variance: [1.7075946757793425, 1.3626135077476502, 1.5438250172138215], time: 179.0
Running avgs for agent 0: q_loss: 39.29169464111328, p_loss: -5.439474582672119, mean_rew: -10.78933667350702, variance: 6.830379009246826, lamda: 1.609298825263977
Running avgs for agent 1: q_loss: 29.019123077392578, p_loss: -5.480857849121094, mean_rew: -10.792692698680403, variance: 5.450454235076904, lamda: 1.9854795932769775
Running avgs for agent 2: q_loss: 41.081398010253906, p_loss: -5.446131706237793, mean_rew: -10.796682853674872, variance: 6.175299644470215, lamda: 1.7670609951019287

steps: 2049975, episodes: 82000, mean episode reward: -835.04807650521, agent episode reward: [-278.34935883507006, -278.34935883507006, -278.34935883507006], time: 162.797
steps: 2049975, episodes: 82000, mean episode variance: 4.558602968931198, agent episode variance: [1.6862132833003998, 1.3570787854194641, 1.5153109002113343], time: 162.798
Running avgs for agent 0: q_loss: 38.457061767578125, p_loss: -5.438338756561279, mean_rew: -10.784175210778324, variance: 6.744853496551514, lamda: 1.612924337387085
Running avgs for agent 1: q_loss: 31.77381706237793, p_loss: -5.478639602661133, mean_rew: -10.789447186289642, variance: 5.428315162658691, lamda: 2.000354290008545
Running avgs for agent 2: q_loss: 41.8953742980957, p_loss: -5.439268589019775, mean_rew: -10.773121554416415, variance: 6.061244010925293, lamda: 1.7690799236297607

steps: 2074975, episodes: 83000, mean episode reward: -851.808867268961, agent episode reward: [-283.93628908965366, -283.93628908965366, -283.93628908965366], time: 164.574
steps: 2074975, episodes: 83000, mean episode variance: 4.539910389900207, agent episode variance: [1.6657782092094422, 1.3430014607906342, 1.5311307199001312], time: 164.574
Running avgs for agent 0: q_loss: 25.723323822021484, p_loss: -5.438460350036621, mean_rew: -10.766457266988196, variance: 6.663113117218018, lamda: 1.623020052909851
Running avgs for agent 1: q_loss: 32.25404357910156, p_loss: -5.461775302886963, mean_rew: -10.764624776228448, variance: 5.372005939483643, lamda: 2.0186944007873535
Running avgs for agent 2: q_loss: 42.07903289794922, p_loss: -5.436509609222412, mean_rew: -10.770469638344533, variance: 6.124523162841797, lamda: 1.772323727607727

steps: 2099975, episodes: 84000, mean episode reward: -853.4848462877331, agent episode reward: [-284.49494876257774, -284.49494876257774, -284.49494876257774], time: 167.974
steps: 2099975, episodes: 84000, mean episode variance: 4.517644705295563, agent episode variance: [1.6633280127048493, 1.3319480440616607, 1.5223686485290526], time: 167.974
Running avgs for agent 0: q_loss: 32.36651611328125, p_loss: -5.43390417098999, mean_rew: -10.759685863077914, variance: 6.6533122062683105, lamda: 1.6374623775482178
Running avgs for agent 1: q_loss: 48.711124420166016, p_loss: -5.46091890335083, mean_rew: -10.767248149468777, variance: 5.327792167663574, lamda: 2.0322952270507812
Running avgs for agent 2: q_loss: 41.60710144042969, p_loss: -5.432737827301025, mean_rew: -10.76602947444525, variance: 6.089474678039551, lamda: 1.7749526500701904

steps: 2124975, episodes: 85000, mean episode reward: -867.6699029788433, agent episode reward: [-289.22330099294777, -289.22330099294777, -289.22330099294777], time: 169.484
steps: 2124975, episodes: 85000, mean episode variance: 4.476125756502151, agent episode variance: [1.6451984112262725, 1.3096414093971251, 1.5212859358787536], time: 169.484
Running avgs for agent 0: q_loss: 39.49863815307617, p_loss: -5.443088531494141, mean_rew: -10.772737127281578, variance: 6.580793857574463, lamda: 1.6444003582000732
Running avgs for agent 1: q_loss: 48.251258850097656, p_loss: -5.468138217926025, mean_rew: -10.753838414503157, variance: 5.238565444946289, lamda: 2.0357625484466553
Running avgs for agent 2: q_loss: 35.78192901611328, p_loss: -5.426721572875977, mean_rew: -10.746591772930772, variance: 6.085143566131592, lamda: 1.7805231809616089

steps: 2149975, episodes: 86000, mean episode reward: -856.1804942195939, agent episode reward: [-285.393498073198, -285.393498073198, -285.393498073198], time: 167.227
steps: 2149975, episodes: 86000, mean episode variance: 4.473762063026428, agent episode variance: [1.6476853005886078, 1.3220304172039032, 1.5040463452339172], time: 167.228
Running avgs for agent 0: q_loss: 37.469173431396484, p_loss: -5.438839435577393, mean_rew: -10.755673173531395, variance: 6.590741157531738, lamda: 1.6484674215316772
Running avgs for agent 1: q_loss: 48.21159744262695, p_loss: -5.453676700592041, mean_rew: -10.761261892277071, variance: 5.288122177124023, lamda: 2.0398566722869873
Running avgs for agent 2: q_loss: 27.07692527770996, p_loss: -5.43439245223999, mean_rew: -10.749268162531168, variance: 6.016185760498047, lamda: 1.7948312759399414

steps: 2174975, episodes: 87000, mean episode reward: -864.220927916018, agent episode reward: [-288.0736426386727, -288.0736426386727, -288.0736426386727], time: 163.287
steps: 2174975, episodes: 87000, mean episode variance: 4.42151333284378, agent episode variance: [1.613879102230072, 1.3152277739048004, 1.4924064567089081], time: 163.288
Running avgs for agent 0: q_loss: 38.70357894897461, p_loss: -5.462679862976074, mean_rew: -10.781013405064773, variance: 6.455516815185547, lamda: 1.6530810594558716
Running avgs for agent 1: q_loss: 31.611608505249023, p_loss: -5.465392589569092, mean_rew: -10.76173536238974, variance: 5.260910987854004, lamda: 2.050899028778076
Running avgs for agent 2: q_loss: 39.69488525390625, p_loss: -5.427389621734619, mean_rew: -10.747563585127267, variance: 5.969625949859619, lamda: 1.804585576057434

steps: 2199975, episodes: 88000, mean episode reward: -866.8713728711065, agent episode reward: [-288.9571242903689, -288.9571242903689, -288.9571242903689], time: 164.749
steps: 2199975, episodes: 88000, mean episode variance: 4.454992343187332, agent episode variance: [1.6391136255264283, 1.3103297617435454, 1.5055489559173585], time: 164.75
Running avgs for agent 0: q_loss: 35.57499313354492, p_loss: -5.43781852722168, mean_rew: -10.75506915402538, variance: 6.556454658508301, lamda: 1.655923843383789
Running avgs for agent 1: q_loss: 31.01547622680664, p_loss: -5.464588642120361, mean_rew: -10.761283593804608, variance: 5.241319179534912, lamda: 2.0655434131622314
Running avgs for agent 2: q_loss: 40.70509719848633, p_loss: -5.440160274505615, mean_rew: -10.767494621827641, variance: 6.022195816040039, lamda: 1.8160861730575562

steps: 2224975, episodes: 89000, mean episode reward: -871.0922128507509, agent episode reward: [-290.3640709502503, -290.3640709502503, -290.3640709502503], time: 176.941
steps: 2224975, episodes: 89000, mean episode variance: 4.423270564317703, agent episode variance: [1.636121975183487, 1.2955892219543457, 1.4915593671798706], time: 176.941
Running avgs for agent 0: q_loss: 35.192440032958984, p_loss: -5.446782112121582, mean_rew: -10.768810386099249, variance: 6.544488430023193, lamda: 1.6668522357940674
Running avgs for agent 1: q_loss: 36.631168365478516, p_loss: -5.475944995880127, mean_rew: -10.765159936763524, variance: 5.182356834411621, lamda: 2.0831899642944336
Running avgs for agent 2: q_loss: 43.83059310913086, p_loss: -5.435053825378418, mean_rew: -10.758895109915661, variance: 5.9662370681762695, lamda: 1.8211020231246948

steps: 2249975, episodes: 90000, mean episode reward: -874.3851522362793, agent episode reward: [-291.4617174120931, -291.4617174120931, -291.4617174120931], time: 181.179
steps: 2249975, episodes: 90000, mean episode variance: 4.416978939771652, agent episode variance: [1.6249477224349975, 1.3002753851413726, 1.491755832195282], time: 181.179
Running avgs for agent 0: q_loss: 37.8110237121582, p_loss: -5.463472843170166, mean_rew: -10.793663103225901, variance: 6.499791145324707, lamda: 1.671966791152954
Running avgs for agent 1: q_loss: 43.74724578857422, p_loss: -5.4772539138793945, mean_rew: -10.788107650851373, variance: 5.201101779937744, lamda: 2.0924952030181885
Running avgs for agent 2: q_loss: 44.27284240722656, p_loss: -5.452495574951172, mean_rew: -10.781384892660855, variance: 5.9670233726501465, lamda: 1.8257670402526855

steps: 2274975, episodes: 91000, mean episode reward: -871.1550107253381, agent episode reward: [-290.3850035751127, -290.3850035751127, -290.3850035751127], time: 171.145
steps: 2274975, episodes: 91000, mean episode variance: 4.379146478414535, agent episode variance: [1.615641126871109, 1.2888914992809295, 1.474613852262497], time: 171.145
Running avgs for agent 0: q_loss: 37.98051452636719, p_loss: -5.464437484741211, mean_rew: -10.794981203806326, variance: 6.462564468383789, lamda: 1.6741737127304077
Running avgs for agent 1: q_loss: 32.15185546875, p_loss: -5.480236530303955, mean_rew: -10.784844087486242, variance: 5.155566215515137, lamda: 2.104084014892578
Running avgs for agent 2: q_loss: 44.138179779052734, p_loss: -5.460334300994873, mean_rew: -10.78891979990466, variance: 5.8984551429748535, lamda: 1.8297481536865234

steps: 2299975, episodes: 92000, mean episode reward: -879.8967566463125, agent episode reward: [-293.2989188821042, -293.2989188821042, -293.2989188821042], time: 169.429
steps: 2299975, episodes: 92000, mean episode variance: 4.390917168855667, agent episode variance: [1.6342501299381256, 1.2797334101200104, 1.4769336287975312], time: 169.429
Running avgs for agent 0: q_loss: 38.37720489501953, p_loss: -5.463085174560547, mean_rew: -10.793040066484707, variance: 6.53700065612793, lamda: 1.6768786907196045
Running avgs for agent 1: q_loss: 31.56721305847168, p_loss: -5.487682819366455, mean_rew: -10.799236036580183, variance: 5.11893367767334, lamda: 2.1206469535827637
Running avgs for agent 2: q_loss: 43.95307159423828, p_loss: -5.458102703094482, mean_rew: -10.801385823615428, variance: 5.907734394073486, lamda: 1.835149884223938

steps: 2324975, episodes: 93000, mean episode reward: -890.118249608078, agent episode reward: [-296.7060832026927, -296.7060832026927, -296.7060832026927], time: 163.767
steps: 2324975, episodes: 93000, mean episode variance: 4.36567043185234, agent episode variance: [1.6155582933425903, 1.2761390511989594, 1.473973087310791], time: 163.768
Running avgs for agent 0: q_loss: 39.06743621826172, p_loss: -5.477126121520996, mean_rew: -10.80592733991955, variance: 6.462233543395996, lamda: 1.6807337999343872
Running avgs for agent 1: q_loss: 33.580875396728516, p_loss: -5.483592510223389, mean_rew: -10.799953646737297, variance: 5.104556560516357, lamda: 2.138032913208008
Running avgs for agent 2: q_loss: 43.513553619384766, p_loss: -5.464707374572754, mean_rew: -10.816002639405228, variance: 5.895892143249512, lamda: 1.838823676109314

steps: 2349975, episodes: 94000, mean episode reward: -887.1688646913669, agent episode reward: [-295.7229548971223, -295.7229548971223, -295.7229548971223], time: 165.978
steps: 2349975, episodes: 94000, mean episode variance: 4.3641083436012265, agent episode variance: [1.6172738330364227, 1.262918791770935, 1.483915718793869], time: 165.979
Running avgs for agent 0: q_loss: 39.997920989990234, p_loss: -5.4966254234313965, mean_rew: -10.846243634553943, variance: 6.469095706939697, lamda: 1.6859443187713623
Running avgs for agent 1: q_loss: 33.019649505615234, p_loss: -5.492027282714844, mean_rew: -10.81344909893956, variance: 5.051675319671631, lamda: 2.157890796661377
Running avgs for agent 2: q_loss: 45.471343994140625, p_loss: -5.467529296875, mean_rew: -10.834736076273163, variance: 5.935663223266602, lamda: 1.8425319194793701

steps: 2374975, episodes: 95000, mean episode reward: -882.0771430818913, agent episode reward: [-294.0257143606305, -294.0257143606305, -294.0257143606305], time: 162.568
steps: 2374975, episodes: 95000, mean episode variance: 4.349043580532074, agent episode variance: [1.6160579059123994, 1.2593915100097657, 1.473594164609909], time: 162.568
Running avgs for agent 0: q_loss: 39.660804748535156, p_loss: -5.496787071228027, mean_rew: -10.849314372094499, variance: 6.464231491088867, lamda: 1.6909853219985962
Running avgs for agent 1: q_loss: 33.50860595703125, p_loss: -5.503668785095215, mean_rew: -10.862976324863052, variance: 5.037566184997559, lamda: 2.1751961708068848
Running avgs for agent 2: q_loss: 44.5153923034668, p_loss: -5.486725807189941, mean_rew: -10.865084345387409, variance: 5.894376277923584, lamda: 1.8478074073791504

steps: 2399975, episodes: 96000, mean episode reward: -885.8128747722725, agent episode reward: [-295.2709582574242, -295.2709582574242, -295.2709582574242], time: 165.798
steps: 2399975, episodes: 96000, mean episode variance: 4.337640393018723, agent episode variance: [1.6141285057067871, 1.2453239488601684, 1.4781879384517669], time: 165.798
Running avgs for agent 0: q_loss: 39.92053985595703, p_loss: -5.52403450012207, mean_rew: -10.88687801504374, variance: 6.45651388168335, lamda: 1.6964472532272339
Running avgs for agent 1: q_loss: 33.96077346801758, p_loss: -5.531055927276611, mean_rew: -10.890038973880943, variance: 4.981296062469482, lamda: 2.196032762527466
Running avgs for agent 2: q_loss: 45.96460723876953, p_loss: -5.5024566650390625, mean_rew: -10.896177182055748, variance: 5.912752151489258, lamda: 1.8532813787460327

steps: 2424975, episodes: 97000, mean episode reward: -901.0612020271951, agent episode reward: [-300.3537340090651, -300.3537340090651, -300.3537340090651], time: 168.758
steps: 2424975, episodes: 97000, mean episode variance: 4.32963425040245, agent episode variance: [1.6113594589233398, 1.2431633512973785, 1.4751114401817322], time: 168.759
Running avgs for agent 0: q_loss: 36.89851379394531, p_loss: -5.528412818908691, mean_rew: -10.902636258324542, variance: 6.445437908172607, lamda: 1.7003397941589355
Running avgs for agent 1: q_loss: 34.00853729248047, p_loss: -5.523250102996826, mean_rew: -10.895272424622739, variance: 4.972653388977051, lamda: 2.2116708755493164
Running avgs for agent 2: q_loss: 45.284481048583984, p_loss: -5.498548984527588, mean_rew: -10.900195579910365, variance: 5.90044641494751, lamda: 1.8589692115783691

steps: 2449975, episodes: 98000, mean episode reward: -888.7137925253229, agent episode reward: [-296.2379308417743, -296.2379308417743, -296.2379308417743], time: 168.234
steps: 2449975, episodes: 98000, mean episode variance: 4.3101956729888915, agent episode variance: [1.6135484414100647, 1.2266861531734468, 1.4699610784053803], time: 168.234
Running avgs for agent 0: q_loss: 34.47804641723633, p_loss: -5.5502095222473145, mean_rew: -10.939282806641973, variance: 6.454193592071533, lamda: 1.7156271934509277
Running avgs for agent 1: q_loss: 32.05348205566406, p_loss: -5.538390636444092, mean_rew: -10.934938035831205, variance: 4.906744956970215, lamda: 2.227856159210205
Running avgs for agent 2: q_loss: 36.33694839477539, p_loss: -5.52898645401001, mean_rew: -10.946497218646217, variance: 5.8798441886901855, lamda: 1.869650959968567

steps: 2474975, episodes: 99000, mean episode reward: -888.5203563249636, agent episode reward: [-296.1734521083212, -296.1734521083212, -296.1734521083212], time: 168.958
steps: 2474975, episodes: 99000, mean episode variance: 4.29442340683937, agent episode variance: [1.5904548048973084, 1.2289925186634063, 1.474976083278656], time: 168.958
Running avgs for agent 0: q_loss: 26.961156845092773, p_loss: -5.567849159240723, mean_rew: -10.962443718907325, variance: 6.361819267272949, lamda: 1.7345854043960571
Running avgs for agent 1: q_loss: 35.317047119140625, p_loss: -5.551746368408203, mean_rew: -10.964213024476688, variance: 4.9159698486328125, lamda: 2.244678020477295
Running avgs for agent 2: q_loss: 42.02775573730469, p_loss: -5.528836250305176, mean_rew: -10.98523848931122, variance: 5.899904251098633, lamda: 1.8804832696914673

steps: 2499975, episodes: 100000, mean episode reward: -887.1822739432902, agent episode reward: [-295.72742464776337, -295.72742464776337, -295.72742464776337], time: 172.599
steps: 2499975, episodes: 100000, mean episode variance: 4.253552555322647, agent episode variance: [1.5774244804382325, 1.2136493837833404, 1.4624786911010743], time: 172.599
Running avgs for agent 0: q_loss: 27.77122688293457, p_loss: -5.599991321563721, mean_rew: -11.022517898588616, variance: 6.309697151184082, lamda: 1.7527357339859009
Running avgs for agent 1: q_loss: 35.42631912231445, p_loss: -5.580193042755127, mean_rew: -11.024446641092105, variance: 4.854597568511963, lamda: 2.259026288986206
Running avgs for agent 2: q_loss: 38.2417106628418, p_loss: -5.547640800476074, mean_rew: -11.009457871236206, variance: 5.84991455078125, lamda: 1.8985111713409424

steps: 2524975, episodes: 101000, mean episode reward: -901.0892113701278, agent episode reward: [-300.3630704567093, -300.3630704567093, -300.3630704567093], time: 164.562
steps: 2524975, episodes: 101000, mean episode variance: 4.233882751703263, agent episode variance: [1.576087545633316, 1.2125523195266723, 1.4452428865432738], time: 164.562
Running avgs for agent 0: q_loss: 39.180755615234375, p_loss: -5.605988025665283, mean_rew: -11.055766256073097, variance: 6.30435037612915, lamda: 1.7648813724517822
Running avgs for agent 1: q_loss: 35.013118743896484, p_loss: -5.6002278327941895, mean_rew: -11.044411256840034, variance: 4.850208759307861, lamda: 2.2765817642211914
Running avgs for agent 2: q_loss: 47.53514862060547, p_loss: -5.571949481964111, mean_rew: -11.056751389254114, variance: 5.780971527099609, lamda: 1.906612515449524

steps: 2549975, episodes: 102000, mean episode reward: -880.9507207410104, agent episode reward: [-293.6502402470035, -293.6502402470035, -293.6502402470035], time: 163.652
steps: 2549975, episodes: 102000, mean episode variance: 4.225623540401458, agent episode variance: [1.5704887375831604, 1.2005036103725433, 1.454631192445755], time: 163.652
Running avgs for agent 0: q_loss: 40.62283706665039, p_loss: -5.632604598999023, mean_rew: -11.094005063534205, variance: 6.281955242156982, lamda: 1.767693042755127
Running avgs for agent 1: q_loss: 37.62547302246094, p_loss: -5.614152431488037, mean_rew: -11.086924046666054, variance: 4.8020148277282715, lamda: 2.29409122467041
Running avgs for agent 2: q_loss: 48.361942291259766, p_loss: -5.577319622039795, mean_rew: -11.079828044949329, variance: 5.8185248374938965, lamda: 1.91156005859375

steps: 2574975, episodes: 103000, mean episode reward: -888.0117324762942, agent episode reward: [-296.0039108254315, -296.0039108254315, -296.0039108254315], time: 163.457
steps: 2574975, episodes: 103000, mean episode variance: 4.247948752164841, agent episode variance: [1.568709503889084, 1.2197412600517272, 1.4594979882240295], time: 163.457
Running avgs for agent 0: q_loss: 40.622467041015625, p_loss: -5.656741142272949, mean_rew: -11.139413887401359, variance: 6.274837970733643, lamda: 1.7710589170455933
Running avgs for agent 1: q_loss: 50.41196823120117, p_loss: -5.627162933349609, mean_rew: -11.134826199152778, variance: 4.878964900970459, lamda: 2.3097879886627197
Running avgs for agent 2: q_loss: 47.859256744384766, p_loss: -5.596818923950195, mean_rew: -11.125042524730427, variance: 5.837991714477539, lamda: 1.9177019596099854

steps: 2599975, episodes: 104000, mean episode reward: -886.7565686513334, agent episode reward: [-295.58552288377786, -295.58552288377786, -295.58552288377786], time: 168.984
steps: 2599975, episodes: 104000, mean episode variance: 4.249051684975624, agent episode variance: [1.5842944643497467, 1.2017588552236558, 1.4629983654022216], time: 168.985
Running avgs for agent 0: q_loss: 40.65815353393555, p_loss: -5.672846794128418, mean_rew: -11.179251657839039, variance: 6.337177753448486, lamda: 1.7750107049942017
Running avgs for agent 1: q_loss: 57.96422576904297, p_loss: -5.66615104675293, mean_rew: -11.196595099229043, variance: 4.807034969329834, lamda: 2.316413640975952
Running avgs for agent 2: q_loss: 48.9326057434082, p_loss: -5.63096284866333, mean_rew: -11.178818020331223, variance: 5.851993083953857, lamda: 1.9220610857009888

steps: 2624975, episodes: 105000, mean episode reward: -903.4239721110814, agent episode reward: [-301.1413240370271, -301.1413240370271, -301.1413240370271], time: 162.296
steps: 2624975, episodes: 105000, mean episode variance: 4.246621917486191, agent episode variance: [1.579033111333847, 1.2089341728687286, 1.4586546332836152], time: 162.297
Running avgs for agent 0: q_loss: 41.04785919189453, p_loss: -5.7033891677856445, mean_rew: -11.220599564468033, variance: 6.316132545471191, lamda: 1.778566837310791
Running avgs for agent 1: q_loss: 56.58919143676758, p_loss: -5.669671058654785, mean_rew: -11.21484216833681, variance: 4.835736274719238, lamda: 2.321437120437622
Running avgs for agent 2: q_loss: 46.76051712036133, p_loss: -5.648592948913574, mean_rew: -11.222365166682922, variance: 5.83461856842041, lamda: 1.9270776510238647

steps: 2649975, episodes: 106000, mean episode reward: -901.725842490905, agent episode reward: [-300.5752808303016, -300.5752808303016, -300.5752808303016], time: 115.034
steps: 2649975, episodes: 106000, mean episode variance: 4.245396653175354, agent episode variance: [1.5786717271804809, 1.217246162891388, 1.4494787631034851], time: 115.035
Running avgs for agent 0: q_loss: 42.42464065551758, p_loss: -5.731047630310059, mean_rew: -11.289539716679402, variance: 6.3146867752075195, lamda: 1.7825459241867065
Running avgs for agent 1: q_loss: 40.74013900756836, p_loss: -5.693945407867432, mean_rew: -11.272359794663712, variance: 4.868984222412109, lamda: 2.334961414337158
Running avgs for agent 2: q_loss: 35.174461364746094, p_loss: -5.659656047821045, mean_rew: -11.274285863958175, variance: 5.797914981842041, lamda: 1.9422075748443604

steps: 2674975, episodes: 107000, mean episode reward: -902.295103951023, agent episode reward: [-300.76503465034097, -300.76503465034097, -300.76503465034097], time: 105.241
steps: 2674975, episodes: 107000, mean episode variance: 4.26544500720501, agent episode variance: [1.592878422021866, 1.2179756792783738, 1.45459090590477], time: 105.241
Running avgs for agent 0: q_loss: 42.76038360595703, p_loss: -5.73980188369751, mean_rew: -11.312097087775648, variance: 6.371513843536377, lamda: 1.7869387865066528
Running avgs for agent 1: q_loss: 59.868831634521484, p_loss: -5.715852737426758, mean_rew: -11.315006846723168, variance: 4.871902942657471, lamda: 2.3484418392181396
Running avgs for agent 2: q_loss: 37.41507339477539, p_loss: -5.686488151550293, mean_rew: -11.319484796629169, variance: 5.818363189697266, lamda: 1.9630743265151978

steps: 2699975, episodes: 108000, mean episode reward: -915.4515914719813, agent episode reward: [-305.15053049066046, -305.15053049066046, -305.15053049066046], time: 107.437
steps: 2699975, episodes: 108000, mean episode variance: 4.237191564798355, agent episode variance: [1.5855164654254914, 1.205503798007965, 1.4461713013648987], time: 107.437
Running avgs for agent 0: q_loss: 43.14213943481445, p_loss: -5.769318580627441, mean_rew: -11.365626659222617, variance: 6.342065334320068, lamda: 1.7923413515090942
Running avgs for agent 1: q_loss: 42.45491409301758, p_loss: -5.749063968658447, mean_rew: -11.374961017893783, variance: 4.822015285491943, lamda: 2.359748125076294
Running avgs for agent 2: q_loss: 50.45333480834961, p_loss: -5.715862274169922, mean_rew: -11.376022250374984, variance: 5.784685134887695, lamda: 1.9770958423614502

steps: 2724975, episodes: 109000, mean episode reward: -917.69225627794, agent episode reward: [-305.8974187593134, -305.8974187593134, -305.8974187593134], time: 106.251
steps: 2724975, episodes: 109000, mean episode variance: 4.239813045263291, agent episode variance: [1.5829502906799315, 1.2024297850131989, 1.4544329695701599], time: 106.252
Running avgs for agent 0: q_loss: 43.93252182006836, p_loss: -5.81395149230957, mean_rew: -11.420011747125402, variance: 6.331800937652588, lamda: 1.7979650497436523
Running avgs for agent 1: q_loss: 37.63477325439453, p_loss: -5.786205291748047, mean_rew: -11.427771040836625, variance: 4.809719085693359, lamda: 2.3793702125549316
Running avgs for agent 2: q_loss: 49.82434844970703, p_loss: -5.75591516494751, mean_rew: -11.431072463429654, variance: 5.8177313804626465, lamda: 1.9822776317596436

steps: 2749975, episodes: 110000, mean episode reward: -915.611938516299, agent episode reward: [-305.203979505433, -305.203979505433, -305.203979505433], time: 98.534
steps: 2749975, episodes: 110000, mean episode variance: 4.246552633047104, agent episode variance: [1.603418625831604, 1.1977805607318879, 1.445353446483612], time: 98.535
Running avgs for agent 0: q_loss: 43.8560905456543, p_loss: -5.8189311027526855, mean_rew: -11.474320895259886, variance: 6.413674354553223, lamda: 1.8022531270980835
Running avgs for agent 1: q_loss: 60.34524917602539, p_loss: -5.824211120605469, mean_rew: -11.48539594273913, variance: 4.791121959686279, lamda: 2.389277458190918
Running avgs for agent 2: q_loss: 52.513160705566406, p_loss: -5.781043529510498, mean_rew: -11.473565656647832, variance: 5.781414031982422, lamda: 1.987565279006958

steps: 2774975, episodes: 111000, mean episode reward: -920.2842869271748, agent episode reward: [-306.7614289757249, -306.7614289757249, -306.7614289757249], time: 95.465
steps: 2774975, episodes: 111000, mean episode variance: 4.26867620909214, agent episode variance: [1.6046027817726136, 1.2077766155004501, 1.4562968118190764], time: 95.465
Running avgs for agent 0: q_loss: 38.577945709228516, p_loss: -5.852534294128418, mean_rew: -11.537819266489624, variance: 6.4184112548828125, lamda: 1.807956337928772
Running avgs for agent 1: q_loss: 63.35005187988281, p_loss: -5.8380608558654785, mean_rew: -11.517643948178861, variance: 4.831106185913086, lamda: 2.3947036266326904
Running avgs for agent 2: q_loss: 52.8298225402832, p_loss: -5.804342746734619, mean_rew: -11.533837973771883, variance: 5.8251872062683105, lamda: 1.993666410446167

steps: 2799975, episodes: 112000, mean episode reward: -905.6047550366841, agent episode reward: [-301.8682516788947, -301.8682516788947, -301.8682516788947], time: 95.301
steps: 2799975, episodes: 112000, mean episode variance: 4.243405873060227, agent episode variance: [1.5876502137184143, 1.1960406749248504, 1.4597149844169617], time: 95.301
Running avgs for agent 0: q_loss: 31.331958770751953, p_loss: -5.855070114135742, mean_rew: -11.554739866749573, variance: 6.350600719451904, lamda: 1.8255338668823242
Running avgs for agent 1: q_loss: 64.93492126464844, p_loss: -5.873524188995361, mean_rew: -11.579420150535228, variance: 4.784162521362305, lamda: 2.403062582015991
Running avgs for agent 2: q_loss: 51.69565963745117, p_loss: -5.823325157165527, mean_rew: -11.55722657217884, variance: 5.838859558105469, lamda: 1.9981657266616821

steps: 2824975, episodes: 113000, mean episode reward: -928.2776353278243, agent episode reward: [-309.42587844260805, -309.42587844260805, -309.42587844260805], time: 97.664
steps: 2824975, episodes: 113000, mean episode variance: 4.255633366823196, agent episode variance: [1.5809210720062257, 1.2022276730537416, 1.4724846217632295], time: 97.664
Running avgs for agent 0: q_loss: 32.52055358886719, p_loss: -5.888793468475342, mean_rew: -11.605191768080182, variance: 6.3236846923828125, lamda: 1.8434878587722778
Running avgs for agent 1: q_loss: 64.66278839111328, p_loss: -5.897961139678955, mean_rew: -11.635641774190342, variance: 4.808910846710205, lamda: 2.4111552238464355
Running avgs for agent 2: q_loss: 55.4703369140625, p_loss: -5.8433918952941895, mean_rew: -11.606333385928389, variance: 5.8899383544921875, lamda: 2.0057408809661865

steps: 2849975, episodes: 114000, mean episode reward: -928.4074023013969, agent episode reward: [-309.4691341004657, -309.4691341004657, -309.4691341004657], time: 89.001
steps: 2849975, episodes: 114000, mean episode variance: 4.233437939047813, agent episode variance: [1.5712971322536469, 1.210051054596901, 1.4520897521972655], time: 89.001
Running avgs for agent 0: q_loss: 45.591304779052734, p_loss: -5.899409770965576, mean_rew: -11.645412900101608, variance: 6.285188674926758, lamda: 1.8523063659667969
Running avgs for agent 1: q_loss: 63.57160186767578, p_loss: -5.906806468963623, mean_rew: -11.658475479178653, variance: 4.840204238891602, lamda: 2.419563055038452
Running avgs for agent 2: q_loss: 54.60735321044922, p_loss: -5.87649393081665, mean_rew: -11.66027460037616, variance: 5.808358669281006, lamda: 2.0138723850250244

steps: 2874975, episodes: 115000, mean episode reward: -932.0113715054136, agent episode reward: [-310.67045716847116, -310.67045716847116, -310.67045716847116], time: 84.738
steps: 2874975, episodes: 115000, mean episode variance: 4.239599547863007, agent episode variance: [1.5829064183235169, 1.1990431458950044, 1.4576499836444854], time: 84.738
Running avgs for agent 0: q_loss: 41.197471618652344, p_loss: -5.924415111541748, mean_rew: -11.695718073869179, variance: 6.331625461578369, lamda: 1.8556970357894897
Running avgs for agent 1: q_loss: 65.2764892578125, p_loss: -5.927806854248047, mean_rew: -11.697696998477218, variance: 4.796172618865967, lamda: 2.4267609119415283
Running avgs for agent 2: q_loss: 54.307594299316406, p_loss: -5.895106315612793, mean_rew: -11.703890690516488, variance: 5.830599784851074, lamda: 2.018467903137207

steps: 2899975, episodes: 116000, mean episode reward: -932.6108408200187, agent episode reward: [-310.8702802733395, -310.8702802733395, -310.8702802733395], time: 82.639
steps: 2899975, episodes: 116000, mean episode variance: 4.2678598767519, agent episode variance: [1.5931526243686676, 1.20917522251606, 1.4655320298671723], time: 82.639
Running avgs for agent 0: q_loss: 41.508758544921875, p_loss: -5.9593048095703125, mean_rew: -11.764256093164835, variance: 6.372610569000244, lamda: 1.8655232191085815
Running avgs for agent 1: q_loss: 66.81564331054688, p_loss: -5.959090709686279, mean_rew: -11.751888454832649, variance: 4.836701393127441, lamda: 2.433459520339966
Running avgs for agent 2: q_loss: 55.38645935058594, p_loss: -5.923089981079102, mean_rew: -11.746667355007732, variance: 5.862127780914307, lamda: 2.0228166580200195

steps: 2924975, episodes: 117000, mean episode reward: -926.5355219397618, agent episode reward: [-308.84517397992056, -308.84517397992056, -308.84517397992056], time: 82.987
steps: 2924975, episodes: 117000, mean episode variance: 4.25964511024952, agent episode variance: [1.5831007966995239, 1.2146397718191146, 1.4619045417308807], time: 82.988
Running avgs for agent 0: q_loss: 48.033164978027344, p_loss: -5.968352794647217, mean_rew: -11.787362976660257, variance: 6.332403182983398, lamda: 1.87020742893219
Running avgs for agent 1: q_loss: 66.8548355102539, p_loss: -5.971452713012695, mean_rew: -11.78195592744122, variance: 4.8585591316223145, lamda: 2.444875955581665
Running avgs for agent 2: q_loss: 57.687599182128906, p_loss: -5.959073066711426, mean_rew: -11.799633675106264, variance: 5.847618103027344, lamda: 2.027700901031494

steps: 2949975, episodes: 118000, mean episode reward: -928.8290134784521, agent episode reward: [-309.6096711594841, -309.6096711594841, -309.6096711594841], time: 82.995
steps: 2949975, episodes: 118000, mean episode variance: 4.253544740200042, agent episode variance: [1.5810002739429474, 1.2069539165496825, 1.4655905497074126], time: 82.995
Running avgs for agent 0: q_loss: 50.124000549316406, p_loss: -5.997359275817871, mean_rew: -11.83204565918474, variance: 6.324000835418701, lamda: 1.8750035762786865
Running avgs for agent 1: q_loss: 65.52176666259766, p_loss: -5.990990161895752, mean_rew: -11.813611160387325, variance: 4.827815532684326, lamda: 2.452113389968872
Running avgs for agent 2: q_loss: 56.85749053955078, p_loss: -5.961993217468262, mean_rew: -11.827354636704252, variance: 5.862362384796143, lamda: 2.033918619155884

steps: 2974975, episodes: 119000, mean episode reward: -926.6015062528697, agent episode reward: [-308.8671687509565, -308.8671687509565, -308.8671687509565], time: 84.066
steps: 2974975, episodes: 119000, mean episode variance: 4.238071164846421, agent episode variance: [1.5684263603687287, 1.208370695590973, 1.4612741088867187], time: 84.066
Running avgs for agent 0: q_loss: 48.264469146728516, p_loss: -6.017943382263184, mean_rew: -11.868194120925093, variance: 6.273705005645752, lamda: 1.8800342082977295
Running avgs for agent 1: q_loss: 55.946414947509766, p_loss: -6.013937473297119, mean_rew: -11.8500564060081, variance: 4.833482265472412, lamda: 2.460414171218872
Running avgs for agent 2: q_loss: 57.3331413269043, p_loss: -5.987582206726074, mean_rew: -11.86129643538251, variance: 5.845096588134766, lamda: 2.0384163856506348

steps: 2999975, episodes: 120000, mean episode reward: -931.0609910938987, agent episode reward: [-310.35366369796617, -310.35366369796617, -310.35366369796617], time: 86.002
steps: 2999975, episodes: 120000, mean episode variance: 4.251735268115997, agent episode variance: [1.5852101340293885, 1.2073860731124877, 1.459139060974121], time: 86.002
Running avgs for agent 0: q_loss: 49.314735412597656, p_loss: -6.023582458496094, mean_rew: -11.899903629711586, variance: 6.3408403396606445, lamda: 1.8837133646011353
Running avgs for agent 1: q_loss: 51.9351806640625, p_loss: -6.036291599273682, mean_rew: -11.9021783766958, variance: 4.829544544219971, lamda: 2.4773154258728027
Running avgs for agent 2: q_loss: 52.589027404785156, p_loss: -5.9983954429626465, mean_rew: -11.88816989149636, variance: 5.836555480957031, lamda: 2.044189453125

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -936.9036974660106, agent episode reward: [-312.30123248867017, -312.30123248867017, -312.30123248867017], time: 65.06
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 65.061
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -937.1437791757428, agent episode reward: [-312.3812597252476, -312.3812597252476, -312.3812597252476], time: 78.121
steps: 49975, episodes: 2000, mean episode variance: 3.741916356801987, agent episode variance: [1.3122096047401428, 1.557347502231598, 0.8723592498302459], time: 78.121
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.50185473432934, variance: 5.377908229827881, lamda: 1.885545253753662
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.492497833105025, variance: 6.382571220397949, lamda: 2.4856905937194824
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.507182019662503, variance: 3.575242757797241, lamda: 2.0499398708343506

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567760423.1091275: line 9: --exp_var_alpha: command not found
