# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation False --constrained \
    False --exp-name MADDPG_simple_spread_uncons_01 --save-dir \
    models/simple_spread_policies2/01-non-linear-uncons/ --plots-dir \
    models/simple_spread_policies2/01-non-linear-uncons/
Job <1083989> is submitted to queue <x86_6h>.
arglist.u_estimation False
2019-09-06 03:03:10.226074: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -528.3395481723285, agent episode reward: [-176.1131827241095, -176.1131827241095, -176.1131827241095], time: 40.047
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 40.047
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -582.0315616613963, agent episode reward: [-194.01052055379878, -194.01052055379878, -194.01052055379878], time: 65.525
steps: 49975, episodes: 2000, mean episode variance: 2.2939480329453947, agent episode variance: [0.8986574074923992, 0.7277348015606403, 0.667555823892355], time: 65.526
Running avgs for agent 0: q_loss: 2.4607996940612793, p_loss: 11.662797927856445, mean_rew: -7.4409367388582215, variance: 3.6830221618540953, mean_q: -11.748265266418457, std_q: 3.2529971599578857
Running avgs for agent 1: q_loss: 2.1825344562530518, p_loss: 11.153923034667969, mean_rew: -7.447644426984043, variance: 2.9825196785272143, mean_q: -11.249922752380371, std_q: 3.0718986988067627
Running avgs for agent 2: q_loss: 2.0659310817718506, p_loss: 10.486626625061035, mean_rew: -7.45239973887597, variance: 2.7358845241489957, mean_q: -10.582457542419434, std_q: 2.876643419265747

steps: 74975, episodes: 3000, mean episode reward: -459.6513597915642, agent episode reward: [-153.21711993052142, -153.21711993052142, -153.21711993052142], time: 68.43
steps: 74975, episodes: 3000, mean episode variance: 4.790992278128862, agent episode variance: [1.2952400973141194, 1.7616126146316529, 1.7341395661830903], time: 68.431
Running avgs for agent 0: q_loss: 1.29801344871521, p_loss: 23.237119674682617, mean_rew: -7.184439911840231, variance: 5.1809603892564775, mean_q: -23.40699577331543, std_q: 6.445730686187744
Running avgs for agent 1: q_loss: 1.265006184577942, p_loss: 22.2408390045166, mean_rew: -7.174408850617502, variance: 7.0464504585266114, mean_q: -22.45801544189453, std_q: 6.146690368652344
Running avgs for agent 2: q_loss: 1.0409775972366333, p_loss: 21.584331512451172, mean_rew: -7.17478333853711, variance: 6.936558264732361, mean_q: -21.75242042541504, std_q: 5.941623210906982

steps: 99975, episodes: 4000, mean episode reward: -435.82784647051704, agent episode reward: [-145.27594882350567, -145.27594882350567, -145.27594882350567], time: 68.269
steps: 99975, episodes: 4000, mean episode variance: 9.509541760861874, agent episode variance: [3.1122790447473525, 3.0941906616687773, 3.3030720544457437], time: 68.269
Running avgs for agent 0: q_loss: 1.3502511978149414, p_loss: 33.447967529296875, mean_rew: -6.838784388117047, variance: 12.44911617898941, mean_q: -33.680328369140625, std_q: 9.061609268188477
Running avgs for agent 1: q_loss: 1.3783529996871948, p_loss: 32.373836517333984, mean_rew: -6.841827689837915, variance: 12.37676264667511, mean_q: -32.7048454284668, std_q: 8.639627456665039
Running avgs for agent 2: q_loss: 1.1863962411880493, p_loss: 31.921098709106445, mean_rew: -6.83682777014236, variance: 13.212288217782975, mean_q: -32.22506332397461, std_q: 8.767831802368164

steps: 124975, episodes: 5000, mean episode reward: -396.8120584032588, agent episode reward: [-132.2706861344196, -132.2706861344196, -132.2706861344196], time: 67.955
steps: 124975, episodes: 5000, mean episode variance: 14.15379166483879, agent episode variance: [5.075282185316086, 4.094752989768982, 4.983756489753723], time: 67.955
Running avgs for agent 0: q_loss: 1.542449951171875, p_loss: 41.764923095703125, mean_rew: -6.550360338680617, variance: 20.301128741264343, mean_q: -42.06317901611328, std_q: 10.779598236083984
Running avgs for agent 1: q_loss: 1.4615187644958496, p_loss: 40.67668151855469, mean_rew: -6.546739926037597, variance: 16.379011959075928, mean_q: -41.09064865112305, std_q: 10.31650447845459
Running avgs for agent 2: q_loss: 1.4087175130844116, p_loss: 40.08250045776367, mean_rew: -6.544537758019772, variance: 19.935025959014894, mean_q: -40.51856994628906, std_q: 10.583390235900879

steps: 149975, episodes: 6000, mean episode reward: -374.94311563611575, agent episode reward: [-124.9810385453719, -124.9810385453719, -124.9810385453719], time: 68.004
steps: 149975, episodes: 6000, mean episode variance: 22.48456518507004, agent episode variance: [6.080763964414596, 10.192841864585876, 6.210959356069565], time: 68.005
Running avgs for agent 0: q_loss: 1.5857372283935547, p_loss: 48.32083511352539, mean_rew: -6.3002023727496335, variance: 24.323055857658385, mean_q: -48.68128204345703, std_q: 12.132404327392578
Running avgs for agent 1: q_loss: 2.1348676681518555, p_loss: 47.2784309387207, mean_rew: -6.294032953304227, variance: 40.771367458343505, mean_q: -47.75699996948242, std_q: 11.371923446655273
Running avgs for agent 2: q_loss: 1.5581250190734863, p_loss: 46.37019348144531, mean_rew: -6.287896917764752, variance: 24.84383742427826, mean_q: -46.87430953979492, std_q: 11.794177055358887

steps: 174975, episodes: 7000, mean episode reward: -368.1733115814801, agent episode reward: [-122.72443719382669, -122.72443719382669, -122.72443719382669], time: 76.498
steps: 174975, episodes: 7000, mean episode variance: 28.172041867256166, agent episode variance: [7.3336589221954345, 13.452503421783447, 7.385879523277283], time: 76.498
Running avgs for agent 0: q_loss: 1.6775041818618774, p_loss: 53.38438415527344, mean_rew: -6.088835585086471, variance: 29.334635688781738, mean_q: -53.76715850830078, std_q: 12.959951400756836
Running avgs for agent 1: q_loss: 2.469957113265991, p_loss: 52.5590705871582, mean_rew: -6.079887510258186, variance: 53.81001368713379, mean_q: -53.04230499267578, std_q: 12.163837432861328
Running avgs for agent 2: q_loss: 1.730061411857605, p_loss: 51.381256103515625, mean_rew: -6.0844542020468255, variance: 29.54351809310913, mean_q: -51.92108917236328, std_q: 12.830126762390137

steps: 199975, episodes: 8000, mean episode reward: -364.3214973902475, agent episode reward: [-121.44049913008249, -121.44049913008249, -121.44049913008249], time: 71.975
steps: 199975, episodes: 8000, mean episode variance: 30.901659504413605, agent episode variance: [7.68853936958313, 14.673190086364746, 8.53993004846573], time: 71.976
Running avgs for agent 0: q_loss: 1.8300647735595703, p_loss: 57.54371643066406, mean_rew: -5.927120276041809, variance: 30.75415747833252, mean_q: -57.92649841308594, std_q: 13.634336471557617
Running avgs for agent 1: q_loss: 2.521468162536621, p_loss: 56.98746109008789, mean_rew: -5.930098978695656, variance: 58.692760345458986, mean_q: -57.46235656738281, std_q: 12.833362579345703
Running avgs for agent 2: q_loss: 1.8751860857009888, p_loss: 55.29671859741211, mean_rew: -5.921265411473908, variance: 34.15972019386292, mean_q: -55.8613395690918, std_q: 13.195150375366211

steps: 224975, episodes: 9000, mean episode reward: -360.4896566116803, agent episode reward: [-120.16321887056009, -120.16321887056009, -120.16321887056009], time: 71.264
steps: 224975, episodes: 9000, mean episode variance: 33.3241090259552, agent episode variance: [8.433608211517335, 15.695218013763428, 9.195282800674438], time: 71.265
Running avgs for agent 0: q_loss: 1.886511206626892, p_loss: 60.865787506103516, mean_rew: -5.792914250356629, variance: 33.73443284606934, mean_q: -61.24335861206055, std_q: 13.511192321777344
Running avgs for agent 1: q_loss: 2.5606331825256348, p_loss: 60.59893798828125, mean_rew: -5.79544067070136, variance: 62.78087205505371, mean_q: -61.06863784790039, std_q: 13.111605644226074
Running avgs for agent 2: q_loss: 1.9406770467758179, p_loss: 58.67385482788086, mean_rew: -5.80218165162014, variance: 36.78113120269775, mean_q: -59.24274826049805, std_q: 13.513524055480957

steps: 249975, episodes: 10000, mean episode reward: -360.636612676152, agent episode reward: [-120.212204225384, -120.212204225384, -120.212204225384], time: 68.446
steps: 249975, episodes: 10000, mean episode variance: 35.97396404075623, agent episode variance: [9.101925426483154, 16.889383140563965, 9.982655473709107], time: 68.447
Running avgs for agent 0: q_loss: 1.9277751445770264, p_loss: 63.731204986572266, mean_rew: -5.6962451208756395, variance: 36.40770170593262, mean_q: -64.10237121582031, std_q: 13.775775909423828
Running avgs for agent 1: q_loss: 2.6298913955688477, p_loss: 63.59720993041992, mean_rew: -5.694470319655356, variance: 67.55753256225586, mean_q: -64.06147003173828, std_q: 13.268537521362305
Running avgs for agent 2: q_loss: 1.972575306892395, p_loss: 61.36432647705078, mean_rew: -5.687275373645682, variance: 39.93062189483643, mean_q: -61.92186737060547, std_q: 13.527498245239258

steps: 274975, episodes: 11000, mean episode reward: -357.98876879225156, agent episode reward: [-119.3295895974172, -119.3295895974172, -119.3295895974172], time: 68.584
steps: 274975, episodes: 11000, mean episode variance: 37.989598834991455, agent episode variance: [9.747524993896484, 17.920801567077635, 10.321272274017334], time: 68.585
Running avgs for agent 0: q_loss: 2.0080347061157227, p_loss: 66.01567840576172, mean_rew: -5.604659912755088, variance: 38.990099975585935, mean_q: -66.37273406982422, std_q: 13.558056831359863
Running avgs for agent 1: q_loss: 2.6174306869506836, p_loss: 66.12512969970703, mean_rew: -5.607274965238099, variance: 71.68320626831054, mean_q: -66.57034301757812, std_q: 13.2555570602417
Running avgs for agent 2: q_loss: 2.150230884552002, p_loss: 63.852882385253906, mean_rew: -5.6125772271055, variance: 41.285089096069335, mean_q: -64.40128326416016, std_q: 13.849519729614258

steps: 299975, episodes: 12000, mean episode reward: -358.47529955539125, agent episode reward: [-119.49176651846375, -119.49176651846375, -119.49176651846375], time: 68.581
steps: 299975, episodes: 12000, mean episode variance: 48.29947903823852, agent episode variance: [10.02317184829712, 17.511628776550292, 20.764678413391113], time: 68.581
Running avgs for agent 0: q_loss: 1.9952740669250488, p_loss: 68.09986114501953, mean_rew: -5.536259544428986, variance: 40.09268739318848, mean_q: -68.43833923339844, std_q: 13.432947158813477
Running avgs for agent 1: q_loss: 2.5952837467193604, p_loss: 68.39334106445312, mean_rew: -5.536567684486857, variance: 70.04651510620117, mean_q: -68.82943725585938, std_q: 13.153423309326172
Running avgs for agent 2: q_loss: 3.1616523265838623, p_loss: 65.72489166259766, mean_rew: -5.535166071169444, variance: 83.05871365356445, mean_q: -66.24601745605469, std_q: 13.378746032714844

steps: 324975, episodes: 13000, mean episode reward: -356.3816745638793, agent episode reward: [-118.79389152129312, -118.79389152129312, -118.79389152129312], time: 68.634
steps: 324975, episodes: 13000, mean episode variance: 58.75805000305176, agent episode variance: [19.61147727584839, 18.857507358551025, 20.289065368652345], time: 68.635
Running avgs for agent 0: q_loss: 2.904780387878418, p_loss: 69.78797149658203, mean_rew: -5.475066854291828, variance: 78.44590910339356, mean_q: -70.11151885986328, std_q: 12.904149055480957
Running avgs for agent 1: q_loss: 2.6112754344940186, p_loss: 70.2561264038086, mean_rew: -5.473587445053492, variance: 75.4300294342041, mean_q: -70.68221282958984, std_q: 12.87671947479248
Running avgs for agent 2: q_loss: 3.0835037231445312, p_loss: 67.58648681640625, mean_rew: -5.4719394202604255, variance: 81.15626147460938, mean_q: -68.08731079101562, std_q: 13.273324012756348

steps: 349975, episodes: 14000, mean episode reward: -351.9034661798436, agent episode reward: [-117.30115539328119, -117.30115539328119, -117.30115539328119], time: 67.533
steps: 349975, episodes: 14000, mean episode variance: 58.570249710083004, agent episode variance: [19.9690724029541, 18.739657455444338, 19.86151985168457], time: 67.534
Running avgs for agent 0: q_loss: 2.835456132888794, p_loss: 71.2930908203125, mean_rew: -5.418517802714879, variance: 79.8762896118164, mean_q: -71.59896087646484, std_q: 12.693498611450195
Running avgs for agent 1: q_loss: 2.649909496307373, p_loss: 71.97169494628906, mean_rew: -5.421268047230811, variance: 74.95862982177735, mean_q: -72.38853454589844, std_q: 12.892867088317871
Running avgs for agent 2: q_loss: 2.96635365486145, p_loss: 69.24703979492188, mean_rew: -5.414582339084957, variance: 79.44607940673828, mean_q: -69.72176361083984, std_q: 12.851778030395508

steps: 374975, episodes: 15000, mean episode reward: -354.83896725247916, agent episode reward: [-118.2796557508264, -118.2796557508264, -118.2796557508264], time: 67.699
steps: 374975, episodes: 15000, mean episode variance: 55.00314048099518, agent episode variance: [19.44174368286133, 14.894630303382874, 20.666766494750977], time: 67.699
Running avgs for agent 0: q_loss: 2.755899429321289, p_loss: 72.59005737304688, mean_rew: -5.370431100508188, variance: 77.76697473144532, mean_q: -72.88892364501953, std_q: 12.355941772460938
Running avgs for agent 1: q_loss: 2.237933874130249, p_loss: 73.3738021850586, mean_rew: -5.368789555274743, variance: 59.578521213531495, mean_q: -73.77429962158203, std_q: 12.460277557373047
Running avgs for agent 2: q_loss: 2.917433023452759, p_loss: 70.67749786376953, mean_rew: -5.370263956722839, variance: 82.66706597900391, mean_q: -71.12730407714844, std_q: 12.534736633300781

steps: 399975, episodes: 16000, mean episode reward: -354.8835309373514, agent episode reward: [-118.29451031245047, -118.29451031245047, -118.29451031245047], time: 67.967
steps: 399975, episodes: 16000, mean episode variance: 50.52034924507141, agent episode variance: [19.14883130645752, 10.523440305709839, 20.848077632904054], time: 67.968
Running avgs for agent 0: q_loss: 2.699350118637085, p_loss: 73.80950164794922, mean_rew: -5.326679665660367, variance: 76.59532522583008, mean_q: -74.09207916259766, std_q: 12.156166076660156
Running avgs for agent 1: q_loss: 1.850449562072754, p_loss: 74.59087371826172, mean_rew: -5.327875691501491, variance: 42.093761222839355, mean_q: -74.9818115234375, std_q: 12.37025260925293
Running avgs for agent 2: q_loss: 2.8363196849823, p_loss: 71.919921875, mean_rew: -5.327933587518385, variance: 83.39231053161622, mean_q: -72.35257720947266, std_q: 12.012969017028809

steps: 424975, episodes: 17000, mean episode reward: -350.84830577769526, agent episode reward: [-116.94943525923176, -116.94943525923176, -116.94943525923176], time: 67.315
steps: 424975, episodes: 17000, mean episode variance: 50.62884714126587, agent episode variance: [19.63029239654541, 10.64608330154419, 20.35247144317627], time: 67.316
Running avgs for agent 0: q_loss: 2.641181707382202, p_loss: 74.866943359375, mean_rew: -5.290202278634243, variance: 78.52116958618164, mean_q: -75.13488006591797, std_q: 12.096120834350586
Running avgs for agent 1: q_loss: 1.7236921787261963, p_loss: 75.66079711914062, mean_rew: -5.291834104993789, variance: 42.58433320617676, mean_q: -76.05854797363281, std_q: 12.096444129943848
Running avgs for agent 2: q_loss: 2.759845495223999, p_loss: 73.07159423828125, mean_rew: -5.291754488770252, variance: 81.40988577270508, mean_q: -73.49080657958984, std_q: 11.771383285522461

steps: 449975, episodes: 18000, mean episode reward: -350.5254383416657, agent episode reward: [-116.84181278055522, -116.84181278055522, -116.84181278055522], time: 66.114
steps: 449975, episodes: 18000, mean episode variance: 50.91254160881042, agent episode variance: [19.235393730163572, 10.93701314353943, 20.74013473510742], time: 66.114
Running avgs for agent 0: q_loss: 2.621272563934326, p_loss: 75.74960327148438, mean_rew: -5.259604229050518, variance: 76.94157492065429, mean_q: -76.01517486572266, std_q: 12.00794506072998
Running avgs for agent 1: q_loss: 1.7493962049484253, p_loss: 76.53964233398438, mean_rew: -5.256168349714751, variance: 43.74805257415772, mean_q: -76.93102264404297, std_q: 11.740777015686035
Running avgs for agent 2: q_loss: 2.7050533294677734, p_loss: 74.03081512451172, mean_rew: -5.25259196816827, variance: 82.96053894042969, mean_q: -74.4481201171875, std_q: 11.50723648071289

steps: 474975, episodes: 19000, mean episode reward: -348.66482184779153, agent episode reward: [-116.22160728259719, -116.22160728259719, -116.22160728259719], time: 66.752
steps: 474975, episodes: 19000, mean episode variance: 56.01866395759583, agent episode variance: [19.412878456115724, 15.843697099685668, 20.762088401794433], time: 66.752
Running avgs for agent 0: q_loss: 2.658050298690796, p_loss: 76.4778823852539, mean_rew: -5.221130512333873, variance: 77.6515138244629, mean_q: -76.73868560791016, std_q: 11.682805061340332
Running avgs for agent 1: q_loss: 2.183372974395752, p_loss: 77.26750183105469, mean_rew: -5.22233226788142, variance: 63.37478839874267, mean_q: -77.64335632324219, std_q: 11.503296852111816
Running avgs for agent 2: q_loss: 2.647148847579956, p_loss: 74.82256317138672, mean_rew: -5.22093515561188, variance: 83.04835360717773, mean_q: -75.2376480102539, std_q: 11.164718627929688

steps: 499975, episodes: 20000, mean episode reward: -350.9253328925734, agent episode reward: [-116.97511096419112, -116.97511096419112, -116.97511096419112], time: 69.126
steps: 499975, episodes: 20000, mean episode variance: 60.93979267883301, agent episode variance: [20.280631523132325, 20.424724380493164, 20.23443677520752], time: 69.127
Running avgs for agent 0: q_loss: 2.5365281105041504, p_loss: 77.0977783203125, mean_rew: -5.1960569805991295, variance: 81.1225260925293, mean_q: -77.35235595703125, std_q: 11.452718734741211
Running avgs for agent 1: q_loss: 2.588590145111084, p_loss: 77.95325469970703, mean_rew: -5.1963711780644655, variance: 81.69889752197265, mean_q: -78.31922149658203, std_q: 11.76333236694336
Running avgs for agent 2: q_loss: 2.6075031757354736, p_loss: 75.5316162109375, mean_rew: -5.196183105112075, variance: 80.93774710083008, mean_q: -75.9495849609375, std_q: 11.166029930114746

steps: 524975, episodes: 21000, mean episode reward: -349.626960987018, agent episode reward: [-116.54232032900597, -116.54232032900597, -116.54232032900597], time: 68.545
steps: 524975, episodes: 21000, mean episode variance: 60.946701156616214, agent episode variance: [19.58323706817627, 20.638705741882323, 20.724758346557618], time: 68.546
Running avgs for agent 0: q_loss: 2.5471742153167725, p_loss: 77.68572235107422, mean_rew: -5.175216555069395, variance: 78.33294827270508, mean_q: -77.93842315673828, std_q: 11.273499488830566
Running avgs for agent 1: q_loss: 2.5109081268310547, p_loss: 78.42152404785156, mean_rew: -5.173345626642479, variance: 82.55482296752929, mean_q: -78.77340698242188, std_q: 11.32081413269043
Running avgs for agent 2: q_loss: 2.6122794151306152, p_loss: 76.06291961669922, mean_rew: -5.173730772251858, variance: 82.89903338623047, mean_q: -76.48443603515625, std_q: 11.050751686096191

steps: 549975, episodes: 22000, mean episode reward: -348.5720944527308, agent episode reward: [-116.19069815091028, -116.19069815091028, -116.19069815091028], time: 69.455
steps: 549975, episodes: 22000, mean episode variance: 60.46651734924316, agent episode variance: [19.252284194946288, 20.579168479919435, 20.635064674377443], time: 69.455
Running avgs for agent 0: q_loss: 2.5177464485168457, p_loss: 78.0932846069336, mean_rew: -5.143157172389285, variance: 77.00913677978515, mean_q: -78.34014892578125, std_q: 11.282947540283203
Running avgs for agent 1: q_loss: 2.466798782348633, p_loss: 78.8170394897461, mean_rew: -5.147434135674824, variance: 82.31667391967774, mean_q: -79.16209411621094, std_q: 11.431008338928223
Running avgs for agent 2: q_loss: 2.5290863513946533, p_loss: 76.460205078125, mean_rew: -5.15163895037906, variance: 82.54025869750977, mean_q: -76.88536071777344, std_q: 11.076478958129883

steps: 574975, episodes: 23000, mean episode reward: -343.8001559107285, agent episode reward: [-114.60005197024284, -114.60005197024284, -114.60005197024284], time: 68.994
steps: 574975, episodes: 23000, mean episode variance: 59.58382004547119, agent episode variance: [19.52045280456543, 19.934249641418457, 20.129117599487305], time: 68.995
Running avgs for agent 0: q_loss: 2.51438570022583, p_loss: 78.44441986083984, mean_rew: -5.124261500653359, variance: 78.08181121826172, mean_q: -78.69087219238281, std_q: 10.992959022521973
Running avgs for agent 1: q_loss: 2.4595696926116943, p_loss: 79.08356475830078, mean_rew: -5.120385020461932, variance: 79.73699856567383, mean_q: -79.42044067382812, std_q: 11.211214065551758
Running avgs for agent 2: q_loss: 2.522477149963379, p_loss: 76.66226196289062, mean_rew: -5.115247981856354, variance: 80.51647039794922, mean_q: -77.08756256103516, std_q: 10.979719161987305

steps: 599975, episodes: 24000, mean episode reward: -340.4616562640649, agent episode reward: [-113.48721875468829, -113.48721875468829, -113.48721875468829], time: 67.894
steps: 599975, episodes: 24000, mean episode variance: 59.0054458656311, agent episode variance: [19.20475382232666, 19.90528769683838, 19.895404346466066], time: 67.894
Running avgs for agent 0: q_loss: 2.3922407627105713, p_loss: 78.73043823242188, mean_rew: -5.103826681005519, variance: 76.81901528930663, mean_q: -78.97930908203125, std_q: 10.88972282409668
Running avgs for agent 1: q_loss: 2.3776450157165527, p_loss: 79.31132507324219, mean_rew: -5.095817435411914, variance: 79.62115078735351, mean_q: -79.63777923583984, std_q: 11.043048858642578
Running avgs for agent 2: q_loss: 2.5315308570861816, p_loss: 76.87490844726562, mean_rew: -5.1018259718987355, variance: 79.58161738586426, mean_q: -77.30509948730469, std_q: 10.931736946105957

steps: 624975, episodes: 25000, mean episode reward: -343.46656910906967, agent episode reward: [-114.4888563696899, -114.4888563696899, -114.4888563696899], time: 69.197
steps: 624975, episodes: 25000, mean episode variance: 59.38444258117676, agent episode variance: [19.141611110687254, 20.13003302383423, 20.112798446655272], time: 69.198
Running avgs for agent 0: q_loss: 2.3833768367767334, p_loss: 78.86986541748047, mean_rew: -5.082536253901181, variance: 76.56644444274902, mean_q: -79.11122131347656, std_q: 10.705978393554688
Running avgs for agent 1: q_loss: 2.382261276245117, p_loss: 79.47747039794922, mean_rew: -5.078459988713959, variance: 80.52013209533692, mean_q: -79.79915618896484, std_q: 11.142133712768555
Running avgs for agent 2: q_loss: 2.475085973739624, p_loss: 76.90912628173828, mean_rew: -5.079498446073561, variance: 80.45119378662109, mean_q: -77.32990264892578, std_q: 10.855548858642578

steps: 649975, episodes: 26000, mean episode reward: -342.0464839820702, agent episode reward: [-114.01549466069007, -114.01549466069007, -114.01549466069007], time: 65.152
steps: 649975, episodes: 26000, mean episode variance: 58.41121802139282, agent episode variance: [18.886421813964844, 19.73490752029419, 19.78988868713379], time: 65.152
Running avgs for agent 0: q_loss: 2.373023271560669, p_loss: 78.98225402832031, mean_rew: -5.058177711962874, variance: 75.54568725585938, mean_q: -79.2177505493164, std_q: 10.748988151550293
Running avgs for agent 1: q_loss: 2.386608362197876, p_loss: 79.51005554199219, mean_rew: -5.0610137372841, variance: 78.93963008117676, mean_q: -79.8264389038086, std_q: 11.050000190734863
Running avgs for agent 2: q_loss: 2.4633493423461914, p_loss: 76.91651153564453, mean_rew: -5.058378660611413, variance: 79.15955474853516, mean_q: -77.33412170410156, std_q: 10.879083633422852

steps: 674975, episodes: 27000, mean episode reward: -340.465465805904, agent episode reward: [-113.48848860196802, -113.48848860196802, -113.48848860196802], time: 67.445
steps: 674975, episodes: 27000, mean episode variance: 57.66878501224518, agent episode variance: [18.35951179218292, 19.56180069732666, 19.747472522735595], time: 67.445
Running avgs for agent 0: q_loss: 2.4752392768859863, p_loss: 79.03047180175781, mean_rew: -5.034905183102255, variance: 73.43804716873169, mean_q: -79.26534271240234, std_q: 10.599288940429688
Running avgs for agent 1: q_loss: 2.3312878608703613, p_loss: 79.55486297607422, mean_rew: -5.040032205427866, variance: 78.24720278930664, mean_q: -79.86554718017578, std_q: 11.019708633422852
Running avgs for agent 2: q_loss: 2.4570271968841553, p_loss: 76.865234375, mean_rew: -5.0372840307218505, variance: 78.98989009094238, mean_q: -77.27832794189453, std_q: 10.66323471069336

steps: 699975, episodes: 28000, mean episode reward: -340.75187851058064, agent episode reward: [-113.58395950352687, -113.58395950352687, -113.58395950352687], time: 67.461
steps: 699975, episodes: 28000, mean episode variance: 57.63497150421143, agent episode variance: [18.35951064682007, 19.673251655578614, 19.602209201812745], time: 67.461
Running avgs for agent 0: q_loss: 2.2800567150115967, p_loss: 79.12433624267578, mean_rew: -5.025649663806356, variance: 73.43804258728028, mean_q: -79.3590316772461, std_q: 10.636726379394531
Running avgs for agent 1: q_loss: 2.31343936920166, p_loss: 79.5301513671875, mean_rew: -5.020858696814277, variance: 78.69300662231446, mean_q: -79.84015655517578, std_q: 11.0439453125
Running avgs for agent 2: q_loss: 2.4350297451019287, p_loss: 76.83401489257812, mean_rew: -5.022828847345485, variance: 78.40883680725098, mean_q: -77.24117279052734, std_q: 10.961457252502441

steps: 724975, episodes: 29000, mean episode reward: -339.7493296436524, agent episode reward: [-113.24977654788414, -113.24977654788414, -113.24977654788414], time: 68.942
steps: 724975, episodes: 29000, mean episode variance: 56.84426877212525, agent episode variance: [18.293619693756103, 19.13788651275635, 19.412762565612795], time: 68.943
Running avgs for agent 0: q_loss: 2.290513753890991, p_loss: 79.06804656982422, mean_rew: -5.001379653056272, variance: 73.17447877502441, mean_q: -79.29264068603516, std_q: 10.273340225219727
Running avgs for agent 1: q_loss: 2.328474521636963, p_loss: 79.47644805908203, mean_rew: -5.008103614870892, variance: 76.5515460510254, mean_q: -79.78239440917969, std_q: 11.192214965820312
Running avgs for agent 2: q_loss: 2.3937201499938965, p_loss: 76.67650604248047, mean_rew: -5.005210739973183, variance: 77.65105026245118, mean_q: -77.07830047607422, std_q: 10.853919982910156

steps: 749975, episodes: 30000, mean episode reward: -339.0439727728799, agent episode reward: [-113.01465759095994, -113.01465759095994, -113.01465759095994], time: 67.896
steps: 749975, episodes: 30000, mean episode variance: 57.03493533325195, agent episode variance: [17.966892093658448, 19.330055007934572, 19.737988231658935], time: 67.896
Running avgs for agent 0: q_loss: 2.22788667678833, p_loss: 79.10643768310547, mean_rew: -4.986876022531649, variance: 71.86756837463379, mean_q: -79.33216094970703, std_q: 10.211895942687988
Running avgs for agent 1: q_loss: 2.2767014503479004, p_loss: 79.3355484008789, mean_rew: -4.986290893969162, variance: 77.32022003173829, mean_q: -79.63612365722656, std_q: 10.957023620605469
Running avgs for agent 2: q_loss: 2.366332530975342, p_loss: 76.47747039794922, mean_rew: -4.990066772102111, variance: 78.95195292663574, mean_q: -76.86588287353516, std_q: 10.91419506072998

steps: 774975, episodes: 31000, mean episode reward: -338.7489348167252, agent episode reward: [-112.91631160557506, -112.91631160557506, -112.91631160557506], time: 67.624
steps: 774975, episodes: 31000, mean episode variance: 55.814684169769286, agent episode variance: [17.740107444763183, 18.61312696456909, 19.461449760437013], time: 67.625
Running avgs for agent 0: q_loss: 2.1985082626342773, p_loss: 79.119873046875, mean_rew: -4.976001003099792, variance: 70.96042977905273, mean_q: -79.34049987792969, std_q: 10.191950798034668
Running avgs for agent 1: q_loss: 2.2211968898773193, p_loss: 79.29486846923828, mean_rew: -4.969605117296772, variance: 74.45250785827636, mean_q: -79.58422088623047, std_q: 10.674585342407227
Running avgs for agent 2: q_loss: 2.3564960956573486, p_loss: 76.31543731689453, mean_rew: -4.966524012509895, variance: 77.84579904174805, mean_q: -76.69168853759766, std_q: 10.754511833190918

steps: 799975, episodes: 32000, mean episode reward: -340.31479826771914, agent episode reward: [-113.43826608923972, -113.43826608923972, -113.43826608923972], time: 66.864
steps: 799975, episodes: 32000, mean episode variance: 55.13724501991272, agent episode variance: [17.700652444839477, 18.445711196899413, 18.990881378173828], time: 66.864
Running avgs for agent 0: q_loss: 2.2539737224578857, p_loss: 78.99974822998047, mean_rew: -4.957666937642984, variance: 70.80260977935791, mean_q: -79.21575927734375, std_q: 10.181927680969238
Running avgs for agent 1: q_loss: 2.2038352489471436, p_loss: 79.18248748779297, mean_rew: -4.954917484196382, variance: 73.78284478759765, mean_q: -79.4660873413086, std_q: 10.566736221313477
Running avgs for agent 2: q_loss: 2.3215043544769287, p_loss: 76.28984832763672, mean_rew: -4.961797679931271, variance: 75.96352551269531, mean_q: -76.65518951416016, std_q: 10.697978973388672

steps: 824975, episodes: 33000, mean episode reward: -338.0684203643998, agent episode reward: [-112.68947345479994, -112.68947345479994, -112.68947345479994], time: 66.814
steps: 824975, episodes: 33000, mean episode variance: 53.624994306087494, agent episode variance: [15.873762776851654, 18.787821796417237, 18.963409732818604], time: 66.814
Running avgs for agent 0: q_loss: 2.1286118030548096, p_loss: 78.94744110107422, mean_rew: -4.946885253850646, variance: 63.49505110740662, mean_q: -79.15280151367188, std_q: 10.090323448181152
Running avgs for agent 1: q_loss: 2.1756224632263184, p_loss: 79.12943267822266, mean_rew: -4.950140418486995, variance: 75.15128718566895, mean_q: -79.41248321533203, std_q: 10.831677436828613
Running avgs for agent 2: q_loss: 2.375509023666382, p_loss: 76.2016372680664, mean_rew: -4.944071814320401, variance: 75.85363893127442, mean_q: -76.54550170898438, std_q: 10.665124893188477

steps: 849975, episodes: 34000, mean episode reward: -337.09624818185614, agent episode reward: [-112.36541606061871, -112.36541606061871, -112.36541606061871], time: 67.723
steps: 849975, episodes: 34000, mean episode variance: 55.14916202163696, agent episode variance: [17.87407179260254, 18.395467403411864, 18.87962282562256], time: 67.723
Running avgs for agent 0: q_loss: 2.113311290740967, p_loss: 78.82587432861328, mean_rew: -4.933031981862843, variance: 71.49628717041016, mean_q: -79.03157806396484, std_q: 10.132397651672363
Running avgs for agent 1: q_loss: 2.1486287117004395, p_loss: 78.97966003417969, mean_rew: -4.932136125633435, variance: 73.58186961364746, mean_q: -79.25684356689453, std_q: 10.412429809570312
Running avgs for agent 2: q_loss: 2.2588417530059814, p_loss: 76.19461059570312, mean_rew: -4.933410746063832, variance: 75.51849130249023, mean_q: -76.522216796875, std_q: 10.487059593200684

steps: 874975, episodes: 35000, mean episode reward: -336.6546503944979, agent episode reward: [-112.21821679816594, -112.21821679816594, -112.21821679816594], time: 67.675
steps: 874975, episodes: 35000, mean episode variance: 54.40752795982361, agent episode variance: [17.78738318634033, 18.158741811752318, 18.461402961730958], time: 67.675
Running avgs for agent 0: q_loss: 2.1199400424957275, p_loss: 78.62637329101562, mean_rew: -4.919888226130363, variance: 71.14953274536133, mean_q: -78.82476806640625, std_q: 10.004826545715332
Running avgs for agent 1: q_loss: 2.1419076919555664, p_loss: 78.81670379638672, mean_rew: -4.919647759121156, variance: 72.63496724700927, mean_q: -79.09418487548828, std_q: 10.290627479553223
Running avgs for agent 2: q_loss: 2.2765438556671143, p_loss: 76.19170379638672, mean_rew: -4.9221325576637565, variance: 73.84561184692383, mean_q: -76.51361083984375, std_q: 10.561640739440918

steps: 899975, episodes: 36000, mean episode reward: -336.14424962173877, agent episode reward: [-112.04808320724625, -112.04808320724625, -112.04808320724625], time: 67.389
steps: 899975, episodes: 36000, mean episode variance: 44.44989207172394, agent episode variance: [17.004661911010743, 8.936879525184631, 18.508350635528565], time: 67.39
Running avgs for agent 0: q_loss: 2.1090199947357178, p_loss: 78.45233154296875, mean_rew: -4.904143843581793, variance: 68.01864764404297, mean_q: -78.6495132446289, std_q: 9.944602966308594
Running avgs for agent 1: q_loss: 1.7462140321731567, p_loss: 78.73696899414062, mean_rew: -4.908197924202547, variance: 35.747518100738525, mean_q: -79.00939178466797, std_q: 10.36951732635498
Running avgs for agent 2: q_loss: 2.2090280055999756, p_loss: 76.14024353027344, mean_rew: -4.905815785340739, variance: 74.03340254211426, mean_q: -76.45864868164062, std_q: 10.42116641998291

steps: 924975, episodes: 37000, mean episode reward: -339.21753427701685, agent episode reward: [-113.0725114256723, -113.0725114256723, -113.0725114256723], time: 67.452
steps: 924975, episodes: 37000, mean episode variance: 44.46057944869995, agent episode variance: [16.952452381134034, 8.712601867675781, 18.795525199890136], time: 67.453
Running avgs for agent 0: q_loss: 2.0609118938446045, p_loss: 78.30301666259766, mean_rew: -4.895336818851154, variance: 67.80980952453613, mean_q: -78.49651336669922, std_q: 9.995244026184082
Running avgs for agent 1: q_loss: 1.6496936082839966, p_loss: 78.51085662841797, mean_rew: -4.8932682371786616, variance: 34.850407470703125, mean_q: -78.79371643066406, std_q: 10.106210708618164
Running avgs for agent 2: q_loss: 2.1841800212860107, p_loss: 76.04779815673828, mean_rew: -4.894868240273106, variance: 75.18210079956054, mean_q: -76.35619354248047, std_q: 10.267796516418457

steps: 949975, episodes: 38000, mean episode reward: -334.5199324925424, agent episode reward: [-111.50664416418078, -111.50664416418078, -111.50664416418078], time: 67.19
steps: 949975, episodes: 38000, mean episode variance: 38.247534489154816, agent episode variance: [10.418919401168823, 9.944119162082671, 17.88449592590332], time: 67.19
Running avgs for agent 0: q_loss: 1.6659977436065674, p_loss: 78.1397476196289, mean_rew: -4.887686601754073, variance: 41.67567760467529, mean_q: -78.32243347167969, std_q: 9.791953086853027
Running avgs for agent 1: q_loss: 1.389987587928772, p_loss: 78.34992218017578, mean_rew: -4.886052390358221, variance: 39.776476648330686, mean_q: -78.63279724121094, std_q: 10.033677101135254
Running avgs for agent 2: q_loss: 2.254281997680664, p_loss: 76.07039642333984, mean_rew: -4.8863187715607745, variance: 71.53798370361328, mean_q: -76.37860870361328, std_q: 10.477936744689941

steps: 974975, episodes: 39000, mean episode reward: -333.45148275519136, agent episode reward: [-111.1504942517305, -111.1504942517305, -111.1504942517305], time: 80.48
steps: 974975, episodes: 39000, mean episode variance: 37.36860374593735, agent episode variance: [9.483723706722259, 9.724500392913818, 18.16037964630127], time: 80.481
Running avgs for agent 0: q_loss: 1.5965226888656616, p_loss: 77.93070983886719, mean_rew: -4.8724040045863495, variance: 37.934894826889035, mean_q: -78.11328125, std_q: 9.579302787780762
Running avgs for agent 1: q_loss: 1.397127628326416, p_loss: 78.19292449951172, mean_rew: -4.869306395236699, variance: 38.89800157165527, mean_q: -78.47502136230469, std_q: 10.214310646057129
Running avgs for agent 2: q_loss: 2.1731860637664795, p_loss: 75.98994445800781, mean_rew: -4.881205000260278, variance: 72.64151858520508, mean_q: -76.30006408691406, std_q: 10.302096366882324

steps: 999975, episodes: 40000, mean episode reward: -334.25479860280655, agent episode reward: [-111.41826620093552, -111.41826620093552, -111.41826620093552], time: 65.678
steps: 999975, episodes: 40000, mean episode variance: 37.40199701189995, agent episode variance: [13.343882811307907, 9.630806758880615, 14.427307441711426], time: 65.679
Running avgs for agent 0: q_loss: 1.7948524951934814, p_loss: 77.75376892089844, mean_rew: -4.867864347970633, variance: 53.37553124523163, mean_q: -77.9375, std_q: 9.623214721679688
Running avgs for agent 1: q_loss: 1.4232393503189087, p_loss: 78.08844757080078, mean_rew: -4.8688530081670525, variance: 38.52322703552246, mean_q: -78.38066864013672, std_q: 9.967560768127441
Running avgs for agent 2: q_loss: 2.051431894302368, p_loss: 75.85711669921875, mean_rew: -4.862238768885627, variance: 57.7092297668457, mean_q: -76.1729965209961, std_q: 10.2857084274292

steps: 1024975, episodes: 41000, mean episode reward: -334.9356818447181, agent episode reward: [-111.64522728157269, -111.64522728157269, -111.64522728157269], time: 65.518
steps: 1024975, episodes: 41000, mean episode variance: 43.73477775382996, agent episode variance: [16.603205238342284, 9.324233682632446, 17.807338832855226], time: 65.519
Running avgs for agent 0: q_loss: 1.9978278875350952, p_loss: 77.41084289550781, mean_rew: -4.825250931997985, variance: 66.41282095336913, mean_q: -77.58468627929688, std_q: 9.544961929321289
Running avgs for agent 1: q_loss: 1.4116014242172241, p_loss: 77.70673370361328, mean_rew: -4.82101162227024, variance: 37.296934730529784, mean_q: -77.9820785522461, std_q: 9.79653549194336
Running avgs for agent 2: q_loss: 2.066920757293701, p_loss: 75.60386657714844, mean_rew: -4.826684098486921, variance: 71.2293553314209, mean_q: -75.90483093261719, std_q: 10.002663612365723

steps: 1049975, episodes: 42000, mean episode reward: -332.5556898913901, agent episode reward: [-110.85189663046336, -110.85189663046336, -110.85189663046336], time: 65.676
steps: 1049975, episodes: 42000, mean episode variance: 42.73198358726501, agent episode variance: [16.089211261749266, 9.288407392501831, 17.354364933013915], time: 65.676
Running avgs for agent 0: q_loss: 1.896652102470398, p_loss: 76.7472152709961, mean_rew: -4.747300770834538, variance: 64.35684504699707, mean_q: -76.89368438720703, std_q: 7.338596820831299
Running avgs for agent 1: q_loss: 1.3127046823501587, p_loss: 77.06787872314453, mean_rew: -4.747623360295934, variance: 37.153629570007325, mean_q: -77.30901336669922, std_q: 7.7671427726745605
Running avgs for agent 2: q_loss: 2.0097544193267822, p_loss: 74.91118621826172, mean_rew: -4.743064555063111, variance: 69.41745973205566, mean_q: -75.17650604248047, std_q: 7.903941631317139

steps: 1074975, episodes: 43000, mean episode reward: -333.63701558478846, agent episode reward: [-111.2123385282628, -111.2123385282628, -111.2123385282628], time: 65.797
steps: 1074975, episodes: 43000, mean episode variance: 42.4824165649414, agent episode variance: [15.897498542785645, 9.311917915344239, 17.273000106811523], time: 65.798
Running avgs for agent 0: q_loss: 1.810733437538147, p_loss: 76.34689331054688, mean_rew: -4.68812423137869, variance: 63.58999417114258, mean_q: -76.48043823242188, std_q: 6.6771440505981445
Running avgs for agent 1: q_loss: 1.2308251857757568, p_loss: 76.66459655761719, mean_rew: -4.691155501343881, variance: 37.247671661376955, mean_q: -76.87976837158203, std_q: 6.912546157836914
Running avgs for agent 2: q_loss: 1.897469401359558, p_loss: 74.52315521240234, mean_rew: -4.685365835529361, variance: 69.09200042724609, mean_q: -74.75787353515625, std_q: 7.118029594421387

steps: 1099975, episodes: 44000, mean episode reward: -333.58414675766926, agent episode reward: [-111.19471558588977, -111.19471558588977, -111.19471558588977], time: 65.979
steps: 1099975, episodes: 44000, mean episode variance: 41.50116449737549, agent episode variance: [15.162018981933594, 9.4049426612854, 16.934202854156496], time: 65.979
Running avgs for agent 0: q_loss: 1.7353886365890503, p_loss: 76.15520477294922, mean_rew: -4.650268845716083, variance: 60.648075927734375, mean_q: -76.28250885009766, std_q: 6.566948890686035
Running avgs for agent 1: q_loss: 1.1856627464294434, p_loss: 76.381591796875, mean_rew: -4.648422190160658, variance: 37.6197706451416, mean_q: -76.57814025878906, std_q: 6.8237080574035645
Running avgs for agent 2: q_loss: 1.8367664813995361, p_loss: 74.3472671508789, mean_rew: -4.65044192931398, variance: 67.73681141662598, mean_q: -74.56096649169922, std_q: 7.000683784484863

steps: 1124975, episodes: 45000, mean episode reward: -330.6078303872822, agent episode reward: [-110.20261012909404, -110.20261012909404, -110.20261012909404], time: 100.044
steps: 1124975, episodes: 45000, mean episode variance: 40.20148704528808, agent episode variance: [15.070103096008301, 8.900020462036133, 16.23136348724365], time: 100.045
Running avgs for agent 0: q_loss: 1.6889222860336304, p_loss: 76.04515075683594, mean_rew: -4.628209475652355, variance: 60.280412384033205, mean_q: -76.17382049560547, std_q: 6.5620317459106445
Running avgs for agent 1: q_loss: 1.1465076208114624, p_loss: 76.23278045654297, mean_rew: -4.627726645899332, variance: 35.600081848144534, mean_q: -76.41899871826172, std_q: 6.76805305480957
Running avgs for agent 2: q_loss: 1.8166736364364624, p_loss: 74.29585266113281, mean_rew: -4.62791315194575, variance: 64.9254539489746, mean_q: -74.4958267211914, std_q: 6.9326653480529785

steps: 1149975, episodes: 46000, mean episode reward: -330.67622369368524, agent episode reward: [-110.22540789789505, -110.22540789789505, -110.22540789789505], time: 112.866
steps: 1149975, episodes: 46000, mean episode variance: 40.51682461929321, agent episode variance: [15.05243514251709, 8.862652591705322, 16.6017368850708], time: 112.866
Running avgs for agent 0: q_loss: 1.6497955322265625, p_loss: 75.87220001220703, mean_rew: -4.606810305119687, variance: 60.20974057006836, mean_q: -76.0, std_q: 6.5099897384643555
Running avgs for agent 1: q_loss: 1.136926293373108, p_loss: 76.02434539794922, mean_rew: -4.6050260748041705, variance: 35.45061036682129, mean_q: -76.20372009277344, std_q: 6.66170597076416
Running avgs for agent 2: q_loss: 1.7439229488372803, p_loss: 74.18663024902344, mean_rew: -4.6077696256144876, variance: 66.4069475402832, mean_q: -74.38085174560547, std_q: 6.876923561096191

steps: 1174975, episodes: 47000, mean episode reward: -330.7643087314449, agent episode reward: [-110.2547695771483, -110.2547695771483, -110.2547695771483], time: 75.224
steps: 1174975, episodes: 47000, mean episode variance: 39.920849491119384, agent episode variance: [14.458397468566895, 8.727850200653076, 16.734601821899414], time: 75.224
Running avgs for agent 0: q_loss: 1.5904122591018677, p_loss: 75.80493927001953, mean_rew: -4.592048079572479, variance: 57.83358987426758, mean_q: -75.93084716796875, std_q: 6.479855060577393
Running avgs for agent 1: q_loss: 1.1041463613510132, p_loss: 75.86438751220703, mean_rew: -4.594918070750889, variance: 34.911400802612306, mean_q: -76.03973388671875, std_q: 6.6631388664245605
Running avgs for agent 2: q_loss: 1.7104557752609253, p_loss: 74.06648254394531, mean_rew: -4.594141815238772, variance: 66.93840728759766, mean_q: -74.25359344482422, std_q: 6.829115390777588

steps: 1199975, episodes: 48000, mean episode reward: -330.7066446646542, agent episode reward: [-110.23554822155141, -110.23554822155141, -110.23554822155141], time: 67.088
steps: 1199975, episodes: 48000, mean episode variance: 39.2390287361145, agent episode variance: [14.681311725616455, 8.625651313781738, 15.93206569671631], time: 67.088
Running avgs for agent 0: q_loss: 1.56807541847229, p_loss: 75.6973876953125, mean_rew: -4.581430425323863, variance: 58.72524690246582, mean_q: -75.82308959960938, std_q: 6.497153282165527
Running avgs for agent 1: q_loss: 1.0856049060821533, p_loss: 75.77410888671875, mean_rew: -4.5832273869316715, variance: 34.50260525512695, mean_q: -75.94683837890625, std_q: 6.639563083648682
Running avgs for agent 2: q_loss: 1.6706005334854126, p_loss: 73.9959487915039, mean_rew: -4.5781265155623485, variance: 63.72826278686524, mean_q: -74.17658996582031, std_q: 6.801571369171143

steps: 1224975, episodes: 49000, mean episode reward: -328.87301480063985, agent episode reward: [-109.62433826687993, -109.62433826687993, -109.62433826687993], time: 66.749
steps: 1224975, episodes: 49000, mean episode variance: 39.49222521209717, agent episode variance: [14.6231400680542, 8.618090770721436, 16.250994373321532], time: 66.749
Running avgs for agent 0: q_loss: 1.5461806058883667, p_loss: 75.60549926757812, mean_rew: -4.571068046560453, variance: 58.4925602722168, mean_q: -75.7312240600586, std_q: 6.529240131378174
Running avgs for agent 1: q_loss: 1.0682066679000854, p_loss: 75.63591003417969, mean_rew: -4.570573021531628, variance: 34.47236308288574, mean_q: -75.79997253417969, std_q: 6.61973237991333
Running avgs for agent 2: q_loss: 1.635616660118103, p_loss: 73.9461441040039, mean_rew: -4.567971254269914, variance: 65.00397749328613, mean_q: -74.12052917480469, std_q: 6.807544708251953

steps: 1249975, episodes: 50000, mean episode reward: -331.5022798832636, agent episode reward: [-110.50075996108787, -110.50075996108787, -110.50075996108787], time: 66.425
steps: 1249975, episodes: 50000, mean episode variance: 38.206603233337404, agent episode variance: [14.142415004730225, 8.530943824768066, 15.533244403839111], time: 66.426
Running avgs for agent 0: q_loss: 1.5351207256317139, p_loss: 75.5262680053711, mean_rew: -4.562047902779931, variance: 56.5696600189209, mean_q: -75.65570831298828, std_q: 6.546022415161133
Running avgs for agent 1: q_loss: 1.0471652746200562, p_loss: 75.4754638671875, mean_rew: -4.556840671167898, variance: 34.123775299072264, mean_q: -75.63838958740234, std_q: 6.607398509979248
Running avgs for agent 2: q_loss: 1.6178059577941895, p_loss: 73.89600372314453, mean_rew: -4.554804955512608, variance: 62.132977615356445, mean_q: -74.0661392211914, std_q: 6.772217273712158

steps: 1274975, episodes: 51000, mean episode reward: -329.30632916721913, agent episode reward: [-109.76877638907301, -109.76877638907301, -109.76877638907301], time: 65.75
steps: 1274975, episodes: 51000, mean episode variance: 37.84822394180298, agent episode variance: [14.048339138031006, 8.589754600524902, 15.21013020324707], time: 65.75
Running avgs for agent 0: q_loss: 1.5106068849563599, p_loss: 75.38704681396484, mean_rew: -4.547182765889296, variance: 56.19335655212402, mean_q: -75.51203918457031, std_q: 6.544891834259033
Running avgs for agent 1: q_loss: 1.0748066902160645, p_loss: 75.33372497558594, mean_rew: -4.551199936822512, variance: 34.35901840209961, mean_q: -75.49701690673828, std_q: 6.617631912231445
Running avgs for agent 2: q_loss: 1.5983482599258423, p_loss: 73.8375015258789, mean_rew: -4.5457849151035985, variance: 60.84052081298828, mean_q: -74.00763702392578, std_q: 6.793949127197266

steps: 1299975, episodes: 52000, mean episode reward: -327.7800887080958, agent episode reward: [-109.26002956936526, -109.26002956936526, -109.26002956936526], time: 66.62
steps: 1299975, episodes: 52000, mean episode variance: 38.30257786369324, agent episode variance: [14.346408172607422, 8.396364786148071, 15.559804904937744], time: 66.62
Running avgs for agent 0: q_loss: 1.5015950202941895, p_loss: 75.24749755859375, mean_rew: -4.545200297567151, variance: 57.38563269042969, mean_q: -75.37474822998047, std_q: 6.603641510009766
Running avgs for agent 1: q_loss: 1.0507665872573853, p_loss: 75.17855834960938, mean_rew: -4.539597856712873, variance: 33.585459144592285, mean_q: -75.34181213378906, std_q: 6.626089096069336
Running avgs for agent 2: q_loss: 1.5682499408721924, p_loss: 73.73219299316406, mean_rew: -4.53540522948141, variance: 62.239219619750976, mean_q: -73.89077758789062, std_q: 6.746015548706055

steps: 1324975, episodes: 53000, mean episode reward: -330.26485875575145, agent episode reward: [-110.08828625191713, -110.08828625191713, -110.08828625191713], time: 67.4
steps: 1324975, episodes: 53000, mean episode variance: 37.44444171333313, agent episode variance: [13.749569808959961, 8.404188081741333, 15.290683822631836], time: 67.401
Running avgs for agent 0: q_loss: 1.4919862747192383, p_loss: 75.0884017944336, mean_rew: -4.533715049611501, variance: 54.998279235839846, mean_q: -75.21623229980469, std_q: 6.584411144256592
Running avgs for agent 1: q_loss: 1.0632466077804565, p_loss: 75.0289535522461, mean_rew: -4.5313356453718265, variance: 33.61675232696533, mean_q: -75.18318939208984, std_q: 6.66003942489624
Running avgs for agent 2: q_loss: 1.5501377582550049, p_loss: 73.67207336425781, mean_rew: -4.533256988189121, variance: 61.162735290527344, mean_q: -73.82295227050781, std_q: 6.779961109161377

steps: 1349975, episodes: 54000, mean episode reward: -329.197365165189, agent episode reward: [-109.73245505506301, -109.73245505506301, -109.73245505506301], time: 66.439
steps: 1349975, episodes: 54000, mean episode variance: 37.248000663757324, agent episode variance: [13.72817105102539, 8.585720333099365, 14.934109279632569], time: 66.439
Running avgs for agent 0: q_loss: 1.4646846055984497, p_loss: 74.92252349853516, mean_rew: -4.5274370951286595, variance: 54.91268420410156, mean_q: -75.0486068725586, std_q: 6.592978477478027
Running avgs for agent 1: q_loss: 1.0453603267669678, p_loss: 74.84141540527344, mean_rew: -4.5223721958254925, variance: 34.34288133239746, mean_q: -74.9938735961914, std_q: 6.670649528503418
Running avgs for agent 2: q_loss: 1.53146493434906, p_loss: 73.59896087646484, mean_rew: -4.524639546686135, variance: 59.736437118530276, mean_q: -73.74373626708984, std_q: 6.774081230163574

steps: 1374975, episodes: 55000, mean episode reward: -329.273936619965, agent episode reward: [-109.75797887332169, -109.75797887332169, -109.75797887332169], time: 69.132
steps: 1374975, episodes: 55000, mean episode variance: 37.335632865905765, agent episode variance: [13.522570075988769, 8.344257175445557, 15.468805614471435], time: 69.132
Running avgs for agent 0: q_loss: 1.4378366470336914, p_loss: 74.80371856689453, mean_rew: -4.516821573585808, variance: 54.090280303955076, mean_q: -74.93004608154297, std_q: 6.596111297607422
Running avgs for agent 1: q_loss: 1.034353256225586, p_loss: 74.65369415283203, mean_rew: -4.514325193221553, variance: 33.37702870178223, mean_q: -74.80648803710938, std_q: 6.704962730407715
Running avgs for agent 2: q_loss: 1.51064133644104, p_loss: 73.47106170654297, mean_rew: -4.51198977780222, variance: 61.87522245788574, mean_q: -73.60808563232422, std_q: 6.7339653968811035

steps: 1399975, episodes: 56000, mean episode reward: -326.73132162057755, agent episode reward: [-108.9104405401925, -108.9104405401925, -108.9104405401925], time: 75.316
steps: 1399975, episodes: 56000, mean episode variance: 36.30826777267456, agent episode variance: [13.152493515014648, 8.083262969970702, 15.07251128768921], time: 75.317
Running avgs for agent 0: q_loss: 1.4223746061325073, p_loss: 74.73568725585938, mean_rew: -4.511850389521598, variance: 52.60997406005859, mean_q: -74.85546875, std_q: 6.6208415031433105
Running avgs for agent 1: q_loss: 1.0578904151916504, p_loss: 74.46294403076172, mean_rew: -4.506320320614411, variance: 32.33305187988281, mean_q: -74.61146545410156, std_q: 6.707142353057861
Running avgs for agent 2: q_loss: 1.499583125114441, p_loss: 73.32749938964844, mean_rew: -4.503468147291117, variance: 60.29004515075684, mean_q: -73.46199798583984, std_q: 6.685821056365967

steps: 1424975, episodes: 57000, mean episode reward: -326.06175414974047, agent episode reward: [-108.68725138324683, -108.68725138324683, -108.68725138324683], time: 65.958
steps: 1424975, episodes: 57000, mean episode variance: 42.391287303924564, agent episode variance: [13.399868133544922, 14.04447996520996, 14.946939205169677], time: 65.959
Running avgs for agent 0: q_loss: 1.4112495183944702, p_loss: 74.58912658691406, mean_rew: -4.499370641484061, variance: 53.59947253417969, mean_q: -74.70801544189453, std_q: 6.613829135894775
Running avgs for agent 1: q_loss: 1.5058249235153198, p_loss: 74.2315902709961, mean_rew: -4.4952446750991735, variance: 56.17791986083984, mean_q: -74.37934112548828, std_q: 6.751044750213623
Running avgs for agent 2: q_loss: 1.4721801280975342, p_loss: 73.209228515625, mean_rew: -4.4917581398596, variance: 59.78775682067871, mean_q: -73.34429931640625, std_q: 6.649768352508545

steps: 1449975, episodes: 58000, mean episode reward: -326.23247937850505, agent episode reward: [-108.744159792835, -108.744159792835, -108.744159792835], time: 66.075
steps: 1449975, episodes: 58000, mean episode variance: 43.11330043792724, agent episode variance: [13.476087673187255, 15.009460605621339, 14.627752159118652], time: 66.075
Running avgs for agent 0: q_loss: 1.3936214447021484, p_loss: 74.35382843017578, mean_rew: -4.485593848228466, variance: 53.90435069274902, mean_q: -74.47055053710938, std_q: 6.605419158935547
Running avgs for agent 1: q_loss: 1.5132286548614502, p_loss: 73.98422241210938, mean_rew: -4.487678213627548, variance: 60.037842422485355, mean_q: -74.127197265625, std_q: 6.790052890777588
Running avgs for agent 2: q_loss: 1.4432382583618164, p_loss: 73.07593536376953, mean_rew: -4.482081075892129, variance: 58.51100863647461, mean_q: -73.21082305908203, std_q: 6.699309349060059

steps: 1474975, episodes: 59000, mean episode reward: -324.86579853158685, agent episode reward: [-108.28859951052895, -108.28859951052895, -108.28859951052895], time: 66.09
steps: 1474975, episodes: 59000, mean episode variance: 41.37411057281494, agent episode variance: [12.84541987991333, 13.845681232452392, 14.683009460449219], time: 66.09
Running avgs for agent 0: q_loss: 1.3751012086868286, p_loss: 74.20716094970703, mean_rew: -4.4785090638948075, variance: 51.38167951965332, mean_q: -74.32128143310547, std_q: 6.661925792694092
Running avgs for agent 1: q_loss: 1.473238229751587, p_loss: 73.78266906738281, mean_rew: -4.478688244617445, variance: 55.38272492980957, mean_q: -73.91876220703125, std_q: 6.832913875579834
Running avgs for agent 2: q_loss: 1.4621050357818604, p_loss: 72.93545532226562, mean_rew: -4.478682527452036, variance: 58.732037841796874, mean_q: -73.06503295898438, std_q: 6.708705902099609

steps: 1499975, episodes: 60000, mean episode reward: -323.3404754512095, agent episode reward: [-107.78015848373651, -107.78015848373651, -107.78015848373651], time: 66.627
steps: 1499975, episodes: 60000, mean episode variance: 40.74974892044067, agent episode variance: [12.878149200439454, 13.884434616088868, 13.987165103912353], time: 66.627
Running avgs for agent 0: q_loss: 1.3528972864151, p_loss: 74.03652954101562, mean_rew: -4.466820691895938, variance: 51.512596801757816, mean_q: -74.15019989013672, std_q: 6.633745193481445
Running avgs for agent 1: q_loss: 1.4444142580032349, p_loss: 73.58641052246094, mean_rew: -4.474393815974539, variance: 55.53773846435547, mean_q: -73.72102355957031, std_q: 6.831581115722656
Running avgs for agent 2: q_loss: 1.4399828910827637, p_loss: 72.75804901123047, mean_rew: -4.467714718683352, variance: 55.948660415649414, mean_q: -72.88679504394531, std_q: 6.710346698760986

...Finished total of 60001 episodes... Now freezing policy and running for 10000 more episodes to get cvar estimation
steps: 24975, episodes: 1000, mean episode reward: -326.8961724859383, agent episode reward: [-108.96539082864608, -108.96539082864608, -108.96539082864608], time: 50.307
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 50.308
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -325.59956164484004, agent episode reward: [-108.53318721494668, -108.53318721494668, -108.53318721494668], time: 62.905
steps: 49975, episodes: 2000, mean episode variance: 31.295901611328127, agent episode variance: [0.0, 0.0, 31.295901611328127], time: 62.905
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3523626776745745, variance: 0.0, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -74.1019058227539, std_q: 6.566508769989014
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.352082492619955, variance: 0.0, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -73.89213562011719, std_q: 6.713159084320068
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.350835402504151, variance: 128.26190185546875, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -71.66815185546875, std_q: 6.790448188781738

steps: 74975, episodes: 3000, mean episode reward: -325.0111800756782, agent episode reward: [-108.33706002522607, -108.33706002522607, -108.33706002522607], time: 64.63
steps: 74975, episodes: 3000, mean episode variance: 31.880047409057617, agent episode variance: [0.0, 0.0, 31.880047409057617], time: 64.63
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.350055218141252, variance: 0.0, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -74.0927505493164, std_q: 6.604619979858398
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.351093171643789, variance: 0.0, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -73.87003326416016, std_q: 6.752748966217041
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.346744037094317, variance: 127.52018737792969, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -71.62226104736328, std_q: 6.802547931671143

steps: 99975, episodes: 4000, mean episode reward: -324.6906633216699, agent episode reward: [-108.23022110722329, -108.23022110722329, -108.23022110722329], time: 63.272
steps: 99975, episodes: 4000, mean episode variance: 31.842630027770998, agent episode variance: [0.0, 0.0, 31.842630027770998], time: 63.272
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.346836002574198, variance: 0.0, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -74.10887145996094, std_q: 6.615939140319824
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.356451968491742, variance: 0.0, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -73.86580657958984, std_q: 6.769288063049316
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3508203568284625, variance: 127.37051391601562, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -71.65201568603516, std_q: 6.815291881561279

steps: 124975, episodes: 5000, mean episode reward: -323.8065194460158, agent episode reward: [-107.93550648200527, -107.93550648200527, -107.93550648200527], time: 63.083
steps: 124975, episodes: 5000, mean episode variance: 31.733736717224122, agent episode variance: [0.0, 0.0, 31.733736717224122], time: 63.083
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.345804173094719, variance: 0.0, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -74.10067749023438, std_q: 6.618976593017578
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.340926554330775, variance: 0.0, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -73.80885314941406, std_q: 6.743971347808838
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.341137830110425, variance: 126.93494415283203, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -71.59949493408203, std_q: 6.757211685180664

steps: 149975, episodes: 6000, mean episode reward: -324.9267013447656, agent episode reward: [-108.30890044825519, -108.30890044825519, -108.30890044825519], time: 63.003
steps: 149975, episodes: 6000, mean episode variance: 31.764595375061035, agent episode variance: [0.0, 0.0, 31.764595375061035], time: 63.004
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337604318217938, variance: 0.0, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -74.08306121826172, std_q: 6.59855842590332
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.343307775203678, variance: 0.0, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -73.79529571533203, std_q: 6.718966960906982
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.340299135603347, variance: 127.05838012695312, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -71.6021728515625, std_q: 6.7477216720581055

steps: 174975, episodes: 7000, mean episode reward: -323.69382885198047, agent episode reward: [-107.89794295066018, -107.89794295066018, -107.89794295066018], time: 62.462
steps: 174975, episodes: 7000, mean episode variance: 31.66711693572998, agent episode variance: [0.0, 0.0, 31.66711693572998], time: 62.462
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.336398594773185, variance: 0.0, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -74.07444763183594, std_q: 6.5692009925842285
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.336944408024121, variance: 0.0, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -73.759033203125, std_q: 6.691885948181152
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.335377883919123, variance: 126.66846466064453, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -71.56356048583984, std_q: 6.706876754760742

steps: 199975, episodes: 8000, mean episode reward: -323.6543868293853, agent episode reward: [-107.8847956097951, -107.8847956097951, -107.8847956097951], time: 63.423
steps: 199975, episodes: 8000, mean episode variance: 31.751587684631346, agent episode variance: [0.0, 0.0, 31.751587684631346], time: 63.424
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3363940071344365, variance: 0.0, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -74.08157348632812, std_q: 6.57767391204834
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.333798510660116, variance: 0.0, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -73.75811004638672, std_q: 6.695812702178955
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337390461811957, variance: 127.00635528564453, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -71.59349822998047, std_q: 6.745760917663574

steps: 224975, episodes: 9000, mean episode reward: -325.67118846318186, agent episode reward: [-108.55706282106061, -108.55706282106061, -108.55706282106061], time: 64.918
steps: 224975, episodes: 9000, mean episode variance: 31.751367965698243, agent episode variance: [0.0, 0.0, 31.751367965698243], time: 64.918
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.336563080433085, variance: 0.0, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -74.09048461914062, std_q: 6.604466915130615
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337226312103908, variance: 0.0, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -73.77536010742188, std_q: 6.7197394371032715
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3347503243466, variance: 127.0054702758789, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -71.5695571899414, std_q: 6.717228889465332

steps: 249975, episodes: 10000, mean episode reward: -325.4174545936459, agent episode reward: [-108.47248486454865, -108.47248486454865, -108.47248486454865], time: 64.449
steps: 249975, episodes: 10000, mean episode variance: 31.86005958557129, agent episode variance: [0.0, 0.0, 31.86005958557129], time: 64.45
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.335548328337262, variance: 0.0, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -74.11007690429688, std_q: 6.590501308441162
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.335199889727633, variance: 0.0, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -73.76261901855469, std_q: 6.696826934814453
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.334472501833179, variance: 127.44023895263672, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -71.56915283203125, std_q: 6.69155740737915

steps: 274975, episodes: 11000, mean episode reward: -326.28701586008384, agent episode reward: [-108.76233862002796, -108.76233862002796, -108.76233862002796], time: 64.248
steps: 274975, episodes: 11000, mean episode variance: 31.905416290283203, agent episode variance: [0.0, 0.0, 31.905416290283203], time: 64.249
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.336250540155326, variance: 0.0, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -74.09345245361328, std_q: 6.5887064933776855
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.336966281664004, variance: 0.0, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -73.76142883300781, std_q: 6.722508907318115
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337359418131533, variance: 127.62165832519531, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -71.59259033203125, std_q: 6.734696865081787

steps: 299975, episodes: 12000, mean episode reward: -324.8450708124917, agent episode reward: [-108.28169027083055, -108.28169027083055, -108.28169027083055], time: 63.566
steps: 299975, episodes: 12000, mean episode variance: 31.858089195251466, agent episode variance: [0.0, 0.0, 31.858089195251466], time: 63.567
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3398386301988445, variance: 0.0, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -74.11924743652344, std_q: 6.606714725494385
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.338266860409316, variance: 0.0, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -73.7729721069336, std_q: 6.7324628829956055
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337484394212917, variance: 127.43235778808594, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -71.59452056884766, std_q: 6.732852935791016

steps: 324975, episodes: 13000, mean episode reward: -324.1703071173137, agent episode reward: [-108.05676903910458, -108.05676903910458, -108.05676903910458], time: 63.88
steps: 324975, episodes: 13000, mean episode variance: 31.929690063476563, agent episode variance: [0.0, 0.0, 31.929690063476563], time: 63.881
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.338025444882812, variance: 0.0, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -74.10041809082031, std_q: 6.5905375480651855
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.340674318258743, variance: 0.0, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -73.77345275878906, std_q: 6.738502502441406
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337504976645162, variance: 127.71875762939453, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -71.56334686279297, std_q: 6.717021465301514

steps: 349975, episodes: 14000, mean episode reward: -323.54673643374235, agent episode reward: [-107.84891214458078, -107.84891214458078, -107.84891214458078], time: 63.23
steps: 349975, episodes: 14000, mean episode variance: 31.77647900390625, agent episode variance: [0.0, 0.0, 31.77647900390625], time: 63.23
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.338150744326018, variance: 0.0, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -74.11396789550781, std_q: 6.640983581542969
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.338220282398433, variance: 0.0, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -73.77379608154297, std_q: 6.7157745361328125
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.335807397063147, variance: 127.10591125488281, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -71.5758285522461, std_q: 6.7201247215271

steps: 374975, episodes: 15000, mean episode reward: -326.3114950883746, agent episode reward: [-108.77049836279151, -108.77049836279151, -108.77049836279151], time: 64.06
steps: 374975, episodes: 15000, mean episode variance: 31.870697525024415, agent episode variance: [0.0, 0.0, 31.870697525024415], time: 64.061
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337486357715311, variance: 0.0, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -74.1112060546875, std_q: 6.594156265258789
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.333725436139128, variance: 0.0, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -73.75301361083984, std_q: 6.705196380615234
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.335929982737498, variance: 127.48278045654297, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -71.56671905517578, std_q: 6.7222900390625

steps: 399975, episodes: 16000, mean episode reward: -322.6605730971966, agent episode reward: [-107.55352436573223, -107.55352436573223, -107.55352436573223], time: 64.029
steps: 399975, episodes: 16000, mean episode variance: 31.893762672424316, agent episode variance: [0.0, 0.0, 31.893762672424316], time: 64.029
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3366374495554725, variance: 0.0, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -74.12306213378906, std_q: 6.595454692840576
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.333543849989403, variance: 0.0, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -73.77635955810547, std_q: 6.689051628112793
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.338261837642495, variance: 127.5750503540039, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -71.58470916748047, std_q: 6.7125935554504395

steps: 424975, episodes: 17000, mean episode reward: -324.1040633186765, agent episode reward: [-108.03468777289217, -108.03468777289217, -108.03468777289217], time: 64.078
steps: 424975, episodes: 17000, mean episode variance: 31.75993186187744, agent episode variance: [0.0, 0.0, 31.75993186187744], time: 64.078
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.332558149583617, variance: 0.0, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -74.10726928710938, std_q: 6.575134754180908
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.332020664176209, variance: 0.0, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -73.76270294189453, std_q: 6.699820041656494
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.333304127273624, variance: 127.03973388671875, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -71.56306457519531, std_q: 6.713072776794434

steps: 449975, episodes: 18000, mean episode reward: -323.91010796069463, agent episode reward: [-107.97003598689821, -107.97003598689821, -107.97003598689821], time: 64.825
steps: 449975, episodes: 18000, mean episode variance: 31.82514077758789, agent episode variance: [0.0, 0.0, 31.82514077758789], time: 64.826
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.332986178318901, variance: 0.0, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -74.1004638671875, std_q: 6.576684474945068
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.33759532397466, variance: 0.0, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -73.77605438232422, std_q: 6.711520195007324
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.334458822470318, variance: 127.3005599975586, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -71.58599853515625, std_q: 6.716546058654785

steps: 474975, episodes: 19000, mean episode reward: -325.2394431269941, agent episode reward: [-108.41314770899804, -108.41314770899804, -108.41314770899804], time: 67.541
steps: 474975, episodes: 19000, mean episode variance: 31.716100288391115, agent episode variance: [0.0, 0.0, 31.716100288391115], time: 67.542
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.336212607544146, variance: 0.0, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -74.10173797607422, std_q: 6.589967727661133
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.330048037521398, variance: 0.0, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -73.74190521240234, std_q: 6.700831890106201
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.332470537328467, variance: 126.86439514160156, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -71.5730972290039, std_q: 6.716618061065674

steps: 499975, episodes: 20000, mean episode reward: -328.3258777176475, agent episode reward: [-109.44195923921582, -109.44195923921582, -109.44195923921582], time: 64.293
steps: 499975, episodes: 20000, mean episode variance: 31.819127906799316, agent episode variance: [0.0, 0.0, 31.819127906799316], time: 64.293
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3362502710420525, variance: 0.0, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -74.09870147705078, std_q: 6.590475559234619
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.332456821790369, variance: 0.0, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -73.75263977050781, std_q: 6.698276996612549
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3352120394724984, variance: 127.2765121459961, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -71.58346557617188, std_q: 6.699209690093994

steps: 524975, episodes: 21000, mean episode reward: -325.90633125402115, agent episode reward: [-108.63544375134039, -108.63544375134039, -108.63544375134039], time: 65.379
steps: 524975, episodes: 21000, mean episode variance: 31.877520866394043, agent episode variance: [0.0, 0.0, 31.877520866394043], time: 65.379
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.341194396522778, variance: 0.0, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -74.1196060180664, std_q: 6.630056858062744
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3359309395512975, variance: 0.0, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -73.77629852294922, std_q: 6.70499324798584
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337823594330091, variance: 127.51007843017578, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -71.59742736816406, std_q: 6.7240729331970215

steps: 549975, episodes: 22000, mean episode reward: -323.7750700001532, agent episode reward: [-107.92502333338442, -107.92502333338442, -107.92502333338442], time: 65.843
steps: 549975, episodes: 22000, mean episode variance: 31.797084396362305, agent episode variance: [0.0, 0.0, 31.797084396362305], time: 65.843
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.333104861726429, variance: 0.0, cvar: -62.290958404541016, v: -62.290958404541016, mean_q: -74.09858703613281, std_q: 6.602697372436523
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.339936036544439, variance: 0.0, cvar: -62.290618896484375, v: -62.290958404541016, mean_q: -73.77163696289062, std_q: 6.725495338439941
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337414681229997, variance: 127.1883316040039, cvar: -62.271209716796875, v: -62.290958404541016, mean_q: -71.60079956054688, std_q: 6.730783462524414

steps: 574975, episodes: 23000, mean episode reward: -324.85139012224414, agent episode reward: [-108.28379670741474, -108.28379670741474, -108.28379670741474], time: 65.547
steps: 574975, episodes: 23000, mean episode variance: 31.88253730773926, agent episode variance: [0.0, 0.0, 31.88253730773926], time: 65.547
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3341744760421355, variance: 0.0, cvar: -64.77442169189453, v: -64.79768371582031, mean_q: -74.10912322998047, std_q: 6.596749782562256
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3361878883777, variance: 0.0, cvar: -64.6773910522461, v: -64.79736328125, mean_q: -73.76268768310547, std_q: 6.71629524230957
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.333752044146476, variance: 127.5301513671875, cvar: -64.0296630859375, v: -64.54954528808594, mean_q: -71.57414245605469, std_q: 6.707878112792969

steps: 599975, episodes: 24000, mean episode reward: -323.6349151178197, agent episode reward: [-107.87830503927323, -107.87830503927323, -107.87830503927323], time: 68.407
steps: 599975, episodes: 24000, mean episode variance: 31.837366226196288, agent episode variance: [0.0, 0.0, 31.837366226196288], time: 68.408
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.339803603359061, variance: 0.0, cvar: -66.35665130615234, v: -66.96390533447266, mean_q: -74.1240463256836, std_q: 6.60551118850708
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.33307878785395, variance: 0.0, cvar: -65.74040985107422, v: -66.59149169921875, mean_q: -73.7594985961914, std_q: 6.72687292098999
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.335042683383356, variance: 127.34945678710938, cvar: -64.19983673095703, v: -65.05021667480469, mean_q: -71.57972717285156, std_q: 6.725834369659424

steps: 624975, episodes: 25000, mean episode reward: -323.1980037175333, agent episode reward: [-107.73266790584444, -107.73266790584444, -107.73266790584444], time: 65.384
steps: 624975, episodes: 25000, mean episode variance: 31.825745010375975, agent episode variance: [0.0, 0.0, 31.825745010375975], time: 65.384
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.338941463405461, variance: 0.0, cvar: -66.4520034790039, v: -67.40103149414062, mean_q: -74.12332153320312, std_q: 6.6248908042907715
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.339896446242376, variance: 0.0, cvar: -65.75260925292969, v: -66.79529571533203, mean_q: -73.78385925292969, std_q: 6.735921859741211
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.338433262533048, variance: 127.30298614501953, cvar: -64.19620513916016, v: -65.05680847167969, mean_q: -71.5940933227539, std_q: 6.736493110656738

steps: 649975, episodes: 26000, mean episode reward: -325.06604570234566, agent episode reward: [-108.35534856744856, -108.35534856744856, -108.35534856744856], time: 64.833
steps: 649975, episodes: 26000, mean episode variance: 31.77666813659668, agent episode variance: [0.0, 0.0, 31.77666813659668], time: 64.834
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337771308463519, variance: 0.0, cvar: -66.43939971923828, v: -67.38925170898438, mean_q: -74.12774658203125, std_q: 6.624268531799316
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.332961103414292, variance: 0.0, cvar: -65.7523193359375, v: -66.78412628173828, mean_q: -73.75830841064453, std_q: 6.720648288726807
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.33848735038014, variance: 127.10667419433594, cvar: -64.20512390136719, v: -65.06024932861328, mean_q: -71.60625457763672, std_q: 6.736754894256592

steps: 674975, episodes: 27000, mean episode reward: -326.65996395669873, agent episode reward: [-108.8866546522329, -108.8866546522329, -108.8866546522329], time: 65.068
steps: 674975, episodes: 27000, mean episode variance: 31.857907630920412, agent episode variance: [0.0, 0.0, 31.857907630920412], time: 65.068
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.335980419979704, variance: 0.0, cvar: -66.45821380615234, v: -67.4026870727539, mean_q: -74.11331176757812, std_q: 6.602893352508545
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.33912628445498, variance: 0.0, cvar: -65.75970458984375, v: -66.79827117919922, mean_q: -73.76826477050781, std_q: 6.730356216430664
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.337055748145757, variance: 127.43162536621094, cvar: -64.19977569580078, v: -65.0520248413086, mean_q: -71.58780670166016, std_q: 6.727964401245117

steps: 699975, episodes: 28000, mean episode reward: -323.7436275762767, agent episode reward: [-107.91454252542556, -107.91454252542556, -107.91454252542556], time: 66.085
steps: 699975, episodes: 28000, mean episode variance: 31.766085739135743, agent episode variance: [0.0, 0.0, 31.766085739135743], time: 66.085
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.33823554390736, variance: 0.0, cvar: -66.46060943603516, v: -67.40513610839844, mean_q: -74.12882232666016, std_q: 6.6160969734191895
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.335362049710251, variance: 0.0, cvar: -65.77092742919922, v: -66.80447387695312, mean_q: -73.767333984375, std_q: 6.711206912994385
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.338009543395941, variance: 127.06434631347656, cvar: -64.2003402709961, v: -65.0553207397461, mean_q: -71.60334014892578, std_q: 6.711178779602051

steps: 724975, episodes: 29000, mean episode reward: -325.89521493607737, agent episode reward: [-108.63173831202579, -108.63173831202579, -108.63173831202579], time: 71.487
steps: 724975, episodes: 29000, mean episode variance: 31.6780020904541, agent episode variance: [0.0, 0.0, 31.6780020904541], time: 71.488
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.334456036850964, variance: 0.0, cvar: -66.45130920410156, v: -67.40043640136719, mean_q: -74.11388397216797, std_q: 6.599947452545166
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.334558441346717, variance: 0.0, cvar: -65.75238800048828, v: -66.80015563964844, mean_q: -73.7660140991211, std_q: 6.723040580749512
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.331355905489911, variance: 126.71199798583984, cvar: -64.18971252441406, v: -65.0501480102539, mean_q: -71.5877914428711, std_q: 6.718527317047119

some error in feed_dict
Tensor("observation0:0", shape=(?, 16), dtype=float32) :: [[ 0.65454574 -0.01259024 -0.34655086 -0.71615905 -0.11327689  0.89466945
  -0.32598885  0.06170821 -0.16117771  0.313933   -0.1057957   0.9010055
   0.          0.          0.          0.        ]]
Traceback (most recent call last):
  File "train.py", line 493, in <module>
    train(arglist)
  File "train.py", line 192, in train
    action_n = [agent.action(obs) for agent, obs in zip(trainers,obs_n)]
  File "train.py", line 192, in <listcomp>
    action_n = [agent.action(obs) for agent, obs in zip(trainers,obs_n)]
  File "../maddpg/trainer/maddpg.py", line 259, in action
    return self.act(obs[None])[0]
  File "../maddpg/common/tf_util.py", line 289, in <lambda>
    return lambda *args, **kwargs: f(*args, **kwargs)[0]
  File "../maddpg/common/tf_util.py", line 346, in __call__
    return results
UnboundLocalError: local variable 'results' referenced before assignment
