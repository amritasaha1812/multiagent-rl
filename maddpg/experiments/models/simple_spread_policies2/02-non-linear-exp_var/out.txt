# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 23.72 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies2/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies2/02-non-linear-exp_var/
Job <1090532> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc247>>
arglist.u_estimation True
2019-09-06 04:43:48.179938: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -532.3546109484704, agent episode reward: [-177.45153698282346, -177.45153698282346, -177.45153698282346], time: 45.171
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 45.172
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -708.7626331909676, agent episode reward: [-236.25421106365587, -236.25421106365587, -236.25421106365587], time: 74.195
steps: 49975, episodes: 2000, mean episode variance: 8.596636296510697, agent episode variance: [2.9565456270575523, 2.9582150375843046, 2.6818756318688393], time: 74.195
Running avgs for agent 0: q_loss: 89.78154754638672, p_loss: -5.631385803222656, mean_rew: -7.670060174656839, variance: 12.116989135742188, lamda: 1.0100303888320923
Running avgs for agent 1: q_loss: 99.91384887695312, p_loss: -5.568641662597656, mean_rew: -7.666396192518524, variance: 12.123832121247151, lamda: 1.009629487991333
Running avgs for agent 2: q_loss: 88.8861312866211, p_loss: -5.206711769104004, mean_rew: -7.67447765841733, variance: 10.991293573232948, lamda: 1.0099915266036987

steps: 74975, episodes: 3000, mean episode reward: -854.465238736104, agent episode reward: [-284.82174624536805, -284.82174624536805, -284.82174624536805], time: 81.148
steps: 74975, episodes: 3000, mean episode variance: 6.599322356462479, agent episode variance: [2.1985162980556487, 2.202286497592926, 2.198519560813904], time: 81.149
Running avgs for agent 0: q_loss: 50.742305755615234, p_loss: -4.867711067199707, mean_rew: -8.933146017389872, variance: 8.79406452178955, lamda: 1.0331246852874756
Running avgs for agent 1: q_loss: 44.710426330566406, p_loss: -4.921098709106445, mean_rew: -8.934530743262792, variance: 8.809146881103516, lamda: 1.031582236289978
Running avgs for agent 2: q_loss: 51.77737808227539, p_loss: -4.876838207244873, mean_rew: -8.934107897703877, variance: 8.79407787322998, lamda: 1.0327101945877075

steps: 99975, episodes: 4000, mean episode reward: -795.3594339823383, agent episode reward: [-265.1198113274461, -265.1198113274461, -265.1198113274461], time: 77.424
steps: 99975, episodes: 4000, mean episode variance: 6.6343032855987545, agent episode variance: [2.2079554567337034, 2.218293656349182, 2.2080541725158693], time: 77.424
Running avgs for agent 0: q_loss: 46.823795318603516, p_loss: -4.769346714019775, mean_rew: -9.479506424741741, variance: 8.831822395324707, lamda: 1.0581284761428833
Running avgs for agent 1: q_loss: 59.157386779785156, p_loss: -4.831947326660156, mean_rew: -9.48320416743959, variance: 8.873173713684082, lamda: 1.0564767122268677
Running avgs for agent 2: q_loss: 59.91658020019531, p_loss: -4.81532096862793, mean_rew: -9.472305354103806, variance: 8.8322172164917, lamda: 1.0577129125595093

steps: 124975, episodes: 5000, mean episode reward: -713.9113344259404, agent episode reward: [-237.97044480864685, -237.97044480864685, -237.97044480864685], time: 78.114
steps: 124975, episodes: 5000, mean episode variance: 6.711150016307831, agent episode variance: [2.2414840273857117, 2.240069959640503, 2.229596029281616], time: 78.114
Running avgs for agent 0: q_loss: 45.3172607421875, p_loss: -4.879767417907715, mean_rew: -9.668317777211312, variance: 8.965935707092285, lamda: 1.0831327438354492
Running avgs for agent 1: q_loss: 47.3936653137207, p_loss: -4.7381768226623535, mean_rew: -9.66067134568654, variance: 8.96027946472168, lamda: 1.081480860710144
Running avgs for agent 2: q_loss: 43.831085205078125, p_loss: -4.859201431274414, mean_rew: -9.671169024817832, variance: 8.918384552001953, lamda: 1.0827170610427856

steps: 149975, episodes: 6000, mean episode reward: -716.4205895968321, agent episode reward: [-238.80686319894403, -238.80686319894403, -238.80686319894403], time: 76.38
steps: 149975, episodes: 6000, mean episode variance: 6.537119792938232, agent episode variance: [2.1667055177688597, 2.195011267662048, 2.1754030075073243], time: 76.381
Running avgs for agent 0: q_loss: 39.38505554199219, p_loss: -4.834702968597412, mean_rew: -9.590312650741627, variance: 8.66682243347168, lamda: 1.10797119140625
Running avgs for agent 1: q_loss: 37.83256912231445, p_loss: -4.852675914764404, mean_rew: -9.589620785519633, variance: 8.780044555664062, lamda: 1.106471061706543
Running avgs for agent 2: q_loss: 35.36602020263672, p_loss: -4.815786361694336, mean_rew: -9.588172087563532, variance: 8.701611518859863, lamda: 1.1076148748397827

steps: 174975, episodes: 7000, mean episode reward: -789.8216190589606, agent episode reward: [-263.2738730196535, -263.2738730196535, -263.2738730196535], time: 74.093
steps: 174975, episodes: 7000, mean episode variance: 6.445962828159332, agent episode variance: [2.128370852947235, 2.1623579902648924, 2.1552339849472046], time: 74.094
Running avgs for agent 0: q_loss: 33.3669319152832, p_loss: -4.8555731773376465, mean_rew: -9.674335990611457, variance: 8.513483047485352, lamda: 1.1317462921142578
Running avgs for agent 1: q_loss: 28.964582443237305, p_loss: -4.914897441864014, mean_rew: -9.68032551820266, variance: 8.649431228637695, lamda: 1.1307361125946045
Running avgs for agent 2: q_loss: 32.9776496887207, p_loss: -4.8858642578125, mean_rew: -9.714609079029218, variance: 8.620935440063477, lamda: 1.1321496963500977

steps: 199975, episodes: 8000, mean episode reward: -771.8922128839577, agent episode reward: [-257.2974042946525, -257.2974042946525, -257.2974042946525], time: 75.249
steps: 199975, episodes: 8000, mean episode variance: 6.369869770288467, agent episode variance: [2.127005308151245, 2.114842411994934, 2.128022050142288], time: 75.249
Running avgs for agent 0: q_loss: 35.0242805480957, p_loss: -4.906350612640381, mean_rew: -9.767194280189992, variance: 8.508021354675293, lamda: 1.1552916765213013
Running avgs for agent 1: q_loss: 26.027944564819336, p_loss: -4.950669765472412, mean_rew: -9.770793740938947, variance: 8.459368705749512, lamda: 1.1539931297302246
Running avgs for agent 2: q_loss: 34.43159103393555, p_loss: -4.923717021942139, mean_rew: -9.785800122458696, variance: 8.51208782196045, lamda: 1.1547273397445679

steps: 224975, episodes: 9000, mean episode reward: -763.1904362582515, agent episode reward: [-254.3968120860838, -254.3968120860838, -254.3968120860838], time: 74.893
steps: 224975, episodes: 9000, mean episode variance: 6.292024823665619, agent episode variance: [2.0951456122398375, 2.094611280441284, 2.102267930984497], time: 74.893
Running avgs for agent 0: q_loss: 32.414939880371094, p_loss: -4.945834636688232, mean_rew: -9.826470888872736, variance: 8.380582809448242, lamda: 1.1783852577209473
Running avgs for agent 1: q_loss: 24.83167266845703, p_loss: -4.97682523727417, mean_rew: -9.82788090368806, variance: 8.378445625305176, lamda: 1.1749399900436401
Running avgs for agent 2: q_loss: 31.257862091064453, p_loss: -4.941153526306152, mean_rew: -9.834125141322486, variance: 8.409071922302246, lamda: 1.1766266822814941

steps: 249975, episodes: 10000, mean episode reward: -808.6508329593177, agent episode reward: [-269.5502776531059, -269.5502776531059, -269.5502776531059], time: 73.763
steps: 249975, episodes: 10000, mean episode variance: 6.219420814990998, agent episode variance: [2.067147954940796, 2.0776291375160216, 2.0746437225341796], time: 73.763
Running avgs for agent 0: q_loss: 33.2968864440918, p_loss: -5.011716842651367, mean_rew: -9.904592462544102, variance: 8.26859188079834, lamda: 1.2003785371780396
Running avgs for agent 1: q_loss: 25.492647171020508, p_loss: -5.024068355560303, mean_rew: -9.910835406474096, variance: 8.310516357421875, lamda: 1.196303367614746
Running avgs for agent 2: q_loss: 30.68979263305664, p_loss: -5.002851486206055, mean_rew: -9.906808899236406, variance: 8.298574447631836, lamda: 1.199110507965088

steps: 274975, episodes: 11000, mean episode reward: -797.0014229281542, agent episode reward: [-265.66714097605137, -265.66714097605137, -265.66714097605137], time: 76.747
steps: 274975, episodes: 11000, mean episode variance: 6.167929039001465, agent episode variance: [2.060239308834076, 2.055835271835327, 2.0518544583320617], time: 76.748
Running avgs for agent 0: q_loss: 33.9787712097168, p_loss: -5.053744316101074, mean_rew: -9.988091116612466, variance: 8.240957260131836, lamda: 1.2229328155517578
Running avgs for agent 1: q_loss: 26.226343154907227, p_loss: -5.053084373474121, mean_rew: -9.977799350623302, variance: 8.22334098815918, lamda: 1.2180482149124146
Running avgs for agent 2: q_loss: 28.19741439819336, p_loss: -5.053921699523926, mean_rew: -9.986161415413884, variance: 8.207418441772461, lamda: 1.2199182510375977

steps: 299975, episodes: 12000, mean episode reward: -799.0176703104519, agent episode reward: [-266.3392234368173, -266.3392234368173, -266.3392234368173], time: 75.769
steps: 299975, episodes: 12000, mean episode variance: 6.127520912647247, agent episode variance: [2.0322901549339294, 2.047896379470825, 2.0473343782424926], time: 75.769
Running avgs for agent 0: q_loss: 32.43614959716797, p_loss: -5.075509548187256, mean_rew: -10.04243460105726, variance: 8.12916088104248, lamda: 1.2427468299865723
Running avgs for agent 1: q_loss: 35.56725311279297, p_loss: -5.085054874420166, mean_rew: -10.04736929281, variance: 8.191585540771484, lamda: 1.2377718687057495
Running avgs for agent 2: q_loss: 30.83213996887207, p_loss: -5.085485458374023, mean_rew: -10.045720991843602, variance: 8.189337730407715, lamda: 1.239029049873352

steps: 324975, episodes: 13000, mean episode reward: -781.3925980204555, agent episode reward: [-260.4641993401518, -260.4641993401518, -260.4641993401518], time: 77.223
steps: 324975, episodes: 13000, mean episode variance: 6.094701869010925, agent episode variance: [1.9944268183708191, 2.04481632232666, 2.0554587283134462], time: 77.224
Running avgs for agent 0: q_loss: 31.307912826538086, p_loss: -5.107638835906982, mean_rew: -10.083222783072992, variance: 7.977707862854004, lamda: 1.2608309984207153
Running avgs for agent 1: q_loss: 39.69245910644531, p_loss: -5.091636657714844, mean_rew: -10.081102704237175, variance: 8.179265022277832, lamda: 1.2480765581130981
Running avgs for agent 2: q_loss: 44.82107925415039, p_loss: -5.116243362426758, mean_rew: -10.098071222191539, variance: 8.221835136413574, lamda: 1.2510688304901123

steps: 349975, episodes: 14000, mean episode reward: -767.1814351762499, agent episode reward: [-255.72714505875, -255.72714505875, -255.72714505875], time: 78.679
steps: 349975, episodes: 14000, mean episode variance: 5.994943388938903, agent episode variance: [1.984391685962677, 2.005898151397705, 2.0046535515785218], time: 78.68
Running avgs for agent 0: q_loss: 31.968420028686523, p_loss: -5.088345527648926, mean_rew: -10.09662866412676, variance: 7.937566757202148, lamda: 1.2770578861236572
Running avgs for agent 1: q_loss: 31.157413482666016, p_loss: -5.112650394439697, mean_rew: -10.098059738986471, variance: 8.023592948913574, lamda: 1.2608002424240112
Running avgs for agent 2: q_loss: 41.91129684448242, p_loss: -5.1113104820251465, mean_rew: -10.09343810171272, variance: 8.018613815307617, lamda: 1.2556560039520264

steps: 374975, episodes: 15000, mean episode reward: -784.412920110086, agent episode reward: [-261.4709733700287, -261.4709733700287, -261.4709733700287], time: 81.077
steps: 374975, episodes: 15000, mean episode variance: 5.988294055223465, agent episode variance: [1.9625569052696228, 1.988955057144165, 2.036782092809677], time: 81.077
Running avgs for agent 0: q_loss: 31.6544246673584, p_loss: -5.102483749389648, mean_rew: -10.106566245228509, variance: 7.850227355957031, lamda: 1.2947665452957153
Running avgs for agent 1: q_loss: 31.523759841918945, p_loss: -5.113780975341797, mean_rew: -10.115778120610036, variance: 7.955819606781006, lamda: 1.2828623056411743
Running avgs for agent 2: q_loss: 41.37020492553711, p_loss: -5.1147027015686035, mean_rew: -10.114577034677616, variance: 8.147128105163574, lamda: 1.2612509727478027

steps: 399975, episodes: 16000, mean episode reward: -768.5734077460028, agent episode reward: [-256.1911359153342, -256.1911359153342, -256.1911359153342], time: 80.591
steps: 399975, episodes: 16000, mean episode variance: 5.925530789375305, agent episode variance: [1.9507316703796387, 1.9591635813713073, 2.0156355376243593], time: 80.592
Running avgs for agent 0: q_loss: 35.191226959228516, p_loss: -5.109326362609863, mean_rew: -10.12281237349142, variance: 7.802926540374756, lamda: 1.3124459981918335
Running avgs for agent 1: q_loss: 27.5226993560791, p_loss: -5.1263275146484375, mean_rew: -10.134837478140435, variance: 7.836654186248779, lamda: 1.3035889863967896
Running avgs for agent 2: q_loss: 41.44829559326172, p_loss: -5.123922824859619, mean_rew: -10.136840163065587, variance: 8.062542915344238, lamda: 1.2677713632583618

steps: 424975, episodes: 17000, mean episode reward: -797.8904300554633, agent episode reward: [-265.9634766851545, -265.9634766851545, -265.9634766851545], time: 85.224
steps: 424975, episodes: 17000, mean episode variance: 5.827899826526642, agent episode variance: [1.9355196313858032, 1.912142967224121, 1.9802372279167175], time: 85.224
Running avgs for agent 0: q_loss: 50.770896911621094, p_loss: -5.1330437660217285, mean_rew: -10.138059954322136, variance: 7.7420783042907715, lamda: 1.3246495723724365
Running avgs for agent 1: q_loss: 26.90110969543457, p_loss: -5.1256303787231445, mean_rew: -10.146649339396372, variance: 7.648571968078613, lamda: 1.325693130493164
Running avgs for agent 2: q_loss: 36.92799758911133, p_loss: -5.129088878631592, mean_rew: -10.13992577609491, variance: 7.9209489822387695, lamda: 1.2805159091949463

steps: 449975, episodes: 18000, mean episode reward: -806.8944819750491, agent episode reward: [-268.96482732501636, -268.96482732501636, -268.96482732501636], time: 83.006
steps: 449975, episodes: 18000, mean episode variance: 5.841445418834686, agent episode variance: [1.955542848110199, 1.9057462320327758, 1.9801563386917114], time: 83.006
Running avgs for agent 0: q_loss: 52.130733489990234, p_loss: -5.15745210647583, mean_rew: -10.203415895976484, variance: 7.822171688079834, lamda: 1.3329122066497803
Running avgs for agent 1: q_loss: 26.132719039916992, p_loss: -5.143977642059326, mean_rew: -10.194877698197404, variance: 7.622984886169434, lamda: 1.347334623336792
Running avgs for agent 2: q_loss: 30.554595947265625, p_loss: -5.141295433044434, mean_rew: -10.177587385595563, variance: 7.920625686645508, lamda: 1.2961987257003784

steps: 474975, episodes: 19000, mean episode reward: -798.8427123010491, agent episode reward: [-266.28090410034963, -266.28090410034963, -266.28090410034963], time: 85.132
steps: 474975, episodes: 19000, mean episode variance: 5.749987019777298, agent episode variance: [1.913819876909256, 1.8825334305763244, 1.9536337122917176], time: 85.132
Running avgs for agent 0: q_loss: 41.22102355957031, p_loss: -5.16986083984375, mean_rew: -10.21629616712003, variance: 7.655279159545898, lamda: 1.343628168106079
Running avgs for agent 1: q_loss: 26.908050537109375, p_loss: -5.161971569061279, mean_rew: -10.21537163832785, variance: 7.5301337242126465, lamda: 1.3694446086883545
Running avgs for agent 2: q_loss: 31.64990234375, p_loss: -5.158624172210693, mean_rew: -10.21602697525715, variance: 7.814535140991211, lamda: 1.3189150094985962

steps: 499975, episodes: 20000, mean episode reward: -812.8022083848633, agent episode reward: [-270.9340694616211, -270.9340694616211, -270.9340694616211], time: 116.076
steps: 499975, episodes: 20000, mean episode variance: 5.658823987960815, agent episode variance: [1.8881115970611573, 1.8592734184265136, 1.9114389724731444], time: 116.077
Running avgs for agent 0: q_loss: 31.588098526000977, p_loss: -5.17505407333374, mean_rew: -10.254525153520841, variance: 7.552446365356445, lamda: 1.3619604110717773
Running avgs for agent 1: q_loss: 27.383399963378906, p_loss: -5.157048225402832, mean_rew: -10.236178699947642, variance: 7.437093734741211, lamda: 1.3902636766433716
Running avgs for agent 2: q_loss: 30.708805084228516, p_loss: -5.151665210723877, mean_rew: -10.222933778899577, variance: 7.645755767822266, lamda: 1.342056393623352

steps: 524975, episodes: 21000, mean episode reward: -788.3431996255867, agent episode reward: [-262.78106654186223, -262.78106654186223, -262.78106654186223], time: 107.967
steps: 524975, episodes: 21000, mean episode variance: 5.647763097524643, agent episode variance: [1.8982077217102051, 1.8410541791915893, 1.9085011966228485], time: 107.967
Running avgs for agent 0: q_loss: 48.590843200683594, p_loss: -5.201887130737305, mean_rew: -10.274445324723626, variance: 7.592831134796143, lamda: 1.376842975616455
Running avgs for agent 1: q_loss: 27.30759620666504, p_loss: -5.165345191955566, mean_rew: -10.262758294103161, variance: 7.3642168045043945, lamda: 1.4107584953308105
Running avgs for agent 2: q_loss: 34.68920135498047, p_loss: -5.188938617706299, mean_rew: -10.279893454080918, variance: 7.634005069732666, lamda: 1.3636600971221924

steps: 549975, episodes: 22000, mean episode reward: -798.6291233268007, agent episode reward: [-266.20970777560024, -266.20970777560024, -266.20970777560024], time: 107.564
steps: 549975, episodes: 22000, mean episode variance: 5.586962994813919, agent episode variance: [1.8795078320503236, 1.8204728808403015, 1.886982281923294], time: 107.564
Running avgs for agent 0: q_loss: 49.524845123291016, p_loss: -5.20319128036499, mean_rew: -10.272027842398735, variance: 7.518031120300293, lamda: 1.3823212385177612
Running avgs for agent 1: q_loss: 26.43765640258789, p_loss: -5.161064147949219, mean_rew: -10.266008873866046, variance: 7.281891822814941, lamda: 1.4300366640090942
Running avgs for agent 2: q_loss: 40.40932083129883, p_loss: -5.17291784286499, mean_rew: -10.273538429351598, variance: 7.547928810119629, lamda: 1.3744202852249146

steps: 574975, episodes: 23000, mean episode reward: -807.5133869662974, agent episode reward: [-269.17112898876576, -269.17112898876576, -269.17112898876576], time: 109.075
steps: 574975, episodes: 23000, mean episode variance: 5.544737179517746, agent episode variance: [1.8663999953269959, 1.798722265958786, 1.8796149182319641], time: 109.075
Running avgs for agent 0: q_loss: 30.731094360351562, p_loss: -5.216357231140137, mean_rew: -10.295026944876117, variance: 7.465600490570068, lamda: 1.3927150964736938
Running avgs for agent 1: q_loss: 38.2901611328125, p_loss: -5.170366287231445, mean_rew: -10.288257362361135, variance: 7.194888591766357, lamda: 1.4437305927276611
Running avgs for agent 2: q_loss: 29.876535415649414, p_loss: -5.193782329559326, mean_rew: -10.307715284192243, variance: 7.518459320068359, lamda: 1.3889799118041992

steps: 599975, episodes: 24000, mean episode reward: -795.1869158804941, agent episode reward: [-265.062305293498, -265.062305293498, -265.062305293498], time: 110.395
steps: 599975, episodes: 24000, mean episode variance: 5.450361936092377, agent episode variance: [1.8432469577789308, 1.789260835647583, 1.817854142665863], time: 110.396
Running avgs for agent 0: q_loss: 35.13311004638672, p_loss: -5.228024482727051, mean_rew: -10.326206449842678, variance: 7.372988224029541, lamda: 1.4110504388809204
Running avgs for agent 1: q_loss: 30.39414405822754, p_loss: -5.186148166656494, mean_rew: -10.316370546392378, variance: 7.157042980194092, lamda: 1.448772668838501
Running avgs for agent 2: q_loss: 28.413555145263672, p_loss: -5.205653190612793, mean_rew: -10.307458833709246, variance: 7.271416664123535, lamda: 1.4116920232772827

steps: 624975, episodes: 25000, mean episode reward: -800.0033645704244, agent episode reward: [-266.66778819014144, -266.66778819014144, -266.66778819014144], time: 143.593
steps: 624975, episodes: 25000, mean episode variance: 5.424274097442627, agent episode variance: [1.8232687711715698, 1.7807750253677368, 1.8202303009033203], time: 143.594
Running avgs for agent 0: q_loss: 32.3741455078125, p_loss: -5.232542037963867, mean_rew: -10.329736501651936, variance: 7.293075084686279, lamda: 1.4280403852462769
Running avgs for agent 1: q_loss: 25.019214630126953, p_loss: -5.191745281219482, mean_rew: -10.321367788424258, variance: 7.123100280761719, lamda: 1.4612395763397217
Running avgs for agent 2: q_loss: 29.16551971435547, p_loss: -5.2057976722717285, mean_rew: -10.34334416420816, variance: 7.28092098236084, lamda: 1.4321041107177734

steps: 649975, episodes: 26000, mean episode reward: -821.6080933384898, agent episode reward: [-273.8693644461633, -273.8693644461633, -273.8693644461633], time: 164.645
steps: 649975, episodes: 26000, mean episode variance: 5.374681131601333, agent episode variance: [1.798410301208496, 1.7810618288516997, 1.7952090015411377], time: 164.646
Running avgs for agent 0: q_loss: 32.011714935302734, p_loss: -5.237217903137207, mean_rew: -10.348721101307557, variance: 7.193641662597656, lamda: 1.4439536333084106
Running avgs for agent 1: q_loss: 34.559173583984375, p_loss: -5.179029941558838, mean_rew: -10.343917863277348, variance: 7.124247074127197, lamda: 1.4753663539886475
Running avgs for agent 2: q_loss: 29.6436824798584, p_loss: -5.2191691398620605, mean_rew: -10.344837187467412, variance: 7.180835723876953, lamda: 1.4535762071609497

steps: 674975, episodes: 27000, mean episode reward: -840.3005326841851, agent episode reward: [-280.100177561395, -280.100177561395, -280.100177561395], time: 158.403
steps: 674975, episodes: 27000, mean episode variance: 5.331515968561172, agent episode variance: [1.79299862909317, 1.7656721460819245, 1.7728451933860778], time: 158.403
Running avgs for agent 0: q_loss: 41.3758659362793, p_loss: -5.253971099853516, mean_rew: -10.37528443296966, variance: 7.171994209289551, lamda: 1.4614970684051514
Running avgs for agent 1: q_loss: 36.4887809753418, p_loss: -5.223941802978516, mean_rew: -10.387047924393439, variance: 7.06268835067749, lamda: 1.4780385494232178
Running avgs for agent 2: q_loss: 28.70648765563965, p_loss: -5.232724189758301, mean_rew: -10.363290185633495, variance: 7.091381072998047, lamda: 1.4721343517303467

steps: 699975, episodes: 28000, mean episode reward: -827.072966113187, agent episode reward: [-275.6909887043956, -275.6909887043956, -275.6909887043956], time: 158.023
steps: 699975, episodes: 28000, mean episode variance: 5.308513439655304, agent episode variance: [1.7798102016448976, 1.7776642227172852, 1.7510390152931214], time: 158.023
Running avgs for agent 0: q_loss: 42.287742614746094, p_loss: -5.259754180908203, mean_rew: -10.407287094610982, variance: 7.119240760803223, lamda: 1.4752182960510254
Running avgs for agent 1: q_loss: 36.33558654785156, p_loss: -5.225693702697754, mean_rew: -10.408366707783957, variance: 7.11065673828125, lamda: 1.4805924892425537
Running avgs for agent 2: q_loss: 28.085922241210938, p_loss: -5.258143901824951, mean_rew: -10.397708420007366, variance: 7.00415563583374, lamda: 1.4888845682144165

steps: 724975, episodes: 29000, mean episode reward: -784.5583560878064, agent episode reward: [-261.5194520292688, -261.5194520292688, -261.5194520292688], time: 164.052
steps: 724975, episodes: 29000, mean episode variance: 5.29430653309822, agent episode variance: [1.7573422131538392, 1.7887437908649444, 1.7482205290794373], time: 164.053
Running avgs for agent 0: q_loss: 33.137962341308594, p_loss: -5.263192176818848, mean_rew: -10.414614573882664, variance: 7.029369354248047, lamda: 1.494101881980896
Running avgs for agent 1: q_loss: 31.14908790588379, p_loss: -5.241748809814453, mean_rew: -10.423000814007269, variance: 7.154974460601807, lamda: 1.4833711385726929
Running avgs for agent 2: q_loss: 40.79184341430664, p_loss: -5.261549949645996, mean_rew: -10.419702824209583, variance: 6.992882251739502, lamda: 1.5018759965896606

steps: 749975, episodes: 30000, mean episode reward: -800.7954462624198, agent episode reward: [-266.93181542080663, -266.93181542080663, -266.93181542080663], time: 164.625
steps: 749975, episodes: 30000, mean episode variance: 5.242572209358215, agent episode variance: [1.7444540424346924, 1.7661874890327454, 1.7319306778907775], time: 164.625
Running avgs for agent 0: q_loss: 33.043922424316406, p_loss: -5.259647846221924, mean_rew: -10.412616332365829, variance: 6.977816104888916, lamda: 1.5072203874588013
Running avgs for agent 1: q_loss: 24.100358963012695, p_loss: -5.2271409034729, mean_rew: -10.409898864249696, variance: 7.0647501945495605, lamda: 1.4931589365005493
Running avgs for agent 2: q_loss: 42.799068450927734, p_loss: -5.2574872970581055, mean_rew: -10.422333181974173, variance: 6.927722454071045, lamda: 1.5060797929763794

steps: 774975, episodes: 31000, mean episode reward: -817.4624810407397, agent episode reward: [-272.4874936802466, -272.4874936802466, -272.4874936802466], time: 163.529
steps: 774975, episodes: 31000, mean episode variance: 5.2139937114715575, agent episode variance: [1.7242638492584228, 1.7484513955116272, 1.7412784667015075], time: 163.529
Running avgs for agent 0: q_loss: 33.19529724121094, p_loss: -5.289761066436768, mean_rew: -10.429439224204092, variance: 6.897055625915527, lamda: 1.5223487615585327
Running avgs for agent 1: q_loss: 23.020736694335938, p_loss: -5.23676872253418, mean_rew: -10.429240680679483, variance: 6.993805885314941, lamda: 1.5086802244186401
Running avgs for agent 2: q_loss: 42.267616271972656, p_loss: -5.267779350280762, mean_rew: -10.437230489509668, variance: 6.965114116668701, lamda: 1.510416030883789

steps: 799975, episodes: 32000, mean episode reward: -817.1015517520847, agent episode reward: [-272.36718391736156, -272.36718391736156, -272.36718391736156], time: 165.248
steps: 799975, episodes: 32000, mean episode variance: 5.165487261414528, agent episode variance: [1.6939666038751602, 1.7393286819458007, 1.7321919755935669], time: 165.248
Running avgs for agent 0: q_loss: 39.77637481689453, p_loss: -5.2859320640563965, mean_rew: -10.454340201904808, variance: 6.7758660316467285, lamda: 1.5358096361160278
Running avgs for agent 1: q_loss: 22.814374923706055, p_loss: -5.241373062133789, mean_rew: -10.424222192413783, variance: 6.957314968109131, lamda: 1.5206373929977417
Running avgs for agent 2: q_loss: 33.88595199584961, p_loss: -5.272916316986084, mean_rew: -10.4514894227958, variance: 6.928768157958984, lamda: 1.5162413120269775

steps: 824975, episodes: 33000, mean episode reward: -821.8171985931288, agent episode reward: [-273.93906619770956, -273.93906619770956, -273.93906619770956], time: 164.106
steps: 824975, episodes: 33000, mean episode variance: 5.1348225841522215, agent episode variance: [1.6922705845832824, 1.7306019477844239, 1.7119500517845154], time: 164.107
Running avgs for agent 0: q_loss: 36.48233413696289, p_loss: -5.296078205108643, mean_rew: -10.45476505105486, variance: 6.769082069396973, lamda: 1.5478886365890503
Running avgs for agent 1: q_loss: 22.43642234802246, p_loss: -5.257466793060303, mean_rew: -10.45559425534503, variance: 6.922408103942871, lamda: 1.5310769081115723
Running avgs for agent 2: q_loss: 28.511831283569336, p_loss: -5.269420146942139, mean_rew: -10.451614958742692, variance: 6.847800254821777, lamda: 1.5334500074386597

steps: 849975, episodes: 34000, mean episode reward: -840.0434165764503, agent episode reward: [-280.0144721921501, -280.0144721921501, -280.0144721921501], time: 164.138
steps: 849975, episodes: 34000, mean episode variance: 5.098760112047195, agent episode variance: [1.6829657247066498, 1.7203428754806518, 1.6954515118598938], time: 164.138
Running avgs for agent 0: q_loss: 51.01465606689453, p_loss: -5.30719518661499, mean_rew: -10.483462173414853, variance: 6.731863498687744, lamda: 1.5605016946792603
Running avgs for agent 1: q_loss: 21.262096405029297, p_loss: -5.268628120422363, mean_rew: -10.486756019264918, variance: 6.881371021270752, lamda: 1.5376557111740112
Running avgs for agent 2: q_loss: 27.998903274536133, p_loss: -5.2902116775512695, mean_rew: -10.482937568558484, variance: 6.781805515289307, lamda: 1.5481176376342773

steps: 874975, episodes: 35000, mean episode reward: -854.0086773726357, agent episode reward: [-284.6695591242119, -284.6695591242119, -284.6695591242119], time: 159.182
steps: 874975, episodes: 35000, mean episode variance: 5.113512518405915, agent episode variance: [1.6951428365707397, 1.7124436433315278, 1.7059260385036468], time: 159.183
Running avgs for agent 0: q_loss: 51.94666290283203, p_loss: -5.321558952331543, mean_rew: -10.509913133098028, variance: 6.780571460723877, lamda: 1.5651696920394897
Running avgs for agent 1: q_loss: 26.06589126586914, p_loss: -5.271237373352051, mean_rew: -10.505217996730407, variance: 6.849774360656738, lamda: 1.544100284576416
Running avgs for agent 2: q_loss: 41.634830474853516, p_loss: -5.280847549438477, mean_rew: -10.487168351225307, variance: 6.823704242706299, lamda: 1.5605778694152832

steps: 899975, episodes: 36000, mean episode reward: -846.0303024823361, agent episode reward: [-282.0101008274454, -282.0101008274454, -282.0101008274454], time: 159.127
steps: 899975, episodes: 36000, mean episode variance: 5.128248996257782, agent episode variance: [1.7039932775497437, 1.733184181690216, 1.6910715370178222], time: 159.128
Running avgs for agent 0: q_loss: 44.72974395751953, p_loss: -5.338020324707031, mean_rew: -10.537244964638129, variance: 6.815972805023193, lamda: 1.570591688156128
Running avgs for agent 1: q_loss: 36.1973991394043, p_loss: -5.274128437042236, mean_rew: -10.512505399701308, variance: 6.932736873626709, lamda: 1.546205997467041
Running avgs for agent 2: q_loss: 42.71657943725586, p_loss: -5.302203178405762, mean_rew: -10.516075522269006, variance: 6.764286041259766, lamda: 1.5641030073165894

steps: 924975, episodes: 37000, mean episode reward: -821.2864145431879, agent episode reward: [-273.76213818106265, -273.76213818106265, -273.76213818106265], time: 160.51
steps: 924975, episodes: 37000, mean episode variance: 5.102981934785843, agent episode variance: [1.6824511334896088, 1.7141035430431366, 1.7064272582530975], time: 160.51
Running avgs for agent 0: q_loss: 38.9551887512207, p_loss: -5.332862377166748, mean_rew: -10.540069623780438, variance: 6.729804515838623, lamda: 1.5862181186676025
Running avgs for agent 1: q_loss: 35.111610412597656, p_loss: -5.303826332092285, mean_rew: -10.547613225150007, variance: 6.856413841247559, lamda: 1.5473875999450684
Running avgs for agent 2: q_loss: 42.37889099121094, p_loss: -5.313566207885742, mean_rew: -10.544384303606794, variance: 6.825708866119385, lamda: 1.5669026374816895

steps: 949975, episodes: 38000, mean episode reward: -827.0774044300832, agent episode reward: [-275.69246814336105, -275.69246814336105, -275.69246814336105], time: 156.922
steps: 949975, episodes: 38000, mean episode variance: 5.100654128074646, agent episode variance: [1.6804190480709076, 1.7386910281181336, 1.6815440518856049], time: 156.922
Running avgs for agent 0: q_loss: 52.38557052612305, p_loss: -5.336809158325195, mean_rew: -10.548516395680082, variance: 6.721676349639893, lamda: 1.5951943397521973
Running avgs for agent 1: q_loss: 34.94916534423828, p_loss: -5.296841621398926, mean_rew: -10.55898490464694, variance: 6.954764366149902, lamda: 1.5486228466033936
Running avgs for agent 2: q_loss: 38.65888214111328, p_loss: -5.3234992027282715, mean_rew: -10.555951852340204, variance: 6.7261762619018555, lamda: 1.5768226385116577

steps: 974975, episodes: 39000, mean episode reward: -839.002122883022, agent episode reward: [-279.66737429434073, -279.66737429434073, -279.66737429434073], time: 157.938
steps: 974975, episodes: 39000, mean episode variance: 5.0758569195270535, agent episode variance: [1.6649028222560882, 1.723092782497406, 1.6878613147735595], time: 157.938
Running avgs for agent 0: q_loss: 46.991966247558594, p_loss: -5.343794822692871, mean_rew: -10.566431362917589, variance: 6.659611225128174, lamda: 1.603622317314148
Running avgs for agent 1: q_loss: 24.964553833007812, p_loss: -5.297002792358398, mean_rew: -10.559193640401618, variance: 6.89237117767334, lamda: 1.5555235147476196
Running avgs for agent 2: q_loss: 43.85026168823242, p_loss: -5.324472427368164, mean_rew: -10.570710860065116, variance: 6.751445293426514, lamda: 1.5818266868591309

steps: 999975, episodes: 40000, mean episode reward: -839.5297808158442, agent episode reward: [-279.84326027194805, -279.84326027194805, -279.84326027194805], time: 158.44
steps: 999975, episodes: 40000, mean episode variance: 5.047623833179474, agent episode variance: [1.6725639986991883, 1.7035441994667053, 1.6715156350135802], time: 158.44
Running avgs for agent 0: q_loss: 50.470584869384766, p_loss: -5.358152866363525, mean_rew: -10.582826491635675, variance: 6.690255641937256, lamda: 1.6077778339385986
Running avgs for agent 1: q_loss: 21.233745574951172, p_loss: -5.315969944000244, mean_rew: -10.580371214985705, variance: 6.814176559448242, lamda: 1.5663796663284302
Running avgs for agent 2: q_loss: 42.524818420410156, p_loss: -5.335930824279785, mean_rew: -10.589026528098357, variance: 6.686062335968018, lamda: 1.585193157196045

steps: 1024975, episodes: 41000, mean episode reward: -838.0324946003877, agent episode reward: [-279.34416486679595, -279.34416486679595, -279.34416486679595], time: 158.497
steps: 1024975, episodes: 41000, mean episode variance: 5.087238499403, agent episode variance: [1.6671045346260072, 1.7123273148536682, 1.7078066499233246], time: 158.497
Running avgs for agent 0: q_loss: 50.82693862915039, p_loss: -5.399959087371826, mean_rew: -10.66052967142838, variance: 6.668417930603027, lamda: 1.6111282110214233
Running avgs for agent 1: q_loss: 20.841188430786133, p_loss: -5.341975688934326, mean_rew: -10.63214152777791, variance: 6.849308967590332, lamda: 1.5758216381072998
Running avgs for agent 2: q_loss: 39.76165008544922, p_loss: -5.375765323638916, mean_rew: -10.657089195005344, variance: 6.831226348876953, lamda: 1.5872339010238647

steps: 1049975, episodes: 42000, mean episode reward: -874.0338270387452, agent episode reward: [-291.3446090129151, -291.3446090129151, -291.3446090129151], time: 157.62
steps: 1049975, episodes: 42000, mean episode variance: 5.078847968578339, agent episode variance: [1.6764056649208068, 1.7078605382442473, 1.6945817654132842], time: 157.62
Running avgs for agent 0: q_loss: 50.34693908691406, p_loss: -5.42698335647583, mean_rew: -10.721159725573484, variance: 6.705622673034668, lamda: 1.6135708093643188
Running avgs for agent 1: q_loss: 22.700611114501953, p_loss: -5.3881378173828125, mean_rew: -10.723671082930018, variance: 6.831442356109619, lamda: 1.5835049152374268
Running avgs for agent 2: q_loss: 37.51156234741211, p_loss: -5.411248207092285, mean_rew: -10.728390700610438, variance: 6.778326988220215, lamda: 1.5982400178909302

steps: 1074975, episodes: 43000, mean episode reward: -902.4496826465075, agent episode reward: [-300.8165608821692, -300.8165608821692, -300.8165608821692], time: 157.151
steps: 1074975, episodes: 43000, mean episode variance: 5.103397827863693, agent episode variance: [1.682960405111313, 1.72294953417778, 1.6974878885746003], time: 157.151
Running avgs for agent 0: q_loss: 37.09297561645508, p_loss: -5.437840461730957, mean_rew: -10.76014155256554, variance: 6.731841564178467, lamda: 1.6222527027130127
Running avgs for agent 1: q_loss: 34.857421875, p_loss: -5.389429092407227, mean_rew: -10.735095907354621, variance: 6.89179801940918, lamda: 1.589605689048767
Running avgs for agent 2: q_loss: 43.40296173095703, p_loss: -5.4348554611206055, mean_rew: -10.783007385679532, variance: 6.789951801300049, lamda: 1.6027312278747559

steps: 1099975, episodes: 44000, mean episode reward: -879.274355218942, agent episode reward: [-293.0914517396474, -293.0914517396474, -293.0914517396474], time: 162.27
steps: 1099975, episodes: 44000, mean episode variance: 5.0900903673172, agent episode variance: [1.661233655691147, 1.7233490686416626, 1.7055076429843903], time: 162.271
Running avgs for agent 0: q_loss: 47.16118621826172, p_loss: -5.460925579071045, mean_rew: -10.792338912341945, variance: 6.64493465423584, lamda: 1.6373497247695923
Running avgs for agent 1: q_loss: 33.53833770751953, p_loss: -5.423834323883057, mean_rew: -10.796135380370131, variance: 6.893396377563477, lamda: 1.5905439853668213
Running avgs for agent 2: q_loss: 42.86214828491211, p_loss: -5.433075904846191, mean_rew: -10.788474933572836, variance: 6.822030544281006, lamda: 1.6052048206329346

steps: 1124975, episodes: 45000, mean episode reward: -900.9308209572253, agent episode reward: [-300.31027365240845, -300.31027365240845, -300.31027365240845], time: 163.844
steps: 1124975, episodes: 45000, mean episode variance: 5.081830263853073, agent episode variance: [1.6583950369358063, 1.7221548235416413, 1.7012804033756257], time: 163.844
Running avgs for agent 0: q_loss: 50.55781936645508, p_loss: -5.491804599761963, mean_rew: -10.831354312200459, variance: 6.633580207824707, lamda: 1.6408978700637817
Running avgs for agent 1: q_loss: 29.471792221069336, p_loss: -5.442762851715088, mean_rew: -10.829376880736618, variance: 6.888619422912598, lamda: 1.591866135597229
Running avgs for agent 2: q_loss: 41.906063079833984, p_loss: -5.454777717590332, mean_rew: -10.82490918312637, variance: 6.805121898651123, lamda: 1.607171654701233

steps: 1149975, episodes: 46000, mean episode reward: -903.217070787346, agent episode reward: [-301.07235692911536, -301.07235692911536, -301.07235692911536], time: 163.809
steps: 1149975, episodes: 46000, mean episode variance: 5.095403581142426, agent episode variance: [1.67848628282547, 1.7212274832725525, 1.695689815044403], time: 163.809
Running avgs for agent 0: q_loss: 49.26691818237305, p_loss: -5.507059574127197, mean_rew: -10.887461463863401, variance: 6.713945388793945, lamda: 1.6422587633132935
Running avgs for agent 1: q_loss: 20.754005432128906, p_loss: -5.472373008728027, mean_rew: -10.891039475264538, variance: 6.884909629821777, lamda: 1.599739909172058
Running avgs for agent 2: q_loss: 42.94209289550781, p_loss: -5.4806928634643555, mean_rew: -10.884566493077305, variance: 6.782758712768555, lamda: 1.6096161603927612

steps: 1174975, episodes: 47000, mean episode reward: -904.7319070461243, agent episode reward: [-301.5773023487081, -301.5773023487081, -301.5773023487081], time: 162.241
steps: 1174975, episodes: 47000, mean episode variance: 5.123644181251526, agent episode variance: [1.680961296081543, 1.7225672087669373, 1.7201156764030456], time: 162.241
Running avgs for agent 0: q_loss: 49.649986267089844, p_loss: -5.5311174392700195, mean_rew: -10.940918593771327, variance: 6.7238450050354, lamda: 1.6426379680633545
Running avgs for agent 1: q_loss: 22.323440551757812, p_loss: -5.492424964904785, mean_rew: -10.929729815308134, variance: 6.890268325805664, lamda: 1.6041784286499023
Running avgs for agent 2: q_loss: 42.852272033691406, p_loss: -5.517433166503906, mean_rew: -10.948730298766042, variance: 6.880462169647217, lamda: 1.6127675771713257

steps: 1199975, episodes: 48000, mean episode reward: -899.9413441764877, agent episode reward: [-299.98044805882927, -299.98044805882927, -299.98044805882927], time: 157.033
steps: 1199975, episodes: 48000, mean episode variance: 5.1164246485233305, agent episode variance: [1.672766004562378, 1.7200412929058075, 1.7236173510551454], time: 157.033
Running avgs for agent 0: q_loss: 50.26045227050781, p_loss: -5.56427001953125, mean_rew: -10.991649021838068, variance: 6.69106388092041, lamda: 1.643486738204956
Running avgs for agent 1: q_loss: 28.75029754638672, p_loss: -5.517068386077881, mean_rew: -10.981547150586902, variance: 6.880165100097656, lamda: 1.6071635484695435
Running avgs for agent 2: q_loss: 43.53200912475586, p_loss: -5.540269374847412, mean_rew: -10.980069118227256, variance: 6.894469738006592, lamda: 1.6151701211929321

steps: 1224975, episodes: 49000, mean episode reward: -913.0762178393593, agent episode reward: [-304.3587392797864, -304.3587392797864, -304.3587392797864], time: 156.431
steps: 1224975, episodes: 49000, mean episode variance: 5.126870489358902, agent episode variance: [1.6929229283332825, 1.7261669330596925, 1.7077806279659271], time: 156.431
Running avgs for agent 0: q_loss: 48.7205696105957, p_loss: -5.580825328826904, mean_rew: -11.042038599507954, variance: 6.77169132232666, lamda: 1.6441062688827515
Running avgs for agent 1: q_loss: 20.613983154296875, p_loss: -5.543540000915527, mean_rew: -11.035286700544834, variance: 6.904667854309082, lamda: 1.611267328262329
Running avgs for agent 2: q_loss: 42.617374420166016, p_loss: -5.5545125007629395, mean_rew: -11.025947008786586, variance: 6.831122398376465, lamda: 1.6179956197738647

steps: 1249975, episodes: 50000, mean episode reward: -907.88012447346, agent episode reward: [-302.62670815782, -302.62670815782, -302.62670815782], time: 156.219
steps: 1249975, episodes: 50000, mean episode variance: 5.16843064236641, agent episode variance: [1.7082623579502105, 1.7320857810974122, 1.7280825033187865], time: 156.22
Running avgs for agent 0: q_loss: 48.683414459228516, p_loss: -5.598618507385254, mean_rew: -11.082133970186813, variance: 6.833049297332764, lamda: 1.6453396081924438
Running avgs for agent 1: q_loss: 21.961870193481445, p_loss: -5.563418865203857, mean_rew: -11.07238263701755, variance: 6.928343296051025, lamda: 1.6157220602035522
Running avgs for agent 2: q_loss: 42.75592041015625, p_loss: -5.567914009094238, mean_rew: -11.066972550869185, variance: 6.912330150604248, lamda: 1.6210685968399048

steps: 1274975, episodes: 51000, mean episode reward: -916.123878072187, agent episode reward: [-305.37462602406225, -305.37462602406225, -305.37462602406225], time: 155.91
steps: 1274975, episodes: 51000, mean episode variance: 5.144390201091766, agent episode variance: [1.6878620471954346, 1.7276958856582643, 1.7288322682380677], time: 155.911
Running avgs for agent 0: q_loss: 48.43151092529297, p_loss: -5.598569393157959, mean_rew: -11.08554968552863, variance: 6.751447677612305, lamda: 1.6465938091278076
Running avgs for agent 1: q_loss: 20.31342124938965, p_loss: -5.589074611663818, mean_rew: -11.108792842794973, variance: 6.910783767700195, lamda: 1.618734359741211
Running avgs for agent 2: q_loss: 42.8610954284668, p_loss: -5.596709728240967, mean_rew: -11.099338732423297, variance: 6.9153289794921875, lamda: 1.622772455215454

steps: 1299975, episodes: 52000, mean episode reward: -916.0418279930436, agent episode reward: [-305.3472759976812, -305.3472759976812, -305.3472759976812], time: 155.246
steps: 1299975, episodes: 52000, mean episode variance: 5.153075360298157, agent episode variance: [1.7024462625980377, 1.7212001042366027, 1.7294289934635163], time: 155.247
Running avgs for agent 0: q_loss: 50.29951858520508, p_loss: -5.641535758972168, mean_rew: -11.15873937087163, variance: 6.80978536605835, lamda: 1.6490390300750732
Running avgs for agent 1: q_loss: 22.881671905517578, p_loss: -5.5984697341918945, mean_rew: -11.131493654791692, variance: 6.884800910949707, lamda: 1.621608018875122
Running avgs for agent 2: q_loss: 42.653812408447266, p_loss: -5.609019756317139, mean_rew: -11.147186220984985, variance: 6.917716026306152, lamda: 1.6251132488250732

steps: 1324975, episodes: 53000, mean episode reward: -905.793401575041, agent episode reward: [-301.9311338583471, -301.9311338583471, -301.9311338583471], time: 158.571
steps: 1324975, episodes: 53000, mean episode variance: 5.178610273838043, agent episode variance: [1.703207284450531, 1.7191575026512147, 1.7562454867362975], time: 158.571
Running avgs for agent 0: q_loss: 30.969268798828125, p_loss: -5.629985332489014, mean_rew: -11.164993439711292, variance: 6.81282901763916, lamda: 1.656765103340149
Running avgs for agent 1: q_loss: 22.413288116455078, p_loss: -5.618181228637695, mean_rew: -11.18177793499351, variance: 6.876629829406738, lamda: 1.6293931007385254
Running avgs for agent 2: q_loss: 42.765228271484375, p_loss: -5.626214981079102, mean_rew: -11.18833217420668, variance: 7.02498197555542, lamda: 1.6271610260009766

steps: 1349975, episodes: 54000, mean episode reward: -936.2516676350747, agent episode reward: [-312.08388921169154, -312.08388921169154, -312.08388921169154], time: 156.276
steps: 1349975, episodes: 54000, mean episode variance: 5.140494176149368, agent episode variance: [1.68067436170578, 1.725591193675995, 1.7342286207675934], time: 156.276
Running avgs for agent 0: q_loss: 30.534957885742188, p_loss: -5.663349628448486, mean_rew: -11.22054453626973, variance: 6.722697734832764, lamda: 1.6637409925460815
Running avgs for agent 1: q_loss: 23.346878051757812, p_loss: -5.653421878814697, mean_rew: -11.245389032024454, variance: 6.902364730834961, lamda: 1.636355996131897
Running avgs for agent 2: q_loss: 42.914955139160156, p_loss: -5.6543097496032715, mean_rew: -11.230313410141695, variance: 6.936914443969727, lamda: 1.629092812538147

steps: 1374975, episodes: 55000, mean episode reward: -912.4566068209572, agent episode reward: [-304.1522022736524, -304.1522022736524, -304.1522022736524], time: 158.688
steps: 1374975, episodes: 55000, mean episode variance: 5.1930367603302, agent episode variance: [1.7025060534477234, 1.7342985920906067, 1.75623211479187], time: 158.688
Running avgs for agent 0: q_loss: 29.618541717529297, p_loss: -5.702327728271484, mean_rew: -11.29013658236001, variance: 6.810024261474609, lamda: 1.6713920831680298
Running avgs for agent 1: q_loss: 23.822683334350586, p_loss: -5.662882328033447, mean_rew: -11.268671286395001, variance: 6.937194347381592, lamda: 1.6425034999847412
Running avgs for agent 2: q_loss: 41.13376235961914, p_loss: -5.6751909255981445, mean_rew: -11.27527073978332, variance: 7.024928569793701, lamda: 1.6318988800048828

steps: 1399975, episodes: 56000, mean episode reward: -900.755790055885, agent episode reward: [-300.2519300186283, -300.2519300186283, -300.2519300186283], time: 162.641
steps: 1399975, episodes: 56000, mean episode variance: 5.172279249429703, agent episode variance: [1.6994100661277771, 1.744942556142807, 1.7279266271591187], time: 162.641
Running avgs for agent 0: q_loss: 29.82452392578125, p_loss: -5.717474460601807, mean_rew: -11.333643132281304, variance: 6.797640800476074, lamda: 1.678322434425354
Running avgs for agent 1: q_loss: 33.01060104370117, p_loss: -5.696576118469238, mean_rew: -11.332400824135615, variance: 6.979770660400391, lamda: 1.6457668542861938
Running avgs for agent 2: q_loss: 31.217849731445312, p_loss: -5.711208343505859, mean_rew: -11.332308815558932, variance: 6.911706447601318, lamda: 1.6434646844863892

steps: 1424975, episodes: 57000, mean episode reward: -883.9978920747168, agent episode reward: [-294.6659640249056, -294.6659640249056, -294.6659640249056], time: 157.52
steps: 1424975, episodes: 57000, mean episode variance: 5.160393602609634, agent episode variance: [1.6812022140026093, 1.744184290409088, 1.735007098197937], time: 157.52
Running avgs for agent 0: q_loss: 31.761295318603516, p_loss: -5.73214054107666, mean_rew: -11.349315670690078, variance: 6.724808692932129, lamda: 1.682945966720581
Running avgs for agent 1: q_loss: 23.24357032775879, p_loss: -5.719810485839844, mean_rew: -11.38013406911679, variance: 6.9767374992370605, lamda: 1.6516330242156982
Running avgs for agent 2: q_loss: 26.646467208862305, p_loss: -5.739736557006836, mean_rew: -11.385429830089308, variance: 6.940028190612793, lamda: 1.654025673866272

steps: 1449975, episodes: 58000, mean episode reward: -899.5444131792129, agent episode reward: [-299.84813772640433, -299.84813772640433, -299.84813772640433], time: 161.432
steps: 1449975, episodes: 58000, mean episode variance: 5.143223462104797, agent episode variance: [1.6873240404129028, 1.7345420570373535, 1.721357364654541], time: 161.432
Running avgs for agent 0: q_loss: 30.29781723022461, p_loss: -5.733678817749023, mean_rew: -11.376761029827357, variance: 6.74929666519165, lamda: 1.6926594972610474
Running avgs for agent 1: q_loss: 23.502094268798828, p_loss: -5.715059280395508, mean_rew: -11.379742406071058, variance: 6.938168048858643, lamda: 1.6572775840759277
Running avgs for agent 2: q_loss: 29.239124298095703, p_loss: -5.7375712394714355, mean_rew: -11.380482905583314, variance: 6.885429859161377, lamda: 1.6627140045166016

steps: 1474975, episodes: 59000, mean episode reward: -913.1433240677801, agent episode reward: [-304.38110802259337, -304.38110802259337, -304.38110802259337], time: 162.279
steps: 1474975, episodes: 59000, mean episode variance: 5.151760905027389, agent episode variance: [1.7040183997154237, 1.738514657020569, 1.7092278482913972], time: 162.279
Running avgs for agent 0: q_loss: 32.66314697265625, p_loss: -5.762332916259766, mean_rew: -11.445683955624753, variance: 6.816074371337891, lamda: 1.701353907585144
Running avgs for agent 1: q_loss: 27.657503128051758, p_loss: -5.735692024230957, mean_rew: -11.42369452163965, variance: 6.954058647155762, lamda: 1.667356014251709
Running avgs for agent 2: q_loss: 34.596527099609375, p_loss: -5.758375644683838, mean_rew: -11.420698482591524, variance: 6.836911201477051, lamda: 1.6757566928863525

steps: 1499975, episodes: 60000, mean episode reward: -882.3431209032058, agent episode reward: [-294.11437363440194, -294.11437363440194, -294.11437363440194], time: 157.391
steps: 1499975, episodes: 60000, mean episode variance: 5.128578336715698, agent episode variance: [1.676845026254654, 1.7320184333324433, 1.719714877128601], time: 157.392
Running avgs for agent 0: q_loss: 33.523921966552734, p_loss: -5.763922691345215, mean_rew: -11.438975711222918, variance: 6.707380294799805, lamda: 1.7153271436691284
Running avgs for agent 1: q_loss: 39.305843353271484, p_loss: -5.754526138305664, mean_rew: -11.45164304516325, variance: 6.928073883056641, lamda: 1.6706600189208984
Running avgs for agent 2: q_loss: 37.65715789794922, p_loss: -5.775910377502441, mean_rew: -11.462766229761288, variance: 6.878859519958496, lamda: 1.688711166381836

steps: 1524975, episodes: 61000, mean episode reward: -919.768085256394, agent episode reward: [-306.58936175213137, -306.58936175213137, -306.58936175213137], time: 158.106
steps: 1524975, episodes: 61000, mean episode variance: 5.128978610515595, agent episode variance: [1.6750985746383666, 1.7514991567134857, 1.702380879163742], time: 158.107
Running avgs for agent 0: q_loss: 29.663089752197266, p_loss: -5.794562816619873, mean_rew: -11.493899892520862, variance: 6.700394630432129, lamda: 1.7216362953186035
Running avgs for agent 1: q_loss: 39.61614990234375, p_loss: -5.776467800140381, mean_rew: -11.512919249154635, variance: 7.0059967041015625, lamda: 1.6712446212768555
Running avgs for agent 2: q_loss: 43.481422424316406, p_loss: -5.772195339202881, mean_rew: -11.474460102774145, variance: 6.809523582458496, lamda: 1.6930493116378784

steps: 1549975, episodes: 62000, mean episode reward: -909.5107054901218, agent episode reward: [-303.170235163374, -303.170235163374, -303.170235163374], time: 159.355
steps: 1549975, episodes: 62000, mean episode variance: 5.172965086936951, agent episode variance: [1.703768697977066, 1.7480486080646516, 1.7211477808952331], time: 159.356
Running avgs for agent 0: q_loss: 45.10344696044922, p_loss: -5.815835952758789, mean_rew: -11.554264292443214, variance: 6.815075397491455, lamda: 1.727057695388794
Running avgs for agent 1: q_loss: 26.396760940551758, p_loss: -5.793420791625977, mean_rew: -11.538446882459525, variance: 6.992194175720215, lamda: 1.6743271350860596
Running avgs for agent 2: q_loss: 44.667877197265625, p_loss: -5.808884143829346, mean_rew: -11.538096097948575, variance: 6.8845906257629395, lamda: 1.693990707397461

steps: 1574975, episodes: 63000, mean episode reward: -922.3263492781213, agent episode reward: [-307.4421164260404, -307.4421164260404, -307.4421164260404], time: 156.281
steps: 1574975, episodes: 63000, mean episode variance: 5.16714600944519, agent episode variance: [1.7022918543815613, 1.743242332458496, 1.7216118226051331], time: 156.282
Running avgs for agent 0: q_loss: 53.281455993652344, p_loss: -5.822993755340576, mean_rew: -11.574477483533496, variance: 6.809167861938477, lamda: 1.7297230958938599
Running avgs for agent 1: q_loss: 23.647172927856445, p_loss: -5.819522380828857, mean_rew: -11.580036318641685, variance: 6.9729695320129395, lamda: 1.6792197227478027
Running avgs for agent 2: q_loss: 39.02770233154297, p_loss: -5.822865009307861, mean_rew: -11.555193706232835, variance: 6.886447429656982, lamda: 1.6987907886505127

steps: 1599975, episodes: 64000, mean episode reward: -940.1896179319241, agent episode reward: [-313.39653931064134, -313.39653931064134, -313.39653931064134], time: 157.898
steps: 1599975, episodes: 64000, mean episode variance: 5.128092122554779, agent episode variance: [1.688876673221588, 1.7352941150665284, 1.7039213342666626], time: 157.899
Running avgs for agent 0: q_loss: 54.18653106689453, p_loss: -5.843548774719238, mean_rew: -11.621263288233898, variance: 6.755506992340088, lamda: 1.7306098937988281
Running avgs for agent 1: q_loss: 23.794189453125, p_loss: -5.828824043273926, mean_rew: -11.60624401793697, variance: 6.941176414489746, lamda: 1.6842992305755615
Running avgs for agent 2: q_loss: 30.396535873413086, p_loss: -5.846026420593262, mean_rew: -11.598241352913403, variance: 6.815685749053955, lamda: 1.7127341032028198

steps: 1624975, episodes: 65000, mean episode reward: -922.5485162586825, agent episode reward: [-307.5161720862275, -307.5161720862275, -307.5161720862275], time: 156.441
steps: 1624975, episodes: 65000, mean episode variance: 5.16355140542984, agent episode variance: [1.704206051826477, 1.7499559750556946, 1.7093893785476684], time: 156.442
Running avgs for agent 0: q_loss: 54.38193893432617, p_loss: -5.860126972198486, mean_rew: -11.660042524160197, variance: 6.816824436187744, lamda: 1.7321476936340332
Running avgs for agent 1: q_loss: 26.40437126159668, p_loss: -5.849710464477539, mean_rew: -11.656534749733034, variance: 6.999824047088623, lamda: 1.6931995153427124
Running avgs for agent 2: q_loss: 46.038917541503906, p_loss: -5.874382972717285, mean_rew: -11.663976766224001, variance: 6.837557792663574, lamda: 1.7183077335357666

steps: 1649975, episodes: 66000, mean episode reward: -946.2915771645962, agent episode reward: [-315.43052572153204, -315.43052572153204, -315.43052572153204], time: 156.451
steps: 1649975, episodes: 66000, mean episode variance: 5.136174622535705, agent episode variance: [1.69049658203125, 1.7331628575325013, 1.7125151829719543], time: 156.451
Running avgs for agent 0: q_loss: 54.077911376953125, p_loss: -5.903621196746826, mean_rew: -11.720221058659622, variance: 6.761986255645752, lamda: 1.734117865562439
Running avgs for agent 1: q_loss: 31.296627044677734, p_loss: -5.8739166259765625, mean_rew: -11.706980571627486, variance: 6.932651519775391, lamda: 1.7029176950454712
Running avgs for agent 2: q_loss: 46.46168518066406, p_loss: -5.891391754150391, mean_rew: -11.703952594547733, variance: 6.85006046295166, lamda: 1.7202731370925903

steps: 1674975, episodes: 67000, mean episode reward: -946.6275837775314, agent episode reward: [-315.54252792584384, -315.54252792584384, -315.54252792584384], time: 156.217
steps: 1674975, episodes: 67000, mean episode variance: 5.1562864992618564, agent episode variance: [1.7048098075389861, 1.7472964494228362, 1.7041802423000336], time: 156.217
Running avgs for agent 0: q_loss: 54.196083068847656, p_loss: -5.911795616149902, mean_rew: -11.748447549407159, variance: 6.819239139556885, lamda: 1.7359265089035034
Running avgs for agent 1: q_loss: 42.63071060180664, p_loss: -5.893446445465088, mean_rew: -11.761275229421235, variance: 6.989185333251953, lamda: 1.706700325012207
Running avgs for agent 2: q_loss: 46.34309387207031, p_loss: -5.906274795532227, mean_rew: -11.727595193453023, variance: 6.816720485687256, lamda: 1.7219176292419434

steps: 1699975, episodes: 68000, mean episode reward: -931.7284706632279, agent episode reward: [-310.57615688774257, -310.57615688774257, -310.57615688774257], time: 158.328
steps: 1699975, episodes: 68000, mean episode variance: 5.179023901224136, agent episode variance: [1.7019452562332154, 1.752459342956543, 1.724619302034378], time: 158.328
Running avgs for agent 0: q_loss: 44.21552658081055, p_loss: -5.911510944366455, mean_rew: -11.761643145965861, variance: 6.807781219482422, lamda: 1.7378047704696655
Running avgs for agent 1: q_loss: 43.50669479370117, p_loss: -5.913622856140137, mean_rew: -11.79189776825847, variance: 7.0098371505737305, lamda: 1.7076102495193481
Running avgs for agent 2: q_loss: 46.75409698486328, p_loss: -5.919405937194824, mean_rew: -11.768296858182602, variance: 6.898477077484131, lamda: 1.7238655090332031

steps: 1724975, episodes: 69000, mean episode reward: -931.6801026866872, agent episode reward: [-310.5600342288957, -310.5600342288957, -310.5600342288957], time: 159.086
steps: 1724975, episodes: 69000, mean episode variance: 5.164054212808609, agent episode variance: [1.70452720785141, 1.7358194189071656, 1.7237075860500335], time: 159.086
Running avgs for agent 0: q_loss: 31.394983291625977, p_loss: -5.948487281799316, mean_rew: -11.822571935751078, variance: 6.818108558654785, lamda: 1.7420971393585205
Running avgs for agent 1: q_loss: 43.49580764770508, p_loss: -5.916906356811523, mean_rew: -11.799813526011523, variance: 6.943277835845947, lamda: 1.7079716920852661
Running avgs for agent 2: q_loss: 47.23084259033203, p_loss: -5.938112258911133, mean_rew: -11.807364431035563, variance: 6.894830703735352, lamda: 1.7272120714187622

steps: 1749975, episodes: 70000, mean episode reward: -909.0508913921639, agent episode reward: [-303.016963797388, -303.016963797388, -303.016963797388], time: 157.95
steps: 1749975, episodes: 70000, mean episode variance: 5.179323601722717, agent episode variance: [1.7029157261848449, 1.7505132818222047, 1.7258945937156678], time: 157.951
Running avgs for agent 0: q_loss: 36.27530288696289, p_loss: -5.94965934753418, mean_rew: -11.855560296200132, variance: 6.8116631507873535, lamda: 1.7524921894073486
Running avgs for agent 1: q_loss: 43.134681701660156, p_loss: -5.947755813598633, mean_rew: -11.862752557130293, variance: 7.002053260803223, lamda: 1.7088298797607422
Running avgs for agent 2: q_loss: 47.3557243347168, p_loss: -5.952993392944336, mean_rew: -11.853057169614097, variance: 6.903578281402588, lamda: 1.7288618087768555

steps: 1774975, episodes: 71000, mean episode reward: -889.4943391671167, agent episode reward: [-296.4981130557055, -296.4981130557055, -296.4981130557055], time: 159.009
steps: 1774975, episodes: 71000, mean episode variance: 5.174576721191406, agent episode variance: [1.6871373479366303, 1.756865665435791, 1.730573707818985], time: 159.009
Running avgs for agent 0: q_loss: 45.058712005615234, p_loss: -5.946950912475586, mean_rew: -11.886446992790418, variance: 6.748549938201904, lamda: 1.765371322631836
Running avgs for agent 1: q_loss: 35.21702575683594, p_loss: -5.9508233070373535, mean_rew: -11.874685105724478, variance: 7.027462959289551, lamda: 1.7131751775741577
Running avgs for agent 2: q_loss: 47.03315353393555, p_loss: -5.971004009246826, mean_rew: -11.871317931875284, variance: 6.922295093536377, lamda: 1.7309731245040894

steps: 1799975, episodes: 72000, mean episode reward: -907.5942158983871, agent episode reward: [-302.5314052994624, -302.5314052994624, -302.5314052994624], time: 158.996
steps: 1799975, episodes: 72000, mean episode variance: 5.176928627252579, agent episode variance: [1.7108766202926635, 1.7286030497550964, 1.7374489572048186], time: 158.997
Running avgs for agent 0: q_loss: 58.68661117553711, p_loss: -5.983743667602539, mean_rew: -11.917488623986209, variance: 6.843505859375, lamda: 1.7668471336364746
Running avgs for agent 1: q_loss: 27.83156394958496, p_loss: -5.968962669372559, mean_rew: -11.904629691370957, variance: 6.914412021636963, lamda: 1.7249614000320435
Running avgs for agent 2: q_loss: 48.04957962036133, p_loss: -5.988723278045654, mean_rew: -11.927642093807787, variance: 6.949795722961426, lamda: 1.732885718345642

steps: 1824975, episodes: 73000, mean episode reward: -936.1152058100752, agent episode reward: [-312.03840193669174, -312.03840193669174, -312.03840193669174], time: 159.404
steps: 1824975, episodes: 73000, mean episode variance: 5.201422983884812, agent episode variance: [1.7101782920360564, 1.7396742372512817, 1.751570454597473], time: 159.404
Running avgs for agent 0: q_loss: 58.605857849121094, p_loss: -6.014837741851807, mean_rew: -11.952165250120755, variance: 6.840713024139404, lamda: 1.7677009105682373
Running avgs for agent 1: q_loss: 27.52328109741211, p_loss: -5.984367370605469, mean_rew: -11.941841320248175, variance: 6.9586968421936035, lamda: 1.7333288192749023
Running avgs for agent 2: q_loss: 48.96459197998047, p_loss: -6.010441303253174, mean_rew: -11.958872700602242, variance: 7.006281852722168, lamda: 1.7357333898544312

steps: 1849975, episodes: 74000, mean episode reward: -921.6332533569182, agent episode reward: [-307.211084452306, -307.211084452306, -307.211084452306], time: 158.517
steps: 1849975, episodes: 74000, mean episode variance: 5.19144911980629, agent episode variance: [1.7086869087219239, 1.7438120937347412, 1.7389501173496247], time: 158.518
Running avgs for agent 0: q_loss: 57.62959671020508, p_loss: -6.019659996032715, mean_rew: -11.978140762683907, variance: 6.834748268127441, lamda: 1.7681798934936523
Running avgs for agent 1: q_loss: 25.67330551147461, p_loss: -6.004184722900391, mean_rew: -11.981044149198874, variance: 6.975247859954834, lamda: 1.739162564277649
Running avgs for agent 2: q_loss: 37.04872512817383, p_loss: -6.028183937072754, mean_rew: -11.995183880009266, variance: 6.955801010131836, lamda: 1.7471339702606201

steps: 1874975, episodes: 75000, mean episode reward: -917.3169500446058, agent episode reward: [-305.7723166815353, -305.7723166815353, -305.7723166815353], time: 157.197
steps: 1874975, episodes: 75000, mean episode variance: 5.166990038394928, agent episode variance: [1.7202826976776122, 1.7344413075447083, 1.7122660331726074], time: 157.198
Running avgs for agent 0: q_loss: 57.900272369384766, p_loss: -6.028817176818848, mean_rew: -12.005493312032613, variance: 6.881130695343018, lamda: 1.7688297033309937
Running avgs for agent 1: q_loss: 26.881227493286133, p_loss: -6.025777816772461, mean_rew: -12.011976231585516, variance: 6.937765598297119, lamda: 1.744816541671753
Running avgs for agent 2: q_loss: 40.774078369140625, p_loss: -6.04844856262207, mean_rew: -12.013764974255531, variance: 6.849063873291016, lamda: 1.7602145671844482

steps: 1899975, episodes: 76000, mean episode reward: -928.192102086741, agent episode reward: [-309.397367362247, -309.397367362247, -309.397367362247], time: 162.699
steps: 1899975, episodes: 76000, mean episode variance: 5.1751091516017915, agent episode variance: [1.7117554659843446, 1.735243953704834, 1.7281097319126129], time: 162.699
Running avgs for agent 0: q_loss: 34.84375762939453, p_loss: -6.045286178588867, mean_rew: -12.025448832061091, variance: 6.847021579742432, lamda: 1.7739421129226685
Running avgs for agent 1: q_loss: 26.016040802001953, p_loss: -6.0352091789245605, mean_rew: -12.039264907091724, variance: 6.940975666046143, lamda: 1.7494056224822998
Running avgs for agent 2: q_loss: 47.88917541503906, p_loss: -6.041594505310059, mean_rew: -12.03531898160746, variance: 6.912438869476318, lamda: 1.7640310525894165

steps: 1924975, episodes: 77000, mean episode reward: -929.6463669333091, agent episode reward: [-309.88212231110305, -309.88212231110305, -309.88212231110305], time: 161.244
steps: 1924975, episodes: 77000, mean episode variance: 5.158551990270615, agent episode variance: [1.7045653111934662, 1.733131446838379, 1.7208552322387696], time: 161.245
Running avgs for agent 0: q_loss: 42.257911682128906, p_loss: -6.063648223876953, mean_rew: -12.057647444287715, variance: 6.818261623382568, lamda: 1.7794642448425293
Running avgs for agent 1: q_loss: 25.853195190429688, p_loss: -6.052077293395996, mean_rew: -12.067605059632605, variance: 6.932525157928467, lamda: 1.756213665008545
Running avgs for agent 2: q_loss: 46.67800521850586, p_loss: -6.064251899719238, mean_rew: -12.088187134026867, variance: 6.883420944213867, lamda: 1.7648855447769165

steps: 1949975, episodes: 78000, mean episode reward: -908.4090153224662, agent episode reward: [-302.80300510748873, -302.80300510748873, -302.80300510748873], time: 156.444
steps: 1949975, episodes: 78000, mean episode variance: 5.1755136482715605, agent episode variance: [1.7116725862026214, 1.7386817293167114, 1.7251593327522279], time: 156.444
Running avgs for agent 0: q_loss: 59.18686294555664, p_loss: -6.088109970092773, mean_rew: -12.122109181592958, variance: 6.846690654754639, lamda: 1.782408356666565
Running avgs for agent 1: q_loss: 25.640789031982422, p_loss: -6.060745716094971, mean_rew: -12.094317296923299, variance: 6.9547271728515625, lamda: 1.7609636783599854
Running avgs for agent 2: q_loss: 30.87196159362793, p_loss: -6.075959205627441, mean_rew: -12.08896857004664, variance: 6.900637626647949, lamda: 1.7700579166412354

steps: 1974975, episodes: 79000, mean episode reward: -928.4760006098127, agent episode reward: [-309.4920002032709, -309.4920002032709, -309.4920002032709], time: 157.505
steps: 1974975, episodes: 79000, mean episode variance: 5.167501113891602, agent episode variance: [1.7203002305030823, 1.7345145673751832, 1.712686316013336], time: 157.506
Running avgs for agent 0: q_loss: 58.369754791259766, p_loss: -6.091940402984619, mean_rew: -12.120943646241226, variance: 6.881200790405273, lamda: 1.7835612297058105
Running avgs for agent 1: q_loss: 26.21685791015625, p_loss: -6.076797962188721, mean_rew: -12.128323773285912, variance: 6.938057899475098, lamda: 1.7636210918426514
Running avgs for agent 2: q_loss: 33.41645050048828, p_loss: -6.090931415557861, mean_rew: -12.12499771057527, variance: 6.85074520111084, lamda: 1.780698537826538

steps: 1999975, episodes: 80000, mean episode reward: -911.5360440335547, agent episode reward: [-303.84534801118497, -303.84534801118497, -303.84534801118497], time: 167.647
steps: 1999975, episodes: 80000, mean episode variance: 5.142826759576797, agent episode variance: [1.7132483551502227, 1.7253335537910461, 1.7042448506355286], time: 167.648
Running avgs for agent 0: q_loss: 58.28657913208008, p_loss: -6.103109836578369, mean_rew: -12.136375044951887, variance: 6.852993011474609, lamda: 1.7840908765792847
Running avgs for agent 1: q_loss: 26.13640022277832, p_loss: -6.078658103942871, mean_rew: -12.12645584079666, variance: 6.901333808898926, lamda: 1.7684309482574463
Running avgs for agent 2: q_loss: 40.061519622802734, p_loss: -6.105679988861084, mean_rew: -12.148534401073023, variance: 6.81697940826416, lamda: 1.7914948463439941

steps: 2024975, episodes: 81000, mean episode reward: -906.0063531856161, agent episode reward: [-302.0021177285388, -302.0021177285388, -302.0021177285388], time: 165.505
steps: 2024975, episodes: 81000, mean episode variance: 5.1550099747180935, agent episode variance: [1.721489364862442, 1.727057915210724, 1.706462694644928], time: 165.505
Running avgs for agent 0: q_loss: 58.08944320678711, p_loss: -6.113175868988037, mean_rew: -12.171044409136815, variance: 6.885957717895508, lamda: 1.784288763999939
Running avgs for agent 1: q_loss: 28.091293334960938, p_loss: -6.103402137756348, mean_rew: -12.17473643990819, variance: 6.908231258392334, lamda: 1.7741988897323608
Running avgs for agent 2: q_loss: 30.986032485961914, p_loss: -6.117983818054199, mean_rew: -12.184698821385668, variance: 6.825850486755371, lamda: 1.7995227575302124

steps: 2049975, episodes: 82000, mean episode reward: -915.6296745895511, agent episode reward: [-305.2098915298503, -305.2098915298503, -305.2098915298503], time: 166.239
steps: 2049975, episodes: 82000, mean episode variance: 5.131510305643082, agent episode variance: [1.7073740065097809, 1.724229735851288, 1.699906563282013], time: 166.239
Running avgs for agent 0: q_loss: 58.82463455200195, p_loss: -6.131612777709961, mean_rew: -12.196554134494258, variance: 6.829495906829834, lamda: 1.7850902080535889
Running avgs for agent 1: q_loss: 27.087440490722656, p_loss: -6.118380069732666, mean_rew: -12.192784641525625, variance: 6.896918296813965, lamda: 1.781207799911499
Running avgs for agent 2: q_loss: 32.266456604003906, p_loss: -6.127677917480469, mean_rew: -12.203360801574108, variance: 6.799625873565674, lamda: 1.8079482316970825

steps: 2074975, episodes: 83000, mean episode reward: -903.3779199832214, agent episode reward: [-301.12597332774044, -301.12597332774044, -301.12597332774044], time: 168.289
steps: 2074975, episodes: 83000, mean episode variance: 5.130365471124649, agent episode variance: [1.7257028896808624, 1.7159739532470704, 1.6886886281967164], time: 168.29
Running avgs for agent 0: q_loss: 56.00008773803711, p_loss: -6.131178855895996, mean_rew: -12.191078003477074, variance: 6.902811527252197, lamda: 1.7874151468276978
Running avgs for agent 1: q_loss: 29.275829315185547, p_loss: -6.123483180999756, mean_rew: -12.210827675722797, variance: 6.863895893096924, lamda: 1.787329077720642
Running avgs for agent 2: q_loss: 30.737104415893555, p_loss: -6.1247172355651855, mean_rew: -12.201704087292445, variance: 6.754755020141602, lamda: 1.8185511827468872

steps: 2099975, episodes: 84000, mean episode reward: -898.7961220994695, agent episode reward: [-299.5987073664898, -299.5987073664898, -299.5987073664898], time: 168.123
steps: 2099975, episodes: 84000, mean episode variance: 5.095616284847259, agent episode variance: [1.6982542390823365, 1.7149726853370666, 1.6823893604278564], time: 168.123
Running avgs for agent 0: q_loss: 53.565006256103516, p_loss: -6.113074779510498, mean_rew: -12.177296307983584, variance: 6.7930169105529785, lamda: 1.792665719985962
Running avgs for agent 1: q_loss: 27.391653060913086, p_loss: -6.120090007781982, mean_rew: -12.201715221029016, variance: 6.859890460968018, lamda: 1.794504165649414
Running avgs for agent 2: q_loss: 30.881086349487305, p_loss: -6.120941638946533, mean_rew: -12.20267524120175, variance: 6.729557514190674, lamda: 1.8256721496582031

steps: 2124975, episodes: 85000, mean episode reward: -910.7095992825762, agent episode reward: [-303.5698664275254, -303.5698664275254, -303.5698664275254], time: 165.705
steps: 2124975, episodes: 85000, mean episode variance: 5.082711672067642, agent episode variance: [1.7072007558345794, 1.7072045965194702, 1.6683063197135926], time: 165.705
Running avgs for agent 0: q_loss: 59.936256408691406, p_loss: -6.136080265045166, mean_rew: -12.219548316210863, variance: 6.828802585601807, lamda: 1.7939448356628418
Running avgs for agent 1: q_loss: 28.24506950378418, p_loss: -6.125432014465332, mean_rew: -12.214964279726416, variance: 6.828818321228027, lamda: 1.8004461526870728
Running avgs for agent 2: q_loss: 41.12139892578125, p_loss: -6.121790409088135, mean_rew: -12.195435657509321, variance: 6.673225402832031, lamda: 1.833524465560913

steps: 2149975, episodes: 86000, mean episode reward: -918.235067870517, agent episode reward: [-306.07835595683906, -306.07835595683906, -306.07835595683906], time: 165.771
steps: 2149975, episodes: 86000, mean episode variance: 5.095182740449905, agent episode variance: [1.7124398972988129, 1.703113380432129, 1.6796294627189636], time: 165.771
Running avgs for agent 0: q_loss: 59.531883239746094, p_loss: -6.129999160766602, mean_rew: -12.205271672345102, variance: 6.849759578704834, lamda: 1.7951692342758179
Running avgs for agent 1: q_loss: 27.424617767333984, p_loss: -6.113229751586914, mean_rew: -12.208126449507857, variance: 6.812453746795654, lamda: 1.8074296712875366
Running avgs for agent 2: q_loss: 51.9237174987793, p_loss: -6.123260021209717, mean_rew: -12.211062278794186, variance: 6.718518257141113, lamda: 1.8375412225723267

steps: 2174975, episodes: 87000, mean episode reward: -926.0199863152045, agent episode reward: [-308.6733287717348, -308.6733287717348, -308.6733287717348], time: 165.798
steps: 2174975, episodes: 87000, mean episode variance: 5.059732465267182, agent episode variance: [1.7093257780075073, 1.6928793544769287, 1.6575273327827453], time: 165.799
Running avgs for agent 0: q_loss: 59.6661376953125, p_loss: -6.148213863372803, mean_rew: -12.225704804156209, variance: 6.837303161621094, lamda: 1.798101782798767
Running avgs for agent 1: q_loss: 26.865934371948242, p_loss: -6.129133224487305, mean_rew: -12.220461589150466, variance: 6.771517753601074, lamda: 1.8120520114898682
Running avgs for agent 2: q_loss: 51.39505386352539, p_loss: -6.125692367553711, mean_rew: -12.209058271971914, variance: 6.6301093101501465, lamda: 1.8392170667648315

steps: 2199975, episodes: 88000, mean episode reward: -918.5765414502677, agent episode reward: [-306.1921804834225, -306.1921804834225, -306.1921804834225], time: 167.162
steps: 2199975, episodes: 88000, mean episode variance: 5.0715310213565825, agent episode variance: [1.6956870949268341, 1.6940032458305359, 1.6818406805992125], time: 167.163
Running avgs for agent 0: q_loss: 58.659523010253906, p_loss: -6.149529457092285, mean_rew: -12.220410109473725, variance: 6.782748699188232, lamda: 1.7997201681137085
Running avgs for agent 1: q_loss: 26.241668701171875, p_loss: -6.123687267303467, mean_rew: -12.220218494283788, variance: 6.776012897491455, lamda: 1.81400728225708
Running avgs for agent 2: q_loss: 52.04014587402344, p_loss: -6.12428092956543, mean_rew: -12.216159616985315, variance: 6.727362155914307, lamda: 1.8412445783615112

steps: 2224975, episodes: 89000, mean episode reward: -940.9465409942812, agent episode reward: [-313.64884699809375, -313.64884699809375, -313.64884699809375], time: 164.789
steps: 2224975, episodes: 89000, mean episode variance: 5.046887243509293, agent episode variance: [1.7006181981563568, 1.6974607610702515, 1.6488082842826843], time: 164.789
Running avgs for agent 0: q_loss: 59.620967864990234, p_loss: -6.14611291885376, mean_rew: -12.228798711202609, variance: 6.8024725914001465, lamda: 1.80089271068573
Running avgs for agent 1: q_loss: 28.525325775146484, p_loss: -6.130616188049316, mean_rew: -12.230022707055884, variance: 6.78984260559082, lamda: 1.818367838859558
Running avgs for agent 2: q_loss: 51.67158508300781, p_loss: -6.139707565307617, mean_rew: -12.240761523991647, variance: 6.59523344039917, lamda: 1.8432387113571167

steps: 2249975, episodes: 90000, mean episode reward: -936.1953870040809, agent episode reward: [-312.0651290013603, -312.0651290013603, -312.0651290013603], time: 164.722
steps: 2249975, episodes: 90000, mean episode variance: 5.074424889326096, agent episode variance: [1.715673395872116, 1.6893289403915406, 1.669422553062439], time: 164.722
Running avgs for agent 0: q_loss: 58.526309967041016, p_loss: -6.15014123916626, mean_rew: -12.240988006422478, variance: 6.8626933097839355, lamda: 1.801669716835022
Running avgs for agent 1: q_loss: 26.890718460083008, p_loss: -6.13437032699585, mean_rew: -12.235993942427688, variance: 6.7573161125183105, lamda: 1.824294924736023
Running avgs for agent 2: q_loss: 52.0206184387207, p_loss: -6.1343302726745605, mean_rew: -12.240060523641333, variance: 6.677690505981445, lamda: 1.8448808193206787

steps: 2274975, episodes: 91000, mean episode reward: -949.1719794129389, agent episode reward: [-316.390659804313, -316.390659804313, -316.390659804313], time: 163.761
steps: 2274975, episodes: 91000, mean episode variance: 5.0532168135643, agent episode variance: [1.7069350290298462, 1.6838257808685302, 1.6624560036659242], time: 163.761
Running avgs for agent 0: q_loss: 59.33393859863281, p_loss: -6.163597583770752, mean_rew: -12.261664770762229, variance: 6.82774019241333, lamda: 1.8030630350112915
Running avgs for agent 1: q_loss: 39.45654296875, p_loss: -6.13100004196167, mean_rew: -12.244392138721635, variance: 6.7353034019470215, lamda: 1.831518530845642
Running avgs for agent 2: q_loss: 49.80168914794922, p_loss: -6.133690357208252, mean_rew: -12.234259393053495, variance: 6.649824142456055, lamda: 1.8498066663742065

steps: 2299975, episodes: 92000, mean episode reward: -949.9092899951842, agent episode reward: [-316.6364299983947, -316.6364299983947, -316.6364299983947], time: 166.165
steps: 2299975, episodes: 92000, mean episode variance: 5.049365216732025, agent episode variance: [1.714877795934677, 1.6689995589256286, 1.6654878618717193], time: 166.166
Running avgs for agent 0: q_loss: 59.213172912597656, p_loss: -6.162257671356201, mean_rew: -12.263155784249529, variance: 6.859511375427246, lamda: 1.8043506145477295
Running avgs for agent 1: q_loss: 27.107746124267578, p_loss: -6.147221565246582, mean_rew: -12.26284877190816, variance: 6.675998687744141, lamda: 1.8364641666412354
Running avgs for agent 2: q_loss: 51.52495193481445, p_loss: -6.147731304168701, mean_rew: -12.26482086400315, variance: 6.661951065063477, lamda: 1.853368878364563

steps: 2324975, episodes: 93000, mean episode reward: -947.7276973063391, agent episode reward: [-315.9092324354464, -315.9092324354464, -315.9092324354464], time: 166.532
steps: 2324975, episodes: 93000, mean episode variance: 5.045440859794617, agent episode variance: [1.6988200175762176, 1.675639081954956, 1.670981760263443], time: 166.533
Running avgs for agent 0: q_loss: 59.5854377746582, p_loss: -6.182532787322998, mean_rew: -12.282477879134946, variance: 6.795280456542969, lamda: 1.8059523105621338
Running avgs for agent 1: q_loss: 28.701427459716797, p_loss: -6.138257026672363, mean_rew: -12.260966727160683, variance: 6.702556610107422, lamda: 1.8433418273925781
Running avgs for agent 2: q_loss: 45.525421142578125, p_loss: -6.162380218505859, mean_rew: -12.288230739566508, variance: 6.683927536010742, lamda: 1.8565635681152344

steps: 2349975, episodes: 94000, mean episode reward: -933.4024853597948, agent episode reward: [-311.13416178659827, -311.13416178659827, -311.13416178659827], time: 166.639
steps: 2349975, episodes: 94000, mean episode variance: 5.043760954141617, agent episode variance: [1.7083734524250032, 1.67178178358078, 1.6636057181358337], time: 166.639
Running avgs for agent 0: q_loss: 59.643898010253906, p_loss: -6.179409027099609, mean_rew: -12.293069582877964, variance: 6.833493709564209, lamda: 1.80808424949646
Running avgs for agent 1: q_loss: 27.122976303100586, p_loss: -6.155519008636475, mean_rew: -12.286612291243625, variance: 6.687127113342285, lamda: 1.8487434387207031
Running avgs for agent 2: q_loss: 40.38511657714844, p_loss: -6.160055637359619, mean_rew: -12.273619206724721, variance: 6.654422760009766, lamda: 1.866798758506775

steps: 2374975, episodes: 95000, mean episode reward: -914.7826907374509, agent episode reward: [-304.9275635791504, -304.9275635791504, -304.9275635791504], time: 162.783
steps: 2374975, episodes: 95000, mean episode variance: 5.026900661230087, agent episode variance: [1.7089846227169037, 1.6690096230506897, 1.648906415462494], time: 162.783
Running avgs for agent 0: q_loss: 59.847938537597656, p_loss: -6.168496131896973, mean_rew: -12.272064207395834, variance: 6.835938453674316, lamda: 1.8099526166915894
Running avgs for agent 1: q_loss: 27.816829681396484, p_loss: -6.157157897949219, mean_rew: -12.281339532901692, variance: 6.67603874206543, lamda: 1.852579951286316
Running avgs for agent 2: q_loss: 33.70515060424805, p_loss: -6.15935754776001, mean_rew: -12.283469957846151, variance: 6.595625400543213, lamda: 1.8787161111831665

steps: 2399975, episodes: 96000, mean episode reward: -901.2647108229706, agent episode reward: [-300.42157027432347, -300.42157027432347, -300.42157027432347], time: 160.622
steps: 2399975, episodes: 96000, mean episode variance: 5.011785687685013, agent episode variance: [1.703759917974472, 1.6649462666511536, 1.6430795030593872], time: 160.623
Running avgs for agent 0: q_loss: 58.59431076049805, p_loss: -6.177346229553223, mean_rew: -12.281359613106815, variance: 6.81503963470459, lamda: 1.8115234375
Running avgs for agent 1: q_loss: 27.72454261779785, p_loss: -6.159825801849365, mean_rew: -12.297021054841633, variance: 6.659785270690918, lamda: 1.858114242553711
Running avgs for agent 2: q_loss: 48.920108795166016, p_loss: -6.1688151359558105, mean_rew: -12.28295366418323, variance: 6.572318077087402, lamda: 1.8883610963821411

steps: 2424975, episodes: 97000, mean episode reward: -927.1478528707985, agent episode reward: [-309.0492842902661, -309.0492842902661, -309.0492842902661], time: 157.527
steps: 2424975, episodes: 97000, mean episode variance: 4.996321782588959, agent episode variance: [1.7100516567230224, 1.6543675384521483, 1.6319025874137878], time: 157.528
Running avgs for agent 0: q_loss: 58.95757293701172, p_loss: -6.1827921867370605, mean_rew: -12.290256907255475, variance: 6.840206146240234, lamda: 1.8122212886810303
Running avgs for agent 1: q_loss: 26.240703582763672, p_loss: -6.150504112243652, mean_rew: -12.272873322658933, variance: 6.6174702644348145, lamda: 1.859863519668579
Running avgs for agent 2: q_loss: 35.10716247558594, p_loss: -6.168623447418213, mean_rew: -12.290388121160191, variance: 6.5276103019714355, lamda: 1.897279977798462

steps: 2449975, episodes: 98000, mean episode reward: -938.5853778485337, agent episode reward: [-312.8617926161779, -312.8617926161779, -312.8617926161779], time: 157.988
steps: 2449975, episodes: 98000, mean episode variance: 5.001113882780075, agent episode variance: [1.7051057817935944, 1.667581358909607, 1.6284267420768739], time: 157.988
Running avgs for agent 0: q_loss: 59.69229507446289, p_loss: -6.186299800872803, mean_rew: -12.308782872684802, variance: 6.820422649383545, lamda: 1.81325364112854
Running avgs for agent 1: q_loss: 28.687644958496094, p_loss: -6.172903060913086, mean_rew: -12.328106740880923, variance: 6.67032527923584, lamda: 1.863468050956726
Running avgs for agent 2: q_loss: 33.558616638183594, p_loss: -6.166581153869629, mean_rew: -12.291877831095393, variance: 6.513707160949707, lamda: 1.9086331129074097

steps: 2474975, episodes: 99000, mean episode reward: -938.8462544049794, agent episode reward: [-312.94875146832646, -312.94875146832646, -312.94875146832646], time: 159.049
steps: 2474975, episodes: 99000, mean episode variance: 4.987148259401321, agent episode variance: [1.7062866899967193, 1.6552360768318177, 1.6256254925727844], time: 159.049
Running avgs for agent 0: q_loss: 49.09035110473633, p_loss: -6.19936990737915, mean_rew: -12.31799935503632, variance: 6.825146675109863, lamda: 1.8164905309677124
Running avgs for agent 1: q_loss: 30.880474090576172, p_loss: -6.17631196975708, mean_rew: -12.321433795269977, variance: 6.620944499969482, lamda: 1.8687095642089844
Running avgs for agent 2: q_loss: 52.86953353881836, p_loss: -6.181581497192383, mean_rew: -12.324850503083713, variance: 6.502501964569092, lamda: 1.9188517332077026

steps: 2499975, episodes: 100000, mean episode reward: -936.786242950169, agent episode reward: [-312.2620809833897, -312.2620809833897, -312.2620809833897], time: 159.716
steps: 2499975, episodes: 100000, mean episode variance: 4.949642250776291, agent episode variance: [1.6916109371185302, 1.653364501953125, 1.6046668117046357], time: 159.716
Running avgs for agent 0: q_loss: 36.258934020996094, p_loss: -6.18964958190918, mean_rew: -12.322009359491108, variance: 6.766443729400635, lamda: 1.827110767364502
Running avgs for agent 1: q_loss: 28.275062561035156, p_loss: -6.169494152069092, mean_rew: -12.322361674353642, variance: 6.613458156585693, lamda: 1.8772697448730469
Running avgs for agent 2: q_loss: 54.13119125366211, p_loss: -6.182740688323975, mean_rew: -12.331207205534719, variance: 6.418666839599609, lamda: 1.920764446258545

steps: 2524975, episodes: 101000, mean episode reward: -932.0943687083824, agent episode reward: [-310.6981229027941, -310.6981229027941, -310.6981229027941], time: 164.923
steps: 2524975, episodes: 101000, mean episode variance: 4.954308490276337, agent episode variance: [1.6940986075401305, 1.6509237365722655, 1.6092861461639405], time: 164.923
Running avgs for agent 0: q_loss: 37.369632720947266, p_loss: -6.196735382080078, mean_rew: -12.334113133389238, variance: 6.776394367218018, lamda: 1.8360766172409058
Running avgs for agent 1: q_loss: 26.721128463745117, p_loss: -6.1655449867248535, mean_rew: -12.306940450829389, variance: 6.603694915771484, lamda: 1.8798422813415527
Running avgs for agent 2: q_loss: 54.78327941894531, p_loss: -6.177698135375977, mean_rew: -12.316578687796435, variance: 6.437144756317139, lamda: 1.9223042726516724

steps: 2549975, episodes: 102000, mean episode reward: -943.4476109135609, agent episode reward: [-314.48253697118696, -314.48253697118696, -314.48253697118696], time: 162.251
steps: 2549975, episodes: 102000, mean episode variance: 4.968334496021271, agent episode variance: [1.68233917760849, 1.654374834060669, 1.6316204843521118], time: 162.251
Running avgs for agent 0: q_loss: 38.735626220703125, p_loss: -6.206629753112793, mean_rew: -12.339041887576961, variance: 6.729356288909912, lamda: 1.851770043373108
Running avgs for agent 1: q_loss: 28.73365592956543, p_loss: -6.181650161743164, mean_rew: -12.346807211908283, variance: 6.617498874664307, lamda: 1.8833400011062622
Running avgs for agent 2: q_loss: 54.070220947265625, p_loss: -6.190746307373047, mean_rew: -12.357686295318643, variance: 6.526482105255127, lamda: 1.9241074323654175

steps: 2574975, episodes: 103000, mean episode reward: -948.4646924931881, agent episode reward: [-316.1548974977294, -316.1548974977294, -316.1548974977294], time: 161.157
steps: 2574975, episodes: 103000, mean episode variance: 4.966561234474182, agent episode variance: [1.6713046417236328, 1.657126226902008, 1.6381303658485413], time: 161.157
Running avgs for agent 0: q_loss: 34.248172760009766, p_loss: -6.209134101867676, mean_rew: -12.361192214871252, variance: 6.685218811035156, lamda: 1.8605741262435913
Running avgs for agent 1: q_loss: 29.684476852416992, p_loss: -6.187690258026123, mean_rew: -12.361138785128707, variance: 6.628504753112793, lamda: 1.8899059295654297
Running avgs for agent 2: q_loss: 55.40420913696289, p_loss: -6.186684608459473, mean_rew: -12.35089638972249, variance: 6.552521705627441, lamda: 1.9252705574035645

steps: 2599975, episodes: 104000, mean episode reward: -940.6309613004084, agent episode reward: [-313.54365376680283, -313.54365376680283, -313.54365376680283], time: 165.748
steps: 2599975, episodes: 104000, mean episode variance: 4.918204550266266, agent episode variance: [1.6682601001262665, 1.631478458404541, 1.6184659917354585], time: 165.748
Running avgs for agent 0: q_loss: 59.228736877441406, p_loss: -6.211845874786377, mean_rew: -12.368672110422795, variance: 6.67303991317749, lamda: 1.8637775182724
Running avgs for agent 1: q_loss: 29.77756118774414, p_loss: -6.195958137512207, mean_rew: -12.364941912133904, variance: 6.525914192199707, lamda: 1.8954620361328125
Running avgs for agent 2: q_loss: 55.29072189331055, p_loss: -6.193668842315674, mean_rew: -12.35377106880644, variance: 6.4738640785217285, lamda: 1.9264472723007202

steps: 2624975, episodes: 105000, mean episode reward: -958.1510556252103, agent episode reward: [-319.3836852084035, -319.3836852084035, -319.3836852084035], time: 166.618
steps: 2624975, episodes: 105000, mean episode variance: 4.9116531734466555, agent episode variance: [1.6628231179714204, 1.6279709129333495, 1.6208591425418855], time: 166.619
Running avgs for agent 0: q_loss: 60.851165771484375, p_loss: -6.216479301452637, mean_rew: -12.360963415007559, variance: 6.651292324066162, lamda: 1.864957332611084
Running avgs for agent 1: q_loss: 29.668201446533203, p_loss: -6.195589065551758, mean_rew: -12.369863845495871, variance: 6.511883735656738, lamda: 1.902887225151062
Running avgs for agent 2: q_loss: 55.50621032714844, p_loss: -6.19301176071167, mean_rew: -12.35574545715982, variance: 6.483436584472656, lamda: 1.9278208017349243

steps: 2649975, episodes: 106000, mean episode reward: -940.2502562107827, agent episode reward: [-313.4167520702609, -313.4167520702609, -313.4167520702609], time: 172.679
steps: 2649975, episodes: 106000, mean episode variance: 4.9252396874427795, agent episode variance: [1.6772257058620452, 1.6379451007843018, 1.6100688807964325], time: 172.679
Running avgs for agent 0: q_loss: 60.2934684753418, p_loss: -6.217805862426758, mean_rew: -12.368513096699543, variance: 6.708902835845947, lamda: 1.8655376434326172
Running avgs for agent 1: q_loss: 27.784652709960938, p_loss: -6.2113871574401855, mean_rew: -12.386242825603345, variance: 6.5517802238464355, lamda: 1.908020257949829
Running avgs for agent 2: q_loss: 38.66859436035156, p_loss: -6.207777976989746, mean_rew: -12.367075173631463, variance: 6.440275192260742, lamda: 1.9335339069366455

steps: 2674975, episodes: 107000, mean episode reward: -924.8997815116531, agent episode reward: [-308.29992717055103, -308.29992717055103, -308.29992717055103], time: 180.246
steps: 2674975, episodes: 107000, mean episode variance: 4.902939750671386, agent episode variance: [1.6741027960777282, 1.6241665301322936, 1.6046704244613648], time: 180.246
Running avgs for agent 0: q_loss: 60.73334503173828, p_loss: -6.23016357421875, mean_rew: -12.390399338056431, variance: 6.6964111328125, lamda: 1.8663856983184814
Running avgs for agent 1: q_loss: 29.88197898864746, p_loss: -6.1891021728515625, mean_rew: -12.351846717647902, variance: 6.496665954589844, lamda: 1.9110857248306274
Running avgs for agent 2: q_loss: 33.87754821777344, p_loss: -6.20296573638916, mean_rew: -12.356047895424396, variance: 6.418681621551514, lamda: 1.9429181814193726

steps: 2699975, episodes: 108000, mean episode reward: -938.7988898302298, agent episode reward: [-312.9329632767433, -312.9329632767433, -312.9329632767433], time: 168.072
steps: 2699975, episodes: 108000, mean episode variance: 4.895687888860703, agent episode variance: [1.6703702199459076, 1.6274819645881653, 1.5978357043266296], time: 168.073
Running avgs for agent 0: q_loss: 61.293663024902344, p_loss: -6.220189571380615, mean_rew: -12.378705340463592, variance: 6.681480884552002, lamda: 1.8678710460662842
Running avgs for agent 1: q_loss: 30.49687957763672, p_loss: -6.193562030792236, mean_rew: -12.358800642096055, variance: 6.509927749633789, lamda: 1.9165297746658325
Running avgs for agent 2: q_loss: 35.06734085083008, p_loss: -6.209569454193115, mean_rew: -12.370368866850727, variance: 6.391342639923096, lamda: 1.95183527469635

steps: 2724975, episodes: 109000, mean episode reward: -946.1041679716653, agent episode reward: [-315.3680559905551, -315.3680559905551, -315.3680559905551], time: 163.313
steps: 2724975, episodes: 109000, mean episode variance: 4.877212864637375, agent episode variance: [1.654583518743515, 1.620217873096466, 1.6024114727973937], time: 163.314
Running avgs for agent 0: q_loss: 50.787261962890625, p_loss: -6.210625171661377, mean_rew: -12.359578822489159, variance: 6.61833381652832, lamda: 1.8721674680709839
Running avgs for agent 1: q_loss: 29.602685928344727, p_loss: -6.187675952911377, mean_rew: -12.341396397032502, variance: 6.480871200561523, lamda: 1.9239697456359863
Running avgs for agent 2: q_loss: 46.0057373046875, p_loss: -6.192524433135986, mean_rew: -12.349548618615506, variance: 6.409646034240723, lamda: 1.9613686800003052

steps: 2749975, episodes: 110000, mean episode reward: -922.6318859787099, agent episode reward: [-307.5439619929033, -307.5439619929033, -307.5439619929033], time: 161.344
steps: 2749975, episodes: 110000, mean episode variance: 4.845926216840744, agent episode variance: [1.653411587715149, 1.6111325569152832, 1.5813820722103118], time: 161.344
Running avgs for agent 0: q_loss: 34.91743850708008, p_loss: -6.213362693786621, mean_rew: -12.353909751770294, variance: 6.613646507263184, lamda: 1.8833669424057007
Running avgs for agent 1: q_loss: 30.88469696044922, p_loss: -6.1978020668029785, mean_rew: -12.371086055484758, variance: 6.444530487060547, lamda: 1.9283969402313232
Running avgs for agent 2: q_loss: 57.00756072998047, p_loss: -6.193325519561768, mean_rew: -12.346622986585325, variance: 6.325528144836426, lamda: 1.964974045753479

steps: 2774975, episodes: 111000, mean episode reward: -931.9640738538003, agent episode reward: [-310.65469128460006, -310.65469128460006, -310.65469128460006], time: 159.455
steps: 2774975, episodes: 111000, mean episode variance: 4.852190480470657, agent episode variance: [1.6424234805107116, 1.6223217906951903, 1.5874452092647553], time: 159.456
Running avgs for agent 0: q_loss: 36.85773468017578, p_loss: -6.215868949890137, mean_rew: -12.37030267204357, variance: 6.5696940422058105, lamda: 1.8895161151885986
Running avgs for agent 1: q_loss: 27.62949562072754, p_loss: -6.201459884643555, mean_rew: -12.389322639389425, variance: 6.48928689956665, lamda: 1.9357783794403076
Running avgs for agent 2: q_loss: 56.6312255859375, p_loss: -6.217082977294922, mean_rew: -12.408278636087358, variance: 6.349780559539795, lamda: 1.965920329093933

steps: 2799975, episodes: 112000, mean episode reward: -943.5590078048097, agent episode reward: [-314.5196692682699, -314.5196692682699, -314.5196692682699], time: 164.844
steps: 2799975, episodes: 112000, mean episode variance: 4.8529193372726445, agent episode variance: [1.6404289927482605, 1.613034713745117, 1.5994556307792664], time: 164.844
Running avgs for agent 0: q_loss: 35.85422897338867, p_loss: -6.226387977600098, mean_rew: -12.390620760055722, variance: 6.561715602874756, lamda: 1.8960810899734497
Running avgs for agent 1: q_loss: 27.454116821289062, p_loss: -6.215249061584473, mean_rew: -12.402154233928064, variance: 6.452138900756836, lamda: 1.9379568099975586
Running avgs for agent 2: q_loss: 56.55438995361328, p_loss: -6.222928524017334, mean_rew: -12.400667154797727, variance: 6.397822380065918, lamda: 1.966794490814209

steps: 2824975, episodes: 113000, mean episode reward: -918.1531727792892, agent episode reward: [-306.0510575930964, -306.0510575930964, -306.0510575930964], time: 162.741
steps: 2824975, episodes: 113000, mean episode variance: 4.844371602296829, agent episode variance: [1.6393826973438264, 1.605087275981903, 1.5999016289710999], time: 162.741
Running avgs for agent 0: q_loss: 46.029361724853516, p_loss: -6.216300010681152, mean_rew: -12.39243355027816, variance: 6.557530403137207, lamda: 1.9038199186325073
Running avgs for agent 1: q_loss: 32.6639404296875, p_loss: -6.213744640350342, mean_rew: -12.394175232062853, variance: 6.420348644256592, lamda: 1.942368984222412
Running avgs for agent 2: q_loss: 56.656524658203125, p_loss: -6.225028038024902, mean_rew: -12.407206128140714, variance: 6.399606227874756, lamda: 1.9676096439361572

steps: 2849975, episodes: 114000, mean episode reward: -929.023631071532, agent episode reward: [-309.6745436905107, -309.6745436905107, -309.6745436905107], time: 165.32
steps: 2849975, episodes: 114000, mean episode variance: 4.8385576171875, agent episode variance: [1.6359813532829284, 1.605317130804062, 1.5972591331005097], time: 165.321
Running avgs for agent 0: q_loss: 63.74798583984375, p_loss: -6.221498966217041, mean_rew: -12.389633782857118, variance: 6.5439252853393555, lamda: 1.9070581197738647
Running avgs for agent 1: q_loss: 41.98967361450195, p_loss: -6.198121547698975, mean_rew: -12.375073026163733, variance: 6.421268463134766, lamda: 1.9499543905258179
Running avgs for agent 2: q_loss: 58.05379867553711, p_loss: -6.2231574058532715, mean_rew: -12.407469241432306, variance: 6.389036655426025, lamda: 1.97018301486969

steps: 2874975, episodes: 115000, mean episode reward: -936.0184770733416, agent episode reward: [-312.0061590244472, -312.0061590244472, -312.0061590244472], time: 168.93
steps: 2874975, episodes: 115000, mean episode variance: 4.827820742845535, agent episode variance: [1.645000849723816, 1.5941823217868805, 1.5886375713348388], time: 168.93
Running avgs for agent 0: q_loss: 63.23211669921875, p_loss: -6.228675842285156, mean_rew: -12.397015763333355, variance: 6.580002784729004, lamda: 1.907538890838623
Running avgs for agent 1: q_loss: 40.77838134765625, p_loss: -6.1993632316589355, mean_rew: -12.36940884161184, variance: 6.3767290115356445, lamda: 1.9528212547302246
Running avgs for agent 2: q_loss: 52.708866119384766, p_loss: -6.21179723739624, mean_rew: -12.386810682779524, variance: 6.354550361633301, lamda: 1.97208833694458

steps: 2899975, episodes: 116000, mean episode reward: -941.0215884596405, agent episode reward: [-313.6738628198802, -313.6738628198802, -313.6738628198802], time: 168.695
steps: 2899975, episodes: 116000, mean episode variance: 4.8247560613155365, agent episode variance: [1.6385872678756714, 1.5988903832435608, 1.5872784101963042], time: 168.695
Running avgs for agent 0: q_loss: 62.74892807006836, p_loss: -6.224696636199951, mean_rew: -12.400633769990467, variance: 6.554348945617676, lamda: 1.9082187414169312
Running avgs for agent 1: q_loss: 30.967119216918945, p_loss: -6.225565433502197, mean_rew: -12.418023046865944, variance: 6.395561695098877, lamda: 1.9601987600326538
Running avgs for agent 2: q_loss: 51.616477966308594, p_loss: -6.231751918792725, mean_rew: -12.425610428613888, variance: 6.349113941192627, lamda: 1.978429913520813

steps: 2924975, episodes: 117000, mean episode reward: -942.6220572068274, agent episode reward: [-314.20735240227583, -314.20735240227583, -314.20735240227583], time: 168.072
steps: 2924975, episodes: 117000, mean episode variance: 4.780849227666855, agent episode variance: [1.6223210232257843, 1.5841068572998047, 1.5744213471412658], time: 168.073
Running avgs for agent 0: q_loss: 62.69094467163086, p_loss: -6.23195219039917, mean_rew: -12.39413362410094, variance: 6.489284038543701, lamda: 1.9091442823410034
Running avgs for agent 1: q_loss: 27.81110382080078, p_loss: -6.2072248458862305, mean_rew: -12.395097321567464, variance: 6.336427688598633, lamda: 1.9650049209594727
Running avgs for agent 2: q_loss: 57.49043655395508, p_loss: -6.227731704711914, mean_rew: -12.417356836652452, variance: 6.297685623168945, lamda: 1.9807296991348267

steps: 2949975, episodes: 118000, mean episode reward: -936.3812221580806, agent episode reward: [-312.12707405269356, -312.12707405269356, -312.12707405269356], time: 166.983
steps: 2949975, episodes: 118000, mean episode variance: 4.843293047428131, agent episode variance: [1.6601950902938842, 1.6030498661994934, 1.5800480909347534], time: 166.983
Running avgs for agent 0: q_loss: 63.11973571777344, p_loss: -6.234580039978027, mean_rew: -12.411784624500651, variance: 6.640780448913574, lamda: 1.9095361232757568
Running avgs for agent 1: q_loss: 28.27981948852539, p_loss: -6.225648403167725, mean_rew: -12.427503015101417, variance: 6.412199020385742, lamda: 1.9660024642944336
Running avgs for agent 2: q_loss: 57.78919982910156, p_loss: -6.232780933380127, mean_rew: -12.419655259406012, variance: 6.320192337036133, lamda: 1.9817261695861816

steps: 2974975, episodes: 119000, mean episode reward: -939.8230024868019, agent episode reward: [-313.2743341622673, -313.2743341622673, -313.2743341622673], time: 164.521
steps: 2974975, episodes: 119000, mean episode variance: 4.782884871482849, agent episode variance: [1.626354262828827, 1.5745836548805237, 1.5819469537734985], time: 164.522
Running avgs for agent 0: q_loss: 63.34185028076172, p_loss: -6.243345260620117, mean_rew: -12.422646964393357, variance: 6.5054168701171875, lamda: 1.9106169939041138
Running avgs for agent 1: q_loss: 32.035743713378906, p_loss: -6.228643417358398, mean_rew: -12.428562838542625, variance: 6.298335075378418, lamda: 1.9725574254989624
Running avgs for agent 2: q_loss: 56.99365234375, p_loss: -6.228818893432617, mean_rew: -12.41921159321031, variance: 6.32778787612915, lamda: 1.9831923246383667

steps: 2999975, episodes: 120000, mean episode reward: -921.2934539017931, agent episode reward: [-307.09781796726435, -307.09781796726435, -307.09781796726435], time: 164.314
steps: 2999975, episodes: 120000, mean episode variance: 4.80580708694458, agent episode variance: [1.6350435416698457, 1.577614921092987, 1.5931486241817474], time: 164.314
Running avgs for agent 0: q_loss: 63.986087799072266, p_loss: -6.245950222015381, mean_rew: -12.428117271846146, variance: 6.54017448425293, lamda: 1.912090539932251
Running avgs for agent 1: q_loss: 29.946300506591797, p_loss: -6.2282185554504395, mean_rew: -12.42686697177304, variance: 6.310459613800049, lamda: 1.9813309907913208
Running avgs for agent 2: q_loss: 57.96217346191406, p_loss: -6.225936412811279, mean_rew: -12.418516759535311, variance: 6.372594833374023, lamda: 1.9847749471664429

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -940.931929329925, agent episode reward: [-313.6439764433083, -313.6439764433083, -313.6439764433083], time: 139.644
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 139.644
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -951.9125815270545, agent episode reward: [-317.30419384235154, -317.30419384235154, -317.30419384235154], time: 155.25
steps: 49975, episodes: 2000, mean episode variance: 4.2772088379859925, agent episode variance: [2.0529407992362976, 1.1912091898918151, 1.0330588488578796], time: 155.25
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.588769230583493, variance: 8.413692474365234, lamda: 1.9128344058990479
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.587485237977784, variance: 4.882004737854004, lamda: 1.983048915863037
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.57561299473668, variance: 4.233847618103027, lamda: 1.9865609407424927

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567759418.1090532: line 9: --exp_var_alpha: command not found
