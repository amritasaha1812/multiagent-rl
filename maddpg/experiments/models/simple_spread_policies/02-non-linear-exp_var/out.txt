# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 23.72 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies/02-non-linear-exp_var/
Job <1090450> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc174>>
arglist.u_estimation True
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -524.5510675035054, agent episode reward: [-174.85035583450178, -174.85035583450178, -174.85035583450178], time: 58.143
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 58.143
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -779.6545020953084, agent episode reward: [-259.8848340317694, -259.8848340317694, -259.8848340317694], time: 79.138
steps: 49975, episodes: 2000, mean episode variance: 7.001827749617398, agent episode variance: [2.9214675546884536, 1.3009419224336742, 2.7794182724952696], time: 79.139
Running avgs for agent 0: q_loss: 99.56717681884766, p_loss: -5.533324241638184, mean_rew: -7.754157112005179, variance: 11.9732276831494, lamda: 1.0102951526641846
Running avgs for agent 1: q_loss: 911.3163452148438, p_loss: 12.335529327392578, mean_rew: -7.744940515067204, variance: 5.331729190301944, lamda: 1.0111236572265625
Running avgs for agent 2: q_loss: 91.16507720947266, p_loss: -5.252249240875244, mean_rew: -7.7404550216595425, variance: 11.391057968139648, lamda: 1.0090312957763672

steps: 74975, episodes: 3000, mean episode reward: -935.6505939320692, agent episode reward: [-311.88353131068976, -311.88353131068976, -311.88353131068976], time: 76.438
steps: 74975, episodes: 3000, mean episode variance: 9.507549377918243, agent episode variance: [2.337764949321747, 4.876860894918442, 2.2929235336780547], time: 76.438
Running avgs for agent 0: q_loss: 43.605281829833984, p_loss: -5.190977573394775, mean_rew: -9.445998235286046, variance: 9.35106086730957, lamda: 1.0337777137756348
Running avgs for agent 1: q_loss: 14599.77734375, p_loss: 32.96963882446289, mean_rew: -9.466178540815282, variance: 19.50744357967377, lamda: 1.0357898473739624
Running avgs for agent 2: q_loss: 64.50361633300781, p_loss: -5.109092712402344, mean_rew: -9.45486424752093, variance: 9.1716947555542, lamda: 1.0312119722366333

steps: 99975, episodes: 4000, mean episode reward: -892.6799679556177, agent episode reward: [-297.5599893185393, -297.5599893185393, -297.5599893185393], time: 84.382
steps: 99975, episodes: 4000, mean episode variance: 17.562214483499528, agent episode variance: [2.391256178379059, 12.756567421913147, 2.414390883207321], time: 84.383
Running avgs for agent 0: q_loss: 50.06740951538086, p_loss: -5.24029016494751, mean_rew: -10.220623581623723, variance: 9.565025329589844, lamda: 1.0567920207977295
Running avgs for agent 1: q_loss: 88485.2890625, p_loss: 57.06944274902344, mean_rew: -10.237514699049145, variance: 51.02626968765259, lamda: 1.0607941150665283
Running avgs for agent 2: q_loss: 60.04105758666992, p_loss: -5.253338813781738, mean_rew: -10.241457420549784, variance: 9.657564163208008, lamda: 1.0555839538574219

steps: 124975, episodes: 5000, mean episode reward: -844.2726826185516, agent episode reward: [-281.4242275395172, -281.4242275395172, -281.4242275395172], time: 81.439
steps: 124975, episodes: 5000, mean episode variance: 44.91991822814941, agent episode variance: [2.4661875243186953, 39.994212896347044, 2.459517807483673], time: 81.439
Running avgs for agent 0: q_loss: 50.46253967285156, p_loss: -5.345930576324463, mean_rew: -10.56187146584648, variance: 9.864749908447266, lamda: 1.0792165994644165
Running avgs for agent 1: q_loss: 398981.875, p_loss: 80.32384490966797, mean_rew: -10.569531566827457, variance: 159.97685158538818, lamda: 1.0857982635498047
Running avgs for agent 2: q_loss: 49.2672004699707, p_loss: -5.375350475311279, mean_rew: -10.56634097971678, variance: 9.838069915771484, lamda: 1.0775095224380493

steps: 149975, episodes: 6000, mean episode reward: -779.331810722645, agent episode reward: [-259.7772702408816, -259.7772702408816, -259.7772702408816], time: 82.213
steps: 149975, episodes: 6000, mean episode variance: 40.98597604139149, agent episode variance: [2.413030291393399, 36.158197279930114, 2.4147484700679778], time: 82.214
Running avgs for agent 0: q_loss: 58.325191497802734, p_loss: -5.373391628265381, mean_rew: -10.595821754972732, variance: 9.652121543884277, lamda: 1.1006243228912354
Running avgs for agent 1: q_loss: 350978.125, p_loss: 102.12174224853516, mean_rew: -10.627384914111218, variance: 144.63278911972046, lamda: 1.1108022928237915
Running avgs for agent 2: q_loss: 46.63673400878906, p_loss: -5.367498874664307, mean_rew: -10.610461588599021, variance: 9.6589937210083, lamda: 1.0947253704071045

steps: 174975, episodes: 7000, mean episode reward: -665.7507870105619, agent episode reward: [-221.9169290035207, -221.9169290035207, -221.9169290035207], time: 80.503
steps: 174975, episodes: 7000, mean episode variance: 82.06254825127125, agent episode variance: [2.3442807995080948, 77.31689506530762, 2.401372386455536], time: 80.503
Running avgs for agent 0: q_loss: 53.63951110839844, p_loss: -5.374100685119629, mean_rew: -10.496293567168044, variance: 9.37712287902832, lamda: 1.1205884218215942
Running avgs for agent 1: q_loss: 1294735.5, p_loss: 118.64350891113281, mean_rew: -10.483113516133718, variance: 309.2675802612305, lamda: 1.1358065605163574
Running avgs for agent 2: q_loss: 45.199031829833984, p_loss: -5.335710048675537, mean_rew: -10.498370473824242, variance: 9.605490684509277, lamda: 1.1097092628479004

steps: 199975, episodes: 8000, mean episode reward: -627.8872128551619, agent episode reward: [-209.29573761838728, -209.29573761838728, -209.29573761838728], time: 81.332
steps: 199975, episodes: 8000, mean episode variance: 63.218940212726594, agent episode variance: [2.262869593143463, 58.701689785003666, 2.2543808345794676], time: 81.332
Running avgs for agent 0: q_loss: 38.1282958984375, p_loss: -5.270771503448486, mean_rew: -10.22429427412049, variance: 9.051478385925293, lamda: 1.1360770463943481
Running avgs for agent 1: q_loss: 1465095.5, p_loss: 128.9473114013672, mean_rew: -10.217749579501854, variance: 234.80675914001466, lamda: 1.1608107089996338
Running avgs for agent 2: q_loss: 48.42609405517578, p_loss: -5.2191619873046875, mean_rew: -10.214653135271108, variance: 9.017523765563965, lamda: 1.131190538406372

steps: 224975, episodes: 9000, mean episode reward: -602.3907893032338, agent episode reward: [-200.7969297677446, -200.7969297677446, -200.7969297677446], time: 80.623
steps: 224975, episodes: 9000, mean episode variance: 73.70014012432098, agent episode variance: [2.205126626968384, 69.31529386901856, 2.179719628334045], time: 80.623
Running avgs for agent 0: q_loss: 39.77888107299805, p_loss: -5.119187355041504, mean_rew: -9.992573617015049, variance: 8.820507049560547, lamda: 1.1561931371688843
Running avgs for agent 1: q_loss: 1800424.5, p_loss: 137.72628784179688, mean_rew: -9.983336013549751, variance: 277.26117547607424, lamda: 1.1858147382736206
Running avgs for agent 2: q_loss: 43.88470458984375, p_loss: -5.085298538208008, mean_rew: -9.98283607253176, variance: 8.718878746032715, lamda: 1.1547160148620605

steps: 249975, episodes: 10000, mean episode reward: -584.6167036297762, agent episode reward: [-194.87223454325874, -194.87223454325874, -194.87223454325874], time: 82.831
steps: 249975, episodes: 10000, mean episode variance: 102.74240038132668, agent episode variance: [2.0941830160617827, 98.58137237739562, 2.0668449878692625], time: 82.831
Running avgs for agent 0: q_loss: 49.698699951171875, p_loss: -4.999598503112793, mean_rew: -9.767588172404665, variance: 8.37673282623291, lamda: 1.1757229566574097
Running avgs for agent 1: q_loss: 2693879.5, p_loss: 144.07460021972656, mean_rew: -9.768549216378076, variance: 394.3254895095825, lamda: 1.210818886756897
Running avgs for agent 2: q_loss: 42.242881774902344, p_loss: -4.978980541229248, mean_rew: -9.767582231715409, variance: 8.267379760742188, lamda: 1.1786054372787476

steps: 274975, episodes: 11000, mean episode reward: -572.3427716905683, agent episode reward: [-190.78092389685608, -190.78092389685608, -190.78092389685608], time: 80.896
steps: 274975, episodes: 11000, mean episode variance: 106.42113947343826, agent episode variance: [2.0552626433372496, 102.37564538574219, 1.9902314443588256], time: 80.896
Running avgs for agent 0: q_loss: 35.40164566040039, p_loss: -4.888863563537598, mean_rew: -9.55858446648046, variance: 8.221051216125488, lamda: 1.1838306188583374
Running avgs for agent 1: q_loss: 3024663.5, p_loss: 149.06825256347656, mean_rew: -9.570425194633353, variance: 409.50258154296876, lamda: 1.235823154449463
Running avgs for agent 2: q_loss: 45.81856155395508, p_loss: -4.9033589363098145, mean_rew: -9.562862327342271, variance: 7.960925579071045, lamda: 1.2028330564498901

steps: 299975, episodes: 12000, mean episode reward: -551.7674695716038, agent episode reward: [-183.92248985720127, -183.92248985720127, -183.92248985720127], time: 84.336
steps: 299975, episodes: 12000, mean episode variance: 105.70023282575607, agent episode variance: [1.9767396211624146, 101.8280222568512, 1.895470947742462], time: 84.337
Running avgs for agent 0: q_loss: 29.534395217895508, p_loss: -4.80277156829834, mean_rew: -9.388241255374592, variance: 7.906959056854248, lamda: 1.2010117769241333
Running avgs for agent 1: q_loss: 3250413.5, p_loss: 152.22450256347656, mean_rew: -9.39010083928494, variance: 407.3120890274048, lamda: 1.2608271837234497
Running avgs for agent 2: q_loss: 40.65243148803711, p_loss: -4.7948994636535645, mean_rew: -9.37502487268014, variance: 7.581883907318115, lamda: 1.2276679277420044

steps: 324975, episodes: 13000, mean episode reward: -555.2148237684164, agent episode reward: [-185.07160792280544, -185.07160792280544, -185.07160792280544], time: 82.837
steps: 324975, episodes: 13000, mean episode variance: 84.08958387160301, agent episode variance: [1.9232503724098207, 80.29851963424683, 1.8678138649463654], time: 82.837
Running avgs for agent 0: q_loss: 33.77106475830078, p_loss: -4.708126544952393, mean_rew: -9.213015463773468, variance: 7.693000793457031, lamda: 1.220281958580017
Running avgs for agent 1: q_loss: 3269973.25, p_loss: 156.94659423828125, mean_rew: -9.238985258591814, variance: 321.1940785369873, lamda: 1.285831332206726
Running avgs for agent 2: q_loss: 41.513641357421875, p_loss: -4.678419589996338, mean_rew: -9.230450854048506, variance: 7.471255779266357, lamda: 1.2524791955947876

steps: 349975, episodes: 14000, mean episode reward: -542.8924833778177, agent episode reward: [-180.96416112593923, -180.96416112593923, -180.96416112593923], time: 83.16
steps: 349975, episodes: 14000, mean episode variance: 104.12869589519501, agent episode variance: [1.8446996495723724, 100.49907454681396, 1.78492169880867], time: 83.16
Running avgs for agent 0: q_loss: 26.551788330078125, p_loss: -4.622483253479004, mean_rew: -9.073364491496468, variance: 7.378798961639404, lamda: 1.2401574850082397
Running avgs for agent 1: q_loss: 3997234.25, p_loss: 158.66281127929688, mean_rew: -9.081581858979861, variance: 401.99629818725583, lamda: 1.3108354806900024
Running avgs for agent 2: q_loss: 35.012813568115234, p_loss: -4.624967098236084, mean_rew: -9.077381616364548, variance: 7.1396870613098145, lamda: 1.2767817974090576

steps: 374975, episodes: 15000, mean episode reward: -562.2977143346029, agent episode reward: [-187.43257144486765, -187.43257144486765, -187.43257144486765], time: 89.883
steps: 374975, episodes: 15000, mean episode variance: 97.07256000065803, agent episode variance: [1.8135490915775299, 93.52558694458008, 1.7334239645004272], time: 89.883
Running avgs for agent 0: q_loss: 24.965473175048828, p_loss: -4.574483871459961, mean_rew: -8.949885126603105, variance: 7.2541961669921875, lamda: 1.2594484090805054
Running avgs for agent 1: q_loss: 4223343.0, p_loss: 161.1037139892578, mean_rew: -8.969933880333972, variance: 374.10234777832034, lamda: 1.3358396291732788
Running avgs for agent 2: q_loss: 30.36163330078125, p_loss: -4.578597545623779, mean_rew: -8.963474938157873, variance: 6.9336957931518555, lamda: 1.3002331256866455

steps: 399975, episodes: 16000, mean episode reward: -569.4151598448469, agent episode reward: [-189.80505328161567, -189.80505328161567, -189.80505328161567], time: 89.654
steps: 399975, episodes: 16000, mean episode variance: 79.2500640323162, agent episode variance: [1.774658679008484, 75.79335390663147, 1.6820514466762542], time: 89.654
Running avgs for agent 0: q_loss: 21.993806838989258, p_loss: -4.54645299911499, mean_rew: -8.86597457826117, variance: 7.098634243011475, lamda: 1.2785634994506836
Running avgs for agent 1: q_loss: 3731414.25, p_loss: 162.8737030029297, mean_rew: -8.874632557504476, variance: 303.17341562652587, lamda: 1.3608437776565552
Running avgs for agent 2: q_loss: 31.038314819335938, p_loss: -4.505391597747803, mean_rew: -8.863226288088246, variance: 6.728206157684326, lamda: 1.32430100440979

steps: 424975, episodes: 17000, mean episode reward: -548.0786252699729, agent episode reward: [-182.69287508999096, -182.69287508999096, -182.69287508999096], time: 89.215
steps: 424975, episodes: 17000, mean episode variance: 77.28300845217704, agent episode variance: [1.715783758878708, 73.93243590164185, 1.634788791656494], time: 89.216
Running avgs for agent 0: q_loss: 29.71102523803711, p_loss: -4.510110378265381, mean_rew: -8.789848129344424, variance: 6.863134860992432, lamda: 1.2983132600784302
Running avgs for agent 1: q_loss: 3360804.0, p_loss: 163.71978759765625, mean_rew: -8.78492661508326, variance: 295.7297436065674, lamda: 1.385848045349121
Running avgs for agent 2: q_loss: 30.69862174987793, p_loss: -4.48717737197876, mean_rew: -8.791023776007185, variance: 6.53915548324585, lamda: 1.3485382795333862

steps: 449975, episodes: 18000, mean episode reward: -558.3127759186674, agent episode reward: [-186.1042586395558, -186.1042586395558, -186.1042586395558], time: 87.164
steps: 449975, episodes: 18000, mean episode variance: 62.851268144607545, agent episode variance: [1.6845271077156068, 59.57146919059753, 1.595271846294403], time: 87.165
Running avgs for agent 0: q_loss: 29.415729522705078, p_loss: -4.4567108154296875, mean_rew: -8.708640989246243, variance: 6.7381086349487305, lamda: 1.3079665899276733
Running avgs for agent 1: q_loss: 3150427.0, p_loss: 164.07595825195312, mean_rew: -8.69555062678111, variance: 238.28587676239013, lamda: 1.410852074623108
Running avgs for agent 2: q_loss: 30.912504196166992, p_loss: -4.437070369720459, mean_rew: -8.707411776864715, variance: 6.381087303161621, lamda: 1.368920922279358

steps: 474975, episodes: 19000, mean episode reward: -550.0035215468661, agent episode reward: [-183.33450718228872, -183.33450718228872, -183.33450718228872], time: 88.202
steps: 474975, episodes: 19000, mean episode variance: 68.57154539656639, agent episode variance: [1.6569534344673156, 65.34906311798096, 1.5655288441181183], time: 88.203
Running avgs for agent 0: q_loss: 22.059558868408203, p_loss: -4.4217424392700195, mean_rew: -8.632160502018905, variance: 6.627813816070557, lamda: 1.3245875835418701
Running avgs for agent 1: q_loss: 3114505.5, p_loss: 165.49896240234375, mean_rew: -8.64423733118612, variance: 261.39625247192384, lamda: 1.4358562231063843
Running avgs for agent 2: q_loss: 37.034515380859375, p_loss: -4.379009246826172, mean_rew: -8.643424712167121, variance: 6.262115001678467, lamda: 1.3913836479187012

steps: 499975, episodes: 20000, mean episode reward: -538.1317906219693, agent episode reward: [-179.37726354065646, -179.37726354065646, -179.37726354065646], time: 119.786
steps: 499975, episodes: 20000, mean episode variance: 94.10093873429298, agent episode variance: [1.6107592661380767, 90.94816271209717, 1.5420167560577394], time: 119.787
Running avgs for agent 0: q_loss: 21.70249366760254, p_loss: -4.377314567565918, mean_rew: -8.56685428940965, variance: 6.443037033081055, lamda: 1.3442304134368896
Running avgs for agent 1: q_loss: 3721656.75, p_loss: 166.15896606445312, mean_rew: -8.572450724361662, variance: 363.7926508483887, lamda: 1.4608603715896606
Running avgs for agent 2: q_loss: 29.28496551513672, p_loss: -4.3394775390625, mean_rew: -8.566100862484982, variance: 6.168067455291748, lamda: 1.4132874011993408

steps: 524975, episodes: 21000, mean episode reward: -525.1236021964307, agent episode reward: [-175.04120073214364, -175.04120073214364, -175.04120073214364], time: 114.41
steps: 524975, episodes: 21000, mean episode variance: 90.2732360265255, agent episode variance: [1.5817298440933227, 87.20679935455323, 1.484706827878952], time: 114.41
Running avgs for agent 0: q_loss: 21.31058120727539, p_loss: -4.335341453552246, mean_rew: -8.496070287529491, variance: 6.326920032501221, lamda: 1.3655487298965454
Running avgs for agent 1: q_loss: 3802014.5, p_loss: 167.72293090820312, mean_rew: -8.487460518261186, variance: 348.8271974182129, lamda: 1.485864520072937
Running avgs for agent 2: q_loss: 29.317941665649414, p_loss: -4.303527355194092, mean_rew: -8.491611946535242, variance: 5.9388275146484375, lamda: 1.4382179975509644

steps: 549975, episodes: 22000, mean episode reward: -535.0002926122215, agent episode reward: [-178.33343087074053, -178.33343087074053, -178.33343087074053], time: 113.76
steps: 549975, episodes: 22000, mean episode variance: 67.40806545829773, agent episode variance: [1.5389885222911834, 64.41431474208832, 1.4547621939182283], time: 113.761
Running avgs for agent 0: q_loss: 20.870464324951172, p_loss: -4.303518772125244, mean_rew: -8.435476396507497, variance: 6.155953884124756, lamda: 1.3879592418670654
Running avgs for agent 1: q_loss: 3072291.25, p_loss: 168.76959228515625, mean_rew: -8.427901961931031, variance: 257.6572589683533, lamda: 1.5108686685562134
Running avgs for agent 2: q_loss: 29.78392219543457, p_loss: -4.2836737632751465, mean_rew: -8.443075061167292, variance: 5.819048881530762, lamda: 1.4628595113754272

steps: 574975, episodes: 23000, mean episode reward: -521.0276725644641, agent episode reward: [-173.67589085482138, -173.67589085482138, -173.67589085482138], time: 114.75
steps: 574975, episodes: 23000, mean episode variance: 87.74953053545951, agent episode variance: [1.5004736082553864, 84.81281943130493, 1.4362374958992004], time: 114.75
Running avgs for agent 0: q_loss: 19.272733688354492, p_loss: -4.273250102996826, mean_rew: -8.37462722543485, variance: 6.001894474029541, lamda: 1.4089196920394897
Running avgs for agent 1: q_loss: 3767171.0, p_loss: 169.64193725585938, mean_rew: -8.389401928276643, variance: 339.2512777252197, lamda: 1.5358729362487793
Running avgs for agent 2: q_loss: 28.414491653442383, p_loss: -4.239055633544922, mean_rew: -8.359094857608449, variance: 5.744950294494629, lamda: 1.4874818325042725

steps: 599975, episodes: 24000, mean episode reward: -527.7127445370328, agent episode reward: [-175.90424817901092, -175.90424817901092, -175.90424817901092], time: 115.108
steps: 599975, episodes: 24000, mean episode variance: 77.3565813343525, agent episode variance: [1.4703051562309266, 74.49665081596375, 1.3896253621578216], time: 115.108
Running avgs for agent 0: q_loss: 18.078683853149414, p_loss: -4.225089073181152, mean_rew: -8.309959962407321, variance: 5.881220817565918, lamda: 1.428977370262146
Running avgs for agent 1: q_loss: 3711870.75, p_loss: 169.59202575683594, mean_rew: -8.312675388329348, variance: 297.986603263855, lamda: 1.5608769655227661
Running avgs for agent 2: q_loss: 26.307241439819336, p_loss: -4.234212875366211, mean_rew: -8.311620709490311, variance: 5.558501243591309, lamda: 1.5120835304260254

steps: 624975, episodes: 25000, mean episode reward: -534.5043303026155, agent episode reward: [-178.16811010087176, -178.16811010087176, -178.16811010087176], time: 166.001
steps: 624975, episodes: 25000, mean episode variance: 84.53553398370742, agent episode variance: [1.442901918888092, 81.71178385925293, 1.3808482055664062], time: 166.002
Running avgs for agent 0: q_loss: 19.206193923950195, p_loss: -4.206507682800293, mean_rew: -8.260843638556738, variance: 5.771607875823975, lamda: 1.4498871564865112
Running avgs for agent 1: q_loss: 3795558.5, p_loss: 170.17596435546875, mean_rew: -8.254221968516557, variance: 326.84713543701173, lamda: 1.5858811140060425
Running avgs for agent 2: q_loss: 30.101776123046875, p_loss: -4.201950550079346, mean_rew: -8.268868331136382, variance: 5.523392677307129, lamda: 1.5365906953811646

steps: 649975, episodes: 26000, mean episode reward: -539.3214082167293, agent episode reward: [-179.77380273890975, -179.77380273890975, -179.77380273890975], time: 174.064
steps: 649975, episodes: 26000, mean episode variance: 62.868853975057604, agent episode variance: [1.418747013092041, 60.119747077941895, 1.3303598840236663], time: 174.065
Running avgs for agent 0: q_loss: 17.863157272338867, p_loss: -4.1843438148498535, mean_rew: -8.214972134697549, variance: 5.674988269805908, lamda: 1.467085599899292
Running avgs for agent 1: q_loss: 3382418.0, p_loss: 172.03819274902344, mean_rew: -8.211696005987841, variance: 240.47898831176758, lamda: 1.6108852624893188
Running avgs for agent 2: q_loss: 28.844423294067383, p_loss: -4.179299831390381, mean_rew: -8.202758898420592, variance: 5.321439266204834, lamda: 1.5611616373062134

steps: 674975, episodes: 27000, mean episode reward: -544.0171072297654, agent episode reward: [-181.33903574325515, -181.33903574325515, -181.33903574325515], time: 166.754
steps: 674975, episodes: 27000, mean episode variance: 45.51228785610199, agent episode variance: [1.3870939366817474, 42.81012413406372, 1.3150697853565216], time: 166.755
Running avgs for agent 0: q_loss: 17.706663131713867, p_loss: -4.153976917266846, mean_rew: -8.16429792345199, variance: 5.548376083374023, lamda: 1.4868801832199097
Running avgs for agent 1: q_loss: 2420379.5, p_loss: 174.11431884765625, mean_rew: -8.177003109980559, variance: 171.24049653625488, lamda: 1.6358894109725952
Running avgs for agent 2: q_loss: 26.758943557739258, p_loss: -4.173822402954102, mean_rew: -8.181415348274866, variance: 5.260279178619385, lamda: 1.5860357284545898

steps: 699975, episodes: 28000, mean episode reward: -535.4985796742056, agent episode reward: [-178.4995265580685, -178.4995265580685, -178.4995265580685], time: 173.131
steps: 699975, episodes: 28000, mean episode variance: 58.57514082479477, agent episode variance: [1.3699052844047546, 55.919463567733764, 1.28577197265625], time: 173.132
Running avgs for agent 0: q_loss: 17.719812393188477, p_loss: -4.14482307434082, mean_rew: -8.147047527117657, variance: 5.479620933532715, lamda: 1.508151888847351
Running avgs for agent 1: q_loss: 4147319.0, p_loss: 176.82460021972656, mean_rew: -8.152035035617258, variance: 223.67785427093506, lamda: 1.6608935594558716
Running avgs for agent 2: q_loss: 26.324737548828125, p_loss: -4.163022518157959, mean_rew: -8.142509055371834, variance: 5.143087863922119, lamda: 1.6107301712036133

steps: 724975, episodes: 29000, mean episode reward: -514.7450907227146, agent episode reward: [-171.5816969075715, -171.5816969075715, -171.5816969075715], time: 168.418
steps: 724975, episodes: 29000, mean episode variance: 89.29248021137714, agent episode variance: [1.3443527715206147, 86.69446310806275, 1.253664331793785], time: 168.418
Running avgs for agent 0: q_loss: 15.951183319091797, p_loss: -4.124558448791504, mean_rew: -8.104035694340816, variance: 5.377411365509033, lamda: 1.5276310443878174
Running avgs for agent 1: q_loss: 5418241.0, p_loss: 177.2305145263672, mean_rew: -8.105458719894672, variance: 346.777852432251, lamda: 1.685897707939148
Running avgs for agent 2: q_loss: 26.44546127319336, p_loss: -4.13967752456665, mean_rew: -8.104539164830578, variance: 5.014657020568848, lamda: 1.6349948644638062

steps: 749975, episodes: 30000, mean episode reward: -523.1585147412603, agent episode reward: [-174.38617158042007, -174.38617158042007, -174.38617158042007], time: 167.522
steps: 749975, episodes: 30000, mean episode variance: 89.81861931729317, agent episode variance: [1.316486095905304, 87.26917672348023, 1.2329564979076386], time: 167.523
Running avgs for agent 0: q_loss: 17.142011642456055, p_loss: -4.1029181480407715, mean_rew: -8.06202655246877, variance: 5.265944480895996, lamda: 1.547086477279663
Running avgs for agent 1: q_loss: 5974843.5, p_loss: 178.04522705078125, mean_rew: -8.069619684848384, variance: 349.0767068939209, lamda: 1.7109018564224243
Running avgs for agent 2: q_loss: 22.696836471557617, p_loss: -4.121282577514648, mean_rew: -8.058547759421087, variance: 4.931826114654541, lamda: 1.6584433317184448

steps: 774975, episodes: 31000, mean episode reward: -518.0855161827329, agent episode reward: [-172.69517206091098, -172.69517206091098, -172.69517206091098], time: 166.384
steps: 774975, episodes: 31000, mean episode variance: 91.13255388307572, agent episode variance: [1.3111136736869813, 88.61613246154785, 1.2053077478408813], time: 166.385
Running avgs for agent 0: q_loss: 17.90193748474121, p_loss: -4.091818332672119, mean_rew: -8.038119214794387, variance: 5.244454383850098, lamda: 1.5612741708755493
Running avgs for agent 1: q_loss: 6030424.0, p_loss: 178.70419311523438, mean_rew: -8.029577790352553, variance: 354.4645298461914, lamda: 1.7359060049057007
Running avgs for agent 2: q_loss: 25.76931381225586, p_loss: -4.105347633361816, mean_rew: -8.027263973018753, variance: 4.821231365203857, lamda: 1.6823506355285645

steps: 799975, episodes: 32000, mean episode reward: -518.900214287562, agent episode reward: [-172.96673809585397, -172.96673809585397, -172.96673809585397], time: 168.854
steps: 799975, episodes: 32000, mean episode variance: 68.30117656302453, agent episode variance: [1.2936554279327392, 65.81910909461975, 1.1884120404720306], time: 168.855
Running avgs for agent 0: q_loss: 15.692179679870605, p_loss: -4.074229717254639, mean_rew: -8.001595354587122, variance: 5.17462158203125, lamda: 1.5803660154342651
Running avgs for agent 1: q_loss: 5532076.5, p_loss: 178.10682678222656, mean_rew: -7.986818093312405, variance: 263.276436378479, lamda: 1.760910153388977
Running avgs for agent 2: q_loss: 23.335878372192383, p_loss: -4.103909969329834, mean_rew: -7.997169287280753, variance: 4.753648281097412, lamda: 1.7062612771987915

steps: 824975, episodes: 33000, mean episode reward: -532.5131745626333, agent episode reward: [-177.50439152087773, -177.50439152087773, -177.50439152087773], time: 168.681
steps: 824975, episodes: 33000, mean episode variance: 82.72074501228333, agent episode variance: [1.2692720918655396, 80.2800892868042, 1.1713836336135863], time: 168.682
Running avgs for agent 0: q_loss: 15.481454849243164, p_loss: -4.059665679931641, mean_rew: -7.957320560835071, variance: 5.077087879180908, lamda: 1.6000471115112305
Running avgs for agent 1: q_loss: 6182526.0, p_loss: 178.10690307617188, mean_rew: -7.965968671431791, variance: 321.1203571472168, lamda: 1.7859143018722534
Running avgs for agent 2: q_loss: 24.15673828125, p_loss: -4.0837836265563965, mean_rew: -7.967341868231873, variance: 4.6855340003967285, lamda: 1.7299983501434326

steps: 849975, episodes: 34000, mean episode reward: -532.0855409401418, agent episode reward: [-177.36184698004723, -177.36184698004723, -177.36184698004723], time: 168.641
steps: 849975, episodes: 34000, mean episode variance: 90.02455852282047, agent episode variance: [1.2435798261165618, 87.62779452514648, 1.1531841715574265], time: 168.642
Running avgs for agent 0: q_loss: 16.55508041381836, p_loss: -4.044803619384766, mean_rew: -7.93159071941246, variance: 4.9743194580078125, lamda: 1.6193606853485107
Running avgs for agent 1: q_loss: 6239684.0, p_loss: 177.95162963867188, mean_rew: -7.925139277545244, variance: 350.51117810058594, lamda: 1.8109184503555298
Running avgs for agent 2: q_loss: 30.37987518310547, p_loss: -4.053994655609131, mean_rew: -7.936695906918808, variance: 4.612736701965332, lamda: 1.7525222301483154

steps: 874975, episodes: 35000, mean episode reward: -528.5317734177002, agent episode reward: [-176.1772578059001, -176.1772578059001, -176.1772578059001], time: 169.377
steps: 874975, episodes: 35000, mean episode variance: 91.14445582187176, agent episode variance: [1.230238062620163, 88.7816829071045, 1.1325348521471024], time: 169.377
Running avgs for agent 0: q_loss: 19.204429626464844, p_loss: -4.042411804199219, mean_rew: -7.918592395966015, variance: 4.920952320098877, lamda: 1.6385151147842407
Running avgs for agent 1: q_loss: 6624330.0, p_loss: 178.16787719726562, mean_rew: -7.910843692480307, variance: 355.126731628418, lamda: 1.8359225988388062
Running avgs for agent 2: q_loss: 29.459823608398438, p_loss: -4.061598300933838, mean_rew: -7.921043098003289, variance: 4.530138969421387, lamda: 1.7749940156936646

steps: 899975, episodes: 36000, mean episode reward: -532.7975673098634, agent episode reward: [-177.5991891032878, -177.5991891032878, -177.5991891032878], time: 164.547
steps: 899975, episodes: 36000, mean episode variance: 93.27415823054314, agent episode variance: [1.2263863224983216, 90.9277532043457, 1.1200187036991118], time: 164.547
Running avgs for agent 0: q_loss: 20.53264045715332, p_loss: -4.015176296234131, mean_rew: -7.883247742365904, variance: 4.905544757843018, lamda: 1.6463580131530762
Running avgs for agent 1: q_loss: 6708410.0, p_loss: 177.93995666503906, mean_rew: -7.890039376174255, variance: 363.7110128173828, lamda: 1.8609267473220825
Running avgs for agent 2: q_loss: 22.186262130737305, p_loss: -4.053746700286865, mean_rew: -7.8875185300613895, variance: 4.480075359344482, lamda: 1.798922061920166

steps: 924975, episodes: 37000, mean episode reward: -533.8364953971904, agent episode reward: [-177.94549846573017, -177.94549846573017, -177.94549846573017], time: 166.361
steps: 924975, episodes: 37000, mean episode variance: 93.8013602553606, agent episode variance: [1.2108361868858337, 91.50245153808594, 1.0880725303888321], time: 166.361
Running avgs for agent 0: q_loss: 16.228422164916992, p_loss: -4.019026756286621, mean_rew: -7.864459837828688, variance: 4.843344688415527, lamda: 1.6576670408248901
Running avgs for agent 1: q_loss: 6598131.5, p_loss: 178.15611267089844, mean_rew: -7.875778822282903, variance: 366.00980615234374, lamda: 1.8859308958053589
Running avgs for agent 2: q_loss: 29.46735382080078, p_loss: -4.0420989990234375, mean_rew: -7.859313464975698, variance: 4.352290153503418, lamda: 1.8226397037506104

steps: 949975, episodes: 38000, mean episode reward: -538.100270664384, agent episode reward: [-179.36675688812804, -179.36675688812804, -179.36675688812804], time: 166.033
steps: 949975, episodes: 38000, mean episode variance: 91.7179204773903, agent episode variance: [1.1990078337192536, 89.43372862052918, 1.085184023141861], time: 166.034
Running avgs for agent 0: q_loss: 18.070554733276367, p_loss: -3.999192476272583, mean_rew: -7.849583392548832, variance: 4.796031475067139, lamda: 1.6724433898925781
Running avgs for agent 1: q_loss: 6905797.5, p_loss: 178.3587188720703, mean_rew: -7.851839281511608, variance: 357.7349144821167, lamda: 1.9109350442886353
Running avgs for agent 2: q_loss: 27.685855865478516, p_loss: -4.031248092651367, mean_rew: -7.850489855289945, variance: 4.340736389160156, lamda: 1.840973973274231

steps: 974975, episodes: 39000, mean episode reward: -535.4775415630351, agent episode reward: [-178.49251385434508, -178.49251385434508, -178.49251385434508], time: 165.783
steps: 974975, episodes: 39000, mean episode variance: 94.82429894578456, agent episode variance: [1.1878941838741301, 92.5780728302002, 1.0583319317102433], time: 165.783
Running avgs for agent 0: q_loss: 16.79677391052246, p_loss: -3.995954990386963, mean_rew: -7.830805256743833, variance: 4.7515764236450195, lamda: 1.6813949346542358
Running avgs for agent 1: q_loss: 7067007.0, p_loss: 177.8368377685547, mean_rew: -7.830679754315334, variance: 370.3122913208008, lamda: 1.9359391927719116
Running avgs for agent 2: q_loss: 26.70151138305664, p_loss: -4.020908355712891, mean_rew: -7.8268893366549, variance: 4.233327865600586, lamda: 1.8639307022094727

steps: 999975, episodes: 40000, mean episode reward: -535.2355238256334, agent episode reward: [-178.41184127521115, -178.41184127521115, -178.41184127521115], time: 164.11
steps: 999975, episodes: 40000, mean episode variance: 93.55073818564415, agent episode variance: [1.169103759765625, 91.32657724761962, 1.0550571782588958], time: 164.11
Running avgs for agent 0: q_loss: 14.4082612991333, p_loss: -3.996535539627075, mean_rew: -7.822960983555375, variance: 4.676414966583252, lamda: 1.6973620653152466
Running avgs for agent 1: q_loss: 7171843.0, p_loss: 176.9993896484375, mean_rew: -7.819111803711701, variance: 365.3063089904785, lamda: 1.960943341255188
Running avgs for agent 2: q_loss: 30.767229080200195, p_loss: -4.019283294677734, mean_rew: -7.811440619737123, variance: 4.220228672027588, lamda: 1.8830302953720093

steps: 1024975, episodes: 41000, mean episode reward: -546.1949739406202, agent episode reward: [-182.06499131354008, -182.06499131354008, -182.06499131354008], time: 163.986
steps: 1024975, episodes: 41000, mean episode variance: 72.52818706929683, agent episode variance: [1.1558866473436356, 70.32611421966553, 1.046186202287674], time: 163.987
Running avgs for agent 0: q_loss: 15.826578140258789, p_loss: -3.995955228805542, mean_rew: -7.809414726183057, variance: 4.623547077178955, lamda: 1.7139530181884766
Running avgs for agent 1: q_loss: 6743208.0, p_loss: 176.6985626220703, mean_rew: -7.8001660984111, variance: 281.3044568786621, lamda: 1.9859474897384644
Running avgs for agent 2: q_loss: 31.280241012573242, p_loss: -4.017735958099365, mean_rew: -7.811551489719873, variance: 4.1847453117370605, lamda: 1.8995347023010254

steps: 1049975, episodes: 42000, mean episode reward: -535.4651340654126, agent episode reward: [-178.4883780218042, -178.4883780218042, -178.4883780218042], time: 164.192
steps: 1049975, episodes: 42000, mean episode variance: 91.36248539054394, agent episode variance: [1.15171617937088, 89.17673677062989, 1.0340324405431747], time: 164.193
Running avgs for agent 0: q_loss: 17.28472328186035, p_loss: -3.9815776348114014, mean_rew: -7.785037145096364, variance: 4.606864929199219, lamda: 1.7222126722335815
Running avgs for agent 1: q_loss: 7160103.0, p_loss: 176.5637664794922, mean_rew: -7.786047832749025, variance: 356.70694708251955, lamda: 2.0109386444091797
Running avgs for agent 2: q_loss: 30.02519989013672, p_loss: -3.991036891937256, mean_rew: -7.78202766543358, variance: 4.136129856109619, lamda: 1.9149013757705688

steps: 1074975, episodes: 43000, mean episode reward: -526.4802031062883, agent episode reward: [-175.49340103542946, -175.49340103542946, -175.49340103542946], time: 165.108
steps: 1074975, episodes: 43000, mean episode variance: 81.1274217274189, agent episode variance: [1.1255841083526612, 79.00693452262878, 0.9949030964374542], time: 165.109
Running avgs for agent 0: q_loss: 12.773978233337402, p_loss: -3.9219040870666504, mean_rew: -7.65368194806612, variance: 4.5023369789123535, lamda: 1.7364870309829712
Running avgs for agent 1: q_loss: 6599929.0, p_loss: 173.33306884765625, mean_rew: -7.65805685403495, variance: 316.02773809051513, lamda: 2.0359132289886475
Running avgs for agent 2: q_loss: 26.576160430908203, p_loss: -3.941404104232788, mean_rew: -7.652921857499396, variance: 3.979612350463867, lamda: 1.9264737367630005

steps: 1099975, episodes: 44000, mean episode reward: -529.3572402439269, agent episode reward: [-176.45241341464228, -176.45241341464228, -176.45241341464228], time: 162.495
steps: 1099975, episodes: 44000, mean episode variance: 65.52450186443329, agent episode variance: [1.0898409140110017, 63.446863518714906, 0.9877974317073822], time: 162.496
Running avgs for agent 0: q_loss: 9.984155654907227, p_loss: -3.8659393787384033, mean_rew: -7.533113267253002, variance: 4.359363079071045, lamda: 1.7496920824050903
Running avgs for agent 1: q_loss: 5570981.5, p_loss: 169.3460693359375, mean_rew: -7.536367191432699, variance: 253.78745407485962, lamda: 2.060887336730957
Running avgs for agent 2: q_loss: 23.78989028930664, p_loss: -3.8692123889923096, mean_rew: -7.5208884364347215, variance: 3.9511899948120117, lamda: 1.9347361326217651

steps: 1124975, episodes: 45000, mean episode reward: -521.6846847236365, agent episode reward: [-173.89489490787884, -173.89489490787884, -173.89489490787884], time: 163.68
steps: 1124975, episodes: 45000, mean episode variance: 62.326912622570994, agent episode variance: [1.0777427079677582, 60.28600967693329, 0.9631602376699447], time: 163.681
Running avgs for agent 0: q_loss: 10.260944366455078, p_loss: -3.800492763519287, mean_rew: -7.4104708056110296, variance: 4.310970783233643, lamda: 1.7610909938812256
Running avgs for agent 1: q_loss: 3923705.5, p_loss: 165.47486877441406, mean_rew: -7.416918565893133, variance: 241.14403870773316, lamda: 2.085861921310425
Running avgs for agent 2: q_loss: 20.94154930114746, p_loss: -3.8191637992858887, mean_rew: -7.413356829961685, variance: 3.8526411056518555, lamda: 1.941306471824646

steps: 1149975, episodes: 46000, mean episode reward: -511.5575558876177, agent episode reward: [-170.51918529587257, -170.51918529587257, -170.51918529587257], time: 162.882
steps: 1149975, episodes: 46000, mean episode variance: 59.63991933941841, agent episode variance: [1.0566318740844727, 57.6280007019043, 0.9552867634296417], time: 162.883
Running avgs for agent 0: q_loss: 8.439947128295898, p_loss: -3.7531700134277344, mean_rew: -7.315008944501227, variance: 4.226527214050293, lamda: 1.7701208591461182
Running avgs for agent 1: q_loss: 3134463.75, p_loss: 160.7838897705078, mean_rew: -7.317775755458603, variance: 230.5120028076172, lamda: 2.1108365058898926
Running avgs for agent 2: q_loss: 15.173097610473633, p_loss: -3.7759652137756348, mean_rew: -7.3160390989158985, variance: 3.8211469650268555, lamda: 1.9562277793884277

steps: 1174975, episodes: 47000, mean episode reward: -511.27231467683566, agent episode reward: [-170.42410489227856, -170.42410489227856, -170.42410489227856], time: 162.992
steps: 1174975, episodes: 47000, mean episode variance: 75.28347800970077, agent episode variance: [1.0367550961971284, 73.31272496032715, 0.9339979531764984], time: 162.993
Running avgs for agent 0: q_loss: 7.617002010345459, p_loss: -3.7149267196655273, mean_rew: -7.242221469999716, variance: 4.14702033996582, lamda: 1.7769314050674438
Running avgs for agent 1: q_loss: 3125551.0, p_loss: 157.4496612548828, mean_rew: -7.252500137587918, variance: 293.2508998413086, lamda: 2.135810613632202
Running avgs for agent 2: q_loss: 15.765478134155273, p_loss: -3.7394440174102783, mean_rew: -7.2414453390194415, variance: 3.7359917163848877, lamda: 1.9731130599975586

steps: 1199975, episodes: 48000, mean episode reward: -509.83883460206, agent episode reward: [-169.94627820068663, -169.94627820068663, -169.94627820068663], time: 158.947
steps: 1199975, episodes: 48000, mean episode variance: 69.0457627364397, agent episode variance: [1.029270206928253, 67.08102204895019, 0.9354704805612564], time: 158.947
Running avgs for agent 0: q_loss: 6.948573112487793, p_loss: -3.692667245864868, mean_rew: -7.200989891903433, variance: 4.117081165313721, lamda: 1.7830597162246704
Running avgs for agent 1: q_loss: 2668911.5, p_loss: 154.82015991210938, mean_rew: -7.198141287389584, variance: 268.32408819580075, lamda: 2.160784959793091
Running avgs for agent 2: q_loss: 16.845460891723633, p_loss: -3.7177505493164062, mean_rew: -7.204171425240348, variance: 3.741882085800171, lamda: 1.9775888919830322

steps: 1224975, episodes: 49000, mean episode reward: -511.5916951634714, agent episode reward: [-170.53056505449047, -170.53056505449047, -170.53056505449047], time: 162.09
steps: 1224975, episodes: 49000, mean episode variance: 64.45754471683502, agent episode variance: [1.0306815884113312, 62.495212829589846, 0.9316502988338471], time: 162.091
Running avgs for agent 0: q_loss: 6.833415985107422, p_loss: -3.6739211082458496, mean_rew: -7.163885056891041, variance: 4.1227264404296875, lamda: 1.785353422164917
Running avgs for agent 1: q_loss: 2460184.25, p_loss: 152.72572326660156, mean_rew: -7.1673663555097455, variance: 249.98085131835938, lamda: 2.1857593059539795
Running avgs for agent 2: q_loss: 14.834761619567871, p_loss: -3.6998584270477295, mean_rew: -7.168480227729784, variance: 3.7266011238098145, lamda: 1.9808053970336914

steps: 1249975, episodes: 50000, mean episode reward: -522.305909012334, agent episode reward: [-174.101969670778, -174.101969670778, -174.101969670778], time: 161.048
steps: 1249975, episodes: 50000, mean episode variance: 59.96962537050247, agent episode variance: [1.020282469034195, 58.034018417358396, 0.9153244841098785], time: 161.048
Running avgs for agent 0: q_loss: 7.05605936050415, p_loss: -3.6647188663482666, mean_rew: -7.144471192000218, variance: 4.081130027770996, lamda: 1.7904345989227295
Running avgs for agent 1: q_loss: 2245715.0, p_loss: 150.11138916015625, mean_rew: -7.140249230441618, variance: 232.13607366943359, lamda: 2.210733413696289
Running avgs for agent 2: q_loss: 10.383015632629395, p_loss: -3.6839637756347656, mean_rew: -7.139502716394539, variance: 3.6612977981567383, lamda: 1.9919074773788452

steps: 1274975, episodes: 51000, mean episode reward: -533.2815031971007, agent episode reward: [-177.76050106570025, -177.76050106570025, -177.76050106570025], time: 160.974
steps: 1274975, episodes: 51000, mean episode variance: 59.69403348445892, agent episode variance: [1.008047563791275, 57.77434588623047, 0.9116400344371796], time: 160.975
Running avgs for agent 0: q_loss: 9.975605010986328, p_loss: -3.6711173057556152, mean_rew: -7.123841291788279, variance: 4.032190322875977, lamda: 1.79351806640625
Running avgs for agent 1: q_loss: 2131323.25, p_loss: 147.7797088623047, mean_rew: -7.127247321826153, variance: 231.09738354492188, lamda: 2.235707998275757
Running avgs for agent 2: q_loss: 10.038124084472656, p_loss: -3.6786341667175293, mean_rew: -7.130814251628042, variance: 3.646559953689575, lamda: 2.000260353088379

steps: 1299975, episodes: 52000, mean episode reward: -541.4448781697429, agent episode reward: [-180.48162605658092, -180.48162605658092, -180.48162605658092], time: 159.825
steps: 1299975, episodes: 52000, mean episode variance: 54.15093146395683, agent episode variance: [1.0115382313728332, 52.23578882598877, 0.9036044065952301], time: 159.826
Running avgs for agent 0: q_loss: 9.463845252990723, p_loss: -3.663174867630005, mean_rew: -7.1185242795244195, variance: 4.0461530685424805, lamda: 1.7935194969177246
Running avgs for agent 1: q_loss: 2057175.75, p_loss: 146.0047149658203, mean_rew: -7.128632950376783, variance: 208.94315530395508, lamda: 2.2606823444366455
Running avgs for agent 2: q_loss: 9.60436725616455, p_loss: -3.6724627017974854, mean_rew: -7.12303832561017, variance: 3.614417552947998, lamda: 2.007658004760742

steps: 1324975, episodes: 53000, mean episode reward: -543.4263035125629, agent episode reward: [-181.1421011708543, -181.1421011708543, -181.1421011708543], time: 160.271
steps: 1324975, episodes: 53000, mean episode variance: 55.070069494962695, agent episode variance: [1.0098062794208527, 53.15917958068847, 0.9010836348533631], time: 160.272
Running avgs for agent 0: q_loss: 9.59507942199707, p_loss: -3.660184621810913, mean_rew: -7.113931855925073, variance: 4.039225101470947, lamda: 1.793546438217163
Running avgs for agent 1: q_loss: 1946133.25, p_loss: 143.73080444335938, mean_rew: -7.122296998859583, variance: 212.6367183227539, lamda: 2.285656452178955
Running avgs for agent 2: q_loss: 9.26435375213623, p_loss: -3.671661138534546, mean_rew: -7.112593950511679, variance: 3.6043343544006348, lamda: 2.01324462890625

steps: 1349975, episodes: 54000, mean episode reward: -543.3168621579923, agent episode reward: [-181.10562071933074, -181.10562071933074, -181.10562071933074], time: 162.501
steps: 1349975, episodes: 54000, mean episode variance: 54.26273620200157, agent episode variance: [1.0008154928684234, 52.36819140625, 0.8937293028831482], time: 162.501
Running avgs for agent 0: q_loss: 9.4126615524292, p_loss: -3.6591618061065674, mean_rew: -7.11080737596136, variance: 4.003261566162109, lamda: 1.7935609817504883
Running avgs for agent 1: q_loss: 1756712.875, p_loss: 141.50685119628906, mean_rew: -7.115234618425567, variance: 209.472765625, lamda: 2.310631036758423
Running avgs for agent 2: q_loss: 9.22803020477295, p_loss: -3.6721274852752686, mean_rew: -7.114478891247283, variance: 3.5749173164367676, lamda: 2.0172338485717773

steps: 1374975, episodes: 55000, mean episode reward: -539.1713577066415, agent episode reward: [-179.72378590221382, -179.72378590221382, -179.72378590221382], time: 168.958
steps: 1374975, episodes: 55000, mean episode variance: 49.65706892609596, agent episode variance: [1.0156977393627167, 47.74235820007324, 0.8990129866600036], time: 168.959
Running avgs for agent 0: q_loss: 8.928193092346191, p_loss: -3.648287057876587, mean_rew: -7.113454406262696, variance: 4.062790870666504, lamda: 1.7935609817504883
Running avgs for agent 1: q_loss: 1663628.75, p_loss: 139.2349395751953, mean_rew: -7.1080723108695265, variance: 190.96943280029296, lamda: 2.3356053829193115
Running avgs for agent 2: q_loss: 8.89893913269043, p_loss: -3.668471574783325, mean_rew: -7.112334582076025, variance: 3.5960519313812256, lamda: 2.021632671356201

steps: 1399975, episodes: 56000, mean episode reward: -545.2858031188708, agent episode reward: [-181.76193437295697, -181.76193437295697, -181.76193437295697], time: 162.132
steps: 1399975, episodes: 56000, mean episode variance: 50.48962546420098, agent episode variance: [1.0103596866130828, 48.580072311401366, 0.8991934661865234], time: 162.133
Running avgs for agent 0: q_loss: 8.945899963378906, p_loss: -3.651071548461914, mean_rew: -7.101483012663031, variance: 4.041438579559326, lamda: 1.7935608625411987
Running avgs for agent 1: q_loss: 1531690.75, p_loss: 137.19760131835938, mean_rew: -7.104325770825599, variance: 194.32028924560547, lamda: 2.360579490661621
Running avgs for agent 2: q_loss: 8.400361061096191, p_loss: -3.666107177734375, mean_rew: -7.0996046911412, variance: 3.596773624420166, lamda: 2.0250415802001953

steps: 1424975, episodes: 57000, mean episode reward: -548.9126277739063, agent episode reward: [-182.97087592463538, -182.97087592463538, -182.97087592463538], time: 158.169
steps: 1424975, episodes: 57000, mean episode variance: 47.979930217385295, agent episode variance: [1.004475082874298, 46.08788844299316, 0.8875666915178299], time: 158.17
Running avgs for agent 0: q_loss: 8.746513366699219, p_loss: -3.648620128631592, mean_rew: -7.099483496153608, variance: 4.017900466918945, lamda: 1.7935609817504883
Running avgs for agent 1: q_loss: 1475797.875, p_loss: 135.0875244140625, mean_rew: -7.1024623368532, variance: 184.35155377197265, lamda: 2.385554313659668
Running avgs for agent 2: q_loss: 14.217944145202637, p_loss: -3.669252395629883, mean_rew: -7.100223477709351, variance: 3.550266742706299, lamda: 2.027907371520996

steps: 1449975, episodes: 58000, mean episode reward: -548.5313531252773, agent episode reward: [-182.8437843750924, -182.8437843750924, -182.8437843750924], time: 169.882
steps: 1449975, episodes: 58000, mean episode variance: 46.286014274954795, agent episode variance: [1.010669054031372, 44.37982186126709, 0.8955233596563339], time: 169.883
Running avgs for agent 0: q_loss: 8.35004711151123, p_loss: -3.645374059677124, mean_rew: -7.097326178660914, variance: 4.0426764488220215, lamda: 1.7935609817504883
Running avgs for agent 1: q_loss: 1411128.75, p_loss: 133.9330596923828, mean_rew: -7.096460962726743, variance: 177.51928744506836, lamda: 2.4105284214019775
Running avgs for agent 2: q_loss: 13.43693733215332, p_loss: -3.654855489730835, mean_rew: -7.095394810832115, variance: 3.5820934772491455, lamda: 2.0281548500061035

steps: 1474975, episodes: 59000, mean episode reward: -549.3243727679594, agent episode reward: [-183.10812425598647, -183.10812425598647, -183.10812425598647], time: 176.068
steps: 1474975, episodes: 59000, mean episode variance: 46.69694560158253, agent episode variance: [1.0014066939353943, 44.81313282775879, 0.8824060798883439], time: 176.068
Running avgs for agent 0: q_loss: 8.230481147766113, p_loss: -3.648930549621582, mean_rew: -7.09907750235538, variance: 4.005627155303955, lamda: 1.7935607433319092
Running avgs for agent 1: q_loss: 1336981.5, p_loss: 133.04254150390625, mean_rew: -7.087297786092334, variance: 179.25253131103517, lamda: 2.435502767562866
Running avgs for agent 2: q_loss: 13.101726531982422, p_loss: -3.662611246109009, mean_rew: -7.089425779532817, variance: 3.5296242237091064, lamda: 2.028266668319702

steps: 1499975, episodes: 60000, mean episode reward: -547.211075984591, agent episode reward: [-182.4036919948637, -182.4036919948637, -182.4036919948637], time: 173.266
steps: 1499975, episodes: 60000, mean episode variance: 46.630764359474185, agent episode variance: [1.0055637905597687, 44.740392807006835, 0.8848077619075775], time: 173.267
Running avgs for agent 0: q_loss: 8.05982494354248, p_loss: -3.6469876766204834, mean_rew: -7.099176812837147, variance: 4.0222554206848145, lamda: 1.7935609817504883
Running avgs for agent 1: q_loss: 1291959.5, p_loss: 132.63418579101562, mean_rew: -7.10392135642401, variance: 178.96157122802734, lamda: 2.460477113723755
Running avgs for agent 2: q_loss: 12.593127250671387, p_loss: -3.6623547077178955, mean_rew: -7.093742377046973, variance: 3.539231061935425, lamda: 2.028337240219116

steps: 1524975, episodes: 61000, mean episode reward: -549.9933440771365, agent episode reward: [-183.33111469237878, -183.33111469237878, -183.33111469237878], time: 173.47
steps: 1524975, episodes: 61000, mean episode variance: 43.16291421079636, agent episode variance: [1.0100886991024018, 41.25606741333008, 0.8967580983638763], time: 173.47
Running avgs for agent 0: q_loss: 7.97368049621582, p_loss: -3.6485557556152344, mean_rew: -7.103071263852037, variance: 4.040355205535889, lamda: 1.7935608625411987
Running avgs for agent 1: q_loss: 1301548.625, p_loss: 132.09288024902344, mean_rew: -7.096837939019, variance: 165.02426965332032, lamda: 2.4854514598846436
Running avgs for agent 2: q_loss: 12.69740104675293, p_loss: -3.6556756496429443, mean_rew: -7.097941250410024, variance: 3.5870323181152344, lamda: 2.028599262237549

steps: 1549975, episodes: 62000, mean episode reward: -545.1656059237075, agent episode reward: [-181.7218686412358, -181.7218686412358, -181.7218686412358], time: 169.706
steps: 1549975, episodes: 62000, mean episode variance: 43.68574113631249, agent episode variance: [1.0118433675765992, 41.78387257385254, 0.8900251948833465], time: 169.706
Running avgs for agent 0: q_loss: 7.5474677085876465, p_loss: -3.6405375003814697, mean_rew: -7.104919053289751, variance: 4.0473737716674805, lamda: 1.7935702800750732
Running avgs for agent 1: q_loss: 1267243.25, p_loss: 131.56146240234375, mean_rew: -7.105196063250614, variance: 167.13549029541016, lamda: 2.5104258060455322
Running avgs for agent 2: q_loss: 11.943131446838379, p_loss: -3.6725451946258545, mean_rew: -7.107319686788423, variance: 3.560100793838501, lamda: 2.028930902481079

steps: 1574975, episodes: 63000, mean episode reward: -547.372328676718, agent episode reward: [-182.4574428922393, -182.4574428922393, -182.4574428922393], time: 166.187
steps: 1574975, episodes: 63000, mean episode variance: 43.244834911465645, agent episode variance: [1.003498696565628, 41.35623249816894, 0.8851037167310715], time: 166.187
Running avgs for agent 0: q_loss: 7.495991230010986, p_loss: -3.654853105545044, mean_rew: -7.115189779895535, variance: 4.0139946937561035, lamda: 1.7935830354690552
Running avgs for agent 1: q_loss: 1229979.25, p_loss: 131.3166961669922, mean_rew: -7.111598463371906, variance: 165.42492999267577, lamda: 2.535400152206421
Running avgs for agent 2: q_loss: 11.973628997802734, p_loss: -3.6766791343688965, mean_rew: -7.108604474371775, variance: 3.540415048599243, lamda: 2.0289411544799805

steps: 1599975, episodes: 64000, mean episode reward: -552.2189470761641, agent episode reward: [-184.0729823587214, -184.0729823587214, -184.0729823587214], time: 177.866
steps: 1599975, episodes: 64000, mean episode variance: 42.15850501489639, agent episode variance: [1.015614283323288, 40.24761701965332, 0.8952737119197846], time: 177.866
Running avgs for agent 0: q_loss: 7.480172634124756, p_loss: -3.655094861984253, mean_rew: -7.124377625611303, variance: 4.062457084655762, lamda: 1.7935829162597656
Running avgs for agent 1: q_loss: 1225558.125, p_loss: 131.437255859375, mean_rew: -7.124730394736556, variance: 160.99046807861328, lamda: 2.5603744983673096
Running avgs for agent 2: q_loss: 11.602469444274902, p_loss: -3.678530216217041, mean_rew: -7.12077420910727, variance: 3.581094741821289, lamda: 2.0290918350219727

steps: 1624975, episodes: 65000, mean episode reward: -552.6434732038914, agent episode reward: [-184.2144910679638, -184.2144910679638, -184.2144910679638], time: 174.649
steps: 1624975, episodes: 65000, mean episode variance: 42.79193208193779, agent episode variance: [1.020015997171402, 40.87644676208496, 0.895469322681427], time: 174.649
Running avgs for agent 0: q_loss: 7.37449312210083, p_loss: -3.655649423599243, mean_rew: -7.13698787748317, variance: 4.080063819885254, lamda: 1.793582797050476
Running avgs for agent 1: q_loss: 1195523.375, p_loss: 131.11814880371094, mean_rew: -7.128018101456851, variance: 163.50578704833984, lamda: 2.585348606109619
Running avgs for agent 2: q_loss: 11.42440128326416, p_loss: -3.68044114112854, mean_rew: -7.127419180807206, variance: 3.5818772315979004, lamda: 2.029092311859131

steps: 1649975, episodes: 66000, mean episode reward: -549.0748544322781, agent episode reward: [-183.024951477426, -183.024951477426, -183.024951477426], time: 173.573
steps: 1649975, episodes: 66000, mean episode variance: 40.16555623805523, agent episode variance: [1.0096577713489532, 38.26068824768066, 0.8952102190256119], time: 173.573
Running avgs for agent 0: q_loss: 7.149766445159912, p_loss: -3.6614060401916504, mean_rew: -7.130850980476704, variance: 4.038630962371826, lamda: 1.7935830354690552
Running avgs for agent 1: q_loss: 1185523.875, p_loss: 131.08285522460938, mean_rew: -7.137374869169487, variance: 153.04275299072265, lamda: 2.610323190689087
Running avgs for agent 2: q_loss: 11.3377103805542, p_loss: -3.6815149784088135, mean_rew: -7.130710031310076, variance: 3.580840826034546, lamda: 2.02909255027771

steps: 1674975, episodes: 67000, mean episode reward: -557.2667102658148, agent episode reward: [-185.75557008860494, -185.75557008860494, -185.75557008860494], time: 168.734
steps: 1674975, episodes: 67000, mean episode variance: 40.596315341472625, agent episode variance: [1.0175432579517365, 38.68262149810791, 0.8961505854129791], time: 168.735
Running avgs for agent 0: q_loss: 6.841202259063721, p_loss: -3.655086040496826, mean_rew: -7.132358427280931, variance: 4.070173263549805, lamda: 1.7935829162597656
Running avgs for agent 1: q_loss: 1106706.125, p_loss: 130.8533172607422, mean_rew: -7.141064311206172, variance: 154.73048599243165, lamda: 2.6352972984313965
Running avgs for agent 2: q_loss: 10.788822174072266, p_loss: -3.6820151805877686, mean_rew: -7.1386988590584775, variance: 3.5846025943756104, lamda: 2.029092311859131

steps: 1699975, episodes: 68000, mean episode reward: -552.2988576321005, agent episode reward: [-184.09961921070015, -184.09961921070015, -184.09961921070015], time: 164.801
steps: 1699975, episodes: 68000, mean episode variance: 41.04090998315811, agent episode variance: [1.0191516330242156, 39.12392280578613, 0.897835544347763], time: 164.801
Running avgs for agent 0: q_loss: 6.857264518737793, p_loss: -3.659151792526245, mean_rew: -7.139620172899247, variance: 4.076606273651123, lamda: 1.793582797050476
Running avgs for agent 1: q_loss: 1064672.75, p_loss: 130.7064666748047, mean_rew: -7.142260743982933, variance: 156.49569122314452, lamda: 2.6602718830108643
Running avgs for agent 2: q_loss: 10.576643943786621, p_loss: -3.6820409297943115, mean_rew: -7.14235471536144, variance: 3.5913422107696533, lamda: 2.029092311859131

steps: 1724975, episodes: 69000, mean episode reward: -555.6054623603394, agent episode reward: [-185.2018207867798, -185.2018207867798, -185.2018207867798], time: 161.796
steps: 1724975, episodes: 69000, mean episode variance: 39.86174258410931, agent episode variance: [1.0144240226745604, 37.95237471008301, 0.8949438513517379], time: 161.796
Running avgs for agent 0: q_loss: 6.791602611541748, p_loss: -3.6677074432373047, mean_rew: -7.156925917909909, variance: 4.0576958656311035, lamda: 1.793582797050476
Running avgs for agent 1: q_loss: 1036491.875, p_loss: 130.5184326171875, mean_rew: -7.1514162839742745, variance: 151.80949884033203, lamda: 2.685245990753174
Running avgs for agent 2: q_loss: 10.473129272460938, p_loss: -3.6909844875335693, mean_rew: -7.1528992020442415, variance: 3.579775333404541, lamda: 2.02909255027771

steps: 1749975, episodes: 70000, mean episode reward: -556.3436306490474, agent episode reward: [-185.4478768830158, -185.4478768830158, -185.4478768830158], time: 164.556
steps: 1749975, episodes: 70000, mean episode variance: 39.554060881495474, agent episode variance: [1.0207621407508851, 37.63103732299805, 0.9022614177465439], time: 164.557
Running avgs for agent 0: q_loss: 6.728483200073242, p_loss: -3.6780571937561035, mean_rew: -7.167420337636894, variance: 4.083048343658447, lamda: 1.7936429977416992
Running avgs for agent 1: q_loss: 1006596.75, p_loss: 130.36880493164062, mean_rew: -7.161674099665552, variance: 150.5241492919922, lamda: 2.7102208137512207
Running avgs for agent 2: q_loss: 10.387327194213867, p_loss: -3.6906046867370605, mean_rew: -7.159019900523941, variance: 3.6090455055236816, lamda: 2.029092311859131

steps: 1774975, episodes: 71000, mean episode reward: -555.9815369970743, agent episode reward: [-185.32717899902474, -185.32717899902474, -185.32717899902474], time: 159.443
steps: 1774975, episodes: 71000, mean episode variance: 39.129967665672304, agent episode variance: [1.0249957840442658, 37.20449284362793, 0.9004790380001068], time: 159.444
Running avgs for agent 0: q_loss: 6.605569362640381, p_loss: -3.6775944232940674, mean_rew: -7.177724818360156, variance: 4.099983215332031, lamda: 1.7936654090881348
Running avgs for agent 1: q_loss: 1010122.4375, p_loss: 130.3433837890625, mean_rew: -7.178592210002713, variance: 148.81797137451173, lamda: 2.7351949214935303
Running avgs for agent 2: q_loss: 10.246511459350586, p_loss: -3.7077629566192627, mean_rew: -7.181458354844073, variance: 3.6019160747528076, lamda: 2.029092311859131

steps: 1799975, episodes: 72000, mean episode reward: -558.294990141649, agent episode reward: [-186.0983300472164, -186.0983300472164, -186.0983300472164], time: 157.635
steps: 1799975, episodes: 72000, mean episode variance: 38.93451758635044, agent episode variance: [1.0210370688438415, 37.01773829650879, 0.8957422209978103], time: 157.636
Running avgs for agent 0: q_loss: 6.658796787261963, p_loss: -3.688399314880371, mean_rew: -7.190238354969029, variance: 4.084148406982422, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 989480.4375, p_loss: 130.48158264160156, mean_rew: -7.190138011742741, variance: 148.07095318603515, lamda: 2.760169267654419
Running avgs for agent 2: q_loss: 10.243912696838379, p_loss: -3.714735746383667, mean_rew: -7.196812499481928, variance: 3.5829687118530273, lamda: 2.02909255027771

steps: 1824975, episodes: 73000, mean episode reward: -556.4854542578145, agent episode reward: [-185.49515141927148, -185.49515141927148, -185.49515141927148], time: 160.695
steps: 1824975, episodes: 73000, mean episode variance: 37.91341688477993, agent episode variance: [1.0265031960010529, 35.98657261657715, 0.9003410722017289], time: 160.695
Running avgs for agent 0: q_loss: 6.499155044555664, p_loss: -3.6879191398620605, mean_rew: -7.194031611642551, variance: 4.10601282119751, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 993800.5625, p_loss: 130.56442260742188, mean_rew: -7.198025259089975, variance: 143.9462904663086, lamda: 2.7851436138153076
Running avgs for agent 2: q_loss: 10.098461151123047, p_loss: -3.712258815765381, mean_rew: -7.200841411378355, variance: 3.6013643741607666, lamda: 2.029092311859131

steps: 1849975, episodes: 74000, mean episode reward: -555.7010410405884, agent episode reward: [-185.2336803468628, -185.2336803468628, -185.2336803468628], time: 159.581
steps: 1849975, episodes: 74000, mean episode variance: 39.090036523342135, agent episode variance: [1.0273525736331939, 37.15869378662109, 0.9039901630878449], time: 159.581
Running avgs for agent 0: q_loss: 6.412167072296143, p_loss: -3.6869194507598877, mean_rew: -7.197219732956854, variance: 4.109410285949707, lamda: 1.7936654090881348
Running avgs for agent 1: q_loss: 991010.625, p_loss: 130.76577758789062, mean_rew: -7.208935644708277, variance: 148.63477514648437, lamda: 2.8101179599761963
Running avgs for agent 2: q_loss: 10.220684051513672, p_loss: -3.713043689727783, mean_rew: -7.210761891837058, variance: 3.6159603595733643, lamda: 2.029092311859131

steps: 1874975, episodes: 75000, mean episode reward: -555.434354870412, agent episode reward: [-185.144784956804, -185.144784956804, -185.144784956804], time: 163.695
steps: 1874975, episodes: 75000, mean episode variance: 37.804637479543686, agent episode variance: [1.0268623144626619, 35.87294609069824, 0.9048290743827819], time: 163.695
Running avgs for agent 0: q_loss: 6.4444193840026855, p_loss: -3.6967294216156006, mean_rew: -7.217330144396615, variance: 4.107449054718018, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 994727.0, p_loss: 131.06158447265625, mean_rew: -7.214513251085753, variance: 143.49178436279297, lamda: 2.835092306137085
Running avgs for agent 2: q_loss: 9.947304725646973, p_loss: -3.715576410293579, mean_rew: -7.214804080319834, variance: 3.619316339492798, lamda: 2.02909255027771

steps: 1899975, episodes: 76000, mean episode reward: -559.1426503483858, agent episode reward: [-186.38088344946192, -186.38088344946192, -186.38088344946192], time: 164.239
steps: 1899975, episodes: 76000, mean episode variance: 36.59024792742729, agent episode variance: [1.0328391189575195, 34.647759994506835, 0.9096488139629364], time: 164.24
Running avgs for agent 0: q_loss: 6.370001316070557, p_loss: -3.708000421524048, mean_rew: -7.227451867876781, variance: 4.131356239318848, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 991142.5, p_loss: 131.44906616210938, mean_rew: -7.227360037080258, variance: 138.59103997802734, lamda: 2.8600664138793945
Running avgs for agent 2: q_loss: 9.911542892456055, p_loss: -3.7178378105163574, mean_rew: -7.225170536094436, variance: 3.6385951042175293, lamda: 2.0291550159454346

steps: 1924975, episodes: 77000, mean episode reward: -558.6469006205956, agent episode reward: [-186.21563354019855, -186.21563354019855, -186.21563354019855], time: 178.664
steps: 1924975, episodes: 77000, mean episode variance: 37.72352348518372, agent episode variance: [1.0347257163524628, 35.781354553222656, 0.9074432156085968], time: 178.664
Running avgs for agent 0: q_loss: 6.207152366638184, p_loss: -3.70740008354187, mean_rew: -7.232281256023301, variance: 4.13890266418457, lamda: 1.7936654090881348
Running avgs for agent 1: q_loss: 984651.4375, p_loss: 131.93740844726562, mean_rew: -7.2375750355246335, variance: 143.12541821289062, lamda: 2.8850409984588623
Running avgs for agent 2: q_loss: 9.904376983642578, p_loss: -3.7270333766937256, mean_rew: -7.234953021854697, variance: 3.629772901535034, lamda: 2.0291588306427

steps: 1949975, episodes: 78000, mean episode reward: -552.3801368175708, agent episode reward: [-184.1267122725236, -184.1267122725236, -184.1267122725236], time: 176.965
steps: 1949975, episodes: 78000, mean episode variance: 36.056113485336304, agent episode variance: [1.031558908700943, 34.11273345947266, 0.9118211171627044], time: 176.965
Running avgs for agent 0: q_loss: 6.126809597015381, p_loss: -3.7060909271240234, mean_rew: -7.239259237844279, variance: 4.126235485076904, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 1014506.6875, p_loss: 132.58489990234375, mean_rew: -7.238924436494395, variance: 136.45093383789063, lamda: 2.910015106201172
Running avgs for agent 2: q_loss: 9.690645217895508, p_loss: -3.724383592605591, mean_rew: -7.245002803183633, variance: 3.6472842693328857, lamda: 2.0291588306427

steps: 1974975, episodes: 79000, mean episode reward: -567.5824488199441, agent episode reward: [-189.1941496066481, -189.1941496066481, -189.1941496066481], time: 174.577
steps: 1974975, episodes: 79000, mean episode variance: 36.187282721042635, agent episode variance: [1.0358303215503692, 34.23026368713379, 0.9211887123584748], time: 174.578
Running avgs for agent 0: q_loss: 5.9828877449035645, p_loss: -3.7161006927490234, mean_rew: -7.251526552525125, variance: 4.143321514129639, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 1028161.4375, p_loss: 133.270263671875, mean_rew: -7.256069696555099, variance: 136.92105474853517, lamda: 2.9349896907806396
Running avgs for agent 2: q_loss: 9.699544906616211, p_loss: -3.7228643894195557, mean_rew: -7.247572317223233, variance: 3.6847548484802246, lamda: 2.0291588306427

steps: 1999975, episodes: 80000, mean episode reward: -558.5416338041315, agent episode reward: [-186.1805446013772, -186.1805446013772, -186.1805446013772], time: 175.684
steps: 1999975, episodes: 80000, mean episode variance: 37.04983239352703, agent episode variance: [1.0356194353103638, 35.106760330200196, 0.9074526280164719], time: 175.685
Running avgs for agent 0: q_loss: 6.007679462432861, p_loss: -3.7168407440185547, mean_rew: -7.254343249657532, variance: 4.142477512359619, lamda: 1.7936654090881348
Running avgs for agent 1: q_loss: 1025178.125, p_loss: 133.6364288330078, mean_rew: -7.249999763653957, variance: 140.42704132080078, lamda: 2.9599640369415283
Running avgs for agent 2: q_loss: 9.642648696899414, p_loss: -3.7371346950531006, mean_rew: -7.2547246588888274, variance: 3.6298105716705322, lamda: 2.0291588306427

steps: 2024975, episodes: 81000, mean episode reward: -559.8852018897056, agent episode reward: [-186.62840062990185, -186.62840062990185, -186.62840062990185], time: 176.735
steps: 2024975, episodes: 81000, mean episode variance: 36.51133462524414, agent episode variance: [1.0337920670509337, 34.56537649536133, 0.9121660628318786], time: 176.736
Running avgs for agent 0: q_loss: 5.835108280181885, p_loss: -3.7145638465881348, mean_rew: -7.256643964483596, variance: 4.135168075561523, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 1032565.625, p_loss: 134.07708740234375, mean_rew: -7.266460000092884, variance: 138.2615059814453, lamda: 2.984938144683838
Running avgs for agent 2: q_loss: 9.473835945129395, p_loss: -3.736706018447876, mean_rew: -7.267066923350775, variance: 3.6486642360687256, lamda: 2.0291588306427

steps: 2049975, episodes: 82000, mean episode reward: -557.3451929145667, agent episode reward: [-185.78173097152222, -185.78173097152222, -185.78173097152222], time: 176.309
steps: 2049975, episodes: 82000, mean episode variance: 36.58308099579811, agent episode variance: [1.040391451358795, 34.6233358001709, 0.9193537442684173], time: 176.31
Running avgs for agent 0: q_loss: 5.89584493637085, p_loss: -3.721686601638794, mean_rew: -7.26958343837187, variance: 4.161565780639648, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 1058556.375, p_loss: 134.6277313232422, mean_rew: -7.268528709557837, variance: 138.4933432006836, lamda: 3.0099127292633057
Running avgs for agent 2: q_loss: 9.414015769958496, p_loss: -3.7348101139068604, mean_rew: -7.2676398007817555, variance: 3.677415132522583, lamda: 2.0291588306427

steps: 2074975, episodes: 83000, mean episode reward: -561.5008759011072, agent episode reward: [-187.16695863370245, -187.16695863370245, -187.16695863370245], time: 172.456
steps: 2074975, episodes: 83000, mean episode variance: 37.34138475251198, agent episode variance: [1.044100357055664, 35.38309934997559, 0.9141850454807281], time: 172.456
Running avgs for agent 0: q_loss: 5.854798316955566, p_loss: -3.7248432636260986, mean_rew: -7.279765402316751, variance: 4.176401138305664, lamda: 1.7936654090881348
Running avgs for agent 1: q_loss: 1046574.125, p_loss: 134.99935913085938, mean_rew: -7.277254693858494, variance: 141.53239739990235, lamda: 3.0348870754241943
Running avgs for agent 2: q_loss: 9.31674575805664, p_loss: -3.744863986968994, mean_rew: -7.279508184055018, variance: 3.656740188598633, lamda: 2.0291588306427

steps: 2099975, episodes: 84000, mean episode reward: -559.6184978678467, agent episode reward: [-186.53949928928225, -186.53949928928225, -186.53949928928225], time: 174.2
steps: 2099975, episodes: 84000, mean episode variance: 36.272609395265576, agent episode variance: [1.0379951510429382, 34.313647148132326, 0.9209670960903168], time: 174.2
Running avgs for agent 0: q_loss: 5.848016262054443, p_loss: -3.731318950653076, mean_rew: -7.289853947084485, variance: 4.151980400085449, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 1068244.875, p_loss: 135.44281005859375, mean_rew: -7.291599147572589, variance: 137.2545885925293, lamda: 3.059861421585083
Running avgs for agent 2: q_loss: 9.34886360168457, p_loss: -3.7492001056671143, mean_rew: -7.288176822377949, variance: 3.683868169784546, lamda: 2.0291588306427

steps: 2124975, episodes: 85000, mean episode reward: -564.6157213779943, agent episode reward: [-188.20524045933143, -188.20524045933143, -188.20524045933143], time: 176.566
steps: 2124975, episodes: 85000, mean episode variance: 37.13939430689812, agent episode variance: [1.0440260064601898, 35.17556730651855, 0.9198009939193725], time: 176.566
Running avgs for agent 0: q_loss: 5.9276123046875, p_loss: -3.740089178085327, mean_rew: -7.303373510892187, variance: 4.1761040687561035, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 1071401.75, p_loss: 135.70822143554688, mean_rew: -7.305136919055805, variance: 140.7022692260742, lamda: 3.084836006164551
Running avgs for agent 2: q_loss: 9.253205299377441, p_loss: -3.7572109699249268, mean_rew: -7.299698475251208, variance: 3.679203987121582, lamda: 2.0291588306427

steps: 2149975, episodes: 86000, mean episode reward: -557.9910537459037, agent episode reward: [-185.99701791530126, -185.99701791530126, -185.99701791530126], time: 172.751
steps: 2149975, episodes: 86000, mean episode variance: 36.25156206202507, agent episode variance: [1.0460488295555115, 34.27783512878418, 0.9276781036853791], time: 172.752
Running avgs for agent 0: q_loss: 5.964329242706299, p_loss: -3.747959613800049, mean_rew: -7.319594432993632, variance: 4.184195518493652, lamda: 1.7936654090881348
Running avgs for agent 1: q_loss: 1076222.5, p_loss: 136.11512756347656, mean_rew: -7.322709854724048, variance: 137.11134051513673, lamda: 3.1098101139068604
Running avgs for agent 2: q_loss: 9.177732467651367, p_loss: -3.759932518005371, mean_rew: -7.3243461019460065, variance: 3.710712432861328, lamda: 2.0291588306427

steps: 2174975, episodes: 87000, mean episode reward: -561.4466113796769, agent episode reward: [-187.14887045989227, -187.14887045989227, -187.14887045989227], time: 174.428
steps: 2174975, episodes: 87000, mean episode variance: 37.16371242880821, agent episode variance: [1.0441327390670776, 35.189716835021976, 0.929862854719162], time: 174.429
Running avgs for agent 0: q_loss: 5.9184489250183105, p_loss: -3.7566802501678467, mean_rew: -7.330358008638149, variance: 4.1765313148498535, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 1088226.0, p_loss: 136.5344696044922, mean_rew: -7.333640384756003, variance: 140.7588673400879, lamda: 3.134784460067749
Running avgs for agent 2: q_loss: 9.111780166625977, p_loss: -3.7697880268096924, mean_rew: -7.335800742369685, variance: 3.7194511890411377, lamda: 2.0291588306427

steps: 2199975, episodes: 88000, mean episode reward: -558.2956878358048, agent episode reward: [-186.09856261193497, -186.09856261193497, -186.09856261193497], time: 173.074
steps: 2199975, episodes: 88000, mean episode variance: 36.05434839820862, agent episode variance: [1.0550755054950713, 34.073028274536135, 0.926244618177414], time: 173.075
Running avgs for agent 0: q_loss: 5.8032732009887695, p_loss: -3.7626748085021973, mean_rew: -7.349926575738657, variance: 4.220301628112793, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 1096174.125, p_loss: 137.0203399658203, mean_rew: -7.34819132213422, variance: 136.29211309814454, lamda: 3.1597588062286377
Running avgs for agent 2: q_loss: 9.167474746704102, p_loss: -3.779109477996826, mean_rew: -7.348208063407828, variance: 3.7049784660339355, lamda: 2.0291588306427

steps: 2224975, episodes: 89000, mean episode reward: -563.1040614174995, agent episode reward: [-187.70135380583315, -187.70135380583315, -187.70135380583315], time: 173.995
steps: 2224975, episodes: 89000, mean episode variance: 36.93569058537483, agent episode variance: [1.057457813501358, 34.94969542694092, 0.9285373449325561], time: 173.995
Running avgs for agent 0: q_loss: 5.937048435211182, p_loss: -3.780334234237671, mean_rew: -7.374226790636681, variance: 4.229831218719482, lamda: 1.7936654090881348
Running avgs for agent 1: q_loss: 1114929.25, p_loss: 137.3980255126953, mean_rew: -7.372184578127709, variance: 139.79878170776368, lamda: 3.1847329139709473
Running avgs for agent 2: q_loss: 9.188802719116211, p_loss: -3.786064386367798, mean_rew: -7.363069027396899, variance: 3.7141494750976562, lamda: 2.0291588306427

steps: 2249975, episodes: 90000, mean episode reward: -562.0372843312128, agent episode reward: [-187.34576144373762, -187.34576144373762, -187.34576144373762], time: 173.664
steps: 2249975, episodes: 90000, mean episode variance: 37.27098722100258, agent episode variance: [1.0523891401290895, 35.28444761657715, 0.934150464296341], time: 173.665
Running avgs for agent 0: q_loss: 5.8590779304504395, p_loss: -3.7783331871032715, mean_rew: -7.377200829711527, variance: 4.209556579589844, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 1125010.25, p_loss: 137.66455078125, mean_rew: -7.383255536484213, variance: 141.1377904663086, lamda: 3.209707498550415
Running avgs for agent 2: q_loss: 9.169336318969727, p_loss: -3.7951877117156982, mean_rew: -7.38237733245129, variance: 3.7366020679473877, lamda: 2.0291588306427

steps: 2274975, episodes: 91000, mean episode reward: -560.84637834054, agent episode reward: [-186.94879278017996, -186.94879278017996, -186.94879278017996], time: 176.161
steps: 2274975, episodes: 91000, mean episode variance: 36.082928469896316, agent episode variance: [1.0635041241645813, 34.07796762084961, 0.9414567248821258], time: 176.161
Running avgs for agent 0: q_loss: 5.7850799560546875, p_loss: -3.782654285430908, mean_rew: -7.39168132124832, variance: 4.254016399383545, lamda: 1.7936655282974243
Running avgs for agent 1: q_loss: 1130411.75, p_loss: 138.0547332763672, mean_rew: -7.390990550883331, variance: 136.31187048339845, lamda: 3.2346818447113037
Running avgs for agent 2: q_loss: 9.085806846618652, p_loss: -3.79599666595459, mean_rew: -7.390168506124815, variance: 3.765826940536499, lamda: 2.0291588306427

steps: 2299975, episodes: 92000, mean episode reward: -558.0949181842196, agent episode reward: [-186.0316393947399, -186.0316393947399, -186.0316393947399], time: 173.798
steps: 2299975, episodes: 92000, mean episode variance: 36.920749236822125, agent episode variance: [1.0606062746047973, 34.929511642456056, 0.9306313197612762], time: 173.798
Running avgs for agent 0: q_loss: 5.838443756103516, p_loss: -3.799769401550293, mean_rew: -7.405744203944883, variance: 4.242424964904785, lamda: 1.7937383651733398
Running avgs for agent 1: q_loss: 1139801.0, p_loss: 138.30409240722656, mean_rew: -7.40673211377581, variance: 139.71804656982422, lamda: 3.2596559524536133
Running avgs for agent 2: q_loss: 8.478411674499512, p_loss: -3.8072071075439453, mean_rew: -7.400109204997914, variance: 3.7225253582000732, lamda: 2.029407024383545

steps: 2324975, episodes: 93000, mean episode reward: -560.8181635051643, agent episode reward: [-186.9393878350548, -186.9393878350548, -186.9393878350548], time: 174.171
steps: 2324975, episodes: 93000, mean episode variance: 36.74514526939392, agent episode variance: [1.0611722931861878, 34.74585498046875, 0.9381179957389831], time: 174.171
Running avgs for agent 0: q_loss: 5.843828201293945, p_loss: -3.796323776245117, mean_rew: -7.410071940790295, variance: 4.244689464569092, lamda: 1.793745756149292
Running avgs for agent 1: q_loss: 1163350.875, p_loss: 138.6094207763672, mean_rew: -7.412452340224654, variance: 138.983419921875, lamda: 3.284630537033081
Running avgs for agent 2: q_loss: 5.1526079177856445, p_loss: -3.7983624935150146, mean_rew: -7.406573123356162, variance: 3.752471923828125, lamda: 2.030895709991455

steps: 2349975, episodes: 94000, mean episode reward: -559.7540884893921, agent episode reward: [-186.5846961631307, -186.5846961631307, -186.5846961631307], time: 179.357
steps: 2349975, episodes: 94000, mean episode variance: 36.6666038017273, agent episode variance: [1.0645181679725646, 34.66270514678955, 0.9393804869651794], time: 179.357
Running avgs for agent 0: q_loss: 5.792910575866699, p_loss: -3.797220230102539, mean_rew: -7.412869921657292, variance: 4.258072853088379, lamda: 1.7937456369400024
Running avgs for agent 1: q_loss: 1181260.625, p_loss: 138.85240173339844, mean_rew: -7.409074385672319, variance: 138.6508205871582, lamda: 3.3096046447753906
Running avgs for agent 2: q_loss: 5.032207489013672, p_loss: -3.804814100265503, mean_rew: -7.409240529719366, variance: 3.757521867752075, lamda: 2.031243085861206

steps: 2374975, episodes: 95000, mean episode reward: -563.9720004695598, agent episode reward: [-187.9906668231866, -187.9906668231866, -187.9906668231866], time: 172.986
steps: 2374975, episodes: 95000, mean episode variance: 36.308084621429444, agent episode variance: [1.065009921312332, 34.306380813598636, 0.9366938865184784], time: 172.987
Running avgs for agent 0: q_loss: 5.758549690246582, p_loss: -3.7987754344940186, mean_rew: -7.418857790477161, variance: 4.260039806365967, lamda: 1.7937456369400024
Running avgs for agent 1: q_loss: 1165943.0, p_loss: 139.19949340820312, mean_rew: -7.412279334184457, variance: 137.22552325439455, lamda: 3.3345789909362793
Running avgs for agent 2: q_loss: 5.03251314163208, p_loss: -3.8148927688598633, mean_rew: -7.420472122927896, variance: 3.7467753887176514, lamda: 2.0315299034118652

steps: 2399975, episodes: 96000, mean episode reward: -563.3257651258018, agent episode reward: [-187.77525504193392, -187.77525504193392, -187.77525504193392], time: 175.14
steps: 2399975, episodes: 96000, mean episode variance: 35.726321979284286, agent episode variance: [1.0569393961429596, 33.730808044433594, 0.9385745387077331], time: 175.141
Running avgs for agent 0: q_loss: 5.757704734802246, p_loss: -3.805392026901245, mean_rew: -7.422979996429284, variance: 4.227757930755615, lamda: 1.793745756149292
Running avgs for agent 1: q_loss: 1188912.125, p_loss: 139.5186309814453, mean_rew: -7.430717704527107, variance: 134.92323217773438, lamda: 3.359553813934326
Running avgs for agent 2: q_loss: 4.927796840667725, p_loss: -3.8140170574188232, mean_rew: -7.429806479605435, variance: 3.754298448562622, lamda: 2.031656265258789

steps: 2424975, episodes: 97000, mean episode reward: -564.1648315791775, agent episode reward: [-188.05494385972585, -188.05494385972585, -188.05494385972585], time: 179.356
steps: 2424975, episodes: 97000, mean episode variance: 35.239206250429156, agent episode variance: [1.0669509675502777, 33.23313313293457, 0.9391221499443054], time: 179.356
Running avgs for agent 0: q_loss: 5.729578018188477, p_loss: -3.8094332218170166, mean_rew: -7.43606559420388, variance: 4.26780366897583, lamda: 1.7937456369400024
Running avgs for agent 1: q_loss: 1203764.5, p_loss: 139.85955810546875, mean_rew: -7.4310929010818665, variance: 132.93253253173827, lamda: 3.3845279216766357
Running avgs for agent 2: q_loss: 4.850966930389404, p_loss: -3.8181405067443848, mean_rew: -7.432375232717288, variance: 3.756488561630249, lamda: 2.032263994216919

steps: 2449975, episodes: 98000, mean episode reward: -565.7902942054886, agent episode reward: [-188.59676473516288, -188.59676473516288, -188.59676473516288], time: 182.284
steps: 2449975, episodes: 98000, mean episode variance: 36.42065663933754, agent episode variance: [1.067416918516159, 34.40919067382813, 0.9440490469932556], time: 182.284
Running avgs for agent 0: q_loss: 5.73747444152832, p_loss: -3.81246280670166, mean_rew: -7.438730625651332, variance: 4.269668102264404, lamda: 1.7937456369400024
Running avgs for agent 1: q_loss: 1211306.0, p_loss: 140.22274780273438, mean_rew: -7.439206921570205, variance: 137.6367626953125, lamda: 3.4095025062561035
Running avgs for agent 2: q_loss: 4.922384262084961, p_loss: -3.8133797645568848, mean_rew: -7.432466711943886, variance: 3.776196241378784, lamda: 2.0324952602386475

steps: 2474975, episodes: 99000, mean episode reward: -559.5867711659279, agent episode reward: [-186.52892372197596, -186.52892372197596, -186.52892372197596], time: 178.838
steps: 2474975, episodes: 99000, mean episode variance: 36.369865336656574, agent episode variance: [1.062966787815094, 34.365861068725586, 0.9410374801158905], time: 178.839
Running avgs for agent 0: q_loss: 5.741547107696533, p_loss: -3.811583995819092, mean_rew: -7.441606626984409, variance: 4.251867294311523, lamda: 1.793745756149292
Running avgs for agent 1: q_loss: 1226439.75, p_loss: 140.39984130859375, mean_rew: -7.440147316818625, variance: 137.46344427490234, lamda: 3.434476613998413
Running avgs for agent 2: q_loss: 4.867569446563721, p_loss: -3.821732759475708, mean_rew: -7.4386655155965595, variance: 3.7641499042510986, lamda: 2.0332345962524414

steps: 2499975, episodes: 100000, mean episode reward: -564.4010934111659, agent episode reward: [-188.13369780372193, -188.13369780372193, -188.13369780372193], time: 177.211
steps: 2499975, episodes: 100000, mean episode variance: 35.63093881917, agent episode variance: [1.0711613147258758, 33.619484199523924, 0.9402933049201966], time: 177.212
Running avgs for agent 0: q_loss: 5.701455116271973, p_loss: -3.811117172241211, mean_rew: -7.443472362846238, variance: 4.2846455574035645, lamda: 1.7937456369400024
Running avgs for agent 1: q_loss: 1244752.625, p_loss: 140.5963134765625, mean_rew: -7.447055235611561, variance: 134.4779367980957, lamda: 3.4594509601593018
Running avgs for agent 2: q_loss: 4.7711405754089355, p_loss: -3.821817636489868, mean_rew: -7.445884739012051, variance: 3.7611732482910156, lamda: 2.0333545207977295

steps: 2524975, episodes: 101000, mean episode reward: -570.3791378678969, agent episode reward: [-190.12637928929897, -190.12637928929897, -190.12637928929897], time: 167.889
steps: 2524975, episodes: 101000, mean episode variance: 36.81651436471939, agent episode variance: [1.0682740027904511, 34.8063747253418, 0.941865636587143], time: 167.889
Running avgs for agent 0: q_loss: 5.748907566070557, p_loss: -3.8159799575805664, mean_rew: -7.44591243025314, variance: 4.273095607757568, lamda: 1.7937456369400024
Running avgs for agent 1: q_loss: 1263709.75, p_loss: 140.85162353515625, mean_rew: -7.454155828639394, variance: 139.2254989013672, lamda: 3.4844253063201904
Running avgs for agent 2: q_loss: 4.778313636779785, p_loss: -3.825800895690918, mean_rew: -7.452630782959863, variance: 3.7674624919891357, lamda: 2.0333657264709473

steps: 2549975, episodes: 102000, mean episode reward: -558.6357680893645, agent episode reward: [-186.21192269645482, -186.21192269645482, -186.21192269645482], time: 186.046
steps: 2549975, episodes: 102000, mean episode variance: 36.32968563747406, agent episode variance: [1.0731430640220643, 34.3094080657959, 0.9471345076560974], time: 186.046
Running avgs for agent 0: q_loss: 5.700112342834473, p_loss: -3.816512107849121, mean_rew: -7.459073956957271, variance: 4.292572498321533, lamda: 1.793745756149292
Running avgs for agent 1: q_loss: 1256471.5, p_loss: 140.97438049316406, mean_rew: -7.459375736707239, variance: 137.2376322631836, lamda: 3.509399652481079
Running avgs for agent 2: q_loss: 4.973720073699951, p_loss: -3.8303136825561523, mean_rew: -7.468287193435368, variance: 3.7885379791259766, lamda: 2.034397602081299

steps: 2574975, episodes: 103000, mean episode reward: -563.658757192271, agent episode reward: [-187.88625239742365, -187.88625239742365, -187.88625239742365], time: 193.265
steps: 2574975, episodes: 103000, mean episode variance: 34.72459143447876, agent episode variance: [1.0702964010238647, 32.713169708251954, 0.9411253252029419], time: 193.266
Running avgs for agent 0: q_loss: 5.662618160247803, p_loss: -3.823922872543335, mean_rew: -7.462826034987011, variance: 4.281185626983643, lamda: 1.7937456369400024
Running avgs for agent 1: q_loss: 1274337.375, p_loss: 141.20361328125, mean_rew: -7.4593972554747525, variance: 130.85267883300781, lamda: 3.5343739986419678
Running avgs for agent 2: q_loss: 4.852163314819336, p_loss: -3.829010009765625, mean_rew: -7.45854757593646, variance: 3.7645013332366943, lamda: 2.0345406532287598

steps: 2599975, episodes: 104000, mean episode reward: -562.4219804735732, agent episode reward: [-187.47399349119107, -187.47399349119107, -187.47399349119107], time: 172.302
steps: 2599975, episodes: 104000, mean episode variance: 35.80613581204415, agent episode variance: [1.06843638586998, 33.79578858947754, 0.9419108366966248], time: 172.303
Running avgs for agent 0: q_loss: 5.821091651916504, p_loss: -3.8329243659973145, mean_rew: -7.47818126651465, variance: 4.273745059967041, lamda: 1.793812870979309
Running avgs for agent 1: q_loss: 1271935.5, p_loss: 141.4857177734375, mean_rew: -7.476336318085069, variance: 135.18315435791015, lamda: 3.5593483448028564
Running avgs for agent 2: q_loss: 4.734473705291748, p_loss: -3.830963611602783, mean_rew: -7.464545316957866, variance: 3.767643451690674, lamda: 2.034543752670288

steps: 2624975, episodes: 105000, mean episode reward: -571.5539451283086, agent episode reward: [-190.5179817094362, -190.5179817094362, -190.5179817094362], time: 164.016
steps: 2624975, episodes: 105000, mean episode variance: 35.75655441880226, agent episode variance: [1.0714564497470855, 33.738804885864255, 0.946293083190918], time: 164.016
Running avgs for agent 0: q_loss: 5.706662654876709, p_loss: -3.828843832015991, mean_rew: -7.4779725320136965, variance: 4.285825729370117, lamda: 1.7939465045928955
Running avgs for agent 1: q_loss: 1298313.75, p_loss: 141.79959106445312, mean_rew: -7.478964821946445, variance: 134.95521954345702, lamda: 3.584322690963745
Running avgs for agent 2: q_loss: 4.880736351013184, p_loss: -3.836042642593384, mean_rew: -7.478145357089925, variance: 3.785172462463379, lamda: 2.034838914871216

steps: 2649975, episodes: 106000, mean episode reward: -568.5895574252723, agent episode reward: [-189.52985247509076, -189.52985247509076, -189.52985247509076], time: 162.415
steps: 2649975, episodes: 106000, mean episode variance: 35.50290771317482, agent episode variance: [1.0716723718643189, 33.48402458190918, 0.9472107594013214], time: 162.415
Running avgs for agent 0: q_loss: 5.57684850692749, p_loss: -3.831205129623413, mean_rew: -7.474919060778444, variance: 4.286689281463623, lamda: 1.7939465045928955
Running avgs for agent 1: q_loss: 1309930.375, p_loss: 142.03627014160156, mean_rew: -7.483448355168821, variance: 133.93609832763673, lamda: 3.6092967987060547
Running avgs for agent 2: q_loss: 4.9191203117370605, p_loss: -3.839365243911743, mean_rew: -7.481934280641633, variance: 3.7888431549072266, lamda: 2.0350449085235596

steps: 2674975, episodes: 107000, mean episode reward: -568.5136326829752, agent episode reward: [-189.50454422765836, -189.50454422765836, -189.50454422765836], time: 164.751
steps: 2674975, episodes: 107000, mean episode variance: 36.074481122493744, agent episode variance: [1.0705403678417207, 34.05806410217285, 0.9458766524791717], time: 164.752
Running avgs for agent 0: q_loss: 5.686932563781738, p_loss: -3.8342032432556152, mean_rew: -7.483258685525717, variance: 4.282161235809326, lamda: 1.7939462661743164
Running avgs for agent 1: q_loss: 1338416.875, p_loss: 142.27378845214844, mean_rew: -7.478196812263465, variance: 136.2322564086914, lamda: 3.6342713832855225
Running avgs for agent 2: q_loss: 4.908778667449951, p_loss: -3.8396286964416504, mean_rew: -7.4789345910736795, variance: 3.7835068702697754, lamda: 2.035156011581421

steps: 2699975, episodes: 108000, mean episode reward: -565.7263511402032, agent episode reward: [-188.5754503800677, -188.5754503800677, -188.5754503800677], time: 159.574
steps: 2699975, episodes: 108000, mean episode variance: 36.932599274635315, agent episode variance: [1.0733162503242493, 34.91261682128906, 0.9466662030220032], time: 159.574
Running avgs for agent 0: q_loss: 5.767183303833008, p_loss: -3.8345518112182617, mean_rew: -7.490693479072744, variance: 4.293264865875244, lamda: 1.7939465045928955
Running avgs for agent 1: q_loss: 1332249.25, p_loss: 142.4532470703125, mean_rew: -7.486885418667671, variance: 139.65046728515625, lamda: 3.659245491027832
Running avgs for agent 2: q_loss: 4.922970771789551, p_loss: -3.8433189392089844, mean_rew: -7.491322660533582, variance: 3.7866647243499756, lamda: 2.035156011581421

steps: 2724975, episodes: 109000, mean episode reward: -566.6308360052647, agent episode reward: [-188.87694533508818, -188.87694533508818, -188.87694533508818], time: 165.722
steps: 2724975, episodes: 109000, mean episode variance: 36.26175919508934, agent episode variance: [1.077341433763504, 34.24142129516601, 0.9429964661598206], time: 165.722
Running avgs for agent 0: q_loss: 5.677995204925537, p_loss: -3.8317270278930664, mean_rew: -7.4917641264097155, variance: 4.309365749359131, lamda: 1.7939465045928955
Running avgs for agent 1: q_loss: 1365459.625, p_loss: 142.68031311035156, mean_rew: -7.496767518452097, variance: 136.96568518066405, lamda: 3.684220314025879
Running avgs for agent 2: q_loss: 4.941694736480713, p_loss: -3.850478172302246, mean_rew: -7.496938339970301, variance: 3.7719857692718506, lamda: 2.035322427749634

steps: 2749975, episodes: 110000, mean episode reward: -571.9643310703156, agent episode reward: [-190.65477702343847, -190.65477702343847, -190.65477702343847], time: 167.951
steps: 2749975, episodes: 110000, mean episode variance: 36.78125699281693, agent episode variance: [1.0744780445098876, 34.75570922088623, 0.9510697274208069], time: 167.952
Running avgs for agent 0: q_loss: 5.6669158935546875, p_loss: -3.836055278778076, mean_rew: -7.497123808816508, variance: 4.297912120819092, lamda: 1.7939462661743164
Running avgs for agent 1: q_loss: 1371238.5, p_loss: 142.97891235351562, mean_rew: -7.498256206225308, variance: 139.02283688354493, lamda: 3.7091944217681885
Running avgs for agent 2: q_loss: 5.014050006866455, p_loss: -3.8440864086151123, mean_rew: -7.498979691401446, variance: 3.80427885055542, lamda: 2.035386085510254

steps: 2774975, episodes: 111000, mean episode reward: -570.7911472202317, agent episode reward: [-190.2637157400772, -190.2637157400772, -190.2637157400772], time: 166.747
steps: 2774975, episodes: 111000, mean episode variance: 36.118324693441394, agent episode variance: [1.0712477955818176, 34.0997520904541, 0.9473248074054718], time: 166.747
Running avgs for agent 0: q_loss: 5.766936779022217, p_loss: -3.841378927230835, mean_rew: -7.503831561982917, variance: 4.284991264343262, lamda: 1.7939465045928955
Running avgs for agent 1: q_loss: 1389305.625, p_loss: 143.29310607910156, mean_rew: -7.505324303247711, variance: 136.3990083618164, lamda: 3.734168767929077
Running avgs for agent 2: q_loss: 4.863962173461914, p_loss: -3.848984956741333, mean_rew: -7.502421037525919, variance: 3.789299249649048, lamda: 2.0353896617889404

steps: 2799975, episodes: 112000, mean episode reward: -566.5194207497236, agent episode reward: [-188.83980691657453, -188.83980691657453, -188.83980691657453], time: 162.654
steps: 2799975, episodes: 112000, mean episode variance: 35.35638323521614, agent episode variance: [1.0798611674308778, 33.327507766723635, 0.9490143010616302], time: 162.654
Running avgs for agent 0: q_loss: 5.771920680999756, p_loss: -3.839352607727051, mean_rew: -7.505118910132922, variance: 4.31944465637207, lamda: 1.7939465045928955
Running avgs for agent 1: q_loss: 1396996.375, p_loss: 143.5391845703125, mean_rew: -7.500725151762348, variance: 133.31003106689454, lamda: 3.759143114089966
Running avgs for agent 2: q_loss: 4.991323947906494, p_loss: -3.8492047786712646, mean_rew: -7.504164730234639, variance: 3.7960572242736816, lamda: 2.0354197025299072

steps: 2824975, episodes: 113000, mean episode reward: -570.2988332356127, agent episode reward: [-190.0996110785376, -190.0996110785376, -190.0996110785376], time: 165.915
steps: 2824975, episodes: 113000, mean episode variance: 35.717927260160444, agent episode variance: [1.0800498118400574, 33.688567276000974, 0.9493101723194123], time: 165.915
Running avgs for agent 0: q_loss: 5.780763626098633, p_loss: -3.842312812805176, mean_rew: -7.5116339722923176, variance: 4.320199012756348, lamda: 1.7939462661743164
Running avgs for agent 1: q_loss: 1386120.5, p_loss: 143.76614379882812, mean_rew: -7.513623388711008, variance: 134.7542691040039, lamda: 3.7841174602508545
Running avgs for agent 2: q_loss: 5.559841156005859, p_loss: -3.8474979400634766, mean_rew: -7.507773652648345, variance: 3.797240734100342, lamda: 2.035888195037842

steps: 2849975, episodes: 114000, mean episode reward: -578.5470789044833, agent episode reward: [-192.84902630149446, -192.84902630149446, -192.84902630149446], time: 166.034
steps: 2849975, episodes: 114000, mean episode variance: 35.653423742771146, agent episode variance: [1.0766250443458558, 33.62756388092041, 0.9492348175048828], time: 166.035
Running avgs for agent 0: q_loss: 5.77581787109375, p_loss: -3.845538854598999, mean_rew: -7.518471238056934, variance: 4.30649995803833, lamda: 1.7939465045928955
Running avgs for agent 1: q_loss: 1404194.75, p_loss: 143.9832000732422, mean_rew: -7.517081795319946, variance: 134.51025552368165, lamda: 3.809091806411743
Running avgs for agent 2: q_loss: 9.036005973815918, p_loss: -3.8563945293426514, mean_rew: -7.512658261184917, variance: 3.7969391345977783, lamda: 2.0402486324310303

steps: 2874975, episodes: 115000, mean episode reward: -566.0304760464815, agent episode reward: [-188.67682534882715, -188.67682534882715, -188.67682534882715], time: 162.591
steps: 2874975, episodes: 115000, mean episode variance: 35.45877354216576, agent episode variance: [1.0832908563613892, 33.42685061645508, 0.9486320693492889], time: 162.592
Running avgs for agent 0: q_loss: 5.739478588104248, p_loss: -3.8450567722320557, mean_rew: -7.522869126196468, variance: 4.333163261413574, lamda: 1.7939465045928955
Running avgs for agent 1: q_loss: 1404696.625, p_loss: 144.1019287109375, mean_rew: -7.514508514118041, variance: 133.7074024658203, lamda: 3.8340659141540527
Running avgs for agent 2: q_loss: 8.926788330078125, p_loss: -3.859159469604492, mean_rew: -7.5204993254900945, variance: 3.7945282459259033, lamda: 2.040356159210205

steps: 2899975, episodes: 116000, mean episode reward: -571.7344973245437, agent episode reward: [-190.5781657748479, -190.5781657748479, -190.5781657748479], time: 160.379
steps: 2899975, episodes: 116000, mean episode variance: 35.477760050058365, agent episode variance: [1.083722752571106, 33.44968305969238, 0.9443542377948762], time: 160.38
Running avgs for agent 0: q_loss: 5.778665065765381, p_loss: -3.8506455421447754, mean_rew: -7.522025033055757, variance: 4.334891319274902, lamda: 1.7939462661743164
Running avgs for agent 1: q_loss: 1408143.625, p_loss: 144.28640747070312, mean_rew: -7.519575569361874, variance: 133.79873223876953, lamda: 3.8590404987335205
Running avgs for agent 2: q_loss: 8.98156452178955, p_loss: -3.8540806770324707, mean_rew: -7.520010883404533, variance: 3.777416706085205, lamda: 2.040356397628784

steps: 2924975, episodes: 117000, mean episode reward: -574.14210002664, agent episode reward: [-191.38070000887998, -191.38070000887998, -191.38070000887998], time: 181.215
steps: 2924975, episodes: 117000, mean episode variance: 35.43846050834656, agent episode variance: [1.0784366419315339, 33.41143821716309, 0.9485856492519379], time: 181.216
Running avgs for agent 0: q_loss: 5.8213114738464355, p_loss: -3.8523752689361572, mean_rew: -7.527308177121817, variance: 4.313746929168701, lamda: 1.793994665145874
Running avgs for agent 1: q_loss: 1408649.75, p_loss: 144.44931030273438, mean_rew: -7.529282219256494, variance: 133.64575286865235, lamda: 3.88401460647583
Running avgs for agent 2: q_loss: 8.978896141052246, p_loss: -3.857736349105835, mean_rew: -7.527912094423185, variance: 3.794342517852783, lamda: 2.040356397628784

steps: 2949975, episodes: 118000, mean episode reward: -573.6674079529713, agent episode reward: [-191.22246931765704, -191.22246931765704, -191.22246931765704], time: 160.657
steps: 2949975, episodes: 118000, mean episode variance: 34.88031671833992, agent episode variance: [1.0821602849960328, 32.85267944335938, 0.9454769899845124], time: 160.658
Running avgs for agent 0: q_loss: 5.780682563781738, p_loss: -3.8588037490844727, mean_rew: -7.542745211405887, variance: 4.328640937805176, lamda: 1.794006109237671
Running avgs for agent 1: q_loss: 1426899.75, p_loss: 144.4802703857422, mean_rew: -7.537047765448407, variance: 131.4107177734375, lamda: 3.908989191055298
Running avgs for agent 2: q_loss: 9.027813911437988, p_loss: -3.8619325160980225, mean_rew: -7.5326263195792205, variance: 3.7819080352783203, lamda: 2.040356159210205

steps: 2974975, episodes: 119000, mean episode reward: -562.1101363028655, agent episode reward: [-187.37004543428847, -187.37004543428847, -187.37004543428847], time: 121.124
steps: 2974975, episodes: 119000, mean episode variance: 35.54607275223732, agent episode variance: [1.0799546041488648, 33.52337115478515, 0.9427469933032989], time: 121.124
Running avgs for agent 0: q_loss: 5.932133197784424, p_loss: -3.851593255996704, mean_rew: -7.537237035728175, variance: 4.319818496704102, lamda: 1.7944257259368896
Running avgs for agent 1: q_loss: 1429450.5, p_loss: 144.6265411376953, mean_rew: -7.53830838491698, variance: 134.0934846191406, lamda: 3.9339632987976074
Running avgs for agent 2: q_loss: 9.159931182861328, p_loss: -3.8621182441711426, mean_rew: -7.532435235297219, variance: 3.7709879875183105, lamda: 2.040356397628784

steps: 2999975, episodes: 120000, mean episode reward: -570.3235246874314, agent episode reward: [-190.10784156247715, -190.10784156247715, -190.10784156247715], time: 118.328
steps: 2999975, episodes: 120000, mean episode variance: 35.22755455303192, agent episode variance: [1.0809095737934113, 33.198278259277345, 0.9483667199611664], time: 118.329
Running avgs for agent 0: q_loss: 5.7146687507629395, p_loss: -3.8476669788360596, mean_rew: -7.533213266453481, variance: 4.323637962341309, lamda: 1.7963988780975342
Running avgs for agent 1: q_loss: 1448423.25, p_loss: 144.68663024902344, mean_rew: -7.536956542956048, variance: 132.79311303710938, lamda: 3.958937883377075
Running avgs for agent 2: q_loss: 9.152830123901367, p_loss: -3.866614818572998, mean_rew: -7.537007236700257, variance: 3.7934670448303223, lamda: 2.040356397628784

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -564.0459287662981, agent episode reward: [-188.0153095887661, -188.0153095887661, -188.0153095887661], time: 90.982
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 90.982
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -568.4257441974896, agent episode reward: [-189.47524806582985, -189.47524806582985, -189.47524806582985], time: 101.784
steps: 49975, episodes: 2000, mean episode variance: 49.20055116891861, agent episode variance: [0.7479747800827027, 47.90303996276855, 0.5495364260673523], time: 101.784
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.5473081009346945, variance: 3.0654702186584473, lamda: 1.7964062690734863
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.5420117445282, variance: 196.3239288330078, lamda: 3.9714744091033936
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.547984783334773, variance: 2.2521982192993164, lamda: 2.040363073348999

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567759303.1090450: line 9: --exp_var_alpha: command not found
