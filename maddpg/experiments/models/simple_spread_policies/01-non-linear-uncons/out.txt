# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation False --constrained \
    False --exp-name MADDPG_simple_spread_uncons_01 --save-dir \
    models/simple_spread_policies/01-non-linear-uncons/ --plots-dir \
    models/simple_spread_policies/01-non-linear-uncons/
Job <1083852> is submitted to queue <x86_6h>.
arglist.u_estimation False
2019-09-06 02:59:50.620052: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -522.8291656005848, agent episode reward: [-174.27638853352826, -174.27638853352826, -174.27638853352826], time: 32.752
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 32.752
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -559.9807202999763, agent episode reward: [-186.66024009999208, -186.66024009999208, -186.66024009999208], time: 56.102
steps: 49975, episodes: 2000, mean episode variance: 2.0020302303284407, agent episode variance: [0.6585749166309833, 0.6626698381304741, 0.6807854755669832], time: 56.103
Running avgs for agent 0: q_loss: 2.076706886291504, p_loss: 9.8892183303833, mean_rew: -7.166279710269015, variance: 2.6990775271761613, mean_q: -9.955778121948242, std_q: 2.0666069984436035
Running avgs for agent 1: q_loss: 1.9613763093948364, p_loss: 10.539530754089355, mean_rew: -7.158219600512724, variance: 2.7158599923380087, mean_q: -10.605472564697266, std_q: 2.171126127243042
Running avgs for agent 2: q_loss: 2.4117588996887207, p_loss: 11.021636009216309, mean_rew: -7.167906809544156, variance: 2.790104408061407, mean_q: -11.092345237731934, std_q: 2.3235597610473633

steps: 74975, episodes: 3000, mean episode reward: -478.0796415480269, agent episode reward: [-159.35988051600896, -159.35988051600896, -159.35988051600896], time: 58.372
steps: 74975, episodes: 3000, mean episode variance: 4.79348699760437, agent episode variance: [1.3815409280657769, 1.596618238389492, 1.8153278311491012], time: 58.373
Running avgs for agent 0: q_loss: 1.1496752500534058, p_loss: 21.22697639465332, mean_rew: -7.094518527765702, variance: 5.5261637122631075, mean_q: -21.393957138061523, std_q: 4.534404277801514
Running avgs for agent 1: q_loss: 1.1060575246810913, p_loss: 22.357955932617188, mean_rew: -7.097104706257911, variance: 6.386472953557968, mean_q: -22.451303482055664, std_q: 4.777281761169434
Running avgs for agent 2: q_loss: 1.2083048820495605, p_loss: 22.860519409179688, mean_rew: -7.09777432076795, variance: 7.261311324596405, mean_q: -23.008834838867188, std_q: 4.834990978240967

steps: 99975, episodes: 4000, mean episode reward: -443.66715913832354, agent episode reward: [-147.88905304610788, -147.88905304610788, -147.88905304610788], time: 59.703
steps: 99975, episodes: 4000, mean episode variance: 12.639048176288604, agent episode variance: [3.1333927683830263, 3.5969873881340027, 5.908668019771576], time: 59.703
Running avgs for agent 0: q_loss: 1.2635935544967651, p_loss: 31.838661193847656, mean_rew: -6.806367936239494, variance: 12.533571073532105, mean_q: -32.16556167602539, std_q: 6.9676642417907715
Running avgs for agent 1: q_loss: 1.235580563545227, p_loss: 32.789852142333984, mean_rew: -6.811382948060489, variance: 14.38794955253601, mean_q: -32.965904235839844, std_q: 7.196106433868408
Running avgs for agent 2: q_loss: 1.8103598356246948, p_loss: 33.312625885009766, mean_rew: -6.8086715073850375, variance: 23.634672079086304, mean_q: -33.525630950927734, std_q: 7.265880107879639

steps: 124975, episodes: 5000, mean episode reward: -403.7558592273751, agent episode reward: [-134.58528640912502, -134.58528640912502, -134.58528640912502], time: 66.01
steps: 124975, episodes: 5000, mean episode variance: 15.04005825650692, agent episode variance: [4.8208770656585695, 5.373782760620117, 4.845398430228233], time: 66.011
Running avgs for agent 0: q_loss: 1.3962322473526, p_loss: 40.22551727294922, mean_rew: -6.547654637901873, variance: 19.283508262634278, mean_q: -40.67994689941406, std_q: 8.786599159240723
Running avgs for agent 1: q_loss: 1.4912736415863037, p_loss: 40.932098388671875, mean_rew: -6.554895503333876, variance: 21.49513104248047, mean_q: -41.21469497680664, std_q: 8.854949951171875
Running avgs for agent 2: q_loss: 1.5296293497085571, p_loss: 41.43503189086914, mean_rew: -6.551027060432917, variance: 19.38159372091293, mean_q: -41.80515670776367, std_q: 9.086788177490234

steps: 149975, episodes: 6000, mean episode reward: -379.65030716088654, agent episode reward: [-126.55010238696217, -126.55010238696217, -126.55010238696217], time: 69.152
steps: 149975, episodes: 6000, mean episode variance: 24.55604910373688, agent episode variance: [6.10704195022583, 12.348534454345703, 6.100472699165344], time: 69.152
Running avgs for agent 0: q_loss: 1.524468183517456, p_loss: 46.70623016357422, mean_rew: -6.313501721278785, variance: 24.42816780090332, mean_q: -47.241912841796875, std_q: 9.947967529296875
Running avgs for agent 1: q_loss: 2.424790620803833, p_loss: 47.22682571411133, mean_rew: -6.310793433903795, variance: 49.39413781738281, mean_q: -47.597999572753906, std_q: 9.870018005371094
Running avgs for agent 2: q_loss: 1.6894915103912354, p_loss: 47.623538970947266, mean_rew: -6.311582495137027, variance: 24.401890796661377, mean_q: -48.094810485839844, std_q: 10.088531494140625

steps: 174975, episodes: 7000, mean episode reward: -372.1104201000829, agent episode reward: [-124.03680670002764, -124.03680670002764, -124.03680670002764], time: 70.48
steps: 174975, episodes: 7000, mean episode variance: 28.5034287109375, agent episode variance: [7.273138058662415, 14.082452926635742, 7.147837725639343], time: 70.48
Running avgs for agent 0: q_loss: 1.637066125869751, p_loss: 51.837974548339844, mean_rew: -6.103850534779162, variance: 29.09255223464966, mean_q: -52.39567947387695, std_q: 10.496214866638184
Running avgs for agent 1: q_loss: 2.5431573390960693, p_loss: 52.23739242553711, mean_rew: -6.104576171010282, variance: 56.32981170654297, mean_q: -52.63173294067383, std_q: 10.422987937927246
Running avgs for agent 2: q_loss: 1.7767219543457031, p_loss: 52.6038818359375, mean_rew: -6.107210893688977, variance: 28.591350902557373, mean_q: -53.08295440673828, std_q: 10.6513671875

steps: 199975, episodes: 8000, mean episode reward: -374.7035909268687, agent episode reward: [-124.90119697562288, -124.90119697562288, -124.90119697562288], time: 66.615
steps: 199975, episodes: 8000, mean episode variance: 31.028782775878906, agent episode variance: [8.130143810272218, 14.988171112060547, 7.910467853546143], time: 66.615
Running avgs for agent 0: q_loss: 1.7238777875900269, p_loss: 56.22303009033203, mean_rew: -5.957966134699347, variance: 32.52057524108887, mean_q: -56.79580307006836, std_q: 10.823017120361328
Running avgs for agent 1: q_loss: 2.617161750793457, p_loss: 56.50384521484375, mean_rew: -5.958525001618984, variance: 59.95268444824219, mean_q: -56.90724563598633, std_q: 10.812718391418457
Running avgs for agent 2: q_loss: 1.8310396671295166, p_loss: 56.73796844482422, mean_rew: -5.959668446342785, variance: 31.64187141418457, mean_q: -57.20246124267578, std_q: 10.94143295288086

steps: 224975, episodes: 9000, mean episode reward: -371.9324541272701, agent episode reward: [-123.97748470909004, -123.97748470909004, -123.97748470909004], time: 66.373
steps: 224975, episodes: 9000, mean episode variance: 33.50480832290649, agent episode variance: [8.839747592926026, 15.9934688873291, 8.671591842651367], time: 66.374
Running avgs for agent 0: q_loss: 1.7905499935150146, p_loss: 60.00531005859375, mean_rew: -5.847046583192241, variance: 35.3589903717041, mean_q: -60.57578659057617, std_q: 11.001354217529297
Running avgs for agent 1: q_loss: 2.626406669616699, p_loss: 60.139888763427734, mean_rew: -5.849085250340193, variance: 63.9738755493164, mean_q: -60.541786193847656, std_q: 10.98823356628418
Running avgs for agent 2: q_loss: 1.837365746498108, p_loss: 60.24043655395508, mean_rew: -5.847648106319941, variance: 34.68636737060547, mean_q: -60.71208572387695, std_q: 11.040142059326172

steps: 249975, episodes: 10000, mean episode reward: -367.7170595105792, agent episode reward: [-122.57235317019308, -122.57235317019308, -122.57235317019308], time: 66.779
steps: 249975, episodes: 10000, mean episode variance: 35.49318123054504, agent episode variance: [9.56314623260498, 16.606616645812988, 9.323418352127074], time: 66.78
Running avgs for agent 0: q_loss: 1.809625506401062, p_loss: 63.14129638671875, mean_rew: -5.745534608155442, variance: 38.25258493041992, mean_q: -63.69879150390625, std_q: 10.913002014160156
Running avgs for agent 1: q_loss: 2.621168375015259, p_loss: 63.21030044555664, mean_rew: -5.745205901791344, variance: 66.42646658325195, mean_q: -63.58956527709961, std_q: 10.971144676208496
Running avgs for agent 2: q_loss: 1.8409112691879272, p_loss: 63.18818664550781, mean_rew: -5.743862604527562, variance: 37.2936734085083, mean_q: -63.65312194824219, std_q: 10.963041305541992

steps: 274975, episodes: 11000, mean episode reward: -362.72901670282647, agent episode reward: [-120.90967223427549, -120.90967223427549, -120.90967223427549], time: 72.883
steps: 274975, episodes: 11000, mean episode variance: 37.96829426574707, agent episode variance: [10.373298122406005, 17.681772117614745, 9.913224025726318], time: 72.884
Running avgs for agent 0: q_loss: 1.807111382484436, p_loss: 65.81432342529297, mean_rew: -5.661975559973711, variance: 41.49319248962402, mean_q: -66.35905456542969, std_q: 10.823881149291992
Running avgs for agent 1: q_loss: 2.617306709289551, p_loss: 65.83329010009766, mean_rew: -5.659087447527694, variance: 70.72708847045898, mean_q: -66.1890869140625, std_q: 11.021896362304688
Running avgs for agent 2: q_loss: 1.8521193265914917, p_loss: 65.78034973144531, mean_rew: -5.668146653427062, variance: 39.652896102905274, mean_q: -66.24617004394531, std_q: 11.009001731872559

steps: 299975, episodes: 12000, mean episode reward: -359.8589024898486, agent episode reward: [-119.95296749661621, -119.95296749661621, -119.95296749661621], time: 68.968
steps: 299975, episodes: 12000, mean episode variance: 38.39405944061279, agent episode variance: [10.505754676818848, 17.851136756896974, 10.037168006896973], time: 68.968
Running avgs for agent 0: q_loss: 1.7950096130371094, p_loss: 68.06005096435547, mean_rew: -5.585135885765305, variance: 42.02301870727539, mean_q: -68.5765609741211, std_q: 10.755098342895508
Running avgs for agent 1: q_loss: 2.6050195693969727, p_loss: 68.19781494140625, mean_rew: -5.593077012270905, variance: 71.4045470275879, mean_q: -68.53971862792969, std_q: 11.03199577331543
Running avgs for agent 2: q_loss: 1.8451687097549438, p_loss: 67.95320129394531, mean_rew: -5.590424491396635, variance: 40.14867202758789, mean_q: -68.41410064697266, std_q: 10.79605770111084

steps: 324975, episodes: 13000, mean episode reward: -358.61533526539006, agent episode reward: [-119.53844508846333, -119.53844508846333, -119.53844508846333], time: 70.069
steps: 324975, episodes: 13000, mean episode variance: 39.50176131725311, agent episode variance: [11.128808338165284, 18.200766613006593, 10.172186366081238], time: 70.069
Running avgs for agent 0: q_loss: 1.7464028596878052, p_loss: 69.95906829833984, mean_rew: -5.526882412757714, variance: 44.515233352661134, mean_q: -70.46526336669922, std_q: 10.601473808288574
Running avgs for agent 1: q_loss: 2.5734317302703857, p_loss: 70.1507339477539, mean_rew: -5.52595108825177, variance: 72.80306645202637, mean_q: -70.47766876220703, std_q: 10.861410140991211
Running avgs for agent 2: q_loss: 1.878267526626587, p_loss: 69.89567565917969, mean_rew: -5.52997484388181, variance: 40.68874546432495, mean_q: -70.33426666259766, std_q: 10.62883186340332

steps: 349975, episodes: 14000, mean episode reward: -355.8589752117017, agent episode reward: [-118.61965840390054, -118.61965840390054, -118.61965840390054], time: 66.905
steps: 349975, episodes: 14000, mean episode variance: 40.06764795398712, agent episode variance: [10.839099240303039, 18.61228672027588, 10.616261993408203], time: 66.905
Running avgs for agent 0: q_loss: 1.758588433265686, p_loss: 71.56887817382812, mean_rew: -5.4696347445986975, variance: 43.356396961212155, mean_q: -72.06366729736328, std_q: 10.4893217086792
Running avgs for agent 1: q_loss: 2.526609420776367, p_loss: 71.8754653930664, mean_rew: -5.466026170989222, variance: 74.44914688110352, mean_q: -72.18421173095703, std_q: 10.793607711791992
Running avgs for agent 2: q_loss: 1.7841023206710815, p_loss: 71.616943359375, mean_rew: -5.467537077958925, variance: 42.465047973632814, mean_q: -72.037841796875, std_q: 10.459332466125488

steps: 374975, episodes: 15000, mean episode reward: -351.87229552720896, agent episode reward: [-117.29076517573631, -117.29076517573631, -117.29076517573631], time: 66.74
steps: 374975, episodes: 15000, mean episode variance: 40.81915919494629, agent episode variance: [11.431969509124755, 18.65874100494385, 10.728448680877685], time: 66.741
Running avgs for agent 0: q_loss: 1.7313412427902222, p_loss: 72.87811279296875, mean_rew: -5.416272094698149, variance: 45.72787803649902, mean_q: -73.36480712890625, std_q: 10.367284774780273
Running avgs for agent 1: q_loss: 2.4761924743652344, p_loss: 73.43860626220703, mean_rew: -5.419485954684195, variance: 74.6349640197754, mean_q: -73.73333740234375, std_q: 10.760677337646484
Running avgs for agent 2: q_loss: 1.7707221508026123, p_loss: 73.10379791259766, mean_rew: -5.423737028418672, variance: 42.91379472351074, mean_q: -73.51512145996094, std_q: 10.383980751037598

steps: 399975, episodes: 16000, mean episode reward: -352.00924953237654, agent episode reward: [-117.3364165107922, -117.3364165107922, -117.3364165107922], time: 65.991
steps: 399975, episodes: 16000, mean episode variance: 41.314525369644166, agent episode variance: [11.669823322296143, 18.850223472595214, 10.794478574752807], time: 65.991
Running avgs for agent 0: q_loss: 1.6807721853256226, p_loss: 73.99128723144531, mean_rew: -5.375628059174357, variance: 46.67929328918457, mean_q: -74.46744537353516, std_q: 10.244196891784668
Running avgs for agent 1: q_loss: 2.4365079402923584, p_loss: 74.7462158203125, mean_rew: -5.373290440868603, variance: 75.40089389038086, mean_q: -75.02496337890625, std_q: 10.649481773376465
Running avgs for agent 2: q_loss: 1.777757167816162, p_loss: 74.27252197265625, mean_rew: -5.37501450660817, variance: 43.17791429901123, mean_q: -74.67029571533203, std_q: 10.28662395477295

steps: 424975, episodes: 17000, mean episode reward: -349.22649527145256, agent episode reward: [-116.40883175715084, -116.40883175715084, -116.40883175715084], time: 63.918
steps: 424975, episodes: 17000, mean episode variance: 42.74262585449219, agent episode variance: [12.040218730926513, 19.4963995513916, 11.206007572174073], time: 63.918
Running avgs for agent 0: q_loss: 1.695009469985962, p_loss: 74.84210968017578, mean_rew: -5.331416300794361, variance: 48.160874923706054, mean_q: -75.30741119384766, std_q: 10.08803939819336
Running avgs for agent 1: q_loss: 2.412691116333008, p_loss: 75.90471649169922, mean_rew: -5.334171138076328, variance: 77.9855982055664, mean_q: -76.17920684814453, std_q: 10.601130485534668
Running avgs for agent 2: q_loss: 1.6840014457702637, p_loss: 75.26783752441406, mean_rew: -5.326985837732817, variance: 44.82403028869629, mean_q: -75.64147186279297, std_q: 10.163769721984863

steps: 449975, episodes: 18000, mean episode reward: -347.73660608817255, agent episode reward: [-115.91220202939084, -115.91220202939084, -115.91220202939084], time: 66.186
steps: 449975, episodes: 18000, mean episode variance: 43.312865466117856, agent episode variance: [12.089242641448974, 18.95342148590088, 12.270201338768006], time: 66.187
Running avgs for agent 0: q_loss: 1.7049835920333862, p_loss: 75.60845947265625, mean_rew: -5.301117845999826, variance: 48.3569705657959, mean_q: -76.0723876953125, std_q: 10.155234336853027
Running avgs for agent 1: q_loss: 2.3817245960235596, p_loss: 76.9217529296875, mean_rew: -5.294269791724749, variance: 75.81368594360352, mean_q: -77.18854522705078, std_q: 10.499408721923828
Running avgs for agent 2: q_loss: 1.8678487539291382, p_loss: 76.08012390136719, mean_rew: -5.287176980084081, variance: 49.08080535507202, mean_q: -76.45219421386719, std_q: 10.147066116333008

steps: 474975, episodes: 19000, mean episode reward: -349.1310276729613, agent episode reward: [-116.37700922432043, -116.37700922432043, -116.37700922432043], time: 68.411
steps: 474975, episodes: 19000, mean episode variance: 51.969700469970704, agent episode variance: [12.496402320861817, 19.262854278564454, 20.210443870544434], time: 68.412
Running avgs for agent 0: q_loss: 1.707421898841858, p_loss: 76.06938934326172, mean_rew: -5.255548884142436, variance: 49.98560928344727, mean_q: -76.51982879638672, std_q: 10.015023231506348
Running avgs for agent 1: q_loss: 2.3718504905700684, p_loss: 77.77058410644531, mean_rew: -5.255148224540866, variance: 77.05141711425782, mean_q: -78.02996063232422, std_q: 10.461492538452148
Running avgs for agent 2: q_loss: 2.5993261337280273, p_loss: 76.78528594970703, mean_rew: -5.2629572525160695, variance: 80.84177548217774, mean_q: -77.14546966552734, std_q: 10.162670135498047

steps: 499975, episodes: 20000, mean episode reward: -347.2828417234873, agent episode reward: [-115.76094724116244, -115.76094724116244, -115.76094724116244], time: 69.18
steps: 499975, episodes: 20000, mean episode variance: 51.49701059722901, agent episode variance: [11.936914306640626, 19.59992024230957, 19.960176048278807], time: 69.181
Running avgs for agent 0: q_loss: 1.6918537616729736, p_loss: 76.56173706054688, mean_rew: -5.223702221608098, variance: 47.7476572265625, mean_q: -77.01763916015625, std_q: 9.982617378234863
Running avgs for agent 1: q_loss: 2.368802785873413, p_loss: 78.47042846679688, mean_rew: -5.226686574910882, variance: 78.39968096923828, mean_q: -78.72857666015625, std_q: 10.36072063446045
Running avgs for agent 2: q_loss: 2.519023895263672, p_loss: 77.35396575927734, mean_rew: -5.223734568449525, variance: 79.84070419311523, mean_q: -77.69639587402344, std_q: 10.103921890258789

steps: 524975, episodes: 21000, mean episode reward: -344.4190838891283, agent episode reward: [-114.8063612963761, -114.8063612963761, -114.8063612963761], time: 69.492
steps: 524975, episodes: 21000, mean episode variance: 50.53258660125732, agent episode variance: [12.107225276947021, 19.006231285095215, 19.419130039215087], time: 69.492
Running avgs for agent 0: q_loss: 1.7242602109909058, p_loss: 76.94546508789062, mean_rew: -5.197223990907966, variance: 48.428901107788086, mean_q: -77.39524841308594, std_q: 10.029888153076172
Running avgs for agent 1: q_loss: 2.3351681232452393, p_loss: 79.12416076660156, mean_rew: -5.194279045942229, variance: 76.02492514038086, mean_q: -79.3757553100586, std_q: 10.300515174865723
Running avgs for agent 2: q_loss: 2.4888031482696533, p_loss: 77.80045318603516, mean_rew: -5.188329982566905, variance: 77.67652015686035, mean_q: -78.13548278808594, std_q: 10.141173362731934

steps: 549975, episodes: 22000, mean episode reward: -345.4133306774141, agent episode reward: [-115.13777689247138, -115.13777689247138, -115.13777689247138], time: 69.807
steps: 549975, episodes: 22000, mean episode variance: 50.83078303527832, agent episode variance: [11.947603908538818, 19.22250040435791, 19.660678722381594], time: 69.808
Running avgs for agent 0: q_loss: 1.709115743637085, p_loss: 77.26740264892578, mean_rew: -5.164116314964872, variance: 47.79041563415527, mean_q: -77.72013854980469, std_q: 10.021387100219727
Running avgs for agent 1: q_loss: 2.3390843868255615, p_loss: 79.5965347290039, mean_rew: -5.170673430229076, variance: 76.89000161743164, mean_q: -79.84321594238281, std_q: 10.307605743408203
Running avgs for agent 2: q_loss: 2.4636058807373047, p_loss: 78.1783676147461, mean_rew: -5.169989903570578, variance: 78.64271488952637, mean_q: -78.50806427001953, std_q: 10.144205093383789

steps: 574975, episodes: 23000, mean episode reward: -344.34000594927835, agent episode reward: [-114.78000198309279, -114.78000198309279, -114.78000198309279], time: 68.927
steps: 574975, episodes: 23000, mean episode variance: 51.38714167785645, agent episode variance: [12.405811916351318, 19.271187492370604, 19.710142269134522], time: 68.928
Running avgs for agent 0: q_loss: 1.7196210622787476, p_loss: 77.47845458984375, mean_rew: -5.14292408198583, variance: 49.62324766540527, mean_q: -77.91838836669922, std_q: 9.941381454467773
Running avgs for agent 1: q_loss: 2.3379852771759033, p_loss: 79.91670989990234, mean_rew: -5.1464650112366686, variance: 77.08474996948242, mean_q: -80.16502380371094, std_q: 10.216629981994629
Running avgs for agent 2: q_loss: 2.424879312515259, p_loss: 78.45915222167969, mean_rew: -5.143211094177044, variance: 78.84056907653809, mean_q: -78.78376770019531, std_q: 10.06620979309082

steps: 599975, episodes: 24000, mean episode reward: -344.4269497271737, agent episode reward: [-114.80898324239121, -114.80898324239121, -114.80898324239121], time: 68.617
steps: 599975, episodes: 24000, mean episode variance: 50.69544635772705, agent episode variance: [12.458103206634522, 19.02055265045166, 19.21679050064087], time: 68.618
Running avgs for agent 0: q_loss: 1.6951640844345093, p_loss: 77.65548706054688, mean_rew: -5.115248354534626, variance: 49.83241282653809, mean_q: -78.09471893310547, std_q: 9.928728103637695
Running avgs for agent 1: q_loss: 2.309267282485962, p_loss: 80.16436004638672, mean_rew: -5.121796367808986, variance: 76.08221060180664, mean_q: -80.40631103515625, std_q: 10.135885238647461
Running avgs for agent 2: q_loss: 2.370803117752075, p_loss: 78.6705551147461, mean_rew: -5.11438431117138, variance: 76.86716200256348, mean_q: -78.99470520019531, std_q: 10.078347206115723

steps: 624975, episodes: 25000, mean episode reward: -345.3070336546755, agent episode reward: [-115.1023445515585, -115.1023445515585, -115.1023445515585], time: 68.185
steps: 624975, episodes: 25000, mean episode variance: 51.15510446929932, agent episode variance: [12.590974639892577, 18.87685234832764, 19.6872774810791], time: 68.186
Running avgs for agent 0: q_loss: 1.6930204629898071, p_loss: 77.76177978515625, mean_rew: -5.099375466792562, variance: 50.36389855957031, mean_q: -78.19231414794922, std_q: 9.95816421508789
Running avgs for agent 1: q_loss: 2.3085992336273193, p_loss: 80.3099136352539, mean_rew: -5.101614864424849, variance: 75.50740939331055, mean_q: -80.5453109741211, std_q: 10.147271156311035
Running avgs for agent 2: q_loss: 2.3669590950012207, p_loss: 78.81614685058594, mean_rew: -5.098712254166184, variance: 78.7491099243164, mean_q: -79.13574981689453, std_q: 9.994851112365723

steps: 649975, episodes: 26000, mean episode reward: -342.98647142768584, agent episode reward: [-114.32882380922861, -114.32882380922861, -114.32882380922861], time: 69.678
steps: 649975, episodes: 26000, mean episode variance: 50.85239275932312, agent episode variance: [12.329396032333374, 18.946684143066406, 19.57631258392334], time: 69.679
Running avgs for agent 0: q_loss: 1.699182152748108, p_loss: 77.86495208740234, mean_rew: -5.080843866626525, variance: 49.3175841293335, mean_q: -78.27790832519531, std_q: 9.89525318145752
Running avgs for agent 1: q_loss: 2.250614643096924, p_loss: 80.32259368896484, mean_rew: -5.0760758766248655, variance: 75.78673657226562, mean_q: -80.55315399169922, std_q: 10.005436897277832
Running avgs for agent 2: q_loss: 2.3266220092773438, p_loss: 78.85630798339844, mean_rew: -5.076898835859447, variance: 78.30525033569336, mean_q: -79.17721557617188, std_q: 9.973371505737305

steps: 674975, episodes: 27000, mean episode reward: -342.5247345606784, agent episode reward: [-114.17491152022613, -114.17491152022613, -114.17491152022613], time: 69.651
steps: 674975, episodes: 27000, mean episode variance: 50.14547401809693, agent episode variance: [12.587861114501953, 18.369935390472413, 19.18767751312256], time: 69.651
Running avgs for agent 0: q_loss: 1.6664118766784668, p_loss: 77.89250183105469, mean_rew: -5.059040560770691, variance: 50.35144445800781, mean_q: -78.2872314453125, std_q: 9.86487865447998
Running avgs for agent 1: q_loss: 2.224134683609009, p_loss: 80.31360626220703, mean_rew: -5.0567387385373435, variance: 73.47974156188965, mean_q: -80.54048156738281, std_q: 9.946834564208984
Running avgs for agent 2: q_loss: 2.306978225708008, p_loss: 78.8440933227539, mean_rew: -5.057764670618841, variance: 76.75071005249023, mean_q: -79.1643295288086, std_q: 9.83804988861084

steps: 699975, episodes: 28000, mean episode reward: -342.16098213700246, agent episode reward: [-114.05366071233415, -114.05366071233415, -114.05366071233415], time: 69.509
steps: 699975, episodes: 28000, mean episode variance: 49.762815662384035, agent episode variance: [12.395926666259765, 18.234888236999513, 19.132000759124757], time: 69.51
Running avgs for agent 0: q_loss: 1.6750670671463013, p_loss: 77.93192291259766, mean_rew: -5.043236124894227, variance: 49.58370666503906, mean_q: -78.31565856933594, std_q: 9.730278015136719
Running avgs for agent 1: q_loss: 2.1992080211639404, p_loss: 80.28697204589844, mean_rew: -5.041038042205345, variance: 72.93955294799805, mean_q: -80.50713348388672, std_q: 9.915302276611328
Running avgs for agent 2: q_loss: 2.2900288105010986, p_loss: 78.83084106445312, mean_rew: -5.040847138633948, variance: 76.52800303649903, mean_q: -79.14977264404297, std_q: 9.816652297973633

steps: 724975, episodes: 29000, mean episode reward: -341.63942275893197, agent episode reward: [-113.87980758631063, -113.87980758631063, -113.87980758631063], time: 70.442
steps: 724975, episodes: 29000, mean episode variance: 50.15627731704712, agent episode variance: [12.989027618408203, 18.367847122192384, 18.799402576446532], time: 70.442
Running avgs for agent 0: q_loss: 1.675827145576477, p_loss: 77.90386199951172, mean_rew: -5.02518668406119, variance: 51.95611047363281, mean_q: -78.27958679199219, std_q: 9.651171684265137
Running avgs for agent 1: q_loss: 2.1870250701904297, p_loss: 80.20132446289062, mean_rew: -5.029368296108678, variance: 73.47138848876953, mean_q: -80.41487121582031, std_q: 9.884711265563965
Running avgs for agent 2: q_loss: 2.2799017429351807, p_loss: 78.83573913574219, mean_rew: -5.0241862581785295, variance: 75.19761030578613, mean_q: -79.15023803710938, std_q: 9.83491325378418

steps: 749975, episodes: 30000, mean episode reward: -336.94974598782795, agent episode reward: [-112.31658199594263, -112.31658199594263, -112.31658199594263], time: 71.091
steps: 749975, episodes: 30000, mean episode variance: 48.942036712646484, agent episode variance: [12.475565990447999, 17.74027607345581, 18.726194648742677], time: 71.091
Running avgs for agent 0: q_loss: 1.667229413986206, p_loss: 77.85958099365234, mean_rew: -5.010727018973922, variance: 49.902263961791995, mean_q: -78.23033142089844, std_q: 9.616827964782715
Running avgs for agent 1: q_loss: 2.1532180309295654, p_loss: 80.14862823486328, mean_rew: -5.006188491243657, variance: 70.96110429382324, mean_q: -80.35826110839844, std_q: 9.81084156036377
Running avgs for agent 2: q_loss: 2.2581727504730225, p_loss: 78.82421875, mean_rew: -5.013730117677531, variance: 74.90477859497071, mean_q: -79.1354751586914, std_q: 9.7512788772583

steps: 774975, episodes: 31000, mean episode reward: -336.349808861626, agent episode reward: [-112.11660295387534, -112.11660295387534, -112.11660295387534], time: 72.632
steps: 774975, episodes: 31000, mean episode variance: 48.843425012588504, agent episode variance: [12.30835382080078, 17.59602470397949, 18.939046487808227], time: 72.632
Running avgs for agent 0: q_loss: 1.6508060693740845, p_loss: 77.79720306396484, mean_rew: -4.9903708765056, variance: 49.23341528320312, mean_q: -78.15802764892578, std_q: 9.53381633758545
Running avgs for agent 1: q_loss: 2.121703624725342, p_loss: 80.08309173583984, mean_rew: -4.993367866764882, variance: 70.38409881591797, mean_q: -80.28942108154297, std_q: 9.819990158081055
Running avgs for agent 2: q_loss: 2.2300117015838623, p_loss: 78.74932861328125, mean_rew: -4.994132155708877, variance: 75.75618595123291, mean_q: -79.05995178222656, std_q: 9.659143447875977

steps: 799975, episodes: 32000, mean episode reward: -339.06354893401266, agent episode reward: [-113.02118297800423, -113.02118297800423, -113.02118297800423], time: 71.598
steps: 799975, episodes: 32000, mean episode variance: 48.556209953308105, agent episode variance: [12.613954540252685, 17.5384808883667, 18.40377452468872], time: 71.598
Running avgs for agent 0: q_loss: 1.6214162111282349, p_loss: 77.70532989501953, mean_rew: -4.974691294175322, variance: 50.45581816101074, mean_q: -78.05814361572266, std_q: 9.42377758026123
Running avgs for agent 1: q_loss: 2.1179263591766357, p_loss: 79.96409606933594, mean_rew: -4.980137849765881, variance: 70.1539235534668, mean_q: -80.16901397705078, std_q: 9.756013870239258
Running avgs for agent 2: q_loss: 2.2188966274261475, p_loss: 78.74661254882812, mean_rew: -4.979101493594332, variance: 73.61509809875488, mean_q: -79.05635070800781, std_q: 9.674086570739746

steps: 824975, episodes: 33000, mean episode reward: -338.21571669450867, agent episode reward: [-112.73857223150289, -112.73857223150289, -112.73857223150289], time: 71.912
steps: 824975, episodes: 33000, mean episode variance: 48.10121813964844, agent episode variance: [12.568667587280274, 17.22994096374512, 18.302609588623046], time: 71.913
Running avgs for agent 0: q_loss: 1.6437368392944336, p_loss: 77.6295166015625, mean_rew: -4.9625837406970135, variance: 50.274670349121095, mean_q: -77.97650146484375, std_q: 9.400262832641602
Running avgs for agent 1: q_loss: 2.0761070251464844, p_loss: 79.88951873779297, mean_rew: -4.96642527619331, variance: 68.91976385498047, mean_q: -80.09253692626953, std_q: 9.745060920715332
Running avgs for agent 2: q_loss: 2.16249418258667, p_loss: 78.65874481201172, mean_rew: -4.960340141785031, variance: 73.21043835449218, mean_q: -78.96365356445312, std_q: 9.568337440490723

steps: 849975, episodes: 34000, mean episode reward: -337.68838957560837, agent episode reward: [-112.5627965252028, -112.5627965252028, -112.5627965252028], time: 71.056
steps: 849975, episodes: 34000, mean episode variance: 47.62677810668945, agent episode variance: [12.372887310028077, 17.18373461532593, 18.07015618133545], time: 71.057
Running avgs for agent 0: q_loss: 1.6227668523788452, p_loss: 77.55021667480469, mean_rew: -4.9476719801608295, variance: 49.491549240112306, mean_q: -77.89240264892578, std_q: 9.346683502197266
Running avgs for agent 1: q_loss: 2.06331205368042, p_loss: 79.76087188720703, mean_rew: -4.954271624538808, variance: 68.73493846130371, mean_q: -79.96448516845703, std_q: 9.757492065429688
Running avgs for agent 2: q_loss: 2.1296708583831787, p_loss: 78.60355377197266, mean_rew: -4.945823335703113, variance: 72.2806247253418, mean_q: -78.90473937988281, std_q: 9.591917037963867

steps: 874975, episodes: 35000, mean episode reward: -337.24477632695124, agent episode reward: [-112.41492544231707, -112.41492544231707, -112.41492544231707], time: 70.94
steps: 874975, episodes: 35000, mean episode variance: 48.196733806610105, agent episode variance: [12.558580425262452, 17.304371925354005, 18.333781455993652], time: 70.941
Running avgs for agent 0: q_loss: 1.603972315788269, p_loss: 77.4391098022461, mean_rew: -4.935072942822795, variance: 50.23432170104981, mean_q: -77.7822494506836, std_q: 9.324528694152832
Running avgs for agent 1: q_loss: 2.02882981300354, p_loss: 79.55340576171875, mean_rew: -4.9382700542449465, variance: 69.21748770141602, mean_q: -79.7570571899414, std_q: 9.704349517822266
Running avgs for agent 2: q_loss: 2.1212105751037598, p_loss: 78.52323150634766, mean_rew: -4.9369469268252315, variance: 73.33512582397461, mean_q: -78.81818389892578, std_q: 9.627896308898926

steps: 899975, episodes: 36000, mean episode reward: -339.59694615713875, agent episode reward: [-113.19898205237958, -113.19898205237958, -113.19898205237958], time: 71.932
steps: 899975, episodes: 36000, mean episode variance: 46.60034683609009, agent episode variance: [12.08895877456665, 17.02019895172119, 17.491189109802246], time: 71.933
Running avgs for agent 0: q_loss: 1.591256856918335, p_loss: 77.31659698486328, mean_rew: -4.924554304795409, variance: 48.3558350982666, mean_q: -77.66584014892578, std_q: 9.238840103149414
Running avgs for agent 1: q_loss: 2.0221173763275146, p_loss: 79.41307830810547, mean_rew: -4.928109658001734, variance: 68.08079580688477, mean_q: -79.6170883178711, std_q: 9.722482681274414
Running avgs for agent 2: q_loss: 2.0875844955444336, p_loss: 78.37734985351562, mean_rew: -4.922990490959071, variance: 69.96475643920898, mean_q: -78.66768646240234, std_q: 9.528741836547852

steps: 924975, episodes: 37000, mean episode reward: -339.42633172376225, agent episode reward: [-113.14211057458742, -113.14211057458742, -113.14211057458742], time: 71.41
steps: 924975, episodes: 37000, mean episode variance: 47.17996669387817, agent episode variance: [12.52450942993164, 16.83801540374756, 17.817441860198976], time: 71.411
Running avgs for agent 0: q_loss: 1.5831514596939087, p_loss: 77.15043640136719, mean_rew: -4.908171785912933, variance: 50.09803771972656, mean_q: -77.49482727050781, std_q: 9.238221168518066
Running avgs for agent 1: q_loss: 1.9726523160934448, p_loss: 79.22186279296875, mean_rew: -4.912841471471311, variance: 67.35206161499023, mean_q: -79.42189025878906, std_q: 9.66994571685791
Running avgs for agent 2: q_loss: 2.05889892578125, p_loss: 78.3088607788086, mean_rew: -4.911459911211881, variance: 71.2697674407959, mean_q: -78.60023498535156, std_q: 9.550956726074219

steps: 949975, episodes: 38000, mean episode reward: -335.35575223151477, agent episode reward: [-111.78525074383826, -111.78525074383826, -111.78525074383826], time: 72.476
steps: 949975, episodes: 38000, mean episode variance: 46.7186728515625, agent episode variance: [12.384228534698487, 17.286194358825682, 17.04824995803833], time: 72.476
Running avgs for agent 0: q_loss: 1.5804802179336548, p_loss: 77.08059692382812, mean_rew: -4.90796430514187, variance: 49.53691413879395, mean_q: -77.42121887207031, std_q: 9.21401596069336
Running avgs for agent 1: q_loss: 1.960673451423645, p_loss: 79.07369995117188, mean_rew: -4.90380347910998, variance: 69.14477743530273, mean_q: -79.2740478515625, std_q: 9.67049789428711
Running avgs for agent 2: q_loss: 2.019392728805542, p_loss: 78.25098419189453, mean_rew: -4.8996933524438715, variance: 68.19299983215332, mean_q: -78.53579711914062, std_q: 9.465624809265137

steps: 974975, episodes: 39000, mean episode reward: -339.6813428285718, agent episode reward: [-113.2271142761906, -113.2271142761906, -113.2271142761906], time: 66.697
steps: 974975, episodes: 39000, mean episode variance: 45.389913528442385, agent episode variance: [12.7825584564209, 16.148136238098143, 16.45921883392334], time: 66.698
Running avgs for agent 0: q_loss: 1.553861141204834, p_loss: 76.84900665283203, mean_rew: -4.889809241812209, variance: 51.1302338256836, mean_q: -77.18695068359375, std_q: 9.018877029418945
Running avgs for agent 1: q_loss: 1.9530991315841675, p_loss: 78.97811126708984, mean_rew: -4.891751932882826, variance: 64.59254495239257, mean_q: -79.17731475830078, std_q: 9.605443000793457
Running avgs for agent 2: q_loss: 2.0030484199523926, p_loss: 78.20582580566406, mean_rew: -4.8896353202386145, variance: 65.83687533569336, mean_q: -78.48721313476562, std_q: 9.486072540283203

steps: 999975, episodes: 40000, mean episode reward: -335.0221418607103, agent episode reward: [-111.67404728690342, -111.67404728690342, -111.67404728690342], time: 69.897
steps: 999975, episodes: 40000, mean episode variance: 45.25925014495849, agent episode variance: [12.06585988998413, 16.4020560798645, 16.791334175109863], time: 69.897
Running avgs for agent 0: q_loss: 1.5550458431243896, p_loss: 76.80899810791016, mean_rew: -4.88499705410585, variance: 48.26343955993652, mean_q: -77.14110565185547, std_q: 9.003933906555176
Running avgs for agent 1: q_loss: 1.9356032609939575, p_loss: 78.81230926513672, mean_rew: -4.883822206086813, variance: 65.608224319458, mean_q: -79.01094055175781, std_q: 9.610100746154785
Running avgs for agent 2: q_loss: 1.987549901008606, p_loss: 78.12600708007812, mean_rew: -4.879710808110581, variance: 67.16533670043945, mean_q: -78.40480041503906, std_q: 9.517171859741211

steps: 1024975, episodes: 41000, mean episode reward: -335.5992049543815, agent episode reward: [-111.8664016514605, -111.8664016514605, -111.8664016514605], time: 72.768
steps: 1024975, episodes: 41000, mean episode variance: 44.8969969329834, agent episode variance: [12.38878060913086, 16.107463485717773, 16.400752838134764], time: 72.769
Running avgs for agent 0: q_loss: 1.5443403720855713, p_loss: 76.49334716796875, mean_rew: -4.845642686474451, variance: 49.55512243652344, mean_q: -76.80950164794922, std_q: 8.773543357849121
Running avgs for agent 1: q_loss: 1.9066835641860962, p_loss: 78.47872161865234, mean_rew: -4.843418851524433, variance: 64.42985394287109, mean_q: -78.67108154296875, std_q: 9.367653846740723
Running avgs for agent 2: q_loss: 1.9470664262771606, p_loss: 77.89745330810547, mean_rew: -4.844433690616998, variance: 65.60301135253906, mean_q: -78.16413116455078, std_q: 9.293712615966797

steps: 1049975, episodes: 42000, mean episode reward: -335.41187205788935, agent episode reward: [-111.80395735262981, -111.80395735262981, -111.80395735262981], time: 82.239
steps: 1049975, episodes: 42000, mean episode variance: 43.09167063140869, agent episode variance: [11.714980052947999, 15.747011657714843, 15.62967892074585], time: 82.239
Running avgs for agent 0: q_loss: 1.4987140893936157, p_loss: 76.00845336914062, mean_rew: -4.773755127609888, variance: 46.859920211791994, mean_q: -76.2843017578125, std_q: 7.972179412841797
Running avgs for agent 1: q_loss: 1.841478705406189, p_loss: 77.93219757080078, mean_rew: -4.776182348556623, variance: 62.98804663085937, mean_q: -78.09798431396484, std_q: 8.622989654541016
Running avgs for agent 2: q_loss: 1.8665012121200562, p_loss: 77.4445571899414, mean_rew: -4.777006418500973, variance: 62.5187156829834, mean_q: -77.68447875976562, std_q: 8.585333824157715

steps: 1074975, episodes: 43000, mean episode reward: -335.66340252767935, agent episode reward: [-111.88780084255978, -111.88780084255978, -111.88780084255978], time: 72.501
steps: 1074975, episodes: 43000, mean episode variance: 43.08852596282959, agent episode variance: [12.08166728591919, 15.49242989730835, 15.514428779602051], time: 72.501
Running avgs for agent 0: q_loss: 1.4370983839035034, p_loss: 75.56661987304688, mean_rew: -4.711037338516706, variance: 48.32666914367676, mean_q: -75.80938720703125, std_q: 7.26693058013916
Running avgs for agent 1: q_loss: 1.7775906324386597, p_loss: 77.43000030517578, mean_rew: -4.716263913178532, variance: 61.9697195892334, mean_q: -77.57536315917969, std_q: 7.976473808288574
Running avgs for agent 2: q_loss: 1.7957093715667725, p_loss: 77.0108413696289, mean_rew: -4.713531456787642, variance: 62.057715118408204, mean_q: -77.22201538085938, std_q: 7.88346004486084

steps: 1099975, episodes: 44000, mean episode reward: -339.64119248671454, agent episode reward: [-113.21373082890486, -113.21373082890486, -113.21373082890486], time: 72.7
steps: 1099975, episodes: 44000, mean episode variance: 42.393327613830564, agent episode variance: [11.612605522155762, 15.235528854370116, 15.545193237304687], time: 72.7
Running avgs for agent 0: q_loss: 1.4235092401504517, p_loss: 75.40435028076172, mean_rew: -4.679067733275187, variance: 46.45042208862305, mean_q: -75.62884521484375, std_q: 7.158637523651123
Running avgs for agent 1: q_loss: 1.7457302808761597, p_loss: 77.16647338867188, mean_rew: -4.68246635395227, variance: 60.942115417480466, mean_q: -77.30018615722656, std_q: 7.7936320304870605
Running avgs for agent 2: q_loss: 1.7469896078109741, p_loss: 76.87237548828125, mean_rew: -4.6764632721929384, variance: 62.18077294921875, mean_q: -77.06455993652344, std_q: 7.749820232391357

steps: 1124975, episodes: 45000, mean episode reward: -339.04051013273966, agent episode reward: [-113.0135033775799, -113.0135033775799, -113.0135033775799], time: 72.579
steps: 1124975, episodes: 45000, mean episode variance: 41.364769155502316, agent episode variance: [11.36498465538025, 14.97461831665039, 15.02516618347168], time: 72.579
Running avgs for agent 0: q_loss: 1.3809751272201538, p_loss: 75.2321548461914, mean_rew: -4.642131758245799, variance: 45.459938621521, mean_q: -75.44251251220703, std_q: 7.1003828048706055
Running avgs for agent 1: q_loss: 1.707626461982727, p_loss: 76.9733657836914, mean_rew: -4.648213931410881, variance: 59.89847326660156, mean_q: -77.09698486328125, std_q: 7.741457462310791
Running avgs for agent 2: q_loss: 1.6963387727737427, p_loss: 76.74139404296875, mean_rew: -4.646157935313428, variance: 60.10066473388672, mean_q: -76.91966247558594, std_q: 7.675568103790283

steps: 1149975, episodes: 46000, mean episode reward: -336.86372492917803, agent episode reward: [-112.287908309726, -112.287908309726, -112.287908309726], time: 72.112
steps: 1149975, episodes: 46000, mean episode variance: 41.077321166992185, agent episode variance: [11.439440780639648, 14.851647354125976, 14.786233032226562], time: 72.113
Running avgs for agent 0: q_loss: 1.362640142440796, p_loss: 75.08735656738281, mean_rew: -4.624307771096628, variance: 45.75776312255859, mean_q: -75.2860336303711, std_q: 7.123348712921143
Running avgs for agent 1: q_loss: 1.6774547100067139, p_loss: 76.82279205322266, mean_rew: -4.626899404461739, variance: 59.406589416503905, mean_q: -76.94003295898438, std_q: 7.676069259643555
Running avgs for agent 2: q_loss: 1.6210275888442993, p_loss: 76.68042755126953, mean_rew: -4.631126331650243, variance: 59.14493212890625, mean_q: -76.84815979003906, std_q: 7.642902851104736

steps: 1174975, episodes: 47000, mean episode reward: -335.81999165515185, agent episode reward: [-111.93999721838392, -111.93999721838392, -111.93999721838392], time: 90.445
steps: 1174975, episodes: 47000, mean episode variance: 40.909831703186036, agent episode variance: [11.550528781890868, 14.841497772216798, 14.517805149078368], time: 90.445
Running avgs for agent 0: q_loss: 1.355642318725586, p_loss: 75.01286315917969, mean_rew: -4.62128653084163, variance: 46.202115127563474, mean_q: -75.202392578125, std_q: 7.171989440917969
Running avgs for agent 1: q_loss: 1.6253694295883179, p_loss: 76.67190551757812, mean_rew: -4.611097354017717, variance: 59.36599108886719, mean_q: -76.78527069091797, std_q: 7.631401538848877
Running avgs for agent 2: q_loss: 1.5986112356185913, p_loss: 76.63003540039062, mean_rew: -4.611106750375387, variance: 58.07122059631347, mean_q: -76.79141235351562, std_q: 7.607389450073242

steps: 1199975, episodes: 48000, mean episode reward: -337.5552316745099, agent episode reward: [-112.51841055817, -112.51841055817, -112.51841055817], time: 116.529
steps: 1199975, episodes: 48000, mean episode variance: 39.57026589202881, agent episode variance: [11.236223411560058, 14.080985359191894, 14.253057121276855], time: 116.53
Running avgs for agent 0: q_loss: 1.3573577404022217, p_loss: 74.9061508178711, mean_rew: -4.6019970837128605, variance: 44.94489364624023, mean_q: -75.09031677246094, std_q: 7.151006698608398
Running avgs for agent 1: q_loss: 1.6031088829040527, p_loss: 76.60546875, mean_rew: -4.601403927228517, variance: 56.323941436767576, mean_q: -76.71453094482422, std_q: 7.5723466873168945
Running avgs for agent 2: q_loss: 1.5982263088226318, p_loss: 76.62915802001953, mean_rew: -4.605039652289099, variance: 57.01222848510742, mean_q: -76.78276824951172, std_q: 7.529206275939941

steps: 1224975, episodes: 49000, mean episode reward: -336.9032892732499, agent episode reward: [-112.30109642441664, -112.30109642441664, -112.30109642441664], time: 96.814
steps: 1224975, episodes: 49000, mean episode variance: 39.73153765106201, agent episode variance: [11.154944190979004, 14.214354541778565, 14.362238918304444], time: 96.815
Running avgs for agent 0: q_loss: 1.3469936847686768, p_loss: 74.750732421875, mean_rew: -4.58603462196542, variance: 44.619776763916015, mean_q: -74.92964172363281, std_q: 7.100325584411621
Running avgs for agent 1: q_loss: 1.6052032709121704, p_loss: 76.51301574707031, mean_rew: -4.590168382899604, variance: 56.85741816711426, mean_q: -76.61860656738281, std_q: 7.570793151855469
Running avgs for agent 2: q_loss: 1.5676325559616089, p_loss: 76.54737854003906, mean_rew: -4.592354344760678, variance: 57.448955673217775, mean_q: -76.69265747070312, std_q: 7.488830089569092

steps: 1249975, episodes: 50000, mean episode reward: -332.9371077551064, agent episode reward: [-110.9790359183688, -110.9790359183688, -110.9790359183688], time: 72.996
steps: 1249975, episodes: 50000, mean episode variance: 39.73330191421509, agent episode variance: [11.136909046173095, 14.372180187225341, 14.22421268081665], time: 72.996
Running avgs for agent 0: q_loss: 1.327190637588501, p_loss: 74.62109375, mean_rew: -4.578044421692239, variance: 44.54763618469238, mean_q: -74.79167175292969, std_q: 7.098069667816162
Running avgs for agent 1: q_loss: 1.6073477268218994, p_loss: 76.38568878173828, mean_rew: -4.580911189599101, variance: 57.488720748901365, mean_q: -76.49314880371094, std_q: 7.5733866691589355
Running avgs for agent 2: q_loss: 1.5534101724624634, p_loss: 76.5029067993164, mean_rew: -4.581166526233047, variance: 56.8968507232666, mean_q: -76.64690399169922, std_q: 7.500444412231445

steps: 1274975, episodes: 51000, mean episode reward: -334.2934771475167, agent episode reward: [-111.43115904917225, -111.43115904917225, -111.43115904917225], time: 72.556
steps: 1274975, episodes: 51000, mean episode variance: 39.01220043182373, agent episode variance: [11.214210430145263, 13.873284934997558, 13.924705066680907], time: 72.557
Running avgs for agent 0: q_loss: 1.3088243007659912, p_loss: 74.455078125, mean_rew: -4.567502817515642, variance: 44.85684172058105, mean_q: -74.6254653930664, std_q: 7.114638805389404
Running avgs for agent 1: q_loss: 1.5405720472335815, p_loss: 76.2669677734375, mean_rew: -4.5735277091163, variance: 55.493139739990234, mean_q: -76.37423706054688, std_q: 7.6071906089782715
Running avgs for agent 2: q_loss: 1.53200364112854, p_loss: 76.41112518310547, mean_rew: -4.570032928111573, variance: 55.69882026672363, mean_q: -76.55645751953125, std_q: 7.513197422027588

steps: 1299975, episodes: 52000, mean episode reward: -334.7892927683815, agent episode reward: [-111.59643092279381, -111.59643092279381, -111.59643092279381], time: 72.869
steps: 1299975, episodes: 52000, mean episode variance: 38.57851696014404, agent episode variance: [10.871222229003907, 13.819309036254882, 13.887985694885254], time: 72.869
Running avgs for agent 0: q_loss: 1.3093292713165283, p_loss: 74.32164764404297, mean_rew: -4.5609554683718745, variance: 43.48488891601563, mean_q: -74.4798583984375, std_q: 7.086852550506592
Running avgs for agent 1: q_loss: 1.553667426109314, p_loss: 76.09427642822266, mean_rew: -4.562435024068882, variance: 55.27723614501953, mean_q: -76.19842529296875, std_q: 7.571942329406738
Running avgs for agent 2: q_loss: 1.5091426372528076, p_loss: 76.30581665039062, mean_rew: -4.5621846432585995, variance: 55.551942779541015, mean_q: -76.4474105834961, std_q: 7.54802942276001

steps: 1324975, episodes: 53000, mean episode reward: -334.3368606642093, agent episode reward: [-111.4456202214031, -111.4456202214031, -111.4456202214031], time: 72.189
steps: 1324975, episodes: 53000, mean episode variance: 38.20222118377686, agent episode variance: [10.959444477081298, 13.502509262084962, 13.740267444610597], time: 72.19
Running avgs for agent 0: q_loss: 1.2800439596176147, p_loss: 74.17754364013672, mean_rew: -4.551065254436011, variance: 43.837777908325194, mean_q: -74.3334732055664, std_q: 7.111051559448242
Running avgs for agent 1: q_loss: 1.516412615776062, p_loss: 75.90426635742188, mean_rew: -4.549166109820866, variance: 54.010037048339846, mean_q: -76.0113525390625, std_q: 7.6044816970825195
Running avgs for agent 2: q_loss: 1.4987692832946777, p_loss: 76.2145767211914, mean_rew: -4.551287267125846, variance: 54.961069778442386, mean_q: -76.35250091552734, std_q: 7.5492095947265625

steps: 1349975, episodes: 54000, mean episode reward: -332.72176238978943, agent episode reward: [-110.90725412992981, -110.90725412992981, -110.90725412992981], time: 72.196
steps: 1349975, episodes: 54000, mean episode variance: 37.55131071090698, agent episode variance: [10.867866966247558, 13.241048709869386, 13.442395034790039], time: 72.196
Running avgs for agent 0: q_loss: 1.2545764446258545, p_loss: 74.05194854736328, mean_rew: -4.540689165558815, variance: 43.47146786499023, mean_q: -74.19740295410156, std_q: 7.107310771942139
Running avgs for agent 1: q_loss: 1.4873394966125488, p_loss: 75.71697235107422, mean_rew: -4.539979726047805, variance: 52.96419483947754, mean_q: -75.82064819335938, std_q: 7.6001691818237305
Running avgs for agent 2: q_loss: 1.4756147861480713, p_loss: 76.0936279296875, mean_rew: -4.545709415615857, variance: 53.769580139160155, mean_q: -76.23148345947266, std_q: 7.568700790405273

steps: 1374975, episodes: 55000, mean episode reward: -331.3730313516726, agent episode reward: [-110.45767711722421, -110.45767711722421, -110.45767711722421], time: 72.692
steps: 1374975, episodes: 55000, mean episode variance: 37.395267345428465, agent episode variance: [10.530694576263429, 13.24147904586792, 13.623093723297119], time: 72.693
Running avgs for agent 0: q_loss: 1.2475578784942627, p_loss: 73.98828125, mean_rew: -4.541209596574442, variance: 42.122778305053714, mean_q: -74.13368225097656, std_q: 7.122750759124756
Running avgs for agent 1: q_loss: 1.469867467880249, p_loss: 75.564697265625, mean_rew: -4.5412641116822705, variance: 52.96591618347168, mean_q: -75.67050170898438, std_q: 7.6103925704956055
Running avgs for agent 2: q_loss: 1.4656720161437988, p_loss: 75.93250274658203, mean_rew: -4.529550781210972, variance: 54.492374893188476, mean_q: -76.06370544433594, std_q: 7.559298038482666

steps: 1399975, episodes: 56000, mean episode reward: -331.178712757539, agent episode reward: [-110.39290425251299, -110.39290425251299, -110.39290425251299], time: 72.275
steps: 1399975, episodes: 56000, mean episode variance: 36.533940353393554, agent episode variance: [10.55235718536377, 12.707293689727782, 13.274289478302002], time: 72.275
Running avgs for agent 0: q_loss: 1.2273461818695068, p_loss: 73.8946533203125, mean_rew: -4.534668406049033, variance: 42.20942874145508, mean_q: -74.03367614746094, std_q: 7.159367084503174
Running avgs for agent 1: q_loss: 1.4335627555847168, p_loss: 75.2947998046875, mean_rew: -4.527068339573935, variance: 50.82917475891113, mean_q: -75.4020004272461, std_q: 7.571194648742676
Running avgs for agent 2: q_loss: 1.4649806022644043, p_loss: 75.81930541992188, mean_rew: -4.527906421252851, variance: 53.09715791320801, mean_q: -75.95006561279297, std_q: 7.549161911010742

steps: 1424975, episodes: 57000, mean episode reward: -330.8900195597943, agent episode reward: [-110.29667318659813, -110.29667318659813, -110.29667318659813], time: 78.504
steps: 1424975, episodes: 57000, mean episode variance: 36.890120662689206, agent episode variance: [10.804774280548095, 12.770866786956788, 13.314479595184325], time: 78.505
Running avgs for agent 0: q_loss: 1.1962394714355469, p_loss: 73.69847106933594, mean_rew: -4.5215845813655156, variance: 43.21909712219238, mean_q: -73.83731079101562, std_q: 7.135225772857666
Running avgs for agent 1: q_loss: 1.4058146476745605, p_loss: 75.0821762084961, mean_rew: -4.521874037094707, variance: 51.08346714782715, mean_q: -75.18767547607422, std_q: 7.5573410987854
Running avgs for agent 2: q_loss: 1.457050085067749, p_loss: 75.6796875, mean_rew: -4.526854840986427, variance: 53.2579183807373, mean_q: -75.80699157714844, std_q: 7.578001499176025

steps: 1449975, episodes: 58000, mean episode reward: -331.16561527198746, agent episode reward: [-110.38853842399581, -110.38853842399581, -110.38853842399581], time: 72.581
steps: 1449975, episodes: 58000, mean episode variance: 36.569775272369384, agent episode variance: [10.345736957550049, 12.637265731811523, 13.586772583007813], time: 72.582
Running avgs for agent 0: q_loss: 1.2020944356918335, p_loss: 73.57788848876953, mean_rew: -4.521873086605825, variance: 41.382947830200195, mean_q: -73.71745300292969, std_q: 7.132444381713867
Running avgs for agent 1: q_loss: 1.3906501531600952, p_loss: 74.86790466308594, mean_rew: -4.520605593106719, variance: 50.54906292724609, mean_q: -74.9734878540039, std_q: 7.541865348815918
Running avgs for agent 2: q_loss: 1.4565740823745728, p_loss: 75.47892761230469, mean_rew: -4.51940179918288, variance: 54.34709033203125, mean_q: -75.60173797607422, std_q: 7.56613302230835

steps: 1474975, episodes: 59000, mean episode reward: -325.25741081676324, agent episode reward: [-108.41913693892108, -108.41913693892108, -108.41913693892108], time: 72.868
steps: 1474975, episodes: 59000, mean episode variance: 36.23111389160156, agent episode variance: [10.485750244140625, 12.611805305480956, 13.133558341979981], time: 72.868
Running avgs for agent 0: q_loss: 1.1922340393066406, p_loss: 73.40587615966797, mean_rew: -4.5143241203555915, variance: 41.9430009765625, mean_q: -73.5439224243164, std_q: 7.081880569458008
Running avgs for agent 1: q_loss: 1.3835817575454712, p_loss: 74.6328353881836, mean_rew: -4.5156051147567355, variance: 50.447221221923826, mean_q: -74.73847198486328, std_q: 7.508309364318848
Running avgs for agent 2: q_loss: 1.423287272453308, p_loss: 75.32514953613281, mean_rew: -4.5109782887213425, variance: 52.534233367919924, mean_q: -75.4453353881836, std_q: 7.5771803855896

steps: 1499975, episodes: 60000, mean episode reward: -328.16565021772226, agent episode reward: [-109.38855007257409, -109.38855007257409, -109.38855007257409], time: 71.246
steps: 1499975, episodes: 60000, mean episode variance: 35.51316415786743, agent episode variance: [10.070486103057862, 12.354812023162841, 13.087866031646728], time: 71.246
Running avgs for agent 0: q_loss: 1.1832698583602905, p_loss: 73.30586242675781, mean_rew: -4.5052101104324915, variance: 40.28194441223145, mean_q: -73.44536590576172, std_q: 7.023879528045654
Running avgs for agent 1: q_loss: 1.3620234727859497, p_loss: 74.4128646850586, mean_rew: -4.504816395632263, variance: 49.419248092651365, mean_q: -74.51592254638672, std_q: 7.479183673858643
Running avgs for agent 2: q_loss: 1.408410906791687, p_loss: 75.15662384033203, mean_rew: -4.504743995237503, variance: 52.35146412658691, mean_q: -75.27190399169922, std_q: 7.545546531677246

...Finished total of 60001 episodes... Now freezing policy and running for 10000 more episodes to get cvar estimation
steps: 24975, episodes: 1000, mean episode reward: -326.601830985632, agent episode reward: [-108.86727699521064, -108.86727699521064, -108.86727699521064], time: 53.484
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 53.484
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -328.2186302097519, agent episode reward: [-109.4062100699173, -109.4062100699173, -109.4062100699173], time: 68.757
steps: 49975, episodes: 2000, mean episode variance: 82.87461979675292, agent episode variance: [23.212045738220215, 33.92320195007324, 25.739372108459474], time: 68.758
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.36682399291454, variance: 95.13134002685547, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -71.98411560058594, std_q: 6.825689792633057
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.361843924876332, variance: 139.02952575683594, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -72.4801254272461, std_q: 7.277237415313721
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.362324981955239, variance: 105.4892349243164, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -73.6726303100586, std_q: 7.282092094421387

steps: 74975, episodes: 3000, mean episode reward: -327.39486708898124, agent episode reward: [-109.13162236299374, -109.13162236299374, -109.13162236299374], time: 70.188
steps: 74975, episodes: 3000, mean episode variance: 85.37554552459717, agent episode variance: [23.94771257019043, 34.89509797668457, 26.532734977722168], time: 70.189
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370204235352734, variance: 95.79084014892578, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -72.04618835449219, std_q: 6.924370765686035
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370787360728525, variance: 139.58038330078125, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -72.56055450439453, std_q: 7.342959403991699
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.364085633891784, variance: 106.13093566894531, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -73.72517395019531, std_q: 7.3464179039001465

steps: 99975, episodes: 4000, mean episode reward: -328.2124894211126, agent episode reward: [-109.40416314037085, -109.40416314037085, -109.40416314037085], time: 71.102
steps: 99975, episodes: 4000, mean episode variance: 86.12809476470947, agent episode variance: [24.313829597473145, 34.9455145111084, 26.86875065612793], time: 71.102
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370209268547828, variance: 97.25531005859375, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -72.0615234375, std_q: 6.923915386199951
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370907541919204, variance: 139.7820587158203, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -72.61536407470703, std_q: 7.359403133392334
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.374044406074514, variance: 107.4749984741211, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -73.79594421386719, std_q: 7.387566566467285

steps: 124975, episodes: 5000, mean episode reward: -328.6272923861524, agent episode reward: [-109.54243079538413, -109.54243079538413, -109.54243079538413], time: 70.26
steps: 124975, episodes: 5000, mean episode variance: 86.28799481964111, agent episode variance: [24.240461685180666, 35.195567184448244, 26.851965950012207], time: 70.261
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3726845396598595, variance: 96.96184539794922, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -72.11094665527344, std_q: 6.9445390701293945
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.376379124498221, variance: 140.7822723388672, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -72.67279815673828, std_q: 7.3863677978515625
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373853423208907, variance: 107.4078598022461, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -73.84165954589844, std_q: 7.432236671447754

steps: 149975, episodes: 6000, mean episode reward: -327.5654301843049, agent episode reward: [-109.18847672810165, -109.18847672810165, -109.18847672810165], time: 70.453
steps: 149975, episodes: 6000, mean episode variance: 86.24433857727051, agent episode variance: [24.369000358581545, 35.106745437622074, 26.768592781066893], time: 70.453
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.377234782508234, variance: 97.47599792480469, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -72.13348388671875, std_q: 6.978602409362793
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373629378332465, variance: 140.42698669433594, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -72.66143798828125, std_q: 7.388957977294922
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.375529287813298, variance: 107.0743637084961, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -73.84920501708984, std_q: 7.466180324554443

steps: 174975, episodes: 7000, mean episode reward: -329.00259847002513, agent episode reward: [-109.6675328233417, -109.6675328233417, -109.6675328233417], time: 70.426
steps: 174975, episodes: 7000, mean episode variance: 86.27835415649415, agent episode variance: [24.255139251708986, 35.116267974853514, 26.906946929931642], time: 70.426
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.375265985769864, variance: 97.02056121826172, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -72.1524887084961, std_q: 6.95470666885376
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.377014397453395, variance: 140.46507263183594, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -72.71249389648438, std_q: 7.40101957321167
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373843353381282, variance: 107.62779235839844, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -73.86905670166016, std_q: 7.4635748863220215

steps: 199975, episodes: 8000, mean episode reward: -327.0559676460001, agent episode reward: [-109.01865588200002, -109.01865588200002, -109.01865588200002], time: 70.075
steps: 199975, episodes: 8000, mean episode variance: 86.55406374359131, agent episode variance: [24.48397797393799, 35.15786929321289, 26.91221647644043], time: 70.076
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3782692903348925, variance: 97.93590545654297, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -72.16146850585938, std_q: 6.961404800415039
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.372820177350602, variance: 140.6314697265625, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -72.70985412597656, std_q: 7.3764190673828125
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.37110996908301, variance: 107.64886474609375, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -73.86844635009766, std_q: 7.459184646606445

steps: 224975, episodes: 9000, mean episode reward: -328.13661457620907, agent episode reward: [-109.37887152540301, -109.37887152540301, -109.37887152540301], time: 71.543
steps: 224975, episodes: 9000, mean episode variance: 86.59438745117187, agent episode variance: [24.49588265991211, 35.160062576293946, 26.93844221496582], time: 71.543
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.377613419974414, variance: 97.98352813720703, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -72.16002655029297, std_q: 6.948253154754639
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.378406667113983, variance: 140.64024353027344, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -72.73322296142578, std_q: 7.40453577041626
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.37407574261371, variance: 107.75376892089844, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -73.90009307861328, std_q: 7.429474353790283

steps: 249975, episodes: 10000, mean episode reward: -326.5106637254822, agent episode reward: [-108.83688790849405, -108.83688790849405, -108.83688790849405], time: 72.039
steps: 249975, episodes: 10000, mean episode variance: 86.74579644012451, agent episode variance: [24.443556083679198, 35.24609959411621, 27.0561407623291], time: 72.04
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.376222271024964, variance: 97.77422332763672, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -72.16415405273438, std_q: 6.937070369720459
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.377861957884796, variance: 140.98439025878906, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -72.73878479003906, std_q: 7.398109436035156
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.378628645627103, variance: 108.22456359863281, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -73.91522216796875, std_q: 7.478283882141113

steps: 274975, episodes: 11000, mean episode reward: -326.58862525651034, agent episode reward: [-108.86287508550345, -108.86287508550345, -108.86287508550345], time: 71.394
steps: 274975, episodes: 11000, mean episode variance: 86.55607294464112, agent episode variance: [24.418821334838867, 35.11075885009765, 27.02649275970459], time: 71.394
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.368808262824152, variance: 97.67527770996094, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -72.12422180175781, std_q: 6.906248569488525
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.374946183051798, variance: 140.44302368164062, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -72.72948455810547, std_q: 7.402340888977051
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.375712133344688, variance: 108.10597229003906, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -73.9116439819336, std_q: 7.452579975128174

steps: 299975, episodes: 12000, mean episode reward: -327.0280597535267, agent episode reward: [-109.00935325117558, -109.00935325117558, -109.00935325117558], time: 70.524
steps: 299975, episodes: 12000, mean episode variance: 86.58850679779053, agent episode variance: [24.378843002319336, 35.17497708129883, 27.034686714172363], time: 70.525
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373866931868788, variance: 97.51537322998047, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -72.15459442138672, std_q: 6.935612201690674
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370534725272932, variance: 140.6999053955078, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -72.6964111328125, std_q: 7.360856533050537
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.369066473880293, variance: 108.13874816894531, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -73.87506103515625, std_q: 7.46155309677124

steps: 324975, episodes: 13000, mean episode reward: -328.4055639345537, agent episode reward: [-109.46852131151789, -109.46852131151789, -109.46852131151789], time: 71.298
steps: 324975, episodes: 13000, mean episode variance: 86.48781844329834, agent episode variance: [24.330833503723145, 35.09139204406738, 27.065592895507812], time: 71.298
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.366552737820264, variance: 97.32333374023438, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -72.12370300292969, std_q: 6.913939952850342
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370809346501001, variance: 140.36558532714844, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -72.72660064697266, std_q: 7.385256767272949
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371306096949851, variance: 108.26237487792969, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -73.8792953491211, std_q: 7.447370529174805

steps: 349975, episodes: 14000, mean episode reward: -328.3346963952661, agent episode reward: [-109.44489879842205, -109.44489879842205, -109.44489879842205], time: 70.866
steps: 349975, episodes: 14000, mean episode variance: 86.77705990600586, agent episode variance: [24.48677780151367, 35.27590370178223, 27.014378402709962], time: 70.867
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3667745179613595, variance: 97.94711303710938, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -72.12186431884766, std_q: 6.916814804077148
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373133257560903, variance: 141.10360717773438, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -72.7187271118164, std_q: 7.3669843673706055
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373893767400355, variance: 108.0575180053711, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -73.91676330566406, std_q: 7.439696311950684

steps: 374975, episodes: 15000, mean episode reward: -328.9061553084323, agent episode reward: [-109.63538510281077, -109.63538510281077, -109.63538510281077], time: 71.216
steps: 374975, episodes: 15000, mean episode variance: 86.87825211334228, agent episode variance: [24.519362869262697, 35.33953240966797, 27.01935683441162], time: 71.216
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.376731423911899, variance: 98.07745361328125, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -72.18183135986328, std_q: 6.936344146728516
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.372878590437652, variance: 141.35812377929688, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -72.71707916259766, std_q: 7.362910270690918
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.376381985921439, variance: 108.07742309570312, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -73.90953063964844, std_q: 7.46919584274292

steps: 399975, episodes: 16000, mean episode reward: -326.0479621370622, agent episode reward: [-108.6826540456874, -108.6826540456874, -108.6826540456874], time: 71.03
steps: 399975, episodes: 16000, mean episode variance: 86.67213475799561, agent episode variance: [24.61476234436035, 35.16250931549072, 26.89486309814453], time: 71.031
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.372775730329171, variance: 98.45905303955078, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -72.15107727050781, std_q: 6.908166885375977
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.36886990085549, variance: 140.6500244140625, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -72.69615936279297, std_q: 7.344809532165527
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371875600343484, variance: 107.57945251464844, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -73.8987045288086, std_q: 7.449531555175781

steps: 424975, episodes: 17000, mean episode reward: -328.5401201424331, agent episode reward: [-109.51337338081106, -109.51337338081106, -109.51337338081106], time: 70.982
steps: 424975, episodes: 17000, mean episode variance: 86.68383304595947, agent episode variance: [24.60119297027588, 35.136516296386716, 26.946123779296876], time: 70.983
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.372915846538983, variance: 98.40476989746094, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -72.13980102539062, std_q: 6.917089939117432
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371863323624548, variance: 140.54608154296875, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -72.7107162475586, std_q: 7.358192443847656
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370281191213093, variance: 107.78450012207031, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -73.87078094482422, std_q: 7.413715839385986

steps: 449975, episodes: 18000, mean episode reward: -328.3624564026828, agent episode reward: [-109.4541521342276, -109.4541521342276, -109.4541521342276], time: 75.304
steps: 449975, episodes: 18000, mean episode variance: 86.79516245269775, agent episode variance: [24.5742114944458, 35.239831596374515, 26.98111936187744], time: 75.305
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.376043156184473, variance: 98.29684448242188, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -72.15645599365234, std_q: 6.912548065185547
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.377923984025524, variance: 140.95933532714844, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -72.73528289794922, std_q: 7.374551773071289
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3747021059571685, variance: 107.92446899414062, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -73.90279388427734, std_q: 7.431344032287598

steps: 474975, episodes: 19000, mean episode reward: -326.62501805330373, agent episode reward: [-108.8750060177679, -108.8750060177679, -108.8750060177679], time: 68.555
steps: 474975, episodes: 19000, mean episode variance: 86.65208837890626, agent episode variance: [24.62716481781006, 35.152805603027346, 26.87211795806885], time: 68.556
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373013281131951, variance: 98.50865936279297, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -72.15482330322266, std_q: 6.913811683654785
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.377357788222928, variance: 140.61123657226562, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -72.72189331054688, std_q: 7.37880802154541
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373647034505977, variance: 107.48847198486328, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -73.89291381835938, std_q: 7.424609661102295

steps: 499975, episodes: 20000, mean episode reward: -326.7327681465466, agent episode reward: [-108.91092271551551, -108.91092271551551, -108.91092271551551], time: 65.056
steps: 499975, episodes: 20000, mean episode variance: 86.79540814208984, agent episode variance: [24.561767066955568, 35.25055729675293, 26.98308377838135], time: 65.056
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3760169299289515, variance: 98.24706268310547, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -72.1455307006836, std_q: 6.8958330154418945
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.376447847973362, variance: 141.00221252441406, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -72.69296264648438, std_q: 7.378640651702881
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370987711031181, variance: 107.93232727050781, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -73.8888931274414, std_q: 7.431795120239258

steps: 524975, episodes: 21000, mean episode reward: -328.59937424083586, agent episode reward: [-109.53312474694529, -109.53312474694529, -109.53312474694529], time: 64.062
steps: 524975, episodes: 21000, mean episode variance: 86.81826893615722, agent episode variance: [24.75421565246582, 35.171965812683105, 26.8920874710083], time: 64.063
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3796665698259405, variance: 99.01686096191406, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -72.16248321533203, std_q: 6.925303936004639
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.374838759052778, variance: 140.68788146972656, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -72.70824432373047, std_q: 7.363452434539795
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.374096009034978, variance: 107.56835174560547, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -73.88339233398438, std_q: 7.442361354827881

steps: 549975, episodes: 22000, mean episode reward: -327.1191172686242, agent episode reward: [-109.0397057562081, -109.0397057562081, -109.0397057562081], time: 64.718
steps: 549975, episodes: 22000, mean episode variance: 86.53439761352539, agent episode variance: [24.39921659851074, 35.10536714172363, 27.029813873291015], time: 64.718
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.372711406812234, variance: 97.59686279296875, cvar: -62.286617279052734, v: -62.290958404541016, mean_q: -72.14324951171875, std_q: 6.888382911682129
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.372774654194561, variance: 140.4214630126953, cvar: -62.28238296508789, v: -62.290958404541016, mean_q: -72.71358489990234, std_q: 7.3700852394104
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373281721596472, variance: 108.11925506591797, cvar: -62.290687561035156, v: -62.290958404541016, mean_q: -73.87995147705078, std_q: 7.446776390075684

steps: 574975, episodes: 23000, mean episode reward: -328.2148601249624, agent episode reward: [-109.40495337498747, -109.40495337498747, -109.40495337498747], time: 67.66
steps: 574975, episodes: 23000, mean episode variance: 86.43978461456298, agent episode variance: [24.455790237426758, 35.11436445617676, 26.869629920959472], time: 67.66
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.376097782633341, variance: 97.82315826416016, cvar: -64.23087310791016, v: -64.67115020751953, mean_q: -72.14765930175781, std_q: 6.900748252868652
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373030554511771, variance: 140.4574737548828, cvar: -64.11273193359375, v: -64.62570190429688, mean_q: -72.69029998779297, std_q: 7.340944766998291
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3763245207441415, variance: 107.47852325439453, cvar: -64.67094421386719, v: -64.79694366455078, mean_q: -73.88528442382812, std_q: 7.4279022216796875

steps: 599975, episodes: 24000, mean episode reward: -328.74969481646514, agent episode reward: [-109.5832316054884, -109.5832316054884, -109.5832316054884], time: 64.398
steps: 599975, episodes: 24000, mean episode variance: 86.78868869018555, agent episode variance: [24.647553703308105, 35.15747276306152, 26.98366222381592], time: 64.398
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373645265305923, variance: 98.59021759033203, cvar: -64.5225830078125, v: -65.46459197998047, mean_q: -72.11943817138672, std_q: 6.892486095428467
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.374639656806401, variance: 140.62989807128906, cvar: -64.3302001953125, v: -65.33840942382812, mean_q: -72.71489715576172, std_q: 7.33674955368042
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371570060773755, variance: 107.93464660644531, cvar: -65.61779022216797, v: -66.40694427490234, mean_q: -73.87834167480469, std_q: 7.43549108505249

steps: 624975, episodes: 25000, mean episode reward: -327.1308290067775, agent episode reward: [-109.04360966892584, -109.04360966892584, -109.04360966892584], time: 65.68
steps: 624975, episodes: 25000, mean episode variance: 86.77907781219483, agent episode variance: [24.621775840759277, 35.1293353805542, 27.02796659088135], time: 65.681
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.372031479347955, variance: 98.48709106445312, cvar: -64.54077911376953, v: -65.48186492919922, mean_q: -72.12604522705078, std_q: 6.882922649383545
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.376295429418836, variance: 140.51734924316406, cvar: -64.32780456542969, v: -65.35296630859375, mean_q: -72.70975494384766, std_q: 7.370030403137207
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.376620754320302, variance: 108.11186981201172, cvar: -65.63290405273438, v: -66.51930236816406, mean_q: -73.89681243896484, std_q: 7.451580047607422

steps: 649975, episodes: 26000, mean episode reward: -326.5418656040787, agent episode reward: [-108.8472885346929, -108.8472885346929, -108.8472885346929], time: 65.478
steps: 649975, episodes: 26000, mean episode variance: 86.60647194671631, agent episode variance: [24.542367385864257, 35.104069213867184, 26.960035346984863], time: 65.478
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.377378486139284, variance: 98.16947174072266, cvar: -64.53248596191406, v: -65.49662017822266, mean_q: -72.16409301757812, std_q: 6.920521259307861
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371582508674974, variance: 140.41627502441406, cvar: -64.32196044921875, v: -65.33438873291016, mean_q: -72.69332885742188, std_q: 7.3490118980407715
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371419717150554, variance: 107.84014129638672, cvar: -65.63561248779297, v: -66.51471710205078, mean_q: -73.90679931640625, std_q: 7.45247745513916

steps: 674975, episodes: 27000, mean episode reward: -326.14880291358946, agent episode reward: [-108.71626763786314, -108.71626763786314, -108.71626763786314], time: 66.391
steps: 674975, episodes: 27000, mean episode variance: 86.4978037185669, agent episode variance: [24.517146667480468, 35.09726918029785, 26.883387870788575], time: 66.392
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371073099143832, variance: 98.06858825683594, cvar: -64.54312133789062, v: -65.49032592773438, mean_q: -72.12924194335938, std_q: 6.884340763092041
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373659128584288, variance: 140.3890838623047, cvar: -64.3177261352539, v: -65.32762145996094, mean_q: -72.70050811767578, std_q: 7.378183364868164
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3711029888084525, variance: 107.53355407714844, cvar: -65.62771606445312, v: -66.51121520996094, mean_q: -73.87979888916016, std_q: 7.409948348999023

steps: 699975, episodes: 28000, mean episode reward: -324.72867945736004, agent episode reward: [-108.24289315245335, -108.24289315245335, -108.24289315245335], time: 71.201
steps: 699975, episodes: 28000, mean episode variance: 86.380760597229, agent episode variance: [24.38820532989502, 35.16241580200195, 26.83013946533203], time: 71.202
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371447965769495, variance: 97.55281066894531, cvar: -64.52847290039062, v: -65.4864273071289, mean_q: -72.1324691772461, std_q: 6.87725305557251
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.37501790721279, variance: 140.649658203125, cvar: -64.33783721923828, v: -65.35150909423828, mean_q: -72.70767974853516, std_q: 7.363158226013184
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.368138539175144, variance: 107.320556640625, cvar: -65.63987731933594, v: -66.51820373535156, mean_q: -73.86363220214844, std_q: 7.412996292114258

some error in feed_dict
Tensor("observation0:0", shape=(?, 16), dtype=float32) :: [[ 0.06343741  0.06257223 -0.46532836  0.36699425 -0.429807   -1.18570272
   0.0168148   0.01194537 -0.33646384 -0.72139302 -0.43074554 -1.2592819
   0.          0.          0.          0.        ]]
Traceback (most recent call last):
  File "train.py", line 493, in <module>
    train(arglist)
  File "train.py", line 192, in train
    action_n = [agent.action(obs) for agent, obs in zip(trainers,obs_n)]
  File "train.py", line 192, in <listcomp>
    action_n = [agent.action(obs) for agent, obs in zip(trainers,obs_n)]
  File "../maddpg/trainer/maddpg.py", line 259, in action
    return self.act(obs[None])[0]
  File "../maddpg/common/tf_util.py", line 289, in <lambda>
    return lambda *args, **kwargs: f(*args, **kwargs)[0]
  File "../maddpg/common/tf_util.py", line 346, in __call__
    return results
UnboundLocalError: local variable 'results' referenced before assignment
