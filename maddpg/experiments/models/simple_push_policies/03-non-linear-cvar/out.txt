WARNING: Logging before flag parsing goes to stderr.
W0826 22:50:34.748581 4581688768 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0826 22:50:34.748844 4581688768 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-26 22:50:34.749326: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0826 22:50:34.751436 4581688768 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0826 22:50:34.753437 4581688768 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0826 22:50:34.753546 4581688768 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0826 22:50:34.753654 4581688768 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0826 22:50:35.088142 4581688768 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0826 22:50:35.224375 4581688768 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0826 22:50:35.232197 4581688768 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0826 22:50:35.662833 4581688768 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation False
adversary agent:  1.35
adversary agent:  1.35
good agent:  -0.3
good agent:  -0.3
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -69.56972835156907, agent episode reward: [-6.921470862742025, -6.461080792005449, -27.509185052140296, -28.6779916446813], time: 33.619
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 33.619
steps: 24975, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0, 0.0], time: 33.62
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -55.733426883199265, agent episode reward: [-8.093241366867062, -7.73987903983841, -21.06625351482087, -18.834052961672924], time: 63.617
steps: 49975, episodes: 2000, mean episode variance: 0.22730232925806196, agent episode variance: [0.05778821903467178, 0.04533035745145753, 0.0536699833413586, 0.07051376943057402], time: 63.618
steps: 49975, episodes: 2000, mean episode cvar: 0.5445309246424586, agent episode cvar: [0.293611113011837, 0.2959953788518906, -0.020318969714455307, -0.024756597506813704], time: 63.618
Running avgs for agent 0: q_loss: 0.18063408136367798, p_loss: 0.2366890162229538, mean_rew: -0.2789226122012331, variance: 0.23683696325685158, cvar: 1.203324317932129, v: 0.7214347124099731, mean_q: -0.26467958092689514, std_q: 0.814732551574707, lamda: 1.0046542882919312
Running avgs for agent 1: q_loss: 0.18510669469833374, p_loss: 0.11562836915254593, mean_rew: -0.27111427412878186, variance: 0.18578015348958005, cvar: 1.2130959033966064, v: 0.7452910542488098, mean_q: -0.14062362909317017, std_q: 0.7469064593315125, lamda: 1.005280613899231
Running avgs for agent 2: q_loss: 0.0933951586484909, p_loss: 2.001556396484375, mean_rew: -1.136451737050309, variance: 0.21995894812032213, cvar: -0.08327445387840271, v: -0.15716248750686646, mean_q: -2.057478427886963, std_q: 1.1459553241729736, lamda: 1.0002943277359009
Running avgs for agent 3: q_loss: 0.09371832758188248, p_loss: 1.896645426750183, mean_rew: -1.1199885824960183, variance: 0.2889908583220247, cvar: -0.10146145522594452, v: -0.17670907080173492, mean_q: -1.93836510181427, std_q: 0.9947056770324707, lamda: 1.000940203666687

steps: 74975, episodes: 3000, mean episode reward: -29.198701052854993, agent episode reward: [-7.253891692338752, -6.927043654917741, -7.510522384129038, -7.507243321469462], time: 61.12
steps: 74975, episodes: 3000, mean episode variance: 0.20054724023863674, agent episode variance: [0.04670030665025115, 0.029788738897070288, 0.06528194189630449, 0.05877625279501081], time: 61.121
steps: 74975, episodes: 3000, mean episode cvar: 0.5832913834452629, agent episode cvar: [0.37721312564611437, 0.37487977504730224, -0.08304091277718544, -0.08576060447096824], time: 61.121
Running avgs for agent 0: q_loss: 0.26273179054260254, p_loss: 0.9492390155792236, mean_rew: -0.3031591270544717, variance: 0.1868012266010046, cvar: 1.508852481842041, v: 0.49296483397483826, mean_q: -1.0281766653060913, std_q: 1.4624567031860352, lamda: 1.022387981414795
Running avgs for agent 1: q_loss: 0.1715116947889328, p_loss: 0.8827728629112244, mean_rew: -0.2884183930420184, variance: 0.11915495558828115, cvar: 1.4995189905166626, v: 0.5748967528343201, mean_q: -0.9586238265037537, std_q: 1.405543327331543, lamda: 1.018671989440918
Running avgs for agent 2: q_loss: 0.014047249220311642, p_loss: 2.419527530670166, mean_rew: -0.843955117457958, variance: 0.26112776758521794, cvar: -0.33216363191604614, v: -0.40486791729927063, mean_q: -2.5716779232025146, std_q: 2.4228148460388184, lamda: 1.0009849071502686
Running avgs for agent 3: q_loss: 0.007964127697050571, p_loss: 2.289729595184326, mean_rew: -0.8297787179154136, variance: 0.23510501118004323, cvar: -0.34304243326187134, v: -0.417358934879303, mean_q: -2.432098150253296, std_q: 2.170806646347046, lamda: 1.002608299255371

steps: 99975, episodes: 4000, mean episode reward: -25.652994334524266, agent episode reward: [-6.118404423756835, -5.3471069859934195, -7.250436841989245, -6.937046082784765], time: 61.443
steps: 99975, episodes: 4000, mean episode variance: 0.33714336035912856, agent episode variance: [0.058935148702934384, 0.08190775379538535, 0.10555517189577222, 0.0907452859650366], time: 61.444
steps: 99975, episodes: 4000, mean episode cvar: 0.5845006294250489, agent episode cvar: [0.3870882778763771, 0.3651227424740791, -0.08250320339202881, -0.0852071875333786], time: 61.445
Running avgs for agent 0: q_loss: 0.35521015524864197, p_loss: 1.618154764175415, mean_rew: -0.29041896223521646, variance: 0.23574059481173754, cvar: 1.5483531951904297, v: 0.03560572862625122, mean_q: -1.7473419904708862, std_q: 1.9869924783706665, lamda: 1.0418189764022827
Running avgs for agent 1: q_loss: 0.3455646336078644, p_loss: 1.425118327140808, mean_rew: -0.2710081834515064, variance: 0.3276310151815414, cvar: 1.4604909420013428, v: 0.15062379837036133, mean_q: -1.55521559715271, std_q: 1.8843603134155273, lamda: 1.0374913215637207
Running avgs for agent 2: q_loss: 0.02014944516122341, p_loss: 2.7032744884490967, mean_rew: -0.6873175456185558, variance: 0.4222206875830889, cvar: -0.33001282811164856, v: -0.40331315994262695, mean_q: -2.907081127166748, std_q: 3.4393374919891357, lamda: 1.001821517944336
Running avgs for agent 3: q_loss: 0.014205009676516056, p_loss: 2.5650761127471924, mean_rew: -0.6709150595670685, variance: 0.3629811438601464, cvar: -0.3408287465572357, v: -0.38298043608665466, mean_q: -2.7608327865600586, std_q: 3.143606424331665, lamda: 1.0029702186584473

steps: 124975, episodes: 5000, mean episode reward: -25.29779240093446, agent episode reward: [-5.354753962966536, -5.429558987766859, -7.269114954932285, -7.244364495268777], time: 62.793
steps: 124975, episodes: 5000, mean episode variance: 0.3605119191547856, agent episode variance: [0.12080537802353501, 0.09914170964062213, 0.0681962821809575, 0.07236854930967093], time: 62.793
steps: 124975, episodes: 5000, mean episode cvar: 0.532691120505333, agent episode cvar: [0.3544513186216354, 0.34473321110010147, -0.08241333130002022, -0.08408007791638375], time: 62.794
Running avgs for agent 0: q_loss: 0.3847295641899109, p_loss: 1.9082348346710205, mean_rew: -0.2766851132975045, variance: 0.48322151209414005, cvar: 1.4178051948547363, v: -0.4640571177005768, mean_q: -2.062866687774658, std_q: 2.308032751083374, lamda: 1.0593056678771973
Running avgs for agent 1: q_loss: 0.28781941533088684, p_loss: 1.6074249744415283, mean_rew: -0.2588889980434792, variance: 0.39656683856248853, cvar: 1.3789328336715698, v: -0.23552095890045166, mean_q: -1.7709076404571533, std_q: 2.2360970973968506, lamda: 1.057321310043335
Running avgs for agent 2: q_loss: 0.010639442130923271, p_loss: 2.7061307430267334, mean_rew: -0.5957378924866563, variance: 0.27278512872383, cvar: -0.3296533226966858, v: -0.4094426929950714, mean_q: -2.926410675048828, std_q: 4.015740871429443, lamda: 1.0022250413894653
Running avgs for agent 3: q_loss: 0.009650224819779396, p_loss: 2.5767924785614014, mean_rew: -0.5850268574710245, variance: 0.28947419723868373, cvar: -0.33632031083106995, v: -0.37543538212776184, mean_q: -2.7897744178771973, std_q: 3.752858877182007, lamda: 1.003697156906128

steps: 149975, episodes: 6000, mean episode reward: -24.686029089686738, agent episode reward: [-5.310255447277537, -5.005356157701206, -7.350023126663615, -7.020394358044379], time: 64.989
steps: 149975, episodes: 6000, mean episode variance: 0.5539494882598519, agent episode variance: [0.203435246899724, 0.16745776674151422, 0.11053782742004842, 0.07251864719856531], time: 64.99
steps: 149975, episodes: 6000, mean episode cvar: 0.481017465531826, agent episode cvar: [0.32929059612751005, 0.31975162544846536, -0.08417778277397156, -0.08384697327017784], time: 64.99
Running avgs for agent 0: q_loss: 0.5030294060707092, p_loss: 1.8501183986663818, mean_rew: -0.2632039056879353, variance: 0.813740987598896, cvar: 1.317162275314331, v: -0.6909613013267517, mean_q: -2.0018234252929688, std_q: 2.4285624027252197, lamda: 1.0798054933547974
Running avgs for agent 1: q_loss: 0.366700142621994, p_loss: 1.532625675201416, mean_rew: -0.2528848164326627, variance: 0.6698310669660569, cvar: 1.2790066003799438, v: -0.4011993706226349, mean_q: -1.691436767578125, std_q: 2.3952083587646484, lamda: 1.0767717361450195
Running avgs for agent 2: q_loss: 0.021876171231269836, p_loss: 2.6278700828552246, mean_rew: -0.541390904173137, variance: 0.44215130968019367, cvar: -0.3367111086845398, v: -0.41833585500717163, mean_q: -2.8420629501342773, std_q: 4.353536128997803, lamda: 1.0026568174362183
Running avgs for agent 3: q_loss: 0.011355207301676273, p_loss: 2.4905591011047363, mean_rew: -0.5297299655639177, variance: 0.29007458879426123, cvar: -0.3353878855705261, v: -0.3746844530105591, mean_q: -2.700488328933716, std_q: 4.021020412445068, lamda: 1.003942608833313

steps: 174975, episodes: 7000, mean episode reward: -24.617963837302014, agent episode reward: [-5.3895971136518535, -5.048420953344529, -7.08455013451602, -7.095395635789607], time: 66.139
steps: 174975, episodes: 7000, mean episode variance: 0.5324728456549347, agent episode variance: [0.2171935909949243, 0.1393063639625907, 0.10144319271668792, 0.07452969798073172], time: 66.14
steps: 174975, episodes: 7000, mean episode cvar: 0.4716344450116158, agent episode cvar: [0.31140420013666154, 0.3287759947180748, -0.08446581450104713, -0.08407993534207345], time: 66.14
Running avgs for agent 0: q_loss: 0.4519915282726288, p_loss: 1.6312546730041504, mean_rew: -0.2576917428692291, variance: 0.8687743639796972, cvar: 1.2456169128417969, v: -0.6835436820983887, mean_q: -1.7761321067810059, std_q: 2.4676167964935303, lamda: 1.1003453731536865
Running avgs for agent 1: q_loss: 0.28654754161834717, p_loss: 1.39664626121521, mean_rew: -0.24403306000671335, variance: 0.5572254558503628, cvar: 1.3151040077209473, v: -0.4142729938030243, mean_q: -1.5471336841583252, std_q: 2.453688621520996, lamda: 1.0936903953552246
Running avgs for agent 2: q_loss: 0.017465980723500252, p_loss: 2.493943929672241, mean_rew: -0.5012519350998172, variance: 0.4057727708667517, cvar: -0.3378632366657257, v: -0.43257200717926025, mean_q: -2.6945977210998535, std_q: 4.429234504699707, lamda: 1.0034260749816895
Running avgs for agent 3: q_loss: 0.011760227382183075, p_loss: 2.3647470474243164, mean_rew: -0.49108734231501855, variance: 0.2981187919229269, cvar: -0.33631977438926697, v: -0.37383392453193665, mean_q: -2.561959981918335, std_q: 4.117608547210693, lamda: 1.0043292045593262

steps: 199975, episodes: 8000, mean episode reward: -25.2521437759451, agent episode reward: [-5.726440833394763, -5.457444297950704, -6.979243994594013, -7.089014650005621], time: 61.824
steps: 199975, episodes: 8000, mean episode variance: 0.48409562567900866, agent episode variance: [0.21187217372469605, 0.12757477529719471, 0.0803423786610365, 0.06430629799608141], time: 61.824
steps: 199975, episodes: 8000, mean episode cvar: 0.4731954884827137, agent episode cvar: [0.32269693922996523, 0.32032984620332716, -0.08541718968749046, -0.08441410726308822], time: 61.825
Running avgs for agent 0: q_loss: 0.5233813524246216, p_loss: 1.443527340888977, mean_rew: -0.2527065230408726, variance: 0.8474886948987842, cvar: 1.2907878160476685, v: -0.6232593059539795, mean_q: -1.572883129119873, std_q: 2.429807186126709, lamda: 1.119478464126587
Running avgs for agent 1: q_loss: 0.28337714076042175, p_loss: 1.2696516513824463, mean_rew: -0.23894928992532058, variance: 0.5102991011887789, cvar: 1.2813193798065186, v: -0.40534988045692444, mean_q: -1.4034724235534668, std_q: 2.4098100662231445, lamda: 1.1089818477630615
Running avgs for agent 2: q_loss: 0.013343890197575092, p_loss: 2.3646562099456787, mean_rew: -0.47154724039608226, variance: 0.321369514644146, cvar: -0.3416687548160553, v: -0.43238136172294617, mean_q: -2.5484976768493652, std_q: 4.443353652954102, lamda: 1.0038044452667236
Running avgs for agent 3: q_loss: 0.01054155733436346, p_loss: 2.2557923793792725, mean_rew: -0.46409848520122476, variance: 0.25722519198432564, cvar: -0.3376564383506775, v: -0.3656628131866455, mean_q: -2.4365241527557373, std_q: 4.149043083190918, lamda: 1.0045502185821533

steps: 224975, episodes: 9000, mean episode reward: -25.282201222171114, agent episode reward: [-5.70956949146308, -5.47964845509091, -7.149064262979136, -6.943919012637993], time: 63.225
steps: 224975, episodes: 9000, mean episode variance: 0.48144975104369225, agent episode variance: [0.17425942903012037, 0.14968226035311819, 0.08409976241365075, 0.07340829924680292], time: 63.225
steps: 224975, episodes: 9000, mean episode cvar: 0.4547547946572304, agent episode cvar: [0.3147424196600914, 0.3127549877166748, -0.08607809928059577, -0.08666451343894005], time: 63.226
Running avgs for agent 0: q_loss: 0.4320308268070221, p_loss: 1.3086373805999756, mean_rew: -0.24939010275911147, variance: 0.6970377161204815, cvar: 1.2589695453643799, v: -0.5703480243682861, mean_q: -1.4226560592651367, std_q: 2.3526434898376465, lamda: 1.1378110647201538
Running avgs for agent 1: q_loss: 0.3559180498123169, p_loss: 1.1275050640106201, mean_rew: -0.2365155088712242, variance: 0.5987290414124727, cvar: 1.251020073890686, v: -0.39251843094825745, mean_q: -1.2477984428405762, std_q: 2.302325963973999, lamda: 1.1256681680679321
Running avgs for agent 2: q_loss: 0.01566852629184723, p_loss: 2.2522201538085938, mean_rew: -0.45046865235409084, variance: 0.336399049654603, cvar: -0.3443123698234558, v: -0.4285028278827667, mean_q: -2.4192941188812256, std_q: 4.34136438369751, lamda: 1.0042585134506226
Running avgs for agent 3: q_loss: 0.013986634090542793, p_loss: 2.1469194889068604, mean_rew: -0.4437500711845414, variance: 0.2936331969872117, cvar: -0.3466580808162689, v: -0.36667129397392273, mean_q: -2.3105199337005615, std_q: 4.076170444488525, lamda: 1.0051349401474

steps: 249975, episodes: 10000, mean episode reward: -24.909457308538805, agent episode reward: [-5.560118334762727, -5.24593491656731, -7.028498022762132, -7.074906034446635], time: 62.765
steps: 249975, episodes: 10000, mean episode variance: 0.4687629810012877, agent episode variance: [0.17381921463087202, 0.1525690062791109, 0.0671186479087919, 0.07525611218251288], time: 62.766
steps: 249975, episodes: 10000, mean episode cvar: 0.45644366505742073, agent episode cvar: [0.3140231273472309, 0.31527905640006065, -0.08565925776958466, -0.08719926092028618], time: 62.766
Running avgs for agent 0: q_loss: 0.4410689175128937, p_loss: 1.1541703939437866, mean_rew: -0.24579016622009936, variance: 0.6952768585234881, cvar: 1.2560925483703613, v: -0.5037215352058411, mean_q: -1.2552622556686401, std_q: 2.2497854232788086, lamda: 1.1561028957366943
Running avgs for agent 1: q_loss: 0.3565197288990021, p_loss: 0.9616523385047913, mean_rew: -0.2330941004447506, variance: 0.6102760251164436, cvar: 1.2611162662506104, v: -0.3111100494861603, mean_q: -1.0631217956542969, std_q: 2.1811916828155518, lamda: 1.1426455974578857
Running avgs for agent 2: q_loss: 0.012210174463689327, p_loss: 2.150757312774658, mean_rew: -0.4309098729979392, variance: 0.2684745916351676, cvar: -0.3426370322704315, v: -0.41996705532073975, mean_q: -2.3039040565490723, std_q: 4.271272659301758, lamda: 1.0048702955245972
Running avgs for agent 3: q_loss: 0.014287584461271763, p_loss: 2.047928810119629, mean_rew: -0.42661317354505773, variance: 0.3010244487300515, cvar: -0.3487970232963562, v: -0.36793553829193115, mean_q: -2.1982250213623047, std_q: 3.9846975803375244, lamda: 1.005955457687378

steps: 274975, episodes: 11000, mean episode reward: -25.548346275289987, agent episode reward: [-5.8800654318017695, -5.64187604925797, -6.994400311079984, -7.0320044831502635], time: 63.585
steps: 274975, episodes: 11000, mean episode variance: 0.42716921111755074, agent episode variance: [0.170397214313969, 0.12698597190715372, 0.06397131975274534, 0.06581470514368266], time: 63.585
steps: 274975, episodes: 11000, mean episode cvar: 0.43494282054901123, agent episode cvar: [0.2967640234231949, 0.31098220658302306, -0.08583339175581932, -0.0869700177013874], time: 63.586
Running avgs for agent 0: q_loss: 0.3608950078487396, p_loss: 0.992740273475647, mean_rew: -0.24560949349708944, variance: 0.681588857255876, cvar: 1.1870561838150024, v: -0.3914802074432373, mean_q: -1.0832319259643555, std_q: 2.133582830429077, lamda: 1.1733417510986328
Running avgs for agent 1: q_loss: 0.3059218227863312, p_loss: 0.8646703958511353, mean_rew: -0.23407458199946182, variance: 0.5079438876286149, cvar: 1.2439287900924683, v: -0.2581729590892792, mean_q: -0.9540766477584839, std_q: 2.106628894805908, lamda: 1.1585056781768799
Running avgs for agent 2: q_loss: 0.012201769277453423, p_loss: 2.068380355834961, mean_rew: -0.418195816423955, variance: 0.25588527901098135, cvar: -0.34333354234695435, v: -0.41065821051597595, mean_q: -2.2093613147735596, std_q: 4.141159534454346, lamda: 1.0055476427078247
Running avgs for agent 3: q_loss: 0.013785735704004765, p_loss: 1.9738649129867554, mean_rew: -0.4125222797863425, variance: 0.26325882057473066, cvar: -0.347880095243454, v: -0.3755030035972595, mean_q: -2.112691879272461, std_q: 3.902320146560669, lamda: 1.0077542066574097

steps: 299975, episodes: 12000, mean episode reward: -26.250603588499345, agent episode reward: [-6.117617649847624, -5.927672954525195, -7.030829653776785, -7.174483330349738], time: 63.978
steps: 299975, episodes: 12000, mean episode variance: 0.3968416894944385, agent episode variance: [0.1473699844367802, 0.12843708984926344, 0.06963385469093919, 0.05140076051745564], time: 63.979
steps: 299975, episodes: 12000, mean episode cvar: 0.45159058600664137, agent episode cvar: [0.31309085243940354, 0.3073694457411766, -0.08599405279755593, -0.08287565937638283], time: 63.979
Running avgs for agent 0: q_loss: 0.4198871850967407, p_loss: 0.8686501979827881, mean_rew: -0.24444418197640908, variance: 0.5894799377471208, cvar: 1.2523634433746338, v: -0.32150977849960327, mean_q: -0.9483089447021484, std_q: 2.0608837604522705, lamda: 1.1907316446304321
Running avgs for agent 1: q_loss: 0.31214645504951477, p_loss: 0.7043479681015015, mean_rew: -0.23155055480609915, variance: 0.5137483593970538, cvar: 1.2294777631759644, v: -0.15307022631168365, mean_q: -0.7822020649909973, std_q: 1.9769827127456665, lamda: 1.1748665571212769
Running avgs for agent 2: q_loss: 0.0161378625780344, p_loss: 2.018143653869629, mean_rew: -0.4090093681086807, variance: 0.27853541876375676, cvar: -0.3439761996269226, v: -0.39533886313438416, mean_q: -2.1509649753570557, std_q: 4.153234481811523, lamda: 1.0062543153762817
Running avgs for agent 3: q_loss: 0.014355205930769444, p_loss: 1.918092131614685, mean_rew: -0.4001400019320086, variance: 0.20560304206982255, cvar: -0.33150261640548706, v: -0.3975006639957428, mean_q: -2.0456199645996094, std_q: 3.759352445602417, lamda: 1.0093995332717896

steps: 324975, episodes: 13000, mean episode reward: -25.936154928980926, agent episode reward: [-5.930463108145786, -5.777639624476748, -6.988349837038125, -7.239702359320263], time: 63.411
steps: 324975, episodes: 13000, mean episode variance: 0.35459117918275296, agent episode variance: [0.12448706225678324, 0.1078463894519955, 0.06521934448927641, 0.05703838298469782], time: 63.411
steps: 324975, episodes: 13000, mean episode cvar: 0.4465991740822792, agent episode cvar: [0.3121922585964203, 0.30212688726186754, -0.08586245077848434, -0.08185752099752426], time: 63.412
Running avgs for agent 0: q_loss: 0.3277769684791565, p_loss: 0.832658052444458, mean_rew: -0.24503903457862616, variance: 0.49794824902713297, cvar: 1.2487690448760986, v: -0.3377544581890106, mean_q: -0.9046658277511597, std_q: 2.00919771194458, lamda: 1.2068554162979126
Running avgs for agent 1: q_loss: 0.2405489981174469, p_loss: 0.6001768708229065, mean_rew: -0.23257054611954356, variance: 0.431385557807982, cvar: 1.2085075378417969, v: -0.06157558038830757, mean_q: -0.6698956489562988, std_q: 1.899739384651184, lamda: 1.1901743412017822
Running avgs for agent 2: q_loss: 0.018818408250808716, p_loss: 1.9167497158050537, mean_rew: -0.3944723075782714, variance: 0.26087737795710564, cvar: -0.34344980120658875, v: -0.38054460287094116, mean_q: -2.0404648780822754, std_q: 3.929114818572998, lamda: 1.007534146308899
Running avgs for agent 3: q_loss: 0.01527476403862238, p_loss: 1.875880241394043, mean_rew: -0.3905833299201777, variance: 0.2281535319387913, cvar: -0.32743003964424133, v: -0.41861215233802795, mean_q: -1.99276602268219, std_q: 3.65433406829834, lamda: 1.0110082626342773

steps: 349975, episodes: 14000, mean episode reward: -26.70194425379112, agent episode reward: [-6.519113738655813, -6.138143752943853, -6.913484950155193, -7.13120181203626], time: 63.679
steps: 349975, episodes: 14000, mean episode variance: 0.3433390364125371, agent episode variance: [0.11367134375032037, 0.09113437905069441, 0.0870906104426831, 0.05144270316883921], time: 63.679
steps: 349975, episodes: 14000, mean episode cvar: 0.44993389512598514, agent episode cvar: [0.3151706355512142, 0.30844726181030274, -0.08962364350259304, -0.08406035873293877], time: 63.68
Running avgs for agent 0: q_loss: 0.3190164864063263, p_loss: 0.8135010600090027, mean_rew: -0.24489715773451662, variance: 0.4546853750012815, cvar: 1.2606825828552246, v: -0.3503924608230591, mean_q: -0.8775641918182373, std_q: 1.9699060916900635, lamda: 1.2226368188858032
Running avgs for agent 1: q_loss: 0.23416492342948914, p_loss: 0.5221048593521118, mean_rew: -0.23247468224617468, variance: 0.36453751620277763, cvar: 1.233789086341858, v: 0.005847570486366749, mean_q: -0.5885096192359924, std_q: 1.8772683143615723, lamda: 1.2042869329452515
Running avgs for agent 2: q_loss: 0.028441177681088448, p_loss: 1.8657690286636353, mean_rew: -0.38751872183981273, variance: 0.3483624417707324, cvar: -0.3584945797920227, v: -0.39313846826553345, mean_q: -1.9807562828063965, std_q: 3.8722548484802246, lamda: 1.013152003288269
Running avgs for agent 3: q_loss: 0.010890067555010319, p_loss: 1.8369885683059692, mean_rew: -0.38338586012865106, variance: 0.20577081267535685, cvar: -0.3362414538860321, v: -0.42056936025619507, mean_q: -1.9439213275909424, std_q: 3.5523111820220947, lamda: 1.0118932723999023

steps: 374975, episodes: 15000, mean episode reward: -26.40066720897165, agent episode reward: [-6.3496225503999595, -5.95484628613476, -6.993963560460185, -7.102234811976748], time: 63.755
steps: 374975, episodes: 15000, mean episode variance: 0.30722898177150637, agent episode variance: [0.10477617863751948, 0.08021428766287864, 0.07539059824310243, 0.046847917228005825], time: 63.756
steps: 374975, episodes: 15000, mean episode cvar: 0.45112484659254554, agent episode cvar: [0.312575751721859, 0.31248014318943024, -0.08951328711211681, -0.08441776120662689], time: 63.756
Running avgs for agent 0: q_loss: 0.3289775252342224, p_loss: 0.7952075004577637, mean_rew: -0.24634804712489597, variance: 0.4191047145500779, cvar: 1.2503031492233276, v: -0.34307095408439636, mean_q: -0.857458233833313, std_q: 1.9504375457763672, lamda: 1.2375203371047974
Running avgs for agent 1: q_loss: 0.1857624053955078, p_loss: 0.4987429678440094, mean_rew: -0.2327636160948395, variance: 0.32085715065151454, cvar: 1.2499206066131592, v: 0.024601750075817108, mean_q: -0.5592764616012573, std_q: 1.8229165077209473, lamda: 1.2160797119140625
Running avgs for agent 2: q_loss: 0.01995760202407837, p_loss: 1.8247900009155273, mean_rew: -0.381257112517564, variance: 0.3015623929724097, cvar: -0.35805314779281616, v: -0.40298759937286377, mean_q: -1.933351755142212, std_q: 3.796314001083374, lamda: 1.0175049304962158
Running avgs for agent 3: q_loss: 0.008940895088016987, p_loss: 1.7999900579452515, mean_rew: -0.37625823014159543, variance: 0.1873916689120233, cvar: -0.3376710116863251, v: -0.42374154925346375, mean_q: -1.898042917251587, std_q: 3.4651198387145996, lamda: 1.0128464698791504

steps: 399975, episodes: 16000, mean episode reward: -25.57698005571495, agent episode reward: [-5.829983587455757, -5.819841458563724, -6.9466057419921565, -6.98054926770331], time: 63.731
steps: 399975, episodes: 16000, mean episode variance: 0.3139163723615929, agent episode variance: [0.11869423221051693, 0.08165098051726818, 0.0659371185414493, 0.04763404109235853], time: 63.731
steps: 399975, episodes: 16000, mean episode cvar: 0.44928866943717005, agent episode cvar: [0.30980697873234747, 0.31237886518239977, -0.08803599786758423, -0.08486117660999298], time: 63.732
Running avgs for agent 0: q_loss: 0.3921191096305847, p_loss: 0.7741467356681824, mean_rew: -0.24563818202942087, variance: 0.47477692884206774, cvar: 1.2392280101776123, v: -0.3635737895965576, mean_q: -0.8311027884483337, std_q: 1.8512709140777588, lamda: 1.255257248878479
Running avgs for agent 1: q_loss: 0.22468715906143188, p_loss: 0.49406322836875916, mean_rew: -0.23378554830872894, variance: 0.32660392206907274, cvar: 1.2495155334472656, v: 0.019781433045864105, mean_q: -0.5496613383293152, std_q: 1.7845405340194702, lamda: 1.2278590202331543
Running avgs for agent 2: q_loss: 0.0207913089543581, p_loss: 1.7679299116134644, mean_rew: -0.3723347753231735, variance: 0.2637484741657972, cvar: -0.3521439731121063, v: -0.4004020094871521, mean_q: -1.871451735496521, std_q: 3.6909382343292236, lamda: 1.0218217372894287
Running avgs for agent 3: q_loss: 0.009195522405207157, p_loss: 1.7642693519592285, mean_rew: -0.3689384874293154, variance: 0.19053616436943413, cvar: -0.33944469690322876, v: -0.42232397198677063, mean_q: -1.8563520908355713, std_q: 3.423194646835327, lamda: 1.013832926750183

steps: 424975, episodes: 17000, mean episode reward: -25.730040739802078, agent episode reward: [-5.978715797085337, -5.669012569767675, -7.059792742972016, -7.022519629977052], time: 63.702
steps: 424975, episodes: 17000, mean episode variance: 0.324591625507921, agent episode variance: [0.12166544613614678, 0.08871598382201046, 0.054181018576957285, 0.06002917697280646], time: 63.703
steps: 424975, episodes: 17000, mean episode cvar: 0.43866531905531886, agent episode cvar: [0.3060238053202629, 0.3032311638593674, -0.08603813520073891, -0.08455151492357255], time: 63.703
Running avgs for agent 0: q_loss: 0.36297234892845154, p_loss: 0.697050929069519, mean_rew: -0.24475352852378923, variance: 0.48666178454458714, cvar: 1.2240952253341675, v: -0.2964647114276886, mean_q: -0.7499144673347473, std_q: 1.7845879793167114, lamda: 1.2719048261642456
Running avgs for agent 1: q_loss: 0.2058405876159668, p_loss: 0.38981467485427856, mean_rew: -0.2343839351935223, variance: 0.35486393528804183, cvar: 1.2129247188568115, v: 0.20299823582172394, mean_q: -0.44169825315475464, std_q: 1.7478322982788086, lamda: 1.241409182548523
Running avgs for agent 2: q_loss: 0.016665808856487274, p_loss: 1.7450165748596191, mean_rew: -0.3679320576757737, variance: 0.21672407430782914, cvar: -0.3441525399684906, v: -0.4004473388195038, mean_q: -1.840957760810852, std_q: 3.6648409366607666, lamda: 1.0241347551345825
Running avgs for agent 3: q_loss: 0.020556719973683357, p_loss: 1.7668648958206177, mean_rew: -0.36786204347409646, variance: 0.24011670789122583, cvar: -0.33820605278015137, v: -0.43495801091194153, mean_q: -1.856902837753296, std_q: 3.443483829498291, lamda: 1.015497088432312

steps: 449975, episodes: 18000, mean episode reward: -26.065149034666685, agent episode reward: [-6.284496756482606, -5.6684203128291575, -6.777323722395317, -7.334908242959607], time: 63.993
steps: 449975, episodes: 18000, mean episode variance: 0.28693688931968064, agent episode variance: [0.09773377920687198, 0.07334439713973552, 0.06386068818159402, 0.05199802479147911], time: 63.993
steps: 449975, episodes: 18000, mean episode cvar: 0.4459077280163765, agent episode cvar: [0.30790753197669984, 0.30873387199640273, -0.08580791941285133, -0.08492575654387474], time: 63.994
Running avgs for agent 0: q_loss: 0.26750922203063965, p_loss: 0.6110605597496033, mean_rew: -0.24289419035038343, variance: 0.39093511682748794, cvar: 1.2316300868988037, v: -0.21591848134994507, mean_q: -0.6623114943504333, std_q: 1.7553859949111938, lamda: 1.2879657745361328
Running avgs for agent 1: q_loss: 0.20150738954544067, p_loss: 0.3439900577068329, mean_rew: -0.2332741869871511, variance: 0.2933775885589421, cvar: 1.2349355220794678, v: 0.24532461166381836, mean_q: -0.39396074414253235, std_q: 1.714364767074585, lamda: 1.2527304887771606
Running avgs for agent 2: q_loss: 0.019002988934516907, p_loss: 1.715409517288208, mean_rew: -0.36352925777126266, variance: 0.25544275272637607, cvar: -0.3432316780090332, v: -0.40137866139411926, mean_q: -1.806014895439148, std_q: 3.5463833808898926, lamda: 1.0261684656143188
Running avgs for agent 3: q_loss: 0.012902871705591679, p_loss: 1.7283036708831787, mean_rew: -0.36182842402952864, variance: 0.20799209916591643, cvar: -0.3397030234336853, v: -0.43669506907463074, mean_q: -1.8111416101455688, std_q: 3.3260576725006104, lamda: 1.0173825025558472

steps: 474975, episodes: 19000, mean episode reward: -26.21193294224268, agent episode reward: [-6.290268438465155, -5.750127623739836, -6.946701326827009, -7.224835553210678], time: 63.96
steps: 474975, episodes: 19000, mean episode variance: 0.29432267490681263, agent episode variance: [0.11430358600337058, 0.08532252734061331, 0.0486046359911561, 0.04609192557167262], time: 63.96
steps: 474975, episodes: 19000, mean episode cvar: 0.4501331040859222, agent episode cvar: [0.30837588757276535, 0.3129485840201378, -0.08563485243916512, -0.08555651506781578], time: 63.961
Running avgs for agent 0: q_loss: 0.3476550281047821, p_loss: 0.5280892252922058, mean_rew: -0.2453775789222704, variance: 0.4572143440134823, cvar: 1.2335036993026733, v: -0.1027483269572258, mean_q: -0.5741866230964661, std_q: 1.6784831285476685, lamda: 1.3042572736740112
Running avgs for agent 1: q_loss: 0.2839067876338959, p_loss: 0.3049711287021637, mean_rew: -0.23313631495360687, variance: 0.34129010936245324, cvar: 1.2517943382263184, v: 0.2614009976387024, mean_q: -0.3506258726119995, std_q: 1.6767064332962036, lamda: 1.2673982381820679
Running avgs for agent 2: q_loss: 0.012676859274506569, p_loss: 1.674243450164795, mean_rew: -0.35772797508399196, variance: 0.1944185439646244, cvar: -0.34253939986228943, v: -0.38409364223480225, mean_q: -1.7606830596923828, std_q: 3.499210834503174, lamda: 1.0285751819610596
Running avgs for agent 3: q_loss: 0.007277507800608873, p_loss: 1.7007267475128174, mean_rew: -0.35793778524059994, variance: 0.18436770228669047, cvar: -0.3422260880470276, v: -0.4255902171134949, mean_q: -1.780325174331665, std_q: 3.2756693363189697, lamda: 1.0184705257415771

steps: 499975, episodes: 20000, mean episode reward: -26.418660510557896, agent episode reward: [-6.2705214246444685, -6.205513379030521, -6.648865322953114, -7.293760383929794], time: 63.923
steps: 499975, episodes: 20000, mean episode variance: 0.2613694337967318, agent episode variance: [0.09576389304362237, 0.06968528898153455, 0.04315558453835547, 0.05276466723321937], time: 63.924
steps: 499975, episodes: 20000, mean episode cvar: 0.43652752342820167, agent episode cvar: [0.29975876814126967, 0.30794738352298734, -0.08518429666757583, -0.08599433156847953], time: 63.924
Running avgs for agent 0: q_loss: 0.22643056511878967, p_loss: 0.36824914813041687, mean_rew: -0.24526342716975746, variance: 0.3830555721744895, cvar: 1.1990350484848022, v: 0.06800012290477753, mean_q: -0.4103471636772156, std_q: 1.6391310691833496, lamda: 1.3185813426971436
Running avgs for agent 1: q_loss: 0.16036604344844818, p_loss: 0.21354763209819794, mean_rew: -0.23455968449033138, variance: 0.2787411559261382, cvar: 1.2317895889282227, v: 0.39182424545288086, mean_q: -0.2598651051521301, std_q: 1.665531039237976, lamda: 1.281698226928711
Running avgs for agent 2: q_loss: 0.009933730587363243, p_loss: 1.634637713432312, mean_rew: -0.3536689735625987, variance: 0.17262233815342187, cvar: -0.3407371938228607, v: -0.3649861514568329, mean_q: -1.717849850654602, std_q: 3.396056890487671, lamda: 1.0295566320419312
Running avgs for agent 3: q_loss: 0.013512443751096725, p_loss: 1.6646684408187866, mean_rew: -0.35338373375554033, variance: 0.21105866893287747, cvar: -0.3439773619174957, v: -0.4164636731147766, mean_q: -1.7387295961380005, std_q: 3.2221250534057617, lamda: 1.0196971893310547

steps: 524975, episodes: 21000, mean episode reward: -26.553511965131126, agent episode reward: [-5.836514237989691, -6.0580386821743595, -7.1207337527474905, -7.538225292219585], time: 64.2
steps: 524975, episodes: 21000, mean episode variance: 0.24779663383634762, agent episode variance: [0.0827984559442848, 0.06966818116186187, 0.051605013834312555, 0.04372498289588839], time: 64.201
steps: 524975, episodes: 21000, mean episode cvar: 0.4435354959368706, agent episode cvar: [0.3054802332520485, 0.3097495908737183, -0.08522603389620781, -0.08646829429268837], time: 64.201
Running avgs for agent 0: q_loss: 0.2581973969936371, p_loss: 0.3381475806236267, mean_rew: -0.24536104409179052, variance: 0.3311938237771392, cvar: 1.2219209671020508, v: 0.11783922463655472, mean_q: -0.3791591227054596, std_q: 1.6255921125411987, lamda: 1.3302936553955078
Running avgs for agent 1: q_loss: 0.1853259950876236, p_loss: 0.15279899537563324, mean_rew: -0.2352556212995941, variance: 0.27867272464744747, cvar: 1.238998293876648, v: 0.47224047780036926, mean_q: -0.20154356956481934, std_q: 1.6934115886688232, lamda: 1.2920767068862915
Running avgs for agent 2: q_loss: 0.019038116559386253, p_loss: 1.6003713607788086, mean_rew: -0.3496538105534581, variance: 0.20642005533725022, cvar: -0.3409041464328766, v: -0.35000914335250854, mean_q: -1.6778842210769653, std_q: 3.3123598098754883, lamda: 1.0304920673370361
Running avgs for agent 3: q_loss: 0.0071244449354708195, p_loss: 1.6294329166412354, mean_rew: -0.3499907978399053, variance: 0.17489993158355355, cvar: -0.3458731770515442, v: -0.4039371609687805, mean_q: -1.7019002437591553, std_q: 3.145536422729492, lamda: 1.021026849746704

steps: 549975, episodes: 22000, mean episode reward: -26.56715801848679, agent episode reward: [-6.298726966929858, -6.172553114072594, -6.957237059356183, -7.138640878128152], time: 64.311
steps: 549975, episodes: 22000, mean episode variance: 0.21837027532886713, agent episode variance: [0.07809027054533363, 0.05214242127351463, 0.04057405659928918, 0.04756352691072971], time: 64.312
steps: 549975, episodes: 22000, mean episode cvar: 0.441133328050375, agent episode cvar: [0.30181256288290026, 0.3106096186041832, -0.08487120434641839, -0.08641764909029007], time: 64.312
Running avgs for agent 0: q_loss: 0.2040528953075409, p_loss: 0.22904850542545319, mean_rew: -0.2455960788114233, variance: 0.3123610821813345, cvar: 1.2072502374649048, v: 0.25654518604278564, mean_q: -0.26801446080207825, std_q: 1.6172187328338623, lamda: 1.3426430225372314
Running avgs for agent 1: q_loss: 0.12757408618927002, p_loss: 0.12207522988319397, mean_rew: -0.23265380179929127, variance: 0.20856968509405852, cvar: 1.2424384355545044, v: 0.5033643841743469, mean_q: -0.16804906725883484, std_q: 1.6798683404922485, lamda: 1.3019756078720093
Running avgs for agent 2: q_loss: 0.01114735659211874, p_loss: 1.5646909475326538, mean_rew: -0.3460888290849709, variance: 0.1622962263971567, cvar: -0.339484840631485, v: -0.3445490598678589, mean_q: -1.6406890153884888, std_q: 3.287792444229126, lamda: 1.0325571298599243
Running avgs for agent 3: q_loss: 0.012577466666698456, p_loss: 1.6106001138687134, mean_rew: -0.3484630030021997, variance: 0.19025410764291883, cvar: -0.34567058086395264, v: -0.3925488293170929, mean_q: -1.6814104318618774, std_q: 3.1560635566711426, lamda: 1.0221731662750244

steps: 574975, episodes: 23000, mean episode reward: -26.576684377165872, agent episode reward: [-5.914783346188107, -6.249341044989384, -7.083597643922522, -7.32896234206586], time: 64.234
steps: 574975, episodes: 23000, mean episode variance: 0.20723851634096355, agent episode variance: [0.06616392559371889, 0.055523512507788836, 0.039311979884281756, 0.04623909835517406], time: 64.235
steps: 574975, episodes: 23000, mean episode cvar: 0.4459082859158516, agent episode cvar: [0.3060661863088608, 0.31036053186655044, -0.08373157781362534, -0.0867868544459343], time: 64.235
Running avgs for agent 0: q_loss: 0.170120969414711, p_loss: 0.15053503215312958, mean_rew: -0.24592420760957004, variance: 0.26465570237487557, cvar: 1.2242646217346191, v: 0.3474254906177521, mean_q: -0.18889841437339783, std_q: 1.6258301734924316, lamda: 1.3535041809082031
Running avgs for agent 1: q_loss: 0.13798867166042328, p_loss: 0.11158079653978348, mean_rew: -0.2351783057586252, variance: 0.22209405003115534, cvar: 1.241442084312439, v: 0.5343770980834961, mean_q: -0.15633651614189148, std_q: 1.6681489944458008, lamda: 1.3098928928375244
Running avgs for agent 2: q_loss: 0.011130617000162601, p_loss: 1.5502288341522217, mean_rew: -0.34372627066005845, variance: 0.15724791953712702, cvar: -0.3349263370037079, v: -0.3406161367893219, mean_q: -1.62328040599823, std_q: 3.237581491470337, lamda: 1.0334092378616333
Running avgs for agent 3: q_loss: 0.012027008458971977, p_loss: 1.5803476572036743, mean_rew: -0.3466385712967724, variance: 0.18495639342069625, cvar: -0.3471474051475525, v: -0.38263559341430664, mean_q: -1.6476489305496216, std_q: 3.083327531814575, lamda: 1.0252116918563843

steps: 599975, episodes: 24000, mean episode reward: -25.787583006257606, agent episode reward: [-5.86947520338329, -5.577466212537964, -6.866351750980009, -7.474289839356343], time: 64.104
steps: 599975, episodes: 24000, mean episode variance: 0.18962306126020848, agent episode variance: [0.061140407492406665, 0.04317410262487829, 0.04189117055013776, 0.04341738059278578], time: 64.105
steps: 599975, episodes: 24000, mean episode cvar: 0.4522317670583725, agent episode cvar: [0.3080243273377419, 0.3146076421737671, -0.08449135157465935, -0.08590885087847709], time: 64.105
Running avgs for agent 0: q_loss: 0.17790701985359192, p_loss: 0.10961597412824631, mean_rew: -0.24564505621047414, variance: 0.24456162996962666, cvar: 1.2320972681045532, v: 0.3885202705860138, mean_q: -0.147579625248909, std_q: 1.6273174285888672, lamda: 1.3642972707748413
Running avgs for agent 1: q_loss: 0.10793564468622208, p_loss: 0.12906154990196228, mean_rew: -0.2350650656047369, variance: 0.17269641049951315, cvar: 1.2584306001663208, v: 0.5233131051063538, mean_q: -0.17407704889774323, std_q: 1.702371597290039, lamda: 1.318268060684204
Running avgs for agent 2: q_loss: 0.01515969168394804, p_loss: 1.5371620655059814, mean_rew: -0.342011865412698, variance: 0.16756468220055104, cvar: -0.3379654288291931, v: -0.3440812826156616, mean_q: -1.6079967021942139, std_q: 3.219956874847412, lamda: 1.0344722270965576
Running avgs for agent 3: q_loss: 0.013950658030807972, p_loss: 1.5511819124221802, mean_rew: -0.34271212516631594, variance: 0.1736695223711431, cvar: -0.3436354100704193, v: -0.37654101848602295, mean_q: -1.6146128177642822, std_q: 3.003075122833252, lamda: 1.0272306203842163

steps: 624975, episodes: 25000, mean episode reward: -25.37624142907803, agent episode reward: [-5.620055087686815, -5.311933950123955, -7.277165285949088, -7.167087105318169], time: 64.126
steps: 624975, episodes: 25000, mean episode variance: 0.186283604381606, agent episode variance: [0.05331976470351219, 0.05630008692853153, 0.03615209000185132, 0.040511662747710946], time: 64.127
steps: 624975, episodes: 25000, mean episode cvar: 0.4613411045670509, agent episode cvar: [0.30866619074344637, 0.3221138719916344, -0.08398555952310562, -0.08545339864492417], time: 64.127
Running avgs for agent 0: q_loss: 0.15045210719108582, p_loss: 0.0776977688074112, mean_rew: -0.24432844949174623, variance: 0.21327905881404877, cvar: 1.234664797782898, v: 0.4387270510196686, mean_q: -0.11528809368610382, std_q: 1.6379082202911377, lamda: 1.3749284744262695
Running avgs for agent 1: q_loss: 0.18578429520130157, p_loss: 0.15295395255088806, mean_rew: -0.2343522611097219, variance: 0.22520034771412611, cvar: 1.2884554862976074, v: 0.504575788974762, mean_q: -0.19803164899349213, std_q: 1.7001689672470093, lamda: 1.3273190259933472
Running avgs for agent 2: q_loss: 0.012439247220754623, p_loss: 1.496390461921692, mean_rew: -0.33681957612522906, variance: 0.1446083600074053, cvar: -0.33594223856925964, v: -0.3430400490760803, mean_q: -1.5653809309005737, std_q: 3.140658378601074, lamda: 1.0361803770065308
Running avgs for agent 3: q_loss: 0.011124554090201855, p_loss: 1.5450917482376099, mean_rew: -0.34162492386403254, variance: 0.16204665099084378, cvar: -0.3418135941028595, v: -0.38484352827072144, mean_q: -1.607180118560791, std_q: 2.9910264015197754, lamda: 1.031009554862976

steps: 649975, episodes: 26000, mean episode reward: -25.396731005427753, agent episode reward: [-5.738346301175647, -5.162853913100256, -7.48236191921528, -7.013168871936571], time: 64.331
steps: 649975, episodes: 26000, mean episode variance: 0.18878605374973267, agent episode variance: [0.05431481994595379, 0.06087226561270654, 0.03603717927169055, 0.0375617889193818], time: 64.331
steps: 649975, episodes: 26000, mean episode cvar: 0.4476625771224499, agent episode cvar: [0.3074698229432106, 0.30983150351047517, -0.08414150336384774, -0.08549724596738816], time: 64.332
Running avgs for agent 0: q_loss: 0.14922675490379333, p_loss: -0.015512529760599136, mean_rew: -0.24396347279370567, variance: 0.21725927978381515, cvar: 1.2298792600631714, v: 0.5423523783683777, mean_q: -0.023380329832434654, std_q: 1.6222565174102783, lamda: 1.3844568729400635
Running avgs for agent 1: q_loss: 0.17260730266571045, p_loss: 0.1870330423116684, mean_rew: -0.23324366635186422, variance: 0.24348906245082616, cvar: 1.239326000213623, v: 0.5245028138160706, mean_q: -0.2360617220401764, std_q: 1.6985890865325928, lamda: 1.3413406610488892
Running avgs for agent 2: q_loss: 0.010715761221945286, p_loss: 1.4836101531982422, mean_rew: -0.33763862193823163, variance: 0.1441487170867622, cvar: -0.3365660309791565, v: -0.34160560369491577, mean_q: -1.551809549331665, std_q: 3.060253620147705, lamda: 1.0378642082214355
Running avgs for agent 3: q_loss: 0.006940693128854036, p_loss: 1.5323379039764404, mean_rew: -0.33931820612364805, variance: 0.1502471556775272, cvar: -0.3419889807701111, v: -0.3667801022529602, mean_q: -1.5921542644500732, std_q: 2.944749355316162, lamda: 1.032663106918335

steps: 674975, episodes: 27000, mean episode reward: -25.44094756659125, agent episode reward: [-5.950670170815313, -4.841954676078944, -7.829202577987806, -6.819120141709188], time: 64.324
steps: 674975, episodes: 27000, mean episode variance: 0.17783663710253314, agent episode variance: [0.04572365249833092, 0.05079116414114833, 0.029927210942842068, 0.05139460952021182], time: 64.324
steps: 674975, episodes: 27000, mean episode cvar: 0.45830152156949044, agent episode cvar: [0.3159977390170097, 0.3128344541788101, -0.08295241150259972, -0.08757826012372971], time: 64.325
Running avgs for agent 0: q_loss: 0.18237212300300598, p_loss: 0.0443410649895668, mean_rew: -0.2423037571266183, variance: 0.1828946099933237, cvar: 1.2639909982681274, v: 0.4427040219306946, mean_q: -0.08083749562501907, std_q: 1.618072271347046, lamda: 1.3954449892044067
Running avgs for agent 1: q_loss: 0.13227739930152893, p_loss: 0.1331736445426941, mean_rew: -0.23215781826183562, variance: 0.20316465656459332, cvar: 1.2513377666473389, v: 0.572762668132782, mean_q: -0.17989231646060944, std_q: 1.6858134269714355, lamda: 1.3521267175674438
Running avgs for agent 2: q_loss: 0.007531543727964163, p_loss: 1.4731556177139282, mean_rew: -0.3351477384616015, variance: 0.11970884377136827, cvar: -0.3318096399307251, v: -0.3356751799583435, mean_q: -1.5419392585754395, std_q: 3.0946996212005615, lamda: 1.0387699604034424
Running avgs for agent 3: q_loss: 0.02179822511970997, p_loss: 1.5068227052688599, mean_rew: -0.33855627847676323, variance: 0.20557843808084728, cvar: -0.3503130376338959, v: -0.36108410358428955, mean_q: -1.5626658201217651, std_q: 2.9093401432037354, lamda: 1.0364230871200562

steps: 699975, episodes: 28000, mean episode reward: -25.741184784810024, agent episode reward: [-6.410636460544556, -4.622019823358652, -7.362470096830185, -7.346058404076631], time: 64.248
steps: 699975, episodes: 28000, mean episode variance: 0.17922235132707282, agent episode variance: [0.05814909594319761, 0.04541342253610492, 0.03716154677746818, 0.03849828607030213], time: 64.248
steps: 699975, episodes: 28000, mean episode cvar: 0.46272931537032125, agent episode cvar: [0.3192474210262299, 0.3127252322435379, -0.08329676625132561, -0.08594657164812088], time: 64.248
Running avgs for agent 0: q_loss: 0.21226899325847626, p_loss: 0.14823107421398163, mean_rew: -0.2431180438836191, variance: 0.23259638377279043, cvar: 1.2769896984100342, v: 0.2841365933418274, mean_q: -0.18333718180656433, std_q: 1.6195498704910278, lamda: 1.4116525650024414
Running avgs for agent 1: q_loss: 0.12067240476608276, p_loss: 0.06762845069169998, mean_rew: -0.2304654972110622, variance: 0.18165369014441968, cvar: 1.2509008646011353, v: 0.6432298421859741, mean_q: -0.11347689479589462, std_q: 1.6720480918884277, lamda: 1.361527919769287
Running avgs for agent 2: q_loss: 0.015441404655575752, p_loss: 1.4710476398468018, mean_rew: -0.3334043284562748, variance: 0.1486461871098727, cvar: -0.333187073469162, v: -0.3414705991744995, mean_q: -1.5349252223968506, std_q: 3.0516531467437744, lamda: 1.0405454635620117
Running avgs for agent 3: q_loss: 0.010159037075936794, p_loss: 1.4658424854278564, mean_rew: -0.33480838524908135, variance: 0.15399314428120853, cvar: -0.3437862992286682, v: -0.34970924258232117, mean_q: -1.5210531949996948, std_q: 2.877175807952881, lamda: 1.041432499885559

steps: 724975, episodes: 29000, mean episode reward: -25.565516136754187, agent episode reward: [-6.272993472334578, -4.845909049358772, -6.815679793332612, -7.630933821728226], time: 64.024
steps: 724975, episodes: 29000, mean episode variance: 0.18456434493185953, agent episode variance: [0.06407892636395991, 0.0401274295351468, 0.041005794919095936, 0.039352194113656876], time: 64.025
steps: 724975, episodes: 29000, mean episode cvar: 0.4527803890109062, agent episode cvar: [0.3111660547852516, 0.3125027561187744, -0.08452871751785278, -0.08635970437526704], time: 64.026
Running avgs for agent 0: q_loss: 0.22396522760391235, p_loss: 0.13769836723804474, mean_rew: -0.24374742934420776, variance: 0.25631570545583965, cvar: 1.2446643114089966, v: 0.28995928168296814, mean_q: -0.1702231466770172, std_q: 1.5759724378585815, lamda: 1.4262539148330688
Running avgs for agent 1: q_loss: 0.11233612895011902, p_loss: -0.018158698454499245, mean_rew: -0.22931031439638702, variance: 0.1605097181405872, cvar: 1.2500109672546387, v: 0.7138485312461853, mean_q: -0.026018386706709862, std_q: 1.6410115957260132, lamda: 1.369945764541626
Running avgs for agent 2: q_loss: 0.01820143312215805, p_loss: 1.4541071653366089, mean_rew: -0.3312610192625507, variance: 0.16402317967638375, cvar: -0.338114857673645, v: -0.3480718433856964, mean_q: -1.512768030166626, std_q: 2.9540297985076904, lamda: 1.0447462797164917
Running avgs for agent 3: q_loss: 0.009070048108696938, p_loss: 1.452518105506897, mean_rew: -0.3342032837443972, variance: 0.1574087764546275, cvar: -0.3454388380050659, v: -0.3504329323768616, mean_q: -1.506958246231079, std_q: 2.839862108230591, lamda: 1.042909860610962

steps: 749975, episodes: 30000, mean episode reward: -26.38010386933686, agent episode reward: [-7.172843222894025, -4.795802404192794, -6.730870299146838, -7.680587943103203], time: 64.245
steps: 749975, episodes: 30000, mean episode variance: 0.16970210297452287, agent episode variance: [0.06155647373013198, 0.0342211805973202, 0.034315523894503715, 0.039608924752566964], time: 64.245
steps: 749975, episodes: 30000, mean episode cvar: 0.45649461635947225, agent episode cvar: [0.31307765728235243, 0.3145612472295761, -0.08472795706987381, -0.08641633108258247], time: 64.246
Running avgs for agent 0: q_loss: 0.1972161829471588, p_loss: 0.1447869837284088, mean_rew: -0.24501492474598935, variance: 0.24622589492052793, cvar: 1.2523106336593628, v: 0.2702482044696808, mean_q: -0.17856571078300476, std_q: 1.5470993518829346, lamda: 1.4409438371658325
Running avgs for agent 1: q_loss: 0.09942542761564255, p_loss: -0.0676562637090683, mean_rew: -0.22806544059880798, variance: 0.1368847223892808, cvar: 1.2582449913024902, v: 0.77677983045578, mean_q: 0.02452816441655159, std_q: 1.6273434162139893, lamda: 1.3770211935043335
Running avgs for agent 2: q_loss: 0.008960654959082603, p_loss: 1.4567270278930664, mean_rew: -0.3318104329072997, variance: 0.13726209557801486, cvar: -0.33891183137893677, v: -0.3458610475063324, mean_q: -1.5163670778274536, std_q: 3.006409168243408, lamda: 1.0482077598571777
Running avgs for agent 3: q_loss: 0.008651460520923138, p_loss: 1.4441431760787964, mean_rew: -0.33479031189621744, variance: 0.15843569901026786, cvar: -0.3456653356552124, v: -0.35088208317756653, mean_q: -1.4989608526229858, std_q: 2.8218791484832764, lamda: 1.0455666780471802

steps: 774975, episodes: 31000, mean episode reward: -26.593331585046915, agent episode reward: [-7.0628376424829975, -4.965438148967392, -6.760117381260509, -7.8049384123360195], time: 64.283
steps: 774975, episodes: 31000, mean episode variance: 0.18239960340317338, agent episode variance: [0.06338389449939132, 0.037892471781931815, 0.039928060817532245, 0.04119517630431801], time: 64.283
steps: 774975, episodes: 31000, mean episode cvar: 0.45362388288974764, agent episode cvar: [0.30795891892910005, 0.318658350110054, -0.0861615015566349, -0.08683188459277152], time: 64.284
Running avgs for agent 0: q_loss: 0.23511266708374023, p_loss: 0.07443899661302567, mean_rew: -0.2468731045824778, variance: 0.25353557799756526, cvar: 1.2318357229232788, v: 0.3620353043079376, mean_q: -0.1078246608376503, std_q: 1.5261867046356201, lamda: 1.4545897245407104
Running avgs for agent 1: q_loss: 0.10019152611494064, p_loss: -0.07371186465024948, mean_rew: -0.2275525734630786, variance: 0.15156988712772726, cvar: 1.2746332883834839, v: 0.7926137447357178, mean_q: 0.028699329122900963, std_q: 1.6226727962493896, lamda: 1.3835833072662354
Running avgs for agent 2: q_loss: 0.012392510659992695, p_loss: 1.427351713180542, mean_rew: -0.3277399064200639, variance: 0.15971224327012898, cvar: -0.34464603662490845, v: -0.3548750877380371, mean_q: -1.4836299419403076, std_q: 2.942169427871704, lamda: 1.0507510900497437
Running avgs for agent 3: q_loss: 0.009423681534826756, p_loss: 1.4148750305175781, mean_rew: -0.3319374083801408, variance: 0.16478070521727203, cvar: -0.3473275303840637, v: -0.35348203778266907, mean_q: -1.46854829788208, std_q: 2.7687597274780273, lamda: 1.0480095148086548

steps: 799975, episodes: 32000, mean episode reward: -26.30459416587374, agent episode reward: [-6.735607878521594, -4.655266370176303, -7.1076209862310655, -7.8060989309447795], time: 64.024
steps: 799975, episodes: 32000, mean episode variance: 0.1827990707908757, agent episode variance: [0.057611899660434576, 0.03521037621796131, 0.046724564041942356, 0.04325223087053746], time: 64.025
steps: 799975, episodes: 32000, mean episode cvar: 0.4534340088367462, agent episode cvar: [0.31081802415847776, 0.3184977172613144, -0.08800361821055412, -0.08787811437249184], time: 64.025
Running avgs for agent 0: q_loss: 0.21247337758541107, p_loss: -0.010456313379108906, mean_rew: -0.24837450336234923, variance: 0.2304475986417383, cvar: 1.243272066116333, v: 0.49043145775794983, mean_q: -0.028184635564684868, std_q: 1.4938230514526367, lamda: 1.4688160419464111
Running avgs for agent 1: q_loss: 0.10146601498126984, p_loss: -0.08633290231227875, mean_rew: -0.22557325107283893, variance: 0.14084150487184524, cvar: 1.2739908695220947, v: 0.7893183827400208, mean_q: 0.0431380458176136, std_q: 1.6042338609695435, lamda: 1.3913242816925049
Running avgs for agent 2: q_loss: 0.014817886054515839, p_loss: 1.3948310613632202, mean_rew: -0.32650950275826496, variance: 0.18689825616776942, cvar: -0.3520144522190094, v: -0.3609238862991333, mean_q: -1.4502253532409668, std_q: 2.8961308002471924, lamda: 1.0559790134429932
Running avgs for agent 3: q_loss: 0.009667071513831615, p_loss: 1.3843398094177246, mean_rew: -0.33096779790264397, variance: 0.17300892348214983, cvar: -0.35151246190071106, v: -0.36073389649391174, mean_q: -1.4396027326583862, std_q: 2.7239110469818115, lamda: 1.05271577835083

steps: 824975, episodes: 33000, mean episode reward: -27.070158531424074, agent episode reward: [-7.1377993235587, -4.808547072236778, -6.918529516265899, -8.205282619362698], time: 64.451
steps: 824975, episodes: 33000, mean episode variance: 0.1737834776453674, agent episode variance: [0.048579615539871154, 0.030310097091831266, 0.048175142746418716, 0.04671862226724625], time: 64.452
steps: 824975, episodes: 33000, mean episode cvar: 0.4603053685724735, agent episode cvar: [0.3178033810853958, 0.3197028892040253, -0.08879315599799156, -0.08840774571895599], time: 64.452
Running avgs for agent 0: q_loss: 0.22935956716537476, p_loss: -0.03329882025718689, mean_rew: -0.24788382662365677, variance: 0.19431846215948462, cvar: 1.2712136507034302, v: 0.5554874539375305, mean_q: -0.010896025225520134, std_q: 1.4929049015045166, lamda: 1.4820144176483154
Running avgs for agent 1: q_loss: 0.08914072066545486, p_loss: -0.09356854110956192, mean_rew: -0.2232916728723032, variance: 0.12124038836732506, cvar: 1.2788115739822388, v: 0.7903376221656799, mean_q: 0.052399542182683945, std_q: 1.5917426347732544, lamda: 1.3984296321868896
Running avgs for agent 2: q_loss: 0.015693221241235733, p_loss: 1.3656833171844482, mean_rew: -0.32492548992333464, variance: 0.19270057098567486, cvar: -0.35517263412475586, v: -0.3655761778354645, mean_q: -1.4192790985107422, std_q: 2.846590757369995, lamda: 1.0627260208129883
Running avgs for agent 3: q_loss: 0.014934985898435116, p_loss: 1.3517979383468628, mean_rew: -0.33119914966788117, variance: 0.186874489068985, cvar: -0.35363101959228516, v: -0.363465815782547, mean_q: -1.4089303016662598, std_q: 2.6791129112243652, lamda: 1.0576540231704712

steps: 849975, episodes: 34000, mean episode reward: -26.90644481300612, agent episode reward: [-6.307637234088622, -4.452686209278088, -7.617467165741295, -8.528654203898117], time: 64.131
steps: 849975, episodes: 34000, mean episode variance: 0.17287349975761027, agent episode variance: [0.04331572967302054, 0.03340913101565093, 0.048409893984906374, 0.04773874508403242], time: 64.131
steps: 849975, episodes: 34000, mean episode cvar: 0.45973559373617173, agent episode cvar: [0.31964729392528535, 0.3185409224033356, -0.08866012746095657, -0.08979249513149261], time: 64.132
Running avgs for agent 0: q_loss: 0.1721940040588379, p_loss: -0.03191916644573212, mean_rew: -0.24903327493325644, variance: 0.17326291869208216, cvar: 1.278589129447937, v: 0.6325964331626892, mean_q: -0.017865246161818504, std_q: 1.5271133184432983, lamda: 1.4952001571655273
Running avgs for agent 1: q_loss: 0.09661343693733215, p_loss: -0.091637022793293, mean_rew: -0.22376514759468233, variance: 0.1336365240626037, cvar: 1.2741637229919434, v: 0.8071005940437317, mean_q: 0.05130463093519211, std_q: 1.5999387502670288, lamda: 1.4058698415756226
Running avgs for agent 2: q_loss: 0.016634522005915642, p_loss: 1.3479124307632446, mean_rew: -0.32576921425997313, variance: 0.1936395759396255, cvar: -0.35464051365852356, v: -0.36357682943344116, mean_q: -1.4022578001022339, std_q: 2.8320577144622803, lamda: 1.0682417154312134
Running avgs for agent 3: q_loss: 0.009888730011880398, p_loss: 1.3173401355743408, mean_rew: -0.33193622365437125, variance: 0.19095498033612968, cvar: -0.35916998982429504, v: -0.3668731153011322, mean_q: -1.378847360610962, std_q: 2.659822702407837, lamda: 1.0613375902175903

steps: 874975, episodes: 35000, mean episode reward: -26.511882738486815, agent episode reward: [-5.919388989149632, -3.6759441622010183, -7.814527269861313, -9.102022317274855], time: 64.11
steps: 874975, episodes: 35000, mean episode variance: 0.1716409636763856, agent episode variance: [0.04403743690252304, 0.028514347951859236, 0.04859974015317857, 0.05048943866882473], time: 64.11
steps: 874975, episodes: 35000, mean episode cvar: 0.4625571589767933, agent episode cvar: [0.32314848971366883, 0.3197751569747925, -0.08923533701896667, -0.09113115069270133], time: 64.111
Running avgs for agent 0: q_loss: 0.1609744429588318, p_loss: -0.02005494385957718, mean_rew: -0.24920746459994908, variance: 0.17614974761009217, cvar: 1.2925938367843628, v: 0.6944377422332764, mean_q: -0.032157789915800095, std_q: 1.5785977840423584, lamda: 1.5056897401809692
Running avgs for agent 1: q_loss: 0.09707421809434891, p_loss: -0.12603406608104706, mean_rew: -0.22031232970288495, variance: 0.11405739180743694, cvar: 1.2791005373001099, v: 0.8374960422515869, mean_q: 0.0865635946393013, std_q: 1.5977076292037964, lamda: 1.4126602411270142
Running avgs for agent 2: q_loss: 0.01917847990989685, p_loss: 1.3128622770309448, mean_rew: -0.323288582057289, variance: 0.19439896061271428, cvar: -0.3569413423538208, v: -0.3675461709499359, mean_q: -1.3647589683532715, std_q: 2.758951187133789, lamda: 1.073141098022461
Running avgs for agent 3: q_loss: 0.013112211599946022, p_loss: 1.2699700593948364, mean_rew: -0.33242000154595824, variance: 0.20195775467529892, cvar: -0.36452460289001465, v: -0.37377625703811646, mean_q: -1.3358032703399658, std_q: 2.5849268436431885, lamda: 1.0675129890441895

steps: 899975, episodes: 36000, mean episode reward: -26.07580165570185, agent episode reward: [-4.206719813605928, -3.3163428470835363, -8.347516503749404, -10.205222491262983], time: 64.799
steps: 899975, episodes: 36000, mean episode variance: 0.16899249250115828, agent episode variance: [0.0419367563854903, 0.025374565640930085, 0.04946692322194576, 0.05221424725279212], time: 64.799
steps: 899975, episodes: 36000, mean episode cvar: 0.4525678966641426, agent episode cvar: [0.318124659538269, 0.31657897400856017, -0.0900761477947235, -0.0920595890879631], time: 64.8
Running avgs for agent 0: q_loss: 0.1277368664741516, p_loss: -0.03959773853421211, mean_rew: -0.24752569672062108, variance: 0.1677470255419612, cvar: 1.272498607635498, v: 0.77400803565979, mean_q: -0.014987601898610592, std_q: 1.574578881263733, lamda: 1.5144455432891846
Running avgs for agent 1: q_loss: 0.09334610402584076, p_loss: -0.17719940841197968, mean_rew: -0.2197405106201569, variance: 0.10149826256372034, cvar: 1.2663159370422363, v: 0.872008204460144, mean_q: 0.1375562995672226, std_q: 1.5671889781951904, lamda: 1.4217942953109741
Running avgs for agent 2: q_loss: 0.015691377222537994, p_loss: 1.292014479637146, mean_rew: -0.3241995398219434, variance: 0.19786769288778305, cvar: -0.360304594039917, v: -0.3723660409450531, mean_q: -1.3460792303085327, std_q: 2.7612764835357666, lamda: 1.0778950452804565
Running avgs for agent 3: q_loss: 0.016910359263420105, p_loss: 1.2153218984603882, mean_rew: -0.33350306398157226, variance: 0.20885698901116848, cvar: -0.36823832988739014, v: -0.37682902812957764, mean_q: -1.2831323146820068, std_q: 2.5145349502563477, lamda: 1.0722767114639282

steps: 924975, episodes: 37000, mean episode reward: -26.495093397114847, agent episode reward: [-3.4150779080939455, -3.047608662322273, -8.618802379657911, -11.413604447040722], time: 64.274
steps: 924975, episodes: 37000, mean episode variance: 0.1676377422073856, agent episode variance: [0.03802234742976725, 0.021786064769141376, 0.055546879082918166, 0.052282450925558804], time: 64.275
steps: 924975, episodes: 37000, mean episode cvar: 0.45142756354808805, agent episode cvar: [0.3183571482896805, 0.3180398699045181, -0.09228813818097115, -0.09268131646513939], time: 64.275
Running avgs for agent 0: q_loss: 0.1173539087176323, p_loss: -0.1228925883769989, mean_rew: -0.24437412961143912, variance: 0.152089389719069, cvar: 1.2734285593032837, v: 0.9060330986976624, mean_q: 0.05986485630273819, std_q: 1.5880091190338135, lamda: 1.5225183963775635
Running avgs for agent 1: q_loss: 0.08532996475696564, p_loss: -0.2211168110370636, mean_rew: -0.21744821313137977, variance: 0.0871442590765655, cvar: 1.272159457206726, v: 0.9161728620529175, mean_q: 0.18115133047103882, std_q: 1.5583328008651733, lamda: 1.4286961555480957
Running avgs for agent 2: q_loss: 0.016359470784664154, p_loss: 1.258665680885315, mean_rew: -0.32425364539846385, variance: 0.22218751633167266, cvar: -0.3691525459289551, v: -0.37704095244407654, mean_q: -1.3120452165603638, std_q: 2.667649030685425, lamda: 1.0854661464691162
Running avgs for agent 3: q_loss: 0.014155843295156956, p_loss: 1.1895573139190674, mean_rew: -0.3366297335920105, variance: 0.20912980370223522, cvar: -0.37072527408599854, v: -0.3771393895149231, mean_q: -1.258323311805725, std_q: 2.4847962856292725, lamda: 1.077420949935913

steps: 949975, episodes: 38000, mean episode reward: -26.90166527641817, agent episode reward: [-2.7003499289470545, -2.431815366782946, -9.651755583607677, -12.117744397080493], time: 64.29
steps: 949975, episodes: 38000, mean episode variance: 0.16622853723727166, agent episode variance: [0.03198606428131461, 0.027649861704558135, 0.05660206801444292, 0.049990543236956], time: 64.29
steps: 949975, episodes: 38000, mean episode cvar: 0.46335751116275786, agent episode cvar: [0.326623162984848, 0.32207202863693235, -0.09354589331150055, -0.09179178714752197], time: 64.291
Running avgs for agent 0: q_loss: 0.09202330559492111, p_loss: -0.12684231996536255, mean_rew: -0.24112948247979665, variance: 0.12794425712525845, cvar: 1.3064926862716675, v: 0.989408552646637, mean_q: 0.051540445536375046, std_q: 1.593780517578125, lamda: 1.5281426906585693
Running avgs for agent 1: q_loss: 0.11596014350652695, p_loss: -0.2321203649044037, mean_rew: -0.2147776851774884, variance: 0.11059944681823254, cvar: 1.2882881164550781, v: 0.9646286368370056, mean_q: 0.18708468973636627, std_q: 1.5570718050003052, lamda: 1.436843752861023
Running avgs for agent 2: q_loss: 0.020028263330459595, p_loss: 1.2300724983215332, mean_rew: -0.32564106335796766, variance: 0.22640827205777167, cvar: -0.3741835951805115, v: -0.38185393810272217, mean_q: -1.2811384201049805, std_q: 2.6367313861846924, lamda: 1.0924062728881836
Running avgs for agent 3: q_loss: 0.016943424940109253, p_loss: 1.194564938545227, mean_rew: -0.3399752485439859, variance: 0.199962172947824, cvar: -0.3671671450138092, v: -0.3776310086250305, mean_q: -1.2542307376861572, std_q: 2.378775119781494, lamda: 1.082197904586792

steps: 974975, episodes: 39000, mean episode reward: -25.566794504158917, agent episode reward: [-2.076810014601492, -2.703822857776731, -9.839378720425852, -10.946782911354838], time: 64.356
steps: 974975, episodes: 39000, mean episode variance: 0.1630861436529085, agent episode variance: [0.0303585883481428, 0.024139434251002966, 0.05869762130454183, 0.049890499749220905], time: 64.357
steps: 974975, episodes: 39000, mean episode cvar: 0.47193057990074155, agent episode cvar: [0.33237896871566774, 0.3233757935762405, -0.09342426776885987, -0.09039991462230683], time: 64.357
Running avgs for agent 0: q_loss: 0.08703702688217163, p_loss: -0.10000447928905487, mean_rew: -0.2377403510663135, variance: 0.1214343533925712, cvar: 1.329515814781189, v: 1.0363048315048218, mean_q: 0.006062662228941917, std_q: 1.6247693300247192, lamda: 1.5325138568878174
Running avgs for agent 1: q_loss: 0.1442779302597046, p_loss: -0.21765221655368805, mean_rew: -0.21112666362824717, variance: 0.09655773700401186, cvar: 1.2935031652450562, v: 0.9709946513175964, mean_q: 0.1756383329629898, std_q: 1.5567736625671387, lamda: 1.4499242305755615
Running avgs for agent 2: q_loss: 0.020856987684965134, p_loss: 1.2159453630447388, mean_rew: -0.3273426347652716, variance: 0.2347904852181673, cvar: -0.37369707226753235, v: -0.38597363233566284, mean_q: -1.26567542552948, std_q: 2.611640691757202, lamda: 1.0959630012512207
Running avgs for agent 3: q_loss: 0.018319573253393173, p_loss: 1.2461408376693726, mean_rew: -0.34099399716669954, variance: 0.19956199899688362, cvar: -0.3615996539592743, v: -0.3714243173599243, mean_q: -1.2971856594085693, std_q: 2.329726219177246, lamda: 1.0905661582946777

steps: 999975, episodes: 40000, mean episode reward: -25.455511764326904, agent episode reward: [-3.4962358469179606, -3.542708614242045, -9.799641860630098, -8.616925442536802], time: 64.281
steps: 999975, episodes: 40000, mean episode variance: 0.17190485353767873, agent episode variance: [0.03452049850765616, 0.02716684711817652, 0.06164492966979742, 0.04857257824204862], time: 64.282
steps: 999975, episodes: 40000, mean episode cvar: 0.47536014404892923, agent episode cvar: [0.3378326305150986, 0.32217074537277224, -0.0946034571826458, -0.09003977465629577], time: 64.282
Running avgs for agent 0: q_loss: 0.09501787275075912, p_loss: -0.04184366762638092, mean_rew: -0.23428659386989925, variance: 0.13808199403062463, cvar: 1.3513305187225342, v: 1.032392978668213, mean_q: -0.07492019236087799, std_q: 1.6638959646224976, lamda: 1.5377434492111206
Running avgs for agent 1: q_loss: 0.11867956817150116, p_loss: -0.22781744599342346, mean_rew: -0.20925821982316034, variance: 0.10866738847270609, cvar: 1.2886829376220703, v: 0.9828154444694519, mean_q: 0.18534208834171295, std_q: 1.5554865598678589, lamda: 1.4605205059051514
Running avgs for agent 2: q_loss: 0.02482059970498085, p_loss: 1.1928528547286987, mean_rew: -0.32931812704877, variance: 0.24657971867918968, cvar: -0.3784138262271881, v: -0.38638854026794434, mean_q: -1.244773030281067, std_q: 2.536912202835083, lamda: 1.0997190475463867
Running avgs for agent 3: q_loss: 0.0105224484577775, p_loss: 1.2987565994262695, mean_rew: -0.34325617614145554, variance: 0.1942903129681945, cvar: -0.36015912890434265, v: -0.3659920394420624, mean_q: -1.3566826581954956, std_q: 2.3665332794189453, lamda: 1.0948554277420044

steps: 1024975, episodes: 41000, mean episode reward: -25.63243817424221, agent episode reward: [-3.4129757398819147, -3.9822848230281545, -10.00526929179634, -8.231908319535806], time: 64.394
steps: 1024975, episodes: 41000, mean episode variance: 0.17788493154849858, agent episode variance: [0.040550766761414704, 0.024411090344190596, 0.05819948701560497, 0.05472358742728829], time: 64.395
steps: 1024975, episodes: 41000, mean episode cvar: 0.46803343456983565, agent episode cvar: [0.33347564709186556, 0.32209426355361936, -0.09509821891784669, -0.09243825715780259], time: 64.395
Running avgs for agent 0: q_loss: 0.10819701850414276, p_loss: -0.06823701411485672, mean_rew: -0.23260576966020863, variance: 0.16220306704565882, cvar: 1.3339025974273682, v: 1.0141056776046753, mean_q: -0.05356854200363159, std_q: 1.670235276222229, lamda: 1.5454485416412354
Running avgs for agent 1: q_loss: 0.11842523515224457, p_loss: -0.2530829608440399, mean_rew: -0.20715203918105693, variance: 0.09764436137676238, cvar: 1.2883769273757935, v: 1.0173166990280151, mean_q: 0.22239769995212555, std_q: 1.529974102973938, lamda: 1.4703830480575562
Running avgs for agent 2: q_loss: 0.01859894022345543, p_loss: 1.1244399547576904, mean_rew: -0.32075524250447335, variance: 0.23279794806241988, cvar: -0.3803928792476654, v: -0.3898942172527313, mean_q: -1.1710113286972046, std_q: 2.4360108375549316, lamda: 1.1053247451782227
Running avgs for agent 3: q_loss: 0.013817583210766315, p_loss: 1.2310187816619873, mean_rew: -0.3343559793121862, variance: 0.21889434970915317, cvar: -0.36975303292274475, v: -0.378158301115036, mean_q: -1.2891552448272705, std_q: 2.283329486846924, lamda: 1.100128173828125

steps: 1049975, episodes: 42000, mean episode reward: -25.62578024001302, agent episode reward: [-3.0686083867414826, -3.626841151487951, -10.789964288608036, -8.140366413175553], time: 64.705
steps: 1049975, episodes: 42000, mean episode variance: 0.1385496974582784, agent episode variance: [0.02094861493445933, 0.009228720677550883, 0.052450725853443146, 0.05592163599282503], time: 64.706
steps: 1049975, episodes: 42000, mean episode cvar: 0.4526370271146297, agent episode cvar: [0.32597494077682493, 0.3181876202821732, -0.0958431878387928, -0.09568234610557556], time: 64.706
Running avgs for agent 0: q_loss: 0.0665532574057579, p_loss: -0.23756442964076996, mean_rew: -0.22740829800798104, variance: 0.08379445973783732, cvar: 1.303899884223938, v: 1.0912781953811646, mean_q: 0.13075099885463715, std_q: 1.500871181488037, lamda: 1.5512784719467163
Running avgs for agent 1: q_loss: 0.07541528344154358, p_loss: -0.3272073268890381, mean_rew: -0.20338770412522286, variance: 0.03691488271020353, cvar: 1.2727504968643188, v: 1.062497854232788, mean_q: 0.3053266704082489, std_q: 1.3852728605270386, lamda: 1.477832555770874
Running avgs for agent 2: q_loss: 0.01687934808433056, p_loss: 0.9922337532043457, mean_rew: -0.3034111442992228, variance: 0.20980289578437805, cvar: -0.38337278366088867, v: -0.3896738290786743, mean_q: -1.0185121297836304, std_q: 1.7346317768096924, lamda: 1.1094074249267578
Running avgs for agent 3: q_loss: 0.01417677104473114, p_loss: 1.0702990293502808, mean_rew: -0.31580408676502364, variance: 0.22368654608726501, cvar: -0.3827293813228607, v: -0.3914819359779358, mean_q: -1.1047717332839966, std_q: 1.8027756214141846, lamda: 1.106009840965271

steps: 1074975, episodes: 43000, mean episode reward: -26.439304547145586, agent episode reward: [-2.938383780103899, -3.0132866321931098, -11.63927206876459, -8.848362066083984], time: 64.602
steps: 1074975, episodes: 43000, mean episode variance: 0.13382366320234723, agent episode variance: [0.01610632399143651, 0.009550217781914398, 0.049821110390126704, 0.05834601103886962], time: 64.603
steps: 1074975, episodes: 43000, mean episode cvar: 0.4582299808263779, agent episode cvar: [0.3276933205127716, 0.32220998585224153, -0.09458954864740372, -0.09708377689123154], time: 64.603
Running avgs for agent 0: q_loss: 0.05941291153430939, p_loss: -0.3296809196472168, mean_rew: -0.22172199980727902, variance: 0.06442529596574605, cvar: 1.3107731342315674, v: 1.1130329370498657, mean_q: 0.24153023958206177, std_q: 1.4732389450073242, lamda: 1.5544673204421997
Running avgs for agent 1: q_loss: 0.05914472043514252, p_loss: -0.32921674847602844, mean_rew: -0.19902327122596958, variance: 0.03820087112765759, cvar: 1.2888398170471191, v: 1.0707213878631592, mean_q: 0.3018573224544525, std_q: 1.4006025791168213, lamda: 1.4808647632598877
Running avgs for agent 2: q_loss: 0.01738300360739231, p_loss: 0.9714280962944031, mean_rew: -0.3032974305486421, variance: 0.19928443431854248, cvar: -0.37835821509361267, v: -0.3906764090061188, mean_q: -0.9971452951431274, std_q: 1.6261738538742065, lamda: 1.1121660470962524
Running avgs for agent 3: q_loss: 0.017180748283863068, p_loss: 1.0127729177474976, mean_rew: -0.3128028384795814, variance: 0.23338402807712555, cvar: -0.38833507895469666, v: -0.40189117193222046, mean_q: -1.0394452810287476, std_q: 1.7088441848754883, lamda: 1.1124402284622192

steps: 1099975, episodes: 44000, mean episode reward: -25.072654268488257, agent episode reward: [-3.2908940670340323, -3.167760391907811, -9.683533601203802, -8.93046620834261], time: 64.443
steps: 1099975, episodes: 44000, mean episode variance: 0.13659989874763415, agent episode variance: [0.01720076277339831, 0.013366065603680909, 0.04451882916688919, 0.061514241203665734], time: 64.443
steps: 1099975, episodes: 44000, mean episode cvar: 0.4576654121875763, agent episode cvar: [0.32458846461772917, 0.3247275004386902, -0.09286638575792312, -0.09878416711091995], time: 64.444
Running avgs for agent 0: q_loss: 0.07365076243877411, p_loss: -0.37462133169174194, mean_rew: -0.21878557133563306, variance: 0.06880305109359323, cvar: 1.298353910446167, v: 1.0814327001571655, mean_q: 0.31506189703941345, std_q: 1.4277822971343994, lamda: 1.558815836906433
Running avgs for agent 1: q_loss: 0.06556389480829239, p_loss: -0.3317188322544098, mean_rew: -0.19567230849499764, variance: 0.053464262414723636, cvar: 1.2989100217819214, v: 1.0754145383834839, mean_q: 0.2996288537979126, std_q: 1.4023674726486206, lamda: 1.4837859869003296
Running avgs for agent 2: q_loss: 0.012594436295330524, p_loss: 0.9888503551483154, mean_rew: -0.3095376506443938, variance: 0.17807531356811523, cvar: -0.3714655041694641, v: -0.3763616979122162, mean_q: -1.0185331106185913, std_q: 1.6213188171386719, lamda: 1.1133047342300415
Running avgs for agent 3: q_loss: 0.019703373312950134, p_loss: 0.9896119832992554, mean_rew: -0.31617184353714933, variance: 0.2460569590330124, cvar: -0.39513665437698364, v: -0.409084290266037, mean_q: -1.0129241943359375, std_q: 1.6666518449783325, lamda: 1.113637924194336

steps: 1124975, episodes: 45000, mean episode reward: -24.925693667633656, agent episode reward: [-3.9399223049960836, -3.4062649890002374, -8.132677586058433, -9.446828787578898], time: 64.598
steps: 1124975, episodes: 45000, mean episode variance: 0.12870468734716997, agent episode variance: [0.013072606956586241, 0.015342996038030833, 0.03819099715352058, 0.06209808719903231], time: 64.599
steps: 1124975, episodes: 45000, mean episode cvar: 0.44880154106020925, agent episode cvar: [0.3135003386735916, 0.32525806069374086, -0.09019219914078712, -0.09976465916633606], time: 64.599
Running avgs for agent 0: q_loss: 0.12749983370304108, p_loss: -0.3790760934352875, mean_rew: -0.2164933907636313, variance: 0.052290427826344965, cvar: 1.2540013790130615, v: 1.0241559743881226, mean_q: 0.3419685363769531, std_q: 1.366595983505249, lamda: 1.5660569667816162
Running avgs for agent 1: q_loss: 0.060085806995630264, p_loss: -0.33163654804229736, mean_rew: -0.1939878670881825, variance: 0.06137198415212333, cvar: 1.3010321855545044, v: 1.0690995454788208, mean_q: 0.2965812385082245, std_q: 1.398130178451538, lamda: 1.4874738454818726
Running avgs for agent 2: q_loss: 0.007889237254858017, p_loss: 1.0335739850997925, mean_rew: -0.30893685953703187, variance: 0.15276398861408233, cvar: -0.36076879501342773, v: -0.3668869137763977, mean_q: -1.0587365627288818, std_q: 1.6384307146072388, lamda: 1.1164860725402832
Running avgs for agent 3: q_loss: 0.02004793845117092, p_loss: 0.9601337909698486, mean_rew: -0.31559959186175185, variance: 0.24839234352111816, cvar: -0.39905858039855957, v: -0.4092615842819214, mean_q: -0.9830042719841003, std_q: 1.592386245727539, lamda: 1.1129008531570435

steps: 1149975, episodes: 46000, mean episode reward: -25.77838881852907, agent episode reward: [-5.254550291268815, -2.9656446462120423, -7.837205534234517, -9.720988346813694], time: 64.327
steps: 1149975, episodes: 46000, mean episode variance: 0.15469260211568325, agent episode variance: [0.03713273719511926, 0.015392687593586743, 0.03963307850807905, 0.0625340988188982], time: 64.327
steps: 1149975, episodes: 46000, mean episode cvar: 0.4368431455492973, agent episode cvar: [0.302821202814579, 0.3237639257907867, -0.08999454551935196, -0.09974743753671646], time: 64.328
Running avgs for agent 0: q_loss: 0.4000502824783325, p_loss: -0.24998405575752258, mean_rew: -0.21560543058157414, variance: 0.14853094878047704, cvar: 1.2112847566604614, v: 0.9705345630645752, mean_q: 0.19369027018547058, std_q: 1.384724736213684, lamda: 1.587942361831665
Running avgs for agent 1: q_loss: 0.06222143396735191, p_loss: -0.35292309522628784, mean_rew: -0.19233468047326627, variance: 0.06157075037434697, cvar: 1.295055627822876, v: 1.0764384269714355, mean_q: 0.316811203956604, std_q: 1.4013493061065674, lamda: 1.4918147325515747
Running avgs for agent 2: q_loss: 0.008933790028095245, p_loss: 1.0708346366882324, mean_rew: -0.3101224522653271, variance: 0.1585323065519333, cvar: -0.35997816920280457, v: -0.36730942130088806, mean_q: -1.0960960388183594, std_q: 1.6976560354232788, lamda: 1.1204314231872559
Running avgs for agent 3: q_loss: 0.018722232431173325, p_loss: 0.9760659337043762, mean_rew: -0.3212560261129848, variance: 0.2501364052295685, cvar: -0.398989737033844, v: -0.4091266393661499, mean_q: -0.999509871006012, std_q: 1.604159951210022, lamda: 1.1155062913894653

steps: 1174975, episodes: 47000, mean episode reward: -26.262690891340366, agent episode reward: [-6.1190576087892765, -3.055244778332573, -8.060967384465275, -9.027421119753239], time: 64.616
steps: 1174975, episodes: 47000, mean episode variance: 0.1496640496123582, agent episode variance: [0.030259922673925757, 0.014759401468560099, 0.0438514202889055, 0.06079330518096685], time: 64.616
steps: 1174975, episodes: 47000, mean episode cvar: 0.4445254562199116, agent episode cvar: [0.30997992533445357, 0.3253466935157776, -0.0910932767689228, -0.09970788586139678], time: 64.617
Running avgs for agent 0: q_loss: 1.1414884328842163, p_loss: -0.31711727380752563, mean_rew: -0.21668140724095558, variance: 0.12103969069570303, cvar: 1.239919662475586, v: 0.8983135223388672, mean_q: 0.27682825922966003, std_q: 1.2824301719665527, lamda: 1.6178148984909058
Running avgs for agent 1: q_loss: 0.05830327048897743, p_loss: -0.37070515751838684, mean_rew: -0.18994070844689281, variance: 0.059037605874240395, cvar: 1.3013867139816284, v: 1.08194899559021, mean_q: 0.33524253964424133, std_q: 1.406741976737976, lamda: 1.49625825881958
Running avgs for agent 2: q_loss: 0.01086551882326603, p_loss: 1.085094690322876, mean_rew: -0.3105857737945923, variance: 0.175405681155622, cvar: -0.364373117685318, v: -0.3710944652557373, mean_q: -1.1091264486312866, std_q: 1.7521162033081055, lamda: 1.1231321096420288
Running avgs for agent 3: q_loss: 0.015982413664460182, p_loss: 0.9726096391677856, mean_rew: -0.3201313475982653, variance: 0.24317322671413422, cvar: -0.3988315463066101, v: -0.40639209747314453, mean_q: -0.9971987009048462, std_q: 1.5766781568527222, lamda: 1.121665120124817

steps: 1199975, episodes: 48000, mean episode reward: -28.223893949338986, agent episode reward: [-8.148031273335983, -3.4403473485525775, -8.168366532488067, -8.467148794962363], time: 64.746
steps: 1199975, episodes: 48000, mean episode variance: 0.2645900957239792, agent episode variance: [0.1478996701426804, 0.012451006614603102, 0.04764018466137349, 0.05659923430532217], time: 64.746
steps: 1199975, episodes: 48000, mean episode cvar: 0.4918599009513855, agent episode cvar: [0.3579213012456894, 0.32331497275829313, -0.09236803242564201, -0.09700834062695503], time: 64.747
Running avgs for agent 0: q_loss: 5.528678894042969, p_loss: 0.042619187384843826, mean_rew: -0.21819623234143887, variance: 0.5915986805707216, cvar: 1.4316853284835815, v: 0.7262600064277649, mean_q: -0.14112333953380585, std_q: 1.2575275897979736, lamda: 1.6494553089141846
Running avgs for agent 1: q_loss: 0.05683914199471474, p_loss: -0.4015439450740814, mean_rew: -0.1881195147961975, variance: 0.049804026458412406, cvar: 1.293259859085083, v: 1.0953997373580933, mean_q: 0.36944204568862915, std_q: 1.4061039686203003, lamda: 1.5011012554168701
Running avgs for agent 2: q_loss: 0.01263698935508728, p_loss: 1.0754607915878296, mean_rew: -0.31176885766401613, variance: 0.1905607283115387, cvar: -0.36947211623191833, v: -0.3758886158466339, mean_q: -1.1035710573196411, std_q: 1.7784992456436157, lamda: 1.126435399055481
Running avgs for agent 3: q_loss: 0.011892030015587807, p_loss: 1.0074855089187622, mean_rew: -0.32501511777926667, variance: 0.22639694809913635, cvar: -0.38803336024284363, v: -0.39176231622695923, mean_q: -1.033458948135376, std_q: 1.6281206607818604, lamda: 1.1248565912246704

steps: 1224975, episodes: 49000, mean episode reward: -28.43547440845749, agent episode reward: [-8.179925968766813, -3.2507594271469826, -8.559793959802022, -8.444995052741675], time: 66.061
steps: 1224975, episodes: 49000, mean episode variance: 0.2000932621555403, agent episode variance: [0.07892199322208762, 0.011434165353886782, 0.05264427873678505, 0.05709282484278083], time: 66.062
steps: 1224975, episodes: 49000, mean episode cvar: 0.48873626682162286, agent episode cvar: [0.35551284128427507, 0.3248462884426117, -0.09449644654989242, -0.09712641635537148], time: 66.063
Running avgs for agent 0: q_loss: 2.8327319622039795, p_loss: 0.2976856827735901, mean_rew: -0.2203863913539656, variance: 0.3156879728883505, cvar: 1.4220514297485352, v: 0.4012592136859894, mean_q: -0.3821912109851837, std_q: 1.2423955202102661, lamda: 1.6782898902893066
Running avgs for agent 1: q_loss: 0.05309859290719032, p_loss: -0.4294883608818054, mean_rew: -0.18631011112449083, variance: 0.04573666141554713, cvar: 1.2993849515914917, v: 1.1169610023498535, mean_q: 0.3988392949104309, std_q: 1.411548376083374, lamda: 1.5057538747787476
Running avgs for agent 2: q_loss: 0.015011821873486042, p_loss: 1.040526032447815, mean_rew: -0.3126811921793743, variance: 0.2105771154165268, cvar: -0.37798577547073364, v: -0.3843355178833008, mean_q: -1.0701783895492554, std_q: 1.7514939308166504, lamda: 1.1317881345748901
Running avgs for agent 3: q_loss: 0.012539224699139595, p_loss: 1.0165235996246338, mean_rew: -0.3244890380547603, variance: 0.2283712923526764, cvar: -0.3885056674480438, v: -0.40036654472351074, mean_q: -1.039176344871521, std_q: 1.6354929208755493, lamda: 1.1275906562805176

steps: 1249975, episodes: 50000, mean episode reward: -30.484535458856577, agent episode reward: [-7.99932907277784, -2.6294271488565073, -11.649260992816867, -8.20651824440536], time: 71.608
steps: 1249975, episodes: 50000, mean episode variance: 0.18013929785229266, agent episode variance: [0.05953012773115188, 0.01310875823814422, 0.047427124343812466, 0.060073287539184096], time: 71.608
steps: 1249975, episodes: 50000, mean episode cvar: 0.5158889540731907, agent episode cvar: [0.38095566564798355, 0.32586622631549833, -0.09290554234385491, -0.09802739554643632], time: 71.609
Running avgs for agent 0: q_loss: 2.7641284465789795, p_loss: 0.2684726417064667, mean_rew: -0.22276990813772315, variance: 0.2381205109246075, cvar: 1.523822546005249, v: 0.2649399936199188, mean_q: -0.3094709515571594, std_q: 1.1694445610046387, lamda: 1.703582763671875
Running avgs for agent 1: q_loss: 0.05224568024277687, p_loss: -0.43908724188804626, mean_rew: -0.1836801965097411, variance: 0.05243503295257688, cvar: 1.3034648895263672, v: 1.1390827894210815, mean_q: 0.4084484875202179, std_q: 1.4060935974121094, lamda: 1.5107197761535645
Running avgs for agent 2: q_loss: 0.012849004939198494, p_loss: 1.0128825902938843, mean_rew: -0.3134726949465135, variance: 0.18970850110054016, cvar: -0.3716222047805786, v: -0.38093605637550354, mean_q: -1.046678066253662, std_q: 1.693486213684082, lamda: 1.135886788368225
Running avgs for agent 3: q_loss: 0.012875048443675041, p_loss: 1.0253947973251343, mean_rew: -0.3265678769898033, variance: 0.24029314517974854, cvar: -0.39210957288742065, v: -0.4058797061443329, mean_q: -1.046554684638977, std_q: 1.649228811264038, lamda: 1.1268612146377563

steps: 1274975, episodes: 51000, mean episode reward: -30.72244760688702, agent episode reward: [-8.676374553097574, -2.5979150183382598, -11.010258164312866, -8.437899871138322], time: 65.093
steps: 1274975, episodes: 51000, mean episode variance: 0.2176024887918029, agent episode variance: [0.09744392378255724, 0.014682444084668532, 0.044729565074667334, 0.06074655584990978], time: 65.093
steps: 1274975, episodes: 51000, mean episode cvar: 0.44704544922709466, agent episode cvar: [0.309015759408474, 0.3274048308134079, -0.09178448837995529, -0.09759065261483192], time: 65.094
Running avgs for agent 0: q_loss: 0.7537442445755005, p_loss: 0.24660101532936096, mean_rew: -0.22509461328149943, variance: 0.38977569513022897, cvar: 1.236063003540039, v: 0.24667486548423767, mean_q: -0.2867200970649719, std_q: 1.1452494859695435, lamda: 1.7212334871292114
Running avgs for agent 1: q_loss: 0.05036059394478798, p_loss: -0.4407084286212921, mean_rew: -0.18010247478029895, variance: 0.05872977633867413, cvar: 1.309619426727295, v: 1.1357131004333496, mean_q: 0.40989843010902405, std_q: 1.4182021617889404, lamda: 1.5160831212997437
Running avgs for agent 2: q_loss: 0.009687389247119427, p_loss: 1.0457502603530884, mean_rew: -0.32003474020776335, variance: 0.1789182722568512, cvar: -0.36713793873786926, v: -0.3730018138885498, mean_q: -1.0759549140930176, std_q: 1.656532883644104, lamda: 1.1396257877349854
Running avgs for agent 3: q_loss: 0.013530039228498936, p_loss: 1.0351697206497192, mean_rew: -0.3283504188528127, variance: 0.24298621714115143, cvar: -0.39036262035369873, v: -0.40809524059295654, mean_q: -1.0557650327682495, std_q: 1.6767234802246094, lamda: 1.124124526977539

steps: 1299975, episodes: 52000, mean episode reward: -30.216673905729927, agent episode reward: [-9.79969539118477, -2.601618710992242, -9.163533580552441, -8.651826223000477], time: 66.949
steps: 1299975, episodes: 52000, mean episode variance: 0.19743094257824123, agent episode variance: [0.06900941750407219, 0.010022821115329861, 0.0524721389375627, 0.06592656502127647], time: 66.95
steps: 1299975, episodes: 52000, mean episode cvar: 0.49817036813497545, agent episode cvar: [0.36420479661226274, 0.32830585515499117, -0.09421095001697541, -0.10012933361530305], time: 66.95
Running avgs for agent 0: q_loss: 3.104126453399658, p_loss: 0.16427123546600342, mean_rew: -0.2283311379750832, variance: 0.27603767001628876, cvar: 1.4568191766738892, v: 0.3255328834056854, mean_q: -0.2163904756307602, std_q: 1.1109557151794434, lamda: 1.7366960048675537
Running avgs for agent 1: q_loss: 0.04480297118425369, p_loss: -0.4405536949634552, mean_rew: -0.1768478347790307, variance: 0.040091284461319446, cvar: 1.3132233619689941, v: 1.1345418691635132, mean_q: 0.4101044535636902, std_q: 1.4190443754196167, lamda: 1.5215460062026978
Running avgs for agent 2: q_loss: 0.012217623181641102, p_loss: 1.0685126781463623, mean_rew: -0.323433057826783, variance: 0.2098885557502508, cvar: -0.37684381008148193, v: -0.38638079166412354, mean_q: -1.0976821184158325, std_q: 1.6684566736221313, lamda: 1.1440664529800415
Running avgs for agent 3: q_loss: 0.013443103060126305, p_loss: 1.0346729755401611, mean_rew: -0.32948543738946795, variance: 0.263706237077713, cvar: -0.4005173444747925, v: -0.40948212146759033, mean_q: -1.053634524345398, std_q: 1.6846489906311035, lamda: 1.1225224733352661

steps: 1324975, episodes: 53000, mean episode reward: -31.808067789173446, agent episode reward: [-11.670471504126395, -2.6533109882244204, -8.983698553996318, -8.500586742826314], time: 66.779
steps: 1324975, episodes: 53000, mean episode variance: 0.19150497324299068, agent episode variance: [0.04677781846001744, 0.013428133091889323, 0.06437948462367057, 0.06691953706741333], time: 66.78
steps: 1324975, episodes: 53000, mean episode cvar: 0.5429274266958237, agent episode cvar: [0.41244954758882524, 0.32899999928474427, -0.09786074018478394, -0.10066137999296189], time: 66.78
Running avgs for agent 0: q_loss: 4.793389797210693, p_loss: 0.6028633713722229, mean_rew: -0.2337428546727824, variance: 0.18711127384006976, cvar: 1.6497981548309326, v: -0.03170347213745117, mean_q: -0.6619756817817688, std_q: 1.2317432165145874, lamda: 1.7613904476165771
Running avgs for agent 1: q_loss: 0.04784141480922699, p_loss: -0.4280588626861572, mean_rew: -0.1745691083844336, variance: 0.05371253236755729, cvar: 1.315999984741211, v: 1.127218246459961, mean_q: 0.3984515964984894, std_q: 1.4379438161849976, lamda: 1.527496337890625
Running avgs for agent 2: q_loss: 0.016652647405862808, p_loss: 1.039052963256836, mean_rew: -0.32488011520594673, variance: 0.25751793384552, cvar: -0.3914429545402527, v: -0.40471765398979187, mean_q: -1.067043662071228, std_q: 1.6764426231384277, lamda: 1.1503822803497314
Running avgs for agent 3: q_loss: 0.01336774230003357, p_loss: 1.0325379371643066, mean_rew: -0.3315745971960297, variance: 0.2676781713962555, cvar: -0.4026454985141754, v: -0.4097703695297241, mean_q: -1.0503085851669312, std_q: 1.6790484189987183, lamda: 1.1249332427978516

steps: 1349975, episodes: 54000, mean episode reward: -30.707146394515956, agent episode reward: [-10.959529368042316, -2.480197289091071, -8.983951513615835, -8.283468223766729], time: 66.528
steps: 1349975, episodes: 54000, mean episode variance: 0.23964308706996962, agent episode variance: [0.08766047617793084, 0.012772194999735802, 0.07532707332074642, 0.06388334257155656], time: 66.529
steps: 1349975, episodes: 54000, mean episode cvar: 0.6437954430282116, agent episode cvar: [0.5156333058476448, 0.33082742321491243, -0.10315070626139641, -0.09951457977294922], time: 66.529
Running avgs for agent 0: q_loss: 17.422407150268555, p_loss: 0.9578933715820312, mean_rew: -0.2389778491507143, variance: 0.35064190471172335, cvar: 2.062533140182495, v: -0.42487695813179016, mean_q: -1.0070765018463135, std_q: 1.3711745738983154, lamda: 1.7859030961990356
Running avgs for agent 1: q_loss: 0.04497405141592026, p_loss: -0.415296345949173, mean_rew: -0.17034731543937925, variance: 0.05108877999894321, cvar: 1.3233096599578857, v: 1.1161857843399048, mean_q: 0.385285347700119, std_q: 1.438287377357483, lamda: 1.5333143472671509
Running avgs for agent 2: q_loss: 0.022712688893079758, p_loss: 0.9897114634513855, mean_rew: -0.3260526775667118, variance: 0.301308274269104, cvar: -0.41260284185409546, v: -0.42451658844947815, mean_q: -1.0127604007720947, std_q: 1.63645601272583, lamda: 1.148745059967041
Running avgs for agent 3: q_loss: 0.01134116668254137, p_loss: 1.0362393856048584, mean_rew: -0.3317430329068786, variance: 0.255533367395401, cvar: -0.39805832505226135, v: -0.40669921040534973, mean_q: -1.055114984512329, std_q: 1.6713899374008179, lamda: 1.1264342069625854

steps: 1374975, episodes: 55000, mean episode reward: -31.769667954651794, agent episode reward: [-11.53275180729245, -2.4113197964944595, -9.545933804521134, -8.279662546343753], time: 64.455
steps: 1374975, episodes: 55000, mean episode variance: 0.2970804348923266, agent episode variance: [0.1347982252649963, 0.014570398867130279, 0.08270971497893334, 0.06500209578126669], time: 64.456
steps: 1374975, episodes: 55000, mean episode cvar: 0.6600834848582745, agent episode cvar: [0.5362556996941567, 0.33003072559833524, -0.1064831035733223, -0.09971983686089515], time: 64.456
Running avgs for agent 0: q_loss: 20.89176368713379, p_loss: 1.5033159255981445, mean_rew: -0.24335098677472167, variance: 0.5391929010599852, cvar: 2.1450228691101074, v: -0.7089036107063293, mean_q: -1.613060474395752, std_q: 1.5684194564819336, lamda: 1.8098084926605225
Running avgs for agent 1: q_loss: 0.04934082180261612, p_loss: -0.3932337164878845, mean_rew: -0.16618117494953383, variance: 0.058281595468521116, cvar: 1.3201229572296143, v: 1.0869389772415161, mean_q: 0.3652585446834564, std_q: 1.4338080883026123, lamda: 1.5401015281677246
Running avgs for agent 2: q_loss: 0.02962782047688961, p_loss: 0.9451645016670227, mean_rew: -0.33059168782202186, variance: 0.3308388590812683, cvar: -0.4259324371814728, v: -0.44060778617858887, mean_q: -0.9632893800735474, std_q: 1.582502007484436, lamda: 1.149071216583252
Running avgs for agent 3: q_loss: 0.010442264378070831, p_loss: 1.0518808364868164, mean_rew: -0.3336135817614851, variance: 0.2600083649158478, cvar: -0.3988793194293976, v: -0.4051578938961029, mean_q: -1.071057915687561, std_q: 1.6922485828399658, lamda: 1.1305220127105713

steps: 1399975, episodes: 56000, mean episode reward: -35.96113636881461, agent episode reward: [-14.966557266051597, -2.3741621306469924, -10.15000537119747, -8.470411600918553], time: 64.402
steps: 1399975, episodes: 56000, mean episode variance: 0.32589300329610704, agent episode variance: [0.1569420003918931, 0.014161671572364866, 0.08796545584499836, 0.06682387548685075], time: 64.402
steps: 1399975, episodes: 56000, mean episode cvar: 0.568550781428814, agent episode cvar: [0.44856667149066926, 0.32950528848171234, -0.10958245688676835, -0.09993872165679932], time: 64.403
Running avgs for agent 0: q_loss: 9.015774726867676, p_loss: 1.5186408758163452, mean_rew: -0.2505714250512098, variance: 0.6277680015675724, cvar: 1.794266700744629, v: -0.8019914627075195, mean_q: -1.6975739002227783, std_q: 1.527710199356079, lamda: 1.8325374126434326
Running avgs for agent 1: q_loss: 0.049406830221414566, p_loss: -0.38942602276802063, mean_rew: -0.16291744406776512, variance: 0.056646686289459465, cvar: 1.318021297454834, v: 1.0553104877471924, mean_q: 0.36311426758766174, std_q: 1.4377925395965576, lamda: 1.550108790397644
Running avgs for agent 2: q_loss: 0.03383290767669678, p_loss: 0.905577540397644, mean_rew: -0.33312456517828126, variance: 0.351861834526062, cvar: -0.4383298456668854, v: -0.45180606842041016, mean_q: -0.9189250469207764, std_q: 1.4948891401290894, lamda: 1.1500247716903687
Running avgs for agent 3: q_loss: 0.01177407056093216, p_loss: 1.04921293258667, mean_rew: -0.33408405420061227, variance: 0.2672955095767975, cvar: -0.3997548818588257, v: -0.4097810685634613, mean_q: -1.067697286605835, std_q: 1.6876336336135864, lamda: 1.1340856552124023

steps: 1424975, episodes: 57000, mean episode reward: -35.058223211264384, agent episode reward: [-13.987868002126884, -1.969914676619563, -10.279242291122385, -8.821198241395544], time: 64.328
steps: 1424975, episodes: 57000, mean episode variance: 0.3698416216764599, agent episode variance: [0.19375585621595381, 0.017036975959315897, 0.08582162809371949, 0.07322716140747071], time: 64.329
steps: 1424975, episodes: 57000, mean episode cvar: 0.4837083602249622, agent episode cvar: [0.36431210482120513, 0.33041745483875273, -0.10886946058273315, -0.1021517388522625], time: 64.329
Running avgs for agent 0: q_loss: 3.245166540145874, p_loss: 1.6261398792266846, mean_rew: -0.2601831042953848, variance: 0.7750234248638153, cvar: 1.457248568534851, v: -0.9327813982963562, mean_q: -1.7284222841262817, std_q: 1.4700617790222168, lamda: 1.851891040802002
Running avgs for agent 1: q_loss: 0.050737831741571426, p_loss: -0.3630920350551605, mean_rew: -0.15974259544536326, variance: 0.06814790383726359, cvar: 1.3216698169708252, v: 1.0181254148483276, mean_q: 0.33959609270095825, std_q: 1.4421687126159668, lamda: 1.5596842765808105
Running avgs for agent 2: q_loss: 0.032488174736499786, p_loss: 0.9024561047554016, mean_rew: -0.3377744436410012, variance: 0.3432864844799042, cvar: -0.4354778528213501, v: -0.45343664288520813, mean_q: -0.9168421626091003, std_q: 1.4505597352981567, lamda: 1.149949073791504
Running avgs for agent 3: q_loss: 0.014096515253186226, p_loss: 1.0438450574874878, mean_rew: -0.33691779922414555, variance: 0.292908638715744, cvar: -0.40860694646835327, v: -0.4221275746822357, mean_q: -1.059497356414795, std_q: 1.6918542385101318, lamda: 1.1363221406936646

steps: 1449975, episodes: 58000, mean episode reward: -29.463085742882388, agent episode reward: [-8.937285382666811, -1.9870622587889688, -9.725115397355871, -8.813622704070735], time: 64.362
steps: 1449975, episodes: 58000, mean episode variance: 0.4176192226125859, agent episode variance: [0.24114720830880104, 0.018818797040265053, 0.0787369858622551, 0.07891623140126466], time: 64.362
steps: 1449975, episodes: 58000, mean episode cvar: 0.47999439921975134, agent episode cvar: [0.3631554839015007, 0.3290736442804337, -0.10727041047811509, -0.10496431848406791], time: 64.363
Running avgs for agent 0: q_loss: 3.0029654502868652, p_loss: 1.4184921979904175, mean_rew: -0.26360992917316073, variance: 0.9645888332352042, cvar: 1.4526220560073853, v: -0.7973219752311707, mean_q: -1.4957009553909302, std_q: 1.4570187330245972, lamda: 1.8707282543182373
Running avgs for agent 1: q_loss: 0.05711126700043678, p_loss: -0.3563316762447357, mean_rew: -0.15654785554645256, variance: 0.07527518816106021, cvar: 1.3162946701049805, v: 1.0148869752883911, mean_q: 0.3330496847629547, std_q: 1.4432792663574219, lamda: 1.5717265605926514
Running avgs for agent 2: q_loss: 0.02465924248099327, p_loss: 0.9006677269935608, mean_rew: -0.33784470855995924, variance: 0.3149479329586029, cvar: -0.42908161878585815, v: -0.4341904819011688, mean_q: -0.9203020334243774, std_q: 1.402558445930481, lamda: 1.1516718864440918
Running avgs for agent 3: q_loss: 0.016953974962234497, p_loss: 1.0191102027893066, mean_rew: -0.3372527474577935, variance: 0.3156649172306061, cvar: -0.4198572635650635, v: -0.4320319592952728, mean_q: -1.0318506956100464, std_q: 1.664658546447754, lamda: 1.1399104595184326

steps: 1474975, episodes: 59000, mean episode reward: -27.92967349914209, agent episode reward: [-7.119519077232401, -1.6468757624159314, -9.583639738902974, -9.579638920590781], time: 64.822
steps: 1474975, episodes: 59000, mean episode variance: 0.404268521938473, agent episode variance: [0.2303595133172348, 0.02052003578748554, 0.0693940430432558, 0.08399492979049683], time: 64.823
steps: 1474975, episodes: 59000, mean episode cvar: 0.4446128709614277, agent episode cvar: [0.32523508423566816, 0.32864507937431336, -0.10230921092629433, -0.10695808172225953], time: 64.823
Running avgs for agent 0: q_loss: 1.3228243589401245, p_loss: 1.2107380628585815, mean_rew: -0.26644721530199866, variance: 0.9214380532689392, cvar: 1.3009402751922607, v: -0.6235384345054626, mean_q: -1.2913986444473267, std_q: 1.4281353950500488, lamda: 1.8894180059432983
Running avgs for agent 1: q_loss: 0.059305835515260696, p_loss: -0.35868823528289795, mean_rew: -0.15308651497426806, variance: 0.08208014314994216, cvar: 1.3145803213119507, v: 1.086429238319397, mean_q: 0.3311140835285187, std_q: 1.4825141429901123, lamda: 1.5830583572387695
Running avgs for agent 2: q_loss: 0.019165055826306343, p_loss: 0.9449881315231323, mean_rew: -0.34126267887767053, variance: 0.27757617831230164, cvar: -0.4092368185520172, v: -0.4154890775680542, mean_q: -0.9735926389694214, std_q: 1.4508891105651855, lamda: 1.1541996002197266
Running avgs for agent 3: q_loss: 0.02008138969540596, p_loss: 0.9925070405006409, mean_rew: -0.3391893639165029, variance: 0.33597972989082336, cvar: -0.42783230543136597, v: -0.4425645172595978, mean_q: -1.0037612915039062, std_q: 1.6230168342590332, lamda: 1.1420514583587646

steps: 1499975, episodes: 60000, mean episode reward: -27.388866416922173, agent episode reward: [-5.816119090449585, -2.4185017749409683, -9.403449649477935, -9.750795902053683], time: 65.101
steps: 1499975, episodes: 60000, mean episode variance: 0.3455736147137359, agent episode variance: [0.1709798582866788, 0.025088421110995116, 0.06550535440444946, 0.08399998091161251], time: 65.102
steps: 1499975, episodes: 60000, mean episode cvar: 0.44205956399440766, agent episode cvar: [0.3277809158861637, 0.3215069216489792, -0.09996485590934753, -0.1072634176313877], time: 65.102
Running avgs for agent 0: q_loss: 1.5243651866912842, p_loss: 0.9268102049827576, mean_rew: -0.2679460828778104, variance: 0.6839194331467152, cvar: 1.3111236095428467, v: -0.3442472517490387, mean_q: -1.0151978731155396, std_q: 1.4358693361282349, lamda: 1.906014084815979
Running avgs for agent 1: q_loss: 0.11792432516813278, p_loss: -0.2887114882469177, mean_rew: -0.14804971782971163, variance: 0.10035368444398046, cvar: 1.2860275506973267, v: 1.0109835863113403, mean_q: 0.26689445972442627, std_q: 1.5036396980285645, lamda: 1.6031783819198608
Running avgs for agent 2: q_loss: 0.015012504532933235, p_loss: 0.9832617044448853, mean_rew: -0.34421682341393134, variance: 0.26202142238616943, cvar: -0.39985939860343933, v: -0.40514934062957764, mean_q: -1.0189663171768188, std_q: 1.5065401792526245, lamda: 1.1567530632019043
Running avgs for agent 3: q_loss: 0.020362205803394318, p_loss: 0.9825795292854309, mean_rew: -0.34337563369528323, variance: 0.3359999358654022, cvar: -0.4290536642074585, v: -0.4455985724925995, mean_q: -0.9935060143470764, std_q: 1.5943881273269653, lamda: 1.139761209487915

steps: 1524975, episodes: 61000, mean episode reward: -26.143442026512524, agent episode reward: [-5.595528840187782, -3.14449799407475, -8.234015869531952, -9.169399322718041], time: 64.83
steps: 1524975, episodes: 61000, mean episode variance: 0.3111539359977469, agent episode variance: [0.13841338528878988, 0.019586602655239402, 0.069279856428504, 0.08387409162521363], time: 64.831
steps: 1524975, episodes: 61000, mean episode cvar: 0.44651865899562837, agent episode cvar: [0.348399298787117, 0.3070558249950409, -0.10089506635069848, -0.10804139843583108], time: 64.832
Running avgs for agent 0: q_loss: 2.271605968475342, p_loss: 0.7024200558662415, mean_rew: -0.2658253813239026, variance: 0.5536535411551595, cvar: 1.39359712600708, v: -0.12363651394844055, mean_q: -0.7823531627655029, std_q: 1.4623221158981323, lamda: 1.9226491451263428
Running avgs for agent 1: q_loss: 0.1002504751086235, p_loss: -0.36436188220977783, mean_rew: -0.14632352812291094, variance: 0.07834641062095761, cvar: 1.2282233238220215, v: 1.0739625692367554, mean_q: 0.34573039412498474, std_q: 1.5489448308944702, lamda: 1.623396396636963
Running avgs for agent 2: q_loss: 0.015020187012851238, p_loss: 1.0049488544464111, mean_rew: -0.346506200036288, variance: 0.2771194279193878, cvar: -0.4035802483558655, v: -0.4131792485713959, mean_q: -1.0384854078292847, std_q: 1.5464943647384644, lamda: 1.1608612537384033
Running avgs for agent 3: q_loss: 0.017215702682733536, p_loss: 0.9775834679603577, mean_rew: -0.3446037841819109, variance: 0.33549636602401733, cvar: -0.43216559290885925, v: -0.44191762804985046, mean_q: -0.9890099167823792, std_q: 1.5634868144989014, lamda: 1.1396268606185913

steps: 1549975, episodes: 62000, mean episode reward: -27.714140102160854, agent episode reward: [-6.303661503337104, -3.642471396006202, -8.765252005387826, -9.002755197429725], time: 64.801
steps: 1549975, episodes: 62000, mean episode variance: 0.2988772782655433, agent episode variance: [0.11268186893500387, 0.025835008445195852, 0.07910009664297103, 0.08126030424237252], time: 64.802
steps: 1549975, episodes: 62000, mean episode cvar: 0.5446903505623341, agent episode cvar: [0.4035097285211086, 0.35220678436756137, -0.10408795019984245, -0.10693821212649346], time: 64.802
Running avgs for agent 0: q_loss: 5.476179122924805, p_loss: 0.905971109867096, mean_rew: -0.26457623716282636, variance: 0.4507274757400155, cvar: 1.6140388250350952, v: -0.34056851267814636, mean_q: -0.9763883948326111, std_q: 1.5785902738571167, lamda: 1.9414387941360474
Running avgs for agent 1: q_loss: 1.27024245262146, p_loss: -0.44228532910346985, mean_rew: -0.14373469094216404, variance: 0.10334003378078341, cvar: 1.4088271856307983, v: 1.1869655847549438, mean_q: 0.42038384079933167, std_q: 1.5169626474380493, lamda: 1.6380395889282227
Running avgs for agent 2: q_loss: 0.01791963540017605, p_loss: 1.0019785165786743, mean_rew: -0.34696617216686537, variance: 0.3164003789424896, cvar: -0.4163517951965332, v: -0.42921188473701477, mean_q: -1.027707815170288, std_q: 1.5600433349609375, lamda: 1.1641690731048584
Running avgs for agent 3: q_loss: 0.013944121077656746, p_loss: 0.987554669380188, mean_rew: -0.3456019752083906, variance: 0.3250412344932556, cvar: -0.42775288224220276, v: -0.43451783061027527, mean_q: -1.0006904602050781, std_q: 1.5531069040298462, lamda: 1.143092155456543

steps: 1574975, episodes: 63000, mean episode reward: -28.59179157617847, agent episode reward: [-6.421728284105227, -4.26440857192243, -9.14567674805352, -8.759977972097293], time: 65.566
steps: 1574975, episodes: 63000, mean episode variance: 0.5048432572837919, agent episode variance: [0.20911134220659733, 0.12924653192423283, 0.08712822918593884, 0.07935715396702289], time: 65.566
steps: 1574975, episodes: 63000, mean episode cvar: 0.649535381346941, agent episode cvar: [0.5235772889256477, 0.3379596714377403, -0.10650010827183723, -0.10550147074460983], time: 65.567
Running avgs for agent 0: q_loss: 22.69827651977539, p_loss: 1.8689217567443848, mean_rew: -0.2657776109810012, variance: 0.8364453688263893, cvar: 2.094309091567993, v: -0.8410012722015381, mean_q: -1.9375516176223755, std_q: 1.9393244981765747, lamda: 1.9652559757232666
Running avgs for agent 1: q_loss: 4.965473651885986, p_loss: -0.15423627197742462, mean_rew: -0.14025993578117138, variance: 0.5169861276969313, cvar: 1.3518385887145996, v: 0.8835190534591675, mean_q: 0.13820597529411316, std_q: 1.466827630996704, lamda: 1.6672435998916626
Running avgs for agent 2: q_loss: 0.02324581891298294, p_loss: 0.9890755414962769, mean_rew: -0.3500820800224386, variance: 0.3485129177570343, cvar: -0.4260004162788391, v: -0.44457072019577026, mean_q: -1.0099884271621704, std_q: 1.5774919986724854, lamda: 1.1634870767593384
Running avgs for agent 3: q_loss: 0.012822751887142658, p_loss: 1.0025320053100586, mean_rew: -0.34680081632685705, variance: 0.3174286484718323, cvar: -0.42200589179992676, v: -0.43006378412246704, mean_q: -1.0174728631973267, std_q: 1.5663480758666992, lamda: 1.1463876962661743

steps: 1599975, episodes: 64000, mean episode reward: -31.957136189774815, agent episode reward: [-7.636107519025813, -5.895043736631094, -9.45912642391433, -8.966858510203574], time: 64.7
steps: 1599975, episodes: 64000, mean episode variance: 0.7403079155124724, agent episode variance: [0.45326796194911, 0.10731595620885491, 0.09835866466164589, 0.08136533269286156], time: 64.701
steps: 1599975, episodes: 64000, mean episode cvar: 0.8187600055932999, agent episode cvar: [0.6799800016880035, 0.3566505941748619, -0.11218197548389434, -0.10568861478567124], time: 64.701
Running avgs for agent 0: q_loss: 72.19766998291016, p_loss: 2.959397792816162, mean_rew: -0.2665484503000291, variance: 1.81307184779644, cvar: 2.7199199199676514, v: -1.7592581510543823, mean_q: -3.0261101722717285, std_q: 2.3111884593963623, lamda: 1.9876333475112915
Running avgs for agent 1: q_loss: 4.304433822631836, p_loss: -0.08846897631883621, mean_rew: -0.1410481608720932, variance: 0.42926382483541964, cvar: 1.4266023635864258, v: 1.0078420639038086, mean_q: -0.011022716760635376, std_q: 1.614563226699829, lamda: 1.7049640417099
Running avgs for agent 2: q_loss: 0.028768926858901978, p_loss: 0.9539844989776611, mean_rew: -0.3528172445826602, variance: 0.39343467354774475, cvar: -0.44872790575027466, v: -0.4591723680496216, mean_q: -0.9693242311477661, std_q: 1.535177230834961, lamda: 1.1615839004516602
Running avgs for agent 3: q_loss: 0.013443042524158955, p_loss: 1.0197248458862305, mean_rew: -0.34887501133064375, variance: 0.3254612982273102, cvar: -0.4227544665336609, v: -0.43484368920326233, mean_q: -1.0333969593048096, std_q: 1.5988860130310059, lamda: 1.147493600845337

steps: 1624975, episodes: 65000, mean episode reward: -31.880560904480152, agent episode reward: [-7.967477087933966, -4.58456482644389, -10.091794180465513, -9.236724809636781], time: 65.561
steps: 1624975, episodes: 65000, mean episode variance: 0.9818213658230379, agent episode variance: [0.7710055128633976, 0.022602231686003508, 0.10313981589674949, 0.08507380537688732], time: 65.561
steps: 1624975, episodes: 65000, mean episode cvar: 0.5829633865356445, agent episode cvar: [0.4661463609933853, 0.3385099619626999, -0.11481262648105621, -0.10688030993938447], time: 65.562
Running avgs for agent 0: q_loss: 24.15534019470215, p_loss: 3.8867805004119873, mean_rew: -0.27045608064223964, variance: 3.0840220514535903, cvar: 1.8645853996276855, v: -2.4666967391967773, mean_q: -3.94557523727417, std_q: 2.529449701309204, lamda: 2.0092294216156006
Running avgs for agent 1: q_loss: 0.07581906020641327, p_loss: -0.3172343075275421, mean_rew: -0.1401236104907294, variance: 0.09040892674401403, cvar: 1.3540397882461548, v: 1.262445330619812, mean_q: 0.21283744275569916, std_q: 1.7599927186965942, lamda: 1.7090508937835693
Running avgs for agent 2: q_loss: 0.032685574144124985, p_loss: 0.9173714518547058, mean_rew: -0.3543120217129379, variance: 0.41255927085876465, cvar: -0.45925047993659973, v: -0.4712469279766083, mean_q: -0.9285299181938171, std_q: 1.4743444919586182, lamda: 1.1646716594696045
Running avgs for agent 3: q_loss: 0.015385209582746029, p_loss: 1.017238974571228, mean_rew: -0.3504007708423246, variance: 0.3402952253818512, cvar: -0.4275212287902832, v: -0.44326671957969666, mean_q: -1.0292004346847534, std_q: 1.6011155843734741, lamda: 1.1466046571731567

steps: 1649975, episodes: 66000, mean episode reward: -31.817582717648648, agent episode reward: [-8.619038132157442, -3.103233084573805, -10.35444478578113, -9.740866715136274], time: 65.552
steps: 1649975, episodes: 66000, mean episode variance: 1.1937561826608143, agent episode variance: [0.9805139302536845, 0.019283992437180132, 0.10278821295499802, 0.09117004701495171], time: 65.552
steps: 1649975, episodes: 66000, mean episode cvar: 0.7311630919277669, agent episode cvar: [0.6178639130592346, 0.3385715625286102, -0.11548259273171425, -0.1097897909283638], time: 65.553
Running avgs for agent 0: q_loss: 97.4757080078125, p_loss: 4.051529884338379, mean_rew: -0.2714786677254072, variance: 3.922055721014738, cvar: 2.4714555740356445, v: -2.970327138900757, mean_q: -4.10693883895874, std_q: 2.668339252471924, lamda: 2.030571222305298
Running avgs for agent 1: q_loss: 0.0655682235956192, p_loss: -0.4186374545097351, mean_rew: -0.13695125427537647, variance: 0.07713596974872053, cvar: 1.3542863130569458, v: 1.3049372434616089, mean_q: 0.3529203534126282, std_q: 1.7113550901412964, lamda: 1.7091865539550781
Running avgs for agent 2: q_loss: 0.0339442640542984, p_loss: 0.8940699696540833, mean_rew: -0.35745428245743804, variance: 0.4111528694629669, cvar: -0.46193036437034607, v: -0.47741907835006714, mean_q: -0.905025839805603, std_q: 1.4111055135726929, lamda: 1.163648247718811
Running avgs for agent 3: q_loss: 0.0164930522441864, p_loss: 1.0063666105270386, mean_rew: -0.35274596132374547, variance: 0.3646802008152008, cvar: -0.43915918469429016, v: -0.4493482708930969, mean_q: -1.0166529417037964, std_q: 1.5931464433670044, lamda: 1.145912766456604

steps: 1674975, episodes: 67000, mean episode reward: -34.24433741938122, agent episode reward: [-12.045858562955988, -3.4455309367813576, -9.261472567099974, -9.4914753525439], time: 65.241
steps: 1674975, episodes: 67000, mean episode variance: 1.6192196703520603, agent episode variance: [1.4153058726787566, 0.01756385834189132, 0.09489369212090969, 0.09145624721050262], time: 65.241
steps: 1674975, episodes: 67000, mean episode cvar: 1.0772300811111928, agent episode cvar: [0.9637794387340546, 0.33674616014957426, -0.1134460473060608, -0.10984947046637535], time: 65.242
Running avgs for agent 0: q_loss: 253.66412353515625, p_loss: 4.827124118804932, mean_rew: -0.2753867861805554, variance: 5.6612234907150265, cvar: 3.8551175594329834, v: -3.468031883239746, mean_q: -4.933270454406738, std_q: 3.083383321762085, lamda: 2.0535802841186523
Running avgs for agent 1: q_loss: 0.06082017347216606, p_loss: -0.48961523175239563, mean_rew: -0.13576238235617857, variance: 0.07025543336756528, cvar: 1.3469847440719604, v: 1.2897498607635498, mean_q: 0.43789753317832947, std_q: 1.6349223852157593, lamda: 1.7092841863632202
Running avgs for agent 2: q_loss: 0.024745697155594826, p_loss: 0.9045291543006897, mean_rew: -0.3604667314864517, variance: 0.3795747756958008, cvar: -0.45378419756889343, v: -0.46625566482543945, mean_q: -0.9202934503555298, std_q: 1.3761674165725708, lamda: 1.1619365215301514
Running avgs for agent 3: q_loss: 0.01696663536131382, p_loss: 0.9981423616409302, mean_rew: -0.3555964170942166, variance: 0.36582496762275696, cvar: -0.4393979012966156, v: -0.4546518623828888, mean_q: -1.0082757472991943, std_q: 1.583400011062622, lamda: 1.147113561630249

steps: 1699975, episodes: 68000, mean episode reward: -43.32267503196602, agent episode reward: [-20.65904787698601, -4.46956933807232, -8.42917824290413, -9.764879574003567], time: 64.996
steps: 1699975, episodes: 68000, mean episode variance: 2.001461962669273, agent episode variance: [1.8089982248544694, 0.015346593811293132, 0.08462667466700077, 0.0924904693365097], time: 64.997
steps: 1699975, episodes: 68000, mean episode cvar: 0.9939415632784366, agent episode cvar: [0.8764530425071716, 0.3360533591508865, -0.10831455430388451, -0.110250284075737], time: 64.997
Running avgs for agent 0: q_loss: 209.37303161621094, p_loss: 6.936642646789551, mean_rew: -0.2855916789198692, variance: 7.2359928994178775, cvar: 3.505812168121338, v: -4.079919338226318, mean_q: -7.16635799407959, std_q: 3.9904186725616455, lamda: 2.0754950046539307
Running avgs for agent 1: q_loss: 0.056415196508169174, p_loss: -0.5358856320381165, mean_rew: -0.1349660598880178, variance: 0.06138637524517253, cvar: 1.3442134857177734, v: 1.2813770771026611, mean_q: 0.4892839789390564, std_q: 1.549279808998108, lamda: 1.7094500064849854
Running avgs for agent 2: q_loss: 0.015351459383964539, p_loss: 0.9469141960144043, mean_rew: -0.3614811707579827, variance: 0.33850666880607605, cvar: -0.4332582354545593, v: -0.43934980034828186, mean_q: -0.971823513507843, std_q: 1.4233037233352661, lamda: 1.1653281450271606
Running avgs for agent 3: q_loss: 0.015449907630681992, p_loss: 1.0008541345596313, mean_rew: -0.35875909975268017, variance: 0.36996185779571533, cvar: -0.44100111722946167, v: -0.4538407027721405, mean_q: -1.0115042924880981, std_q: 1.5723518133163452, lamda: 1.1461910009384155

steps: 1724975, episodes: 69000, mean episode reward: -43.6305346189105, agent episode reward: [-21.27325725034773, -4.612471896605417, -8.500173695513777, -9.244631776443589], time: 65.231
steps: 1724975, episodes: 69000, mean episode variance: 3.1900367584466003, agent episode variance: [2.991936275780201, 0.019802501240279526, 0.08535033546388149, 0.09294764596223831], time: 65.231
steps: 1724975, episodes: 69000, mean episode cvar: 0.554208199352026, agent episode cvar: [0.43320135116577146, 0.3394689407348633, -0.10750825163722039, -0.1109538409113884], time: 65.232
Running avgs for agent 0: q_loss: 47.67631149291992, p_loss: 7.844119071960449, mean_rew: -0.30203326546689263, variance: 11.967745103120803, cvar: 1.7328053712844849, v: -5.358550548553467, mean_q: -8.04677963256836, std_q: 4.087103843688965, lamda: 2.092874050140381
Running avgs for agent 1: q_loss: 0.05591428279876709, p_loss: -0.5408742427825928, mean_rew: -0.13491664819508942, variance: 0.0792100049611181, cvar: 1.3578757047653198, v: 1.2877346277236938, mean_q: 0.4773557782173157, std_q: 1.5011128187179565, lamda: 1.7096960544586182
Running avgs for agent 2: q_loss: 0.014383701607584953, p_loss: 0.9795775413513184, mean_rew: -0.3626989087783614, variance: 0.3414013385772705, cvar: -0.43003299832344055, v: -0.43781140446662903, mean_q: -1.0049420595169067, std_q: 1.4782263040542603, lamda: 1.169018268585205
Running avgs for agent 3: q_loss: 0.014389446936547756, p_loss: 0.9993873238563538, mean_rew: -0.35954473771828205, variance: 0.3717905879020691, cvar: -0.44381535053253174, v: -0.4501878619194031, mean_q: -1.0115519762039185, std_q: 1.5583395957946777, lamda: 1.1480615139007568

steps: 1749975, episodes: 70000, mean episode reward: -45.37078511303989, agent episode reward: [-23.12272520415649, -3.9051666210587372, -8.915152429599374, -9.427740858225283], time: 65.419
steps: 1749975, episodes: 70000, mean episode variance: 2.8190769281652757, agent episode variance: [2.6132548735886814, 0.023472308662254363, 0.09097956481575965, 0.09137018109858036], time: 65.419
steps: 1749975, episodes: 70000, mean episode cvar: 0.774919386446476, agent episode cvar: [0.6517033405303955, 0.3417014912366867, -0.10860448879003524, -0.10988095653057098], time: 65.42
Running avgs for agent 0: q_loss: 101.64533996582031, p_loss: 7.10335111618042, mean_rew: -0.31787587122112476, variance: 10.453019494354725, cvar: 2.606813430786133, v: -5.648705005645752, mean_q: -7.193912982940674, std_q: 3.7627856731414795, lamda: 2.112928628921509
Running avgs for agent 1: q_loss: 0.052294567227363586, p_loss: -0.5178951621055603, mean_rew: -0.13506089316599987, variance: 0.09388923464901745, cvar: 1.3668060302734375, v: 1.3180501461029053, mean_q: 0.4529576897621155, std_q: 1.4767332077026367, lamda: 1.7099565267562866
Running avgs for agent 2: q_loss: 0.01675267703831196, p_loss: 0.9924498200416565, mean_rew: -0.36419732974480445, variance: 0.363918274641037, cvar: -0.4344179630279541, v: -0.44773852825164795, mean_q: -1.0144811868667603, std_q: 1.518800973892212, lamda: 1.1703304052352905
Running avgs for agent 3: q_loss: 0.013006674125790596, p_loss: 1.0073204040527344, mean_rew: -0.36131203102315745, variance: 0.3654807209968567, cvar: -0.439523845911026, v: -0.44640931487083435, mean_q: -1.0212198495864868, std_q: 1.5633761882781982, lamda: 1.1514638662338257

steps: 1774975, episodes: 71000, mean episode reward: -38.98313099338894, agent episode reward: [-17.867617475470404, -1.986845094881944, -9.51140921904761, -9.617259203988986], time: 65.29
steps: 1774975, episodes: 71000, mean episode variance: 2.9718916636793873, agent episode variance: [2.7541352061629296, 0.025606539824511856, 0.10005769607424736, 0.09209222161769867], time: 65.291
steps: 1774975, episodes: 71000, mean episode cvar: 0.6321517482399941, agent episode cvar: [0.5075242018699646, 0.3460725364685059, -0.11203822812438011, -0.1094067619740963], time: 65.291
Running avgs for agent 0: q_loss: 64.81705474853516, p_loss: 7.535723686218262, mean_rew: -0.33011665059647577, variance: 11.016540824651718, cvar: 2.0300967693328857, v: -5.911200046539307, mean_q: -7.644834995269775, std_q: 3.759364604949951, lamda: 2.1319515705108643
Running avgs for agent 1: q_loss: 0.048366036266088486, p_loss: -0.45411449670791626, mean_rew: -0.1323004815797393, variance: 0.10242615929804742, cvar: 1.3842902183532715, v: 1.3338786363601685, mean_q: 0.38804545998573303, std_q: 1.482387900352478, lamda: 1.7100622653961182
Running avgs for agent 2: q_loss: 0.020725330337882042, p_loss: 0.9817419052124023, mean_rew: -0.36640362521219855, variance: 0.4002307653427124, cvar: -0.44815289974212646, v: -0.4601273536682129, mean_q: -0.999101459980011, std_q: 1.5298914909362793, lamda: 1.1700607538223267
Running avgs for agent 3: q_loss: 0.013060981407761574, p_loss: 1.0224319696426392, mean_rew: -0.36449740366141975, variance: 0.36836889386177063, cvar: -0.43762707710266113, v: -0.4495624303817749, mean_q: -1.036737322807312, std_q: 1.5792840719223022, lamda: 1.1520761251449585

steps: 1799975, episodes: 72000, mean episode reward: -33.80232542463954, agent episode reward: [-12.770053032390873, -1.6892938108135234, -9.973899147859578, -9.369079433575571], time: 65.119
steps: 1799975, episodes: 72000, mean episode variance: 3.225052681383677, agent episode variance: [2.9966143637895586, 0.025665791523642838, 0.10817286202311516, 0.09459966404736042], time: 65.12
steps: 1799975, episodes: 72000, mean episode cvar: 0.9788244507610798, agent episode cvar: [0.8570946998596192, 0.34818648219108583, -0.11587687185406685, -0.11057985943555831], time: 65.121
Running avgs for agent 0: q_loss: 267.4069519042969, p_loss: 7.83021354675293, mean_rew: -0.3397556016697889, variance: 11.986457455158234, cvar: 3.4283788204193115, v: -6.077968597412109, mean_q: -7.9742608070373535, std_q: 4.165031909942627, lamda: 2.1507115364074707
Running avgs for agent 1: q_loss: 0.050843093544244766, p_loss: -0.3671216666698456, mean_rew: -0.13179741448041207, variance: 0.10266316609457135, cvar: 1.3927459716796875, v: 1.3398115634918213, mean_q: 0.2873454689979553, std_q: 1.5390340089797974, lamda: 1.710246205329895
Running avgs for agent 2: q_loss: 0.02461257390677929, p_loss: 0.9560788869857788, mean_rew: -0.36936657128489364, variance: 0.43269142508506775, cvar: -0.4635075032711029, v: -0.47248077392578125, mean_q: -0.9694657921791077, std_q: 1.5081714391708374, lamda: 1.1729683876037598
Running avgs for agent 3: q_loss: 0.013714093714952469, p_loss: 1.0185695886611938, mean_rew: -0.3649796612703313, variance: 0.37839868664741516, cvar: -0.44231945276260376, v: -0.4523112177848816, mean_q: -1.031766414642334, std_q: 1.577569842338562, lamda: 1.1539899110794067

steps: 1824975, episodes: 73000, mean episode reward: -33.319802685007154, agent episode reward: [-11.553530281653897, -1.5691848212695982, -10.6890236613006, -9.508063920783064], time: 65.21
steps: 1824975, episodes: 73000, mean episode variance: 2.760716336941812, agent episode variance: [2.5343581994920967, 0.017209850183222443, 0.11291903027892113, 0.09622925698757172], time: 65.211
steps: 1824975, episodes: 73000, mean episode cvar: 0.586526339173317, agent episode cvar: [0.47623429298400877, 0.3398399306535721, -0.11871258866786957, -0.11083529579639435], time: 65.211
Running avgs for agent 0: q_loss: 34.62614059448242, p_loss: 7.59909200668335, mean_rew: -0.3430859322600261, variance: 10.137432797968387, cvar: 1.9049372673034668, v: -6.482620716094971, mean_q: -7.775794506072998, std_q: 4.077340602874756, lamda: 2.171088457107544
Running avgs for agent 1: q_loss: 0.050510652363300323, p_loss: -0.4314533770084381, mean_rew: -0.12644554972049724, variance: 0.06883940073288977, cvar: 1.3593597412109375, v: 1.326963186264038, mean_q: 0.3676392734050751, std_q: 1.6045444011688232, lamda: 1.7104800939559937
Running avgs for agent 2: q_loss: 0.02781866118311882, p_loss: 0.9292853474617004, mean_rew: -0.37233911068074393, variance: 0.4516761600971222, cvar: -0.4748503565788269, v: -0.48378536105155945, mean_q: -0.9404195547103882, std_q: 1.4539185762405396, lamda: 1.1750205755233765
Running avgs for agent 3: q_loss: 0.014294919557869434, p_loss: 1.01371431350708, mean_rew: -0.3668161285129969, variance: 0.3849170207977295, cvar: -0.4433411955833435, v: -0.45521464943885803, mean_q: -1.0273427963256836, std_q: 1.5699138641357422, lamda: 1.1554220914840698

steps: 1849975, episodes: 74000, mean episode reward: -35.32243543479087, agent episode reward: [-13.530781726069657, -1.8745897209953974, -10.765879807623788, -9.151184180102023], time: 65.021
steps: 1849975, episodes: 74000, mean episode variance: 1.930439936125651, agent episode variance: [1.7063965338766576, 0.013243878090754151, 0.11384304544329643, 0.09695647871494294], time: 65.022
steps: 1849975, episodes: 74000, mean episode cvar: 0.49442193803191187, agent episode cvar: [0.39065983748435973, 0.3345053049325943, -0.11928841522336006, -0.11145478916168212], time: 65.022
Running avgs for agent 0: q_loss: 27.336381912231445, p_loss: 6.940495491027832, mean_rew: -0.3491619533702101, variance: 6.82558613550663, cvar: 1.5626392364501953, v: -6.140460014343262, mean_q: -7.083001136779785, std_q: 3.9009454250335693, lamda: 2.192640781402588
Running avgs for agent 1: q_loss: 0.05006193369626999, p_loss: -0.5018941164016724, mean_rew: -0.12375582020308278, variance: 0.052975512363016605, cvar: 1.3380212783813477, v: 1.2836636304855347, mean_q: 0.4724758267402649, std_q: 1.6282072067260742, lamda: 1.7109402418136597
Running avgs for agent 2: q_loss: 0.02836025506258011, p_loss: 0.9126316905021667, mean_rew: -0.37583336087512614, variance: 0.4553721845149994, cvar: -0.4771536588668823, v: -0.48747217655181885, mean_q: -0.9233047962188721, std_q: 1.4117213487625122, lamda: 1.1780805587768555
Running avgs for agent 3: q_loss: 0.0140048423781991, p_loss: 1.0116188526153564, mean_rew: -0.3683181936865286, variance: 0.38782593607902527, cvar: -0.4458191394805908, v: -0.4572097659111023, mean_q: -1.0255112648010254, std_q: 1.5667046308517456, lamda: 1.1540777683258057

steps: 1874975, episodes: 75000, mean episode reward: -35.214126172312376, agent episode reward: [-13.83899687698959, -2.1101831557080724, -10.12673665787831, -9.138209481736403], time: 65.131
steps: 1874975, episodes: 75000, mean episode variance: 2.22595872189058, agent episode variance: [2.00742117151618, 0.01304720614803955, 0.10937913772463799, 0.09611120650172233], time: 65.131
steps: 1874975, episodes: 75000, mean episode cvar: 0.8464886858165264, agent episode cvar: [0.7437419819831849, 0.3311878832578659, -0.11721010467410088, -0.11123107475042343], time: 65.132
Running avgs for agent 0: q_loss: 154.13966369628906, p_loss: 7.415841579437256, mean_rew: -0.35507497180374814, variance: 8.02968468606472, cvar: 2.974968194961548, v: -6.275171756744385, mean_q: -7.533537864685059, std_q: 4.403539180755615, lamda: 2.2130627632141113
Running avgs for agent 1: q_loss: 0.05534312129020691, p_loss: -0.5725200772285461, mean_rew: -0.12250491758060277, variance: 0.0521888245921582, cvar: 1.324751615524292, v: 1.2614800930023193, mean_q: 0.5566357374191284, std_q: 1.5808125734329224, lamda: 1.7120405435562134
Running avgs for agent 2: q_loss: 0.023143593221902847, p_loss: 0.9242063164710999, mean_rew: -0.37963700898400565, variance: 0.43751654028892517, cvar: -0.46884042024612427, v: -0.48184898495674133, mean_q: -0.9374280571937561, std_q: 1.3867934942245483, lamda: 1.177343726158142
Running avgs for agent 3: q_loss: 0.013510429300367832, p_loss: 1.005100131034851, mean_rew: -0.3677576756529059, variance: 0.38444483280181885, cvar: -0.4449242949485779, v: -0.4541431963443756, mean_q: -1.0210399627685547, std_q: 1.5563931465148926, lamda: 1.1538279056549072

steps: 1899975, episodes: 76000, mean episode reward: -34.93508517928233, agent episode reward: [-12.90546032599947, -2.7122104217626966, -9.939976682650235, -9.377437748869934], time: 65.108
steps: 1899975, episodes: 76000, mean episode variance: 2.313393043684773, agent episode variance: [2.101227493524551, 0.00994437018316239, 0.10775860020518303, 0.09446257977187633], time: 65.109
steps: 1899975, episodes: 76000, mean episode cvar: 0.5967648875117302, agent episode cvar: [0.4946118941307068, 0.32902582609653475, -0.11602108269929885, -0.11085175001621246], time: 65.109
Running avgs for agent 0: q_loss: 62.82858657836914, p_loss: 6.374696254730225, mean_rew: -0.3669043558490158, variance: 8.404909974098205, cvar: 1.9784475564956665, v: -6.087173938751221, mean_q: -6.467710971832275, std_q: 4.036880970001221, lamda: 2.233445882797241
Running avgs for agent 1: q_loss: 0.056802209466695786, p_loss: -0.6317525506019592, mean_rew: -0.12117337728037804, variance: 0.03977748073264956, cvar: 1.316103219985962, v: 1.230542778968811, mean_q: 0.6209077835083008, std_q: 1.5015534162521362, lamda: 1.713483452796936
Running avgs for agent 2: q_loss: 0.019796913489699364, p_loss: 0.9401262402534485, mean_rew: -0.3802606723136869, variance: 0.4310344159603119, cvar: -0.46408432722091675, v: -0.47590935230255127, mean_q: -0.9559917449951172, std_q: 1.3986890316009521, lamda: 1.1765164136886597
Running avgs for agent 3: q_loss: 0.012534585781395435, p_loss: 1.0080522298812866, mean_rew: -0.3676163097427024, variance: 0.37785032391548157, cvar: -0.44340696930885315, v: -0.45103710889816284, mean_q: -1.0259084701538086, std_q: 1.5590053796768188, lamda: 1.1548489332199097

steps: 1924975, episodes: 77000, mean episode reward: -35.1217810218104, agent episode reward: [-13.535208833145349, -3.169134029754753, -9.760156004914254, -8.65728215399604], time: 65.234
steps: 1924975, episodes: 77000, mean episode variance: 1.449767376575619, agent episode variance: [1.2445431949049235, 0.0073319739587605, 0.11127421721816062, 0.08661799049377442], time: 65.234
steps: 1924975, episodes: 77000, mean episode cvar: 0.6348691714704037, agent episode cvar: [0.5322499775886536, 0.3271891913414001, -0.11694108551740647, -0.10762891194224358], time: 65.235
Running avgs for agent 0: q_loss: 76.13345336914062, p_loss: 6.2805633544921875, mean_rew: -0.3736066737229988, variance: 4.978172779619694, cvar: 2.128999948501587, v: -5.522252559661865, mean_q: -6.377849578857422, std_q: 4.3780927658081055, lamda: 2.255235433578491
Running avgs for agent 1: q_loss: 0.05848679691553116, p_loss: -0.6923725008964539, mean_rew: -0.12000064802074377, variance: 0.029327895835042, cvar: 1.308756709098816, v: 1.219836950302124, mean_q: 0.6833462119102478, std_q: 1.418752908706665, lamda: 1.7158561944961548
Running avgs for agent 2: q_loss: 0.02091972716152668, p_loss: 0.9522596597671509, mean_rew: -0.3819821416584649, variance: 0.44509685039520264, cvar: -0.46776434779167175, v: -0.4806787669658661, mean_q: -0.9665278196334839, std_q: 1.4291049242019653, lamda: 1.1742265224456787
Running avgs for agent 3: q_loss: 0.01024140976369381, p_loss: 1.0219764709472656, mean_rew: -0.36781954695076124, variance: 0.34647196531295776, cvar: -0.4305156171321869, v: -0.4359787702560425, mean_q: -1.045701503753662, std_q: 1.5674127340316772, lamda: 1.1581130027770996

steps: 1949975, episodes: 78000, mean episode reward: -38.101482669104485, agent episode reward: [-14.596230518393677, -3.0367600682010707, -10.491738478581887, -9.976753603927857], time: 65.306
steps: 1949975, episodes: 78000, mean episode variance: 1.6458535322085954, agent episode variance: [1.440604368686676, 0.007357714204583317, 0.11565936425328255, 0.08223208506405354], time: 65.308
steps: 1949975, episodes: 78000, mean episode cvar: 0.5875355904102325, agent episode cvar: [0.48548578763008116, 0.3262290954589844, -0.11837148454785347, -0.10580780813097954], time: 65.309
Running avgs for agent 0: q_loss: 59.023651123046875, p_loss: 6.200295925140381, mean_rew: -0.38687590779779035, variance: 5.762417474746704, cvar: 1.9419431686401367, v: -5.30871057510376, mean_q: -6.31741189956665, std_q: 4.370883941650391, lamda: 2.278221368789673
Running avgs for agent 1: q_loss: 0.061303939670324326, p_loss: -0.7263603806495667, mean_rew: -0.12272882140883178, variance: 0.029430856818333267, cvar: 1.3049163818359375, v: 1.2024140357971191, mean_q: 0.7183088660240173, std_q: 1.3431611061096191, lamda: 1.7191404104232788
Running avgs for agent 2: q_loss: 0.023718779906630516, p_loss: 0.9551417827606201, mean_rew: -0.38453274466640197, variance: 0.46263745427131653, cvar: -0.47348591685295105, v: -0.4931257367134094, mean_q: -0.9663200974464417, std_q: 1.4356462955474854, lamda: 1.1732043027877808
Running avgs for agent 3: q_loss: 0.009643133729696274, p_loss: 1.0278162956237793, mean_rew: -0.36406593830675116, variance: 0.3289283514022827, cvar: -0.42323121428489685, v: -0.42900219559669495, mean_q: -1.056103229522705, std_q: 1.5814664363861084, lamda: 1.1613233089447021

steps: 1974975, episodes: 79000, mean episode reward: -41.53794733074521, agent episode reward: [-17.519421463159343, -3.235165169494741, -10.672792099522333, -10.110568598568793], time: 65.198
steps: 1974975, episodes: 79000, mean episode variance: 1.614355838256888, agent episode variance: [1.4054656545221806, 0.0044751154659315945, 0.12081258863210678, 0.08360247963666916], time: 65.199
steps: 1974975, episodes: 79000, mean episode cvar: 0.7414178703427314, agent episode cvar: [0.6416441135406494, 0.32685442912578583, -0.12093313372135163, -0.10614753860235214], time: 65.199
Running avgs for agent 0: q_loss: 108.68026733398438, p_loss: 6.1564435958862305, mean_rew: -0.3999050556425178, variance: 5.621862618088723, cvar: 2.5665764808654785, v: -5.13142204284668, mean_q: -6.332062721252441, std_q: 4.529858112335205, lamda: 2.3012707233428955
Running avgs for agent 1: q_loss: 0.057376883924007416, p_loss: -0.755010724067688, mean_rew: -0.12194180009216725, variance: 0.017900461863726378, cvar: 1.3074177503585815, v: 1.1983672380447388, mean_q: 0.7472741603851318, std_q: 1.2884231805801392, lamda: 1.7227098941802979
Running avgs for agent 2: q_loss: 0.026078365743160248, p_loss: 0.942396342754364, mean_rew: -0.3855711232800879, variance: 0.48325037956237793, cvar: -0.48373255133628845, v: -0.4973912239074707, mean_q: -0.9514327645301819, std_q: 1.428251028060913, lamda: 1.1701802015304565
Running avgs for agent 3: q_loss: 0.010063646361231804, p_loss: 1.0234023332595825, mean_rew: -0.36224593495003765, variance: 0.3344099223613739, cvar: -0.424590140581131, v: -0.4298204481601715, mean_q: -1.051785945892334, std_q: 1.5973963737487793, lamda: 1.164791226387024

steps: 1999975, episodes: 80000, mean episode reward: -38.277832072382985, agent episode reward: [-14.643612999848205, -3.0987531545744487, -11.42206983026885, -9.113396087691479], time: 65.251
steps: 1999975, episodes: 80000, mean episode variance: 1.4175123408408836, agent episode variance: [1.205777787990868, 0.005742482249625027, 0.12202831754088402, 0.08396375305950642], time: 65.252
steps: 1999975, episodes: 80000, mean episode cvar: 0.48302205055952074, agent episode cvar: [0.38542903876304624, 0.32593089997768404, -0.12228525137901305, -0.10605263680219651], time: 65.253
Running avgs for agent 0: q_loss: 22.494367599487305, p_loss: 5.565192222595215, mean_rew: -0.41415143270091825, variance: 4.823111151963472, cvar: 1.5417160987854004, v: -4.112215042114258, mean_q: -5.82330846786499, std_q: 4.214917182922363, lamda: 2.3237709999084473
Running avgs for agent 1: q_loss: 0.06813149154186249, p_loss: -0.7643296718597412, mean_rew: -0.1204399966055869, variance: 0.02296992899850011, cvar: 1.3037235736846924, v: 1.1693651676177979, mean_q: 0.7569889426231384, std_q: 1.2302109003067017, lamda: 1.730186939239502/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

Running avgs for agent 2: q_loss: 0.0279975738376379, p_loss: 0.9279012680053711, mean_rew: -0.384770043070361, variance: 0.48811328411102295, cvar: -0.4891410171985626, v: -0.5024555325508118, mean_q: -0.9355241656303406, std_q: 1.4042073488235474, lamda: 1.1699817180633545
Running avgs for agent 3: q_loss: 0.011019853875041008, p_loss: 1.01506507396698, mean_rew: -0.3601329557043349, variance: 0.33585503697395325, cvar: -0.4242105484008789, v: -0.4327497184276581, mean_q: -1.0379267930984497, std_q: 1.5835447311401367, lamda: 1.1665935516357422

...Finished total of 80001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -36.550048117944606, agent episode reward: [-12.696028985151676, -2.8260998612591592, -11.251400751017227, -9.77651852051654], time: 43.525
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 43.525
steps: 24975, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0, 0.0], time: 43.526
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -36.804559647823595, agent episode reward: [-13.36117751130836, -2.8116306056162137, -11.159211480795166, -9.47254005010386], time: 57.558
steps: 49975, episodes: 2000, mean episode variance: 1.5396187272500246, agent episode variance: [1.3312880625724792, 0.009591289414092898, 0.13795337122678758, 0.06078600403666496], time: 57.558
steps: 49975, episodes: 2000, mean episode cvar: 0.9589363619685173, agent episode cvar: [0.8534335593581199, 0.324564942240715, -0.11637877786159516, -0.10268336176872253], time: 57.559
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5142440730774646, variance: 5.456098556518555, cvar: 3.497678756713867, v: -1.9513795375823975, mean_q: -3.3787636756896973, std_q: 3.3636584281921387, lamda: 2.3347816467285156
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1125898685630924, variance: 0.03930856317251188, cvar: 1.3301842212677002, v: 1.1798514127731323, mean_q: 0.8233296275138855, std_q: 1.130889654159546, lamda: 1.7331547737121582
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.44964132641064036, variance: 0.565382719039917, cvar: -0.47696220874786377, v: -0.48375388979911804, mean_q: -0.8931425213813782, std_q: 1.356666088104248, lamda: 1.1715545654296875
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3866905169512778, variance: 0.2491229623556137, cvar: -0.4208334684371948, v: -0.4250502586364746, mean_q: -1.0469262599945068, std_q: 1.6267482042312622, lamda: 1.1660326719284058

...Finished total of 2001 episodes with the fixed policy.
