WARNING: Logging before flag parsing goes to stderr.
W0823 08:17:27.082026 4644500928 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0823 08:17:27.082247 4644500928 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-23 08:17:27.082606: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0823 08:17:27.086696 4644500928 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0823 08:17:27.088867 4644500928 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:227: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0823 08:17:27.088984 4644500928 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0823 08:17:27.089061 4644500928 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0823 08:17:27.382451 4644500928 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0823 08:17:27.509050 4644500928 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0823 08:17:27.517186 4644500928 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0823 08:17:27.883279 4644500928 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:259: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation False
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -68.52604193805563, agent episode reward: [-7.492039154290607, -6.522296160893368, -25.98304271734837, -28.52866390552328], time: 29.858
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 29.858
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -50.78503905108417, agent episode reward: [-5.711354056801703, -9.655652999779251, -17.034684372160367, -18.383347622342843], time: 55.532
steps: 49975, episodes: 2000, mean episode variance: 0.04489880771259777, agent episode variance: [0.011738859563367442, 0.012481654103379697, 0.007309583286056295, 0.01336871075979434], time: 55.532
Running avgs for agent 0: q_loss: 0.065567746758461, p_loss: 0.2715549170970917, mean_rew: -0.24471203703073066, variance: 0.048110080177735416, mean_q: -0.30838483572006226, std_q: 0.8842200040817261
Running avgs for agent 1: q_loss: 0.06233443692326546, p_loss: 0.27627795934677124, mean_rew: -0.2951017724355998, variance: 0.051154320095818426, mean_q: -0.3089980185031891, std_q: 0.9089483022689819
Running avgs for agent 2: q_loss: 0.02347469888627529, p_loss: 1.4108378887176514, mean_rew: -1.019236056496795, variance: 0.029957308549411046, mean_q: -1.4625344276428223, std_q: 0.9196375608444214
Running avgs for agent 3: q_loss: 0.030393896624445915, p_loss: 1.684712529182434, mean_rew: -1.120417263309843, variance: 0.05478979819587844, mean_q: -1.7433838844299316, std_q: 1.091491460800171

steps: 74975, episodes: 3000, mean episode reward: -26.42056680800405, agent episode reward: [-5.812584463490009, -6.2746453176182655, -6.94445801239858, -7.388879014497192], time: 54.597
steps: 74975, episodes: 3000, mean episode variance: 0.08336850989761296, agent episode variance: [0.009493904377915896, 0.015960481094196437, 0.0207916052415967, 0.03712251918390393], time: 54.598
Running avgs for agent 0: q_loss: 0.026006869971752167, p_loss: 0.5865480899810791, mean_rew: -0.26170226057495216, variance: 0.03797561751166358, mean_q: -0.6909616589546204, std_q: 1.6323169469833374
Running avgs for agent 1: q_loss: 0.02581983245909214, p_loss: 0.7935078144073486, mean_rew: -0.3145079007904456, variance: 0.06384192437678575, mean_q: -0.9100869297981262, std_q: 1.666922688484192
Running avgs for agent 2: q_loss: 0.005952366627752781, p_loss: 2.0768141746520996, mean_rew: -0.7511135264112268, variance: 0.0831664209663868, mean_q: -2.2080509662628174, std_q: 2.0160694122314453
Running avgs for agent 3: q_loss: 0.010838805697858334, p_loss: 2.338550090789795, mean_rew: -0.8194430082150835, variance: 0.14849007673561573, mean_q: -2.487475633621216, std_q: 2.319305658340454

steps: 99975, episodes: 4000, mean episode reward: -24.169912584484255, agent episode reward: [-4.8542305786908, -5.117303182587173, -7.053674716707973, -7.144704106498312], time: 54.182
steps: 99975, episodes: 4000, mean episode variance: 0.11028055195696652, agent episode variance: [0.01700943786185235, 0.026964232452213764, 0.030627151358872652, 0.03567973028402775], time: 54.182
Running avgs for agent 0: q_loss: 0.03669176995754242, p_loss: 0.8791629076004028, mean_rew: -0.2465240863261924, variance: 0.0680377514474094, mean_q: -1.0412753820419312, std_q: 2.2193479537963867
Running avgs for agent 1: q_loss: 0.04175439476966858, p_loss: 1.128553032875061, mean_rew: -0.28753745250677615, variance: 0.10785692980885506, mean_q: -1.3080670833587646, std_q: 2.2461864948272705
Running avgs for agent 2: q_loss: 0.008458060212433338, p_loss: 2.3741061687469482, mean_rew: -0.6163315649532183, variance: 0.12250860543549061, mean_q: -2.5483591556549072, std_q: 2.8826069831848145
Running avgs for agent 3: q_loss: 0.008810668252408504, p_loss: 2.6670444011688232, mean_rew: -0.6662958879640456, variance: 0.142718921136111, mean_q: -2.867189407348633, std_q: 3.3415513038635254

steps: 124975, episodes: 5000, mean episode reward: -23.806322029482875, agent episode reward: [-5.021678626050738, -4.515146238836347, -6.963804635444209, -7.305692529151577], time: 54.527
steps: 124975, episodes: 5000, mean episode variance: 0.14643914788076653, agent episode variance: [0.0315523085668683, 0.03242108353227377, 0.03169236442865804, 0.05077339135296643], time: 54.527
Running avgs for agent 0: q_loss: 0.053844284266233444, p_loss: 1.071451187133789, mean_rew: -0.23512217918337366, variance: 0.1262092342674732, mean_q: -1.2510570287704468, std_q: 2.5971922874450684
Running avgs for agent 1: q_loss: 0.0488261803984642, p_loss: 1.3071777820587158, mean_rew: -0.26425069753872765, variance: 0.12968433412909508, mean_q: -1.5040439367294312, std_q: 2.6241934299468994
Running avgs for agent 2: q_loss: 0.008799474686384201, p_loss: 2.4315454959869385, mean_rew: -0.5388797440530697, variance: 0.12676945771463216, mean_q: -2.6164658069610596, std_q: 3.362778663635254
Running avgs for agent 3: q_loss: 0.010757673531770706, p_loss: 2.7234134674072266, mean_rew: -0.5785992808849212, variance: 0.2030935654118657, mean_q: -2.9377729892730713, std_q: 3.9302446842193604

steps: 149975, episodes: 6000, mean episode reward: -23.52244519948416, agent episode reward: [-4.84530779300396, -4.764039628121751, -6.9812525232432305, -6.931845255115215], time: 54.852
steps: 149975, episodes: 6000, mean episode variance: 0.15935044843656943, agent episode variance: [0.028645301267504693, 0.03416307465732098, 0.05055652342410758, 0.04598554908763617], time: 54.853
Running avgs for agent 0: q_loss: 0.05747237429022789, p_loss: 1.1825460195541382, mean_rew: -0.22557458914456754, variance: 0.11458120507001877, mean_q: -1.356827974319458, std_q: 2.80147123336792
Running avgs for agent 1: q_loss: 0.05601530522108078, p_loss: 1.4064428806304932, mean_rew: -0.2508146040070308, variance: 0.1366522986292839, mean_q: -1.6008219718933105, std_q: 2.7924585342407227
Running avgs for agent 2: q_loss: 0.015570702031254768, p_loss: 2.407388925552368, mean_rew: -0.4918883041356134, variance: 0.20222609369643033, mean_q: -2.586961030960083, std_q: 3.588916540145874
Running avgs for agent 3: q_loss: 0.011465389281511307, p_loss: 2.687629222869873, mean_rew: -0.5243792102724325, variance: 0.18394219635054468, mean_q: -2.896930456161499, std_q: 4.170775890350342

steps: 174975, episodes: 7000, mean episode reward: -23.250689499911093, agent episode reward: [-4.458492402151322, -4.589553655628659, -7.100371403331787, -7.102272038799325], time: 54.179
steps: 174975, episodes: 7000, mean episode variance: 0.1891736926594749, agent episode variance: [0.027383775744587183, 0.03719663825444877, 0.03749596193805337, 0.08709731672238559], time: 54.179
Running avgs for agent 0: q_loss: 0.059496887028217316, p_loss: 1.2693125009536743, mean_rew: -0.22245493257703994, variance: 0.10953510297834873, mean_q: -1.4322330951690674, std_q: 2.8305792808532715
Running avgs for agent 1: q_loss: 0.06141114979982376, p_loss: 1.4511399269104004, mean_rew: -0.24117140664679657, variance: 0.1487865530177951, mean_q: -1.6288083791732788, std_q: 2.835120439529419
Running avgs for agent 2: q_loss: 0.011339177377521992, p_loss: 2.3382935523986816, mean_rew: -0.4559211541760106, variance: 0.1499838477522135, mean_q: -2.502164125442505, std_q: 3.600491762161255
Running avgs for agent 3: q_loss: 0.022987157106399536, p_loss: 2.6271870136260986, mean_rew: -0.48927258332962775, variance: 0.34838926688954236, mean_q: -2.8234403133392334, std_q: 4.267256736755371

steps: 199975, episodes: 8000, mean episode reward: -23.61808832332653, agent episode reward: [-4.5107965454870715, -4.49766343190782, -7.220270919917848, -7.3893574260137855], time: 54.038
steps: 199975, episodes: 8000, mean episode variance: 0.1385641055563465, agent episode variance: [0.03381917115300894, 0.03393762399163097, 0.03623173918388784, 0.03457557122781873], time: 54.038
Running avgs for agent 0: q_loss: 0.06658567488193512, p_loss: 1.3095781803131104, mean_rew: -0.21668266729681016, variance: 0.13527668461203576, mean_q: -1.4571442604064941, std_q: 2.7934975624084473
Running avgs for agent 1: q_loss: 0.05616267770528793, p_loss: 1.4497637748718262, mean_rew: -0.23346664417769136, variance: 0.1357504959665239, mean_q: -1.613175868988037, std_q: 2.8006980419158936
Running avgs for agent 2: q_loss: 0.01491441112011671, p_loss: 2.3146138191223145, mean_rew: -0.43608079151255946, variance: 0.14492695673555137, mean_q: -2.4639627933502197, std_q: 3.612290143966675
Running avgs for agent 3: q_loss: 0.014356533996760845, p_loss: 2.552988052368164, mean_rew: -0.46072684295090377, variance: 0.13830228491127491, mean_q: -2.7333567142486572, std_q: 4.22493314743042

steps: 224975, episodes: 9000, mean episode reward: -22.771422573527495, agent episode reward: [-4.321259029336448, -4.353855852602032, -7.024570755614441, -7.071736935974574], time: 54.297
steps: 224975, episodes: 9000, mean episode variance: 0.15697607266157865, agent episode variance: [0.029646389216184618, 0.0387592000477016, 0.04760387010220438, 0.04096661329548806], time: 54.297
Running avgs for agent 0: q_loss: 0.05481095239520073, p_loss: 1.331005334854126, mean_rew: -0.2116277369352613, variance: 0.11858555686473847, mean_q: -1.4627646207809448, std_q: 2.7023730278015137
Running avgs for agent 1: q_loss: 0.060155730694532394, p_loss: 1.4444183111190796, mean_rew: -0.2272927774868841, variance: 0.1550368001908064, mean_q: -1.5920747518539429, std_q: 2.7101151943206787
Running avgs for agent 2: q_loss: 0.016835587099194527, p_loss: 2.2748396396636963, mean_rew: -0.41732132746634915, variance: 0.19041548040881753, mean_q: -2.4112367630004883, std_q: 3.5624310970306396
Running avgs for agent 3: q_loss: 0.012896234169602394, p_loss: 2.5174736976623535, mean_rew: -0.4420564350755029, variance: 0.16386645318195223, mean_q: -2.680905818939209, std_q: 4.188811302185059

steps: 249975, episodes: 10000, mean episode reward: -22.913978498150104, agent episode reward: [-4.412375663771085, -4.302787024681031, -7.023980446403498, -7.174835363294489], time: 54.096
steps: 249975, episodes: 10000, mean episode variance: 0.14444347175303845, agent episode variance: [0.02838396865874529, 0.03623277621529997, 0.03320926205534488, 0.046617464823648334], time: 54.097
Running avgs for agent 0: q_loss: 0.06088555231690407, p_loss: 1.34636652469635, mean_rew: -0.2077895762556018, variance: 0.11353587463498116, mean_q: -1.466286063194275, std_q: 2.6534183025360107
Running avgs for agent 1: q_loss: 0.069912850856781, p_loss: 1.4204730987548828, mean_rew: -0.22108017133867686, variance: 0.14493110486119987, mean_q: -1.5573112964630127, std_q: 2.6461894512176514
Running avgs for agent 2: q_loss: 0.012945299968123436, p_loss: 2.260514259338379, mean_rew: -0.4046047165884065, variance: 0.1328370482213795, mean_q: -2.3839340209960938, std_q: 3.533442497253418
Running avgs for agent 3: q_loss: 0.018030552193522453, p_loss: 2.4616568088531494, mean_rew: -0.4235337180580457, variance: 0.18646985929459334, mean_q: -2.610175848007202, std_q: 4.077064037322998

steps: 274975, episodes: 11000, mean episode reward: -22.879546288435535, agent episode reward: [-4.436062831235126, -4.136516622094566, -7.1697542321012335, -7.137212603004607], time: 54.011
steps: 274975, episodes: 11000, mean episode variance: 0.13959753791615367, agent episode variance: [0.029936953684315086, 0.03271077971160412, 0.03107989764865488, 0.04586990687157959], time: 54.011
Running avgs for agent 0: q_loss: 0.06327653676271439, p_loss: 1.3605759143829346, mean_rew: -0.20543904564195767, variance: 0.11974781473726034, mean_q: -1.4708878993988037, std_q: 2.597472667694092
Running avgs for agent 1: q_loss: 0.06277492642402649, p_loss: 1.3914694786071777, mean_rew: -0.21610756867567443, variance: 0.13084311884641647, mean_q: -1.5199931859970093, std_q: 2.6096205711364746
Running avgs for agent 2: q_loss: 0.01200153212994337, p_loss: 2.2369463443756104, mean_rew: -0.3916020478832301, variance: 0.12431959059461951, mean_q: -2.3491475582122803, std_q: 3.3862674236297607
Running avgs for agent 3: q_loss: 0.016012324020266533, p_loss: 2.4270401000976562, mean_rew: -0.4120537048811396, variance: 0.18347962748631835, mean_q: -2.5627083778381348, std_q: 3.959977149963379

steps: 299975, episodes: 12000, mean episode reward: -23.04995913152247, agent episode reward: [-4.41904843396146, -4.159791505578607, -7.241155527535539, -7.229963664446866], time: 54.327
steps: 299975, episodes: 12000, mean episode variance: 0.13361844662204383, agent episode variance: [0.02403830180875957, 0.03728925075940788, 0.034425722397863866, 0.03786517165601253], time: 54.328
Running avgs for agent 0: q_loss: 0.055178094655275345, p_loss: 1.3746106624603271, mean_rew: -0.20217585669017665, variance: 0.09615320723503828, mean_q: -1.4771199226379395, std_q: 2.531522750854492
Running avgs for agent 1: q_loss: 0.06705346703529358, p_loss: 1.3706934452056885, mean_rew: -0.213036391862867, variance: 0.14915700303763152, mean_q: -1.4901505708694458, std_q: 2.5805630683898926
Running avgs for agent 2: q_loss: 0.013507509604096413, p_loss: 2.238393545150757, mean_rew: -0.38256252896145154, variance: 0.13770288959145546, mean_q: -2.341252326965332, std_q: 3.299239158630371
Running avgs for agent 3: q_loss: 0.01745355688035488, p_loss: 2.4057323932647705, mean_rew: -0.4007668055891619, variance: 0.15146068662405013, mean_q: -2.531522750854492, std_q: 3.870283365249634

steps: 324975, episodes: 13000, mean episode reward: -22.64608680596272, agent episode reward: [-4.546213823294268, -4.309393643299077, -6.774359115335495, -7.016120224033883], time: 54.332
steps: 324975, episodes: 13000, mean episode variance: 0.11792075850768015, agent episode variance: [0.033185573576018215, 0.027632141264155505, 0.029912410884164273, 0.027190632783342154], time: 54.332
Running avgs for agent 0: q_loss: 0.07308194786310196, p_loss: 1.3983263969421387, mean_rew: -0.20108263349471248, variance: 0.13274229430407286, mean_q: -1.491204857826233, std_q: 2.459852933883667
Running avgs for agent 1: q_loss: 0.06279318034648895, p_loss: 1.3338786363601685, mean_rew: -0.2085003644598509, variance: 0.11052856505662202, mean_q: -1.4451195001602173, std_q: 2.51058292388916
Running avgs for agent 2: q_loss: 0.014951291494071484, p_loss: 2.2443735599517822, mean_rew: -0.37658365148357037, variance: 0.1196496435366571, mean_q: -2.339277744293213, std_q: 3.2123076915740967
Running avgs for agent 3: q_loss: 0.009931879118084908, p_loss: 2.4018871784210205, mean_rew: -0.39324550973251443, variance: 0.10876253113336862, mean_q: -2.520115613937378, std_q: 3.8070478439331055

steps: 349975, episodes: 14000, mean episode reward: -23.247978863554938, agent episode reward: [-4.312164002896028, -4.186411014074594, -7.475375370759915, -7.274028475824398], time: 54.273
steps: 349975, episodes: 14000, mean episode variance: 0.12028723317757249, agent episode variance: [0.026509717056527732, 0.02747839774098247, 0.028734411048702897, 0.03756470733135939], time: 54.274
Running avgs for agent 0: q_loss: 0.05992811918258667, p_loss: 1.412062168121338, mean_rew: -0.19902922333005488, variance: 0.10603886822611093, mean_q: -1.4999020099639893, std_q: 2.4361631870269775
Running avgs for agent 1: q_loss: 0.05864792689681053, p_loss: 1.3003207445144653, mean_rew: -0.20442338247946337, variance: 0.10991359096392989, mean_q: -1.4067758321762085, std_q: 2.4601757526397705
Running avgs for agent 2: q_loss: 0.017985694110393524, p_loss: 2.236563205718994, mean_rew: -0.36872678870233466, variance: 0.11493764419481159, mean_q: -2.322949171066284, std_q: 3.1665525436401367
Running avgs for agent 3: q_loss: 0.01874573901295662, p_loss: 2.3872029781341553, mean_rew: -0.3846118660945663, variance: 0.15025882932543755, mean_q: -2.4953646659851074, std_q: 3.7148120403289795

steps: 374975, episodes: 15000, mean episode reward: -22.833169464680527, agent episode reward: [-4.270956323835949, -4.151128568007349, -7.314227241544646, -7.096857331292587], time: 54.205
steps: 374975, episodes: 15000, mean episode variance: 0.09981008139625192, agent episode variance: [0.02272827992681414, 0.027574505161494015, 0.022735779136419297, 0.026771517171524466], time: 54.205
Running avgs for agent 0: q_loss: 0.04924096539616585, p_loss: 1.4240003824234009, mean_rew: -0.1977131622054824, variance: 0.09091311970725656, mean_q: -1.5069719552993774, std_q: 2.3972935676574707
Running avgs for agent 1: q_loss: 0.05994390696287155, p_loss: 1.2803359031677246, mean_rew: -0.2029199507663251, variance: 0.11029802064597606, mean_q: -1.3795474767684937, std_q: 2.4162709712982178
Running avgs for agent 2: q_loss: 0.015104483813047409, p_loss: 2.2386255264282227, mean_rew: -0.36309429324928133, variance: 0.09094311654567719, mean_q: -2.318929672241211, std_q: 3.0807108879089355
Running avgs for agent 3: q_loss: 0.008191900327801704, p_loss: 2.380106210708618, mean_rew: -0.3780980873324264, variance: 0.10708606868609787, mean_q: -2.483518123626709, std_q: 3.641693353652954

steps: 399975, episodes: 16000, mean episode reward: -22.71061749020766, agent episode reward: [-4.460910550287639, -4.442309530391605, -6.8387966775569025, -6.968600731971516], time: 54.223
steps: 399975, episodes: 16000, mean episode variance: 0.1128087796671316, agent episode variance: [0.02756266219727695, 0.027920385744422675, 0.024615389654412866, 0.03271034207101911], time: 54.224
Running avgs for agent 0: q_loss: 0.05346947908401489, p_loss: 1.4262216091156006, mean_rew: -0.1955604877293624, variance: 0.1102506487891078, mean_q: -1.5031479597091675, std_q: 2.343087911605835
Running avgs for agent 1: q_loss: 0.05765286460518837, p_loss: 1.241820216178894, mean_rew: -0.199280080892179, variance: 0.1116815429776907, mean_q: -1.3348039388656616, std_q: 2.372194766998291
Running avgs for agent 2: q_loss: 0.011647474952042103, p_loss: 2.238337993621826, mean_rew: -0.3571435805921919, variance: 0.09846155861765146, mean_q: -2.3136954307556152, std_q: 3.0204238891601562
Running avgs for agent 3: q_loss: 0.011049280874431133, p_loss: 2.3536345958709717, mean_rew: -0.369906003760483, variance: 0.13084136828407644, mean_q: -2.448782205581665, std_q: 3.5209157466888428

steps: 424975, episodes: 17000, mean episode reward: -22.617345152817375, agent episode reward: [-4.236080659793968, -4.019245177870898, -7.356754204626255, -7.005265110526254], time: 54.25
steps: 424975, episodes: 17000, mean episode variance: 0.10471768429782241, agent episode variance: [0.023840499360579998, 0.024330656192265453, 0.017474618134554476, 0.03907191061042249], time: 54.251
Running avgs for agent 0: q_loss: 0.04848737642168999, p_loss: 1.434175729751587, mean_rew: -0.19429581702752552, variance: 0.09536199744231999, mean_q: -1.5080907344818115, std_q: 2.3215787410736084
Running avgs for agent 1: q_loss: 0.05614049360156059, p_loss: 1.23598313331604, mean_rew: -0.19964966371939488, variance: 0.09732262476906181, mean_q: -1.3263156414031982, std_q: 2.3856029510498047
Running avgs for agent 2: q_loss: 0.010011597536504269, p_loss: 2.2564995288848877, mean_rew: -0.3538610182216007, variance: 0.0698984725382179, mean_q: -2.330235004425049, std_q: 3.0442421436309814
Running avgs for agent 3: q_loss: 0.01999431848526001, p_loss: 2.355228900909424, mean_rew: -0.3667372619840537, variance: 0.15628764244168997, mean_q: -2.444335699081421, std_q: 3.476813793182373

steps: 449975, episodes: 18000, mean episode reward: -22.869363445425726, agent episode reward: [-4.25864194591337, -4.008234717431791, -7.339739238617256, -7.262747543463312], time: 55.588
steps: 449975, episodes: 18000, mean episode variance: 0.11113819486927241, agent episode variance: [0.029693429194390775, 0.023640973741188644, 0.027029047587886452, 0.03077474434580654], time: 55.589
Running avgs for agent 0: q_loss: 0.06198973208665848, p_loss: 1.4406027793884277, mean_rew: -0.1931229482223306, variance: 0.1187737167775631, mean_q: -1.5112693309783936, std_q: 2.28619384765625
Running avgs for agent 1: q_loss: 0.06014559790492058, p_loss: 1.222145676612854, mean_rew: -0.19761281750923604, variance: 0.09456389496475458, mean_q: -1.3050081729888916, std_q: 2.318763256072998
Running avgs for agent 2: q_loss: 0.014798185788094997, p_loss: 2.2504985332489014, mean_rew: -0.3498659680416637, variance: 0.10811619035154581, mean_q: -2.318258285522461, std_q: 2.924062490463257
Running avgs for agent 3: q_loss: 0.014169647358357906, p_loss: 2.3395819664001465, mean_rew: -0.3603958714525269, variance: 0.12309897738322616, mean_q: -2.42398738861084, std_q: 3.4293642044067383

steps: 474975, episodes: 19000, mean episode reward: -22.560969926493208, agent episode reward: [-4.025568423612569, -3.671693612028247, -7.72907415555411, -7.134633735298281], time: 55.863
steps: 474975, episodes: 19000, mean episode variance: 0.10845533978613094, agent episode variance: [0.024954994581174107, 0.027943491484969853, 0.021518041103146972, 0.034038812616840004], time: 55.863
Running avgs for agent 0: q_loss: 0.05260389298200607, p_loss: 1.4399991035461426, mean_rew: -0.1913247185480778, variance: 0.09981997832469643, mean_q: -1.506036639213562, std_q: 2.2640438079833984
Running avgs for agent 1: q_loss: 0.055599864572286606, p_loss: 1.208929181098938, mean_rew: -0.19506872477688358, variance: 0.11177396593987941, mean_q: -1.2881717681884766, std_q: 2.2965824604034424
Running avgs for agent 2: q_loss: 0.019039541482925415, p_loss: 2.2565953731536865, mean_rew: -0.34878672805158956, variance: 0.08607216441258789, mean_q: -2.3225045204162598, std_q: 2.875357151031494
Running avgs for agent 3: q_loss: 0.016504064202308655, p_loss: 2.3308939933776855, mean_rew: -0.35624142366296074, variance: 0.13615525046736002, mean_q: -2.4127485752105713, std_q: 3.3827691078186035

steps: 499975, episodes: 20000, mean episode reward: -22.341155160294658, agent episode reward: [-3.8752949438136963, -3.8055768911212735, -7.264084290038216, -7.3961990353214695], time: 57.244
steps: 499975, episodes: 20000, mean episode variance: 0.0958056350974366, agent episode variance: [0.02511251073516905, 0.021745732580311598, 0.013240063194185496, 0.03570732858777046], time: 57.244
Running avgs for agent 0: q_loss: 0.05406965687870979, p_loss: 1.4381846189498901, mean_rew: -0.1887455360939673, variance: 0.1004500429406762, mean_q: -1.5013693571090698, std_q: 2.2377593517303467
Running avgs for agent 1: q_loss: 0.05360061675310135, p_loss: 1.2013881206512451, mean_rew: -0.19258939401287753, variance: 0.08698293032124639, mean_q: -1.2768090963363647, std_q: 2.264453649520874
Running avgs for agent 2: q_loss: 0.007471437565982342, p_loss: 2.2442352771759033, mean_rew: -0.3448350157518752, variance: 0.05296025277674198, mean_q: -2.3092212677001953, std_q: 2.8197927474975586
Running avgs for agent 3: q_loss: 0.01970706135034561, p_loss: 2.3185698986053467, mean_rew: -0.3529194652555681, variance: 0.14282931435108184, mean_q: -2.397646427154541, std_q: 3.3361246585845947

steps: 524975, episodes: 21000, mean episode reward: -22.850050381641186, agent episode reward: [-4.074878660907957, -3.8691706404352617, -7.15834217708739, -7.747658903210579], time: 56.214
steps: 524975, episodes: 21000, mean episode variance: 0.08740496626868845, agent episode variance: [0.02391020962409675, 0.019168671435676514, 0.016268075949512423, 0.02805800925940275], time: 56.214
Running avgs for agent 0: q_loss: 0.04987034574151039, p_loss: 1.4387775659561157, mean_rew: -0.1880054398431697, variance: 0.095640838496387, mean_q: -1.500655174255371, std_q: 2.2173876762390137
Running avgs for agent 1: q_loss: 0.04625573754310608, p_loss: 1.1888686418533325, mean_rew: -0.19076234329229116, variance: 0.07667468574270606, mean_q: -1.2602123022079468, std_q: 2.239152669906616
Running avgs for agent 2: q_loss: 0.013667885214090347, p_loss: 2.2410948276519775, mean_rew: -0.34235199937927185, variance: 0.0650723037980497, mean_q: -2.302640199661255, std_q: 2.7742607593536377
Running avgs for agent 3: q_loss: 0.014628400094807148, p_loss: 2.3027596473693848, mean_rew: -0.3497105749989165, variance: 0.112232037037611, mean_q: -2.3792355060577393, std_q: 3.294520616531372

steps: 549975, episodes: 22000, mean episode reward: -22.430689565808017, agent episode reward: [-4.114874013741811, -4.020457469122555, -6.963620574277422, -7.331737508666232], time: 55.652
steps: 549975, episodes: 22000, mean episode variance: 0.10061620299518108, agent episode variance: [0.021960566780995578, 0.02043586426274851, 0.021058887535706162, 0.03716088441573084], time: 55.652
Running avgs for agent 0: q_loss: 0.05033135041594505, p_loss: 1.4294419288635254, mean_rew: -0.18667800349417774, variance: 0.08784226712398231, mean_q: -1.488329291343689, std_q: 2.1961898803710938
Running avgs for agent 1: q_loss: 0.04440472275018692, p_loss: 1.1811606884002686, mean_rew: -0.18865783178318504, variance: 0.08174345705099403, mean_q: -1.2509334087371826, std_q: 2.218580484390259
Running avgs for agent 2: q_loss: 0.0136742377653718, p_loss: 2.222470283508301, mean_rew: -0.3390521297656235, variance: 0.08423555014282465, mean_q: -2.2814929485321045, std_q: 2.7811028957366943
Running avgs for agent 3: q_loss: 0.019478142261505127, p_loss: 2.3131206035614014, mean_rew: -0.35024081986743805, variance: 0.14864353766292335, mean_q: -2.38871431350708, std_q: 3.2976274490356445

steps: 574975, episodes: 23000, mean episode reward: -22.641681645310417, agent episode reward: [-4.184942221936818, -3.688514904734537, -7.3172952941531495, -7.450929224485914], time: 53.344
steps: 574975, episodes: 23000, mean episode variance: 0.07953456405457109, agent episode variance: [0.02280280284024775, 0.024595341932959855, 0.019476816915906966, 0.012659602365456522], time: 53.345
Running avgs for agent 0: q_loss: 0.052528128027915955, p_loss: 1.4264014959335327, mean_rew: -0.18555871651043657, variance: 0.091211211360991, mean_q: -1.4830185174942017, std_q: 2.2017414569854736
Running avgs for agent 1: q_loss: 0.0566553920507431, p_loss: 1.1714963912963867, mean_rew: -0.18760849020233913, variance: 0.09838136773183942, mean_q: -1.2374407052993774, std_q: 2.20452880859375
Running avgs for agent 2: q_loss: 0.014932717196643353, p_loss: 2.2094831466674805, mean_rew: -0.33658411792125437, variance: 0.07790726766362786, mean_q: -2.2652416229248047, std_q: 2.7557106018066406
Running avgs for agent 3: q_loss: 0.007064612582325935, p_loss: 2.3039326667785645, mean_rew: -0.34604843294106197, variance: 0.05063840946182609, mean_q: -2.3744003772735596, std_q: 3.2059717178344727

steps: 599975, episodes: 24000, mean episode reward: -23.364657226285292, agent episode reward: [-4.45844324260857, -4.106975262731399, -7.366717213288373, -7.432521507656955], time: 53.48
steps: 599975, episodes: 24000, mean episode variance: 0.09590831068274565, agent episode variance: [0.02226805724645965, 0.02275903827510774, 0.02171956506697461, 0.029161650094203653], time: 53.481
Running avgs for agent 0: q_loss: 0.051270753145217896, p_loss: 1.428344964981079, mean_rew: -0.18615938625197986, variance: 0.0890722289858386, mean_q: -1.481906533241272, std_q: 2.1561098098754883
Running avgs for agent 1: q_loss: 0.052873365581035614, p_loss: 1.1584230661392212, mean_rew: -0.18654349840367138, variance: 0.09103615310043096, mean_q: -1.2222604751586914, std_q: 2.188957929611206
Running avgs for agent 2: q_loss: 0.014850680716335773, p_loss: 2.217243194580078, mean_rew: -0.3368952760124204, variance: 0.08687826026789844, mean_q: -2.2721445560455322, std_q: 2.763798236846924
Running avgs for agent 3: q_loss: 0.01784890703856945, p_loss: 2.308335781097412, mean_rew: -0.34426017260217523, variance: 0.11664660037681461, mean_q: -2.375361919403076, std_q: 3.187750816345215

steps: 624975, episodes: 25000, mean episode reward: -22.539223908929348, agent episode reward: [-3.850133113140085, -3.6168040833295665, -7.477650441626012, -7.594636270833682], time: 53.487
steps: 624975, episodes: 25000, mean episode variance: 0.08038035288220272, agent episode variance: [0.021402639848180114, 0.024764452780596913, 0.011639362395275384, 0.022573897858150303], time: 53.488
Running avgs for agent 0: q_loss: 0.048234641551971436, p_loss: 1.4155049324035645, mean_rew: -0.18350107124866735, variance: 0.08561055939272046, mean_q: -1.4686180353164673, std_q: 2.1196651458740234
Running avgs for agent 1: q_loss: 0.0506611205637455, p_loss: 1.142004370689392, mean_rew: -0.18438415826479657, variance: 0.09905781112238765, mean_q: -1.204761028289795, std_q: 2.180773973464966
Running avgs for agent 2: q_loss: 0.007900547236204147, p_loss: 2.200326681137085, mean_rew: -0.3344879155680841, variance: 0.046557449581101536, mean_q: -2.2549474239349365, std_q: 2.688430070877075
Running avgs for agent 3: q_loss: 0.010391535237431526, p_loss: 2.3113582134246826, mean_rew: -0.34366491099775176, variance: 0.09029559143260121, mean_q: -2.3774983882904053, std_q: 3.1410000324249268

steps: 649975, episodes: 26000, mean episode reward: -22.46908949755947, agent episode reward: [-3.9989966675767437, -3.661279581582359, -7.590732506148463, -7.218080742251902], time: 53.497
steps: 649975, episodes: 26000, mean episode variance: 0.10485361903253942, agent episode variance: [0.022912647964432837, 0.02715787788759917, 0.020687118615955114, 0.03409597456455231], time: 53.497
Running avgs for agent 0: q_loss: 0.057101309299468994, p_loss: 1.4182097911834717, mean_rew: -0.18408022107323416, variance: 0.09165059185773135, mean_q: -1.4689842462539673, std_q: 2.1272473335266113
Running avgs for agent 1: q_loss: 0.055382199585437775, p_loss: 1.1279240846633911, mean_rew: -0.18281350716173314, variance: 0.10863151155039669, mean_q: -1.1868540048599243, std_q: 2.1561942100524902
Running avgs for agent 2: q_loss: 0.015494363382458687, p_loss: 2.1865615844726562, mean_rew: -0.3318191418650449, variance: 0.08274847446382046, mean_q: -2.240947723388672, std_q: 2.6930794715881348
Running avgs for agent 3: q_loss: 0.014873352833092213, p_loss: 2.311911106109619, mean_rew: -0.3426782719550415, variance: 0.13638389825820924, mean_q: -2.3759360313415527, std_q: 3.116497755050659

steps: 674975, episodes: 27000, mean episode reward: -22.405793844677763, agent episode reward: [-4.005678169537749, -3.840348405734405, -7.294992877638485, -7.264774391767125], time: 53.663
steps: 674975, episodes: 27000, mean episode variance: 0.09764738688664511, agent episode variance: [0.02412929033441469, 0.024004068525508047, 0.016106558105908334, 0.03340746992081404], time: 53.664
Running avgs for agent 0: q_loss: 0.05400191992521286, p_loss: 1.4094443321228027, mean_rew: -0.18235305324425805, variance: 0.09651716133765877, mean_q: -1.4585788249969482, std_q: 2.115950584411621
Running avgs for agent 1: q_loss: 0.055666569620370865, p_loss: 1.1124130487442017, mean_rew: -0.18244362060649164, variance: 0.09601627410203219, mean_q: -1.1694492101669312, std_q: 2.1281776428222656
Running avgs for agent 2: q_loss: 0.012790397740900517, p_loss: 2.1493115425109863, mean_rew: -0.331528193200582, variance: 0.06442623242363334, mean_q: -2.2088940143585205, std_q: 2.6156394481658936
Running avgs for agent 3: q_loss: 0.019546085968613625, p_loss: 2.285125494003296, mean_rew: -0.33717772093858744, variance: 0.13362987968325615, mean_q: -2.345245599746704, std_q: 3.0202066898345947

steps: 699975, episodes: 28000, mean episode reward: -22.3653319876999, agent episode reward: [-3.9020432068216504, -3.7264602520034305, -7.368664890023331, -7.368163638851487], time: 53.623
steps: 699975, episodes: 28000, mean episode variance: 0.08537424970371649, agent episode variance: [0.025976752331480384, 0.01915235562995076, 0.026238247995264828, 0.014006893747020513], time: 53.623
Running avgs for agent 0: q_loss: 0.05242925137281418, p_loss: 1.4209415912628174, mean_rew: -0.18403896750728563, variance: 0.10390700932592153, mean_q: -1.4688891172409058, std_q: 2.1139309406280518
Running avgs for agent 1: q_loss: 0.047941382974386215, p_loss: 1.1018788814544678, mean_rew: -0.18214583479526356, variance: 0.07660942251980304, mean_q: -1.1582409143447876, std_q: 2.123339891433716
Running avgs for agent 2: q_loss: 0.015297211706638336, p_loss: 2.107738733291626, mean_rew: -0.33013205812901275, variance: 0.10495299198105931, mean_q: -2.1712496280670166, std_q: 2.669816732406616
Running avgs for agent 3: q_loss: 0.008659126237034798, p_loss: 2.3018486499786377, mean_rew: -0.33784767614530686, variance: 0.05602757498808205, mean_q: -2.360788106918335, std_q: 3.000384569168091

steps: 724975, episodes: 29000, mean episode reward: -22.540163809484117, agent episode reward: [-4.067633713085819, -3.7785346117496803, -7.1744251561025205, -7.519570328546102], time: 53.64
steps: 724975, episodes: 29000, mean episode variance: 0.07994146591611206, agent episode variance: [0.017573305651545523, 0.025261195599101484, 0.016638351025059818, 0.02046861364040524], time: 53.64
Running avgs for agent 0: q_loss: 0.04448574781417847, p_loss: 1.4154813289642334, mean_rew: -0.18106998963867307, variance: 0.07029322260618209, mean_q: -1.4609304666519165, std_q: 2.0962822437286377
Running avgs for agent 1: q_loss: 0.04944954439997673, p_loss: 1.07417893409729, mean_rew: -0.17980137439937852, variance: 0.10104478239640594, mean_q: -1.1293975114822388, std_q: 2.084266185760498
Running avgs for agent 2: q_loss: 0.015833131968975067, p_loss: 2.0468246936798096, mean_rew: -0.32723097963927894, variance: 0.06655340410023927, mean_q: -2.1094071865081787, std_q: 2.572265148162842
Running avgs for agent 3: q_loss: 0.012233509682118893, p_loss: 2.294529914855957, mean_rew: -0.33499485291862324, variance: 0.08187445456162096, mean_q: -2.3518636226654053, std_q: 2.9879770278930664

steps: 749975, episodes: 30000, mean episode reward: -22.354011823957524, agent episode reward: [-4.113601344954942, -3.9654221747336567, -7.102938727296319, -7.172049576972607], time: 53.779
steps: 749975, episodes: 30000, mean episode variance: 0.07646632345905527, agent episode variance: [0.01984177582943812, 0.02137812701612711, 0.02071045608073473, 0.014535964532755316], time: 53.779
Running avgs for agent 0: q_loss: 0.04367515444755554, p_loss: 1.4138691425323486, mean_rew: -0.18079119247021833, variance: 0.07936710331775249, mean_q: -1.4581228494644165, std_q: 2.054940938949585
Running avgs for agent 1: q_loss: 0.047758299857378006, p_loss: 1.0608599185943604, mean_rew: -0.17738098112123787, variance: 0.08551250806450844, mean_q: -1.1135205030441284, std_q: 2.0908429622650146
Running avgs for agent 2: q_loss: 0.021190427243709564, p_loss: 2.0020456314086914, mean_rew: -0.3261876208280311, variance: 0.08284182432293892, mean_q: -2.0630335807800293, std_q: 2.6250460147857666
Running avgs for agent 3: q_loss: 0.009517023339867592, p_loss: 2.293977975845337, mean_rew: -0.3335810167404512, variance: 0.05814385813102126, mean_q: -2.349540948867798, std_q: 2.9664692878723145

steps: 774975, episodes: 31000, mean episode reward: -22.077259230011258, agent episode reward: [-3.868695175770441, -3.54501620558419, -7.210114903283101, -7.453432945373524], time: 53.656
steps: 774975, episodes: 31000, mean episode variance: 0.09707178366929292, agent episode variance: [0.023966795632615685, 0.01950026207510382, 0.01852478561270982, 0.0350799403488636], time: 53.656
Running avgs for agent 0: q_loss: 0.052706100046634674, p_loss: 1.4206291437149048, mean_rew: -0.17988647722500978, variance: 0.09586718253046274, mean_q: -1.463281512260437, std_q: 2.0613136291503906
Running avgs for agent 1: q_loss: 0.05002598837018013, p_loss: 1.056540608406067, mean_rew: -0.17773017474015837, variance: 0.07800104830041528, mean_q: -1.1081957817077637, std_q: 2.079404830932617
Running avgs for agent 2: q_loss: 0.01773480698466301, p_loss: 1.951166033744812, mean_rew: -0.32657380900937916, variance: 0.07409914245083928, mean_q: -2.020618438720703, std_q: 2.6465439796447754
Running avgs for agent 3: q_loss: 0.020337847992777824, p_loss: 2.296400308609009, mean_rew: -0.332803903978136, variance: 0.1403197613954544, mean_q: -2.351332902908325, std_q: 2.9390947818756104

steps: 799975, episodes: 32000, mean episode reward: -21.982278517893274, agent episode reward: [-3.5935164612092407, -3.5182152665562354, -7.38245210636825, -7.488094683759553], time: 53.645
steps: 799975, episodes: 32000, mean episode variance: 0.08268244774127378, agent episode variance: [0.022090773015283047, 0.02029463049583137, 0.012015740130562336, 0.028281304099597036], time: 53.646
Running avgs for agent 0: q_loss: 0.048312872648239136, p_loss: 1.4238451719284058, mean_rew: -0.1798586920615888, variance: 0.08836309206113219, mean_q: -1.466119408607483, std_q: 2.054206132888794
Running avgs for agent 1: q_loss: 0.05131923779845238, p_loss: 1.0474885702133179, mean_rew: -0.17635498178703005, variance: 0.08117852198332548, mean_q: -1.0987509489059448, std_q: 2.0640206336975098
Running avgs for agent 2: q_loss: 0.014299036003649235, p_loss: 1.8576693534851074, mean_rew: -0.3230823252275428, variance: 0.04806296052224934, mean_q: -1.9327454566955566, std_q: 2.6215262413024902
Running avgs for agent 3: q_loss: 0.019106129184365273, p_loss: 2.297842025756836, mean_rew: -0.33263341762553583, variance: 0.11312521639838814, mean_q: -2.3537089824676514, std_q: 2.91520357131958

steps: 824975, episodes: 33000, mean episode reward: -22.724648494424777, agent episode reward: [-3.607794787609451, -3.3158458676348683, -7.926053414059508, -7.874954425120952], time: 53.52
steps: 824975, episodes: 33000, mean episode variance: 0.077655232838355, agent episode variance: [0.020951047905255107, 0.01925421281065792, 0.017778278119396418, 0.01967169400304556], time: 53.521
Running avgs for agent 0: q_loss: 0.0438682846724987, p_loss: 1.4200530052185059, mean_rew: -0.17880297839823814, variance: 0.08380419162102043, mean_q: -1.4603735208511353, std_q: 2.026122570037842
Running avgs for agent 1: q_loss: 0.045612167567014694, p_loss: 1.0306198596954346, mean_rew: -0.1746394292973347, variance: 0.07701685124263168, mean_q: -1.0805718898773193, std_q: 2.0387697219848633
Running avgs for agent 2: q_loss: 0.017274299636483192, p_loss: 1.7741267681121826, mean_rew: -0.3233718897283751, variance: 0.07111311247758567, mean_q: -1.8541091680526733, std_q: 2.6091835498809814
Running avgs for agent 3: q_loss: 0.013404733501374722, p_loss: 2.294114589691162, mean_rew: -0.33064453595795684, variance: 0.07868677601218224, mean_q: -2.35020112991333, std_q: 2.911975622177124

steps: 849975, episodes: 34000, mean episode reward: -21.92803651610414, agent episode reward: [-3.2734068038839252, -2.747292317668689, -7.823924069149785, -8.083413325401741], time: 53.558
steps: 849975, episodes: 34000, mean episode variance: 0.07992762667918578, agent episode variance: [0.021833430911879986, 0.018757930677384137, 0.018179292326793075, 0.021156972763128577], time: 53.558
Running avgs for agent 0: q_loss: 0.05003302916884422, p_loss: 1.4136704206466675, mean_rew: -0.17691097048221102, variance: 0.08733372364751994, mean_q: -1.4553180932998657, std_q: 2.0369296073913574
Running avgs for agent 1: q_loss: 0.04829967021942139, p_loss: 1.0092229843139648, mean_rew: -0.17217982692069184, variance: 0.07503172270953655, mean_q: -1.0584819316864014, std_q: 2.0395944118499756
Running avgs for agent 2: q_loss: 0.018250498920679092, p_loss: 1.6869546175003052, mean_rew: -0.3236172507433614, variance: 0.0727171693071723, mean_q: -1.7723901271820068, std_q: 2.6203818321228027
Running avgs for agent 3: q_loss: 0.01580805331468582, p_loss: 2.3040919303894043, mean_rew: -0.33158166230918495, variance: 0.08462789105251431, mean_q: -2.361403703689575, std_q: 2.9056451320648193

steps: 874975, episodes: 35000, mean episode reward: -22.079152632566977, agent episode reward: [-3.0149182427482026, -2.6555806674569404, -8.336724309073743, -8.071929413288089], time: 53.687
steps: 874975, episodes: 35000, mean episode variance: 0.07431120030814782, agent episode variance: [0.021158164439722897, 0.018117785427253695, 0.012632453501224519, 0.02240279693994671], time: 53.688
Running avgs for agent 0: q_loss: 0.04663471132516861, p_loss: 1.4074854850769043, mean_rew: -0.17714526118469962, variance: 0.08463265775889159, mean_q: -1.4501430988311768, std_q: 2.050300121307373
Running avgs for agent 1: q_loss: 0.047806207090616226, p_loss: 0.989519476890564, mean_rew: -0.17231644885334202, variance: 0.07247114170901478, mean_q: -1.0369846820831299, std_q: 2.034184217453003
Running avgs for agent 2: q_loss: 0.014094489626586437, p_loss: 1.596545696258545, mean_rew: -0.32315372148148824, variance: 0.050529814004898074, mean_q: -1.6861978769302368, std_q: 2.568913459777832
Running avgs for agent 3: q_loss: 0.01652015931904316, p_loss: 2.305720329284668, mean_rew: -0.33113415432908233, variance: 0.08961118775978684, mean_q: -2.362671136856079, std_q: 2.8558785915374756

steps: 899975, episodes: 36000, mean episode reward: -22.77896797347864, agent episode reward: [-2.691973387949511, -2.695663704467046, -9.127236185691979, -8.264094695370103], time: 53.607
steps: 899975, episodes: 36000, mean episode variance: 0.0754160889768973, agent episode variance: [0.022272189469076692, 0.020609886672347782, 0.013631344016641378, 0.018902668818831445], time: 53.607
Running avgs for agent 0: q_loss: 0.0487196259200573, p_loss: 1.3846365213394165, mean_rew: -0.1748571790586426, variance: 0.08908875787630677, mean_q: -1.4263981580734253, std_q: 2.0035014152526855
Running avgs for agent 1: q_loss: 0.05487373471260071, p_loss: 0.9520837068557739, mean_rew: -0.17102697546642132, variance: 0.08243954668939113, mean_q: -1.000105381011963, std_q: 2.0530567169189453
Running avgs for agent 2: q_loss: 0.012539559975266457, p_loss: 1.5638068914413452, mean_rew: -0.32446000647951206, variance: 0.05452537606656551, mean_q: -1.645283579826355, std_q: 2.568128824234009
Running avgs for agent 3: q_loss: 0.01341284066438675, p_loss: 2.3063247203826904, mean_rew: -0.3298840050974298, variance: 0.07561067527532578, mean_q: -2.363502025604248, std_q: 2.772332191467285

steps: 924975, episodes: 37000, mean episode reward: -23.126391733080382, agent episode reward: [-2.8203894098240627, -2.2340907439839652, -9.500259995198878, -8.57165158407348], time: 53.536
steps: 924975, episodes: 37000, mean episode variance: 0.0750495348374825, agent episode variance: [0.019331978989765047, 0.02060111635597423, 0.019157735392451285, 0.015958704099291936], time: 53.536
Running avgs for agent 0: q_loss: 0.04872852936387062, p_loss: 1.3404850959777832, mean_rew: -0.17226224835692142, variance: 0.07732791595906019, mean_q: -1.3823286294937134, std_q: 2.003695011138916
Running avgs for agent 1: q_loss: 0.05117407813668251, p_loss: 0.8935950398445129, mean_rew: -0.16737751503788434, variance: 0.08240446542389691, mean_q: -0.9416178464889526, std_q: 2.038083076477051
Running avgs for agent 2: q_loss: 0.02154403366148472, p_loss: 1.5971999168395996, mean_rew: -0.3270063586208205, variance: 0.07663094156980514, mean_q: -1.66107177734375, std_q: 2.494584798812866
Running avgs for agent 3: q_loss: 0.01143186166882515, p_loss: 2.337892532348633, mean_rew: -0.331676779034868, variance: 0.06383481639716775, mean_q: -2.396038055419922, std_q: 2.8060145378112793

steps: 949975, episodes: 38000, mean episode reward: -22.602210936040912, agent episode reward: [-2.7039968650820936, -2.252141598236388, -8.772393848057455, -8.873678624664977], time: 53.525
steps: 949975, episodes: 38000, mean episode variance: 0.06924027955951169, agent episode variance: [0.019032418846618385, 0.017823985633905977, 0.013746961173601448, 0.018636913905385882], time: 53.525
Running avgs for agent 0: q_loss: 0.04090482369065285, p_loss: 1.293548345565796, mean_rew: -0.17144717838979415, variance: 0.07612967538647354, mean_q: -1.3330425024032593, std_q: 1.9763673543930054
Running avgs for agent 1: q_loss: 0.046870920807123184, p_loss: 0.8190502524375916, mean_rew: -0.16556888200073422, variance: 0.07129594253562391, mean_q: -0.866513192653656, std_q: 2.0189523696899414
Running avgs for agent 2: q_loss: 0.016119839623570442, p_loss: 1.6581425666809082, mean_rew: -0.3272041357746692, variance: 0.05498784469440579, mean_q: -1.723944902420044, std_q: 2.4170596599578857
Running avgs for agent 3: q_loss: 0.013979345560073853, p_loss: 2.340407609939575, mean_rew: -0.3298983220450732, variance: 0.07454765562154353, mean_q: -2.397221088409424, std_q: 2.7335526943206787

steps: 974975, episodes: 39000, mean episode reward: -22.750679817192445, agent episode reward: [-2.495340643848521, -1.8062321878273269, -8.77837478829097, -9.670732197225629], time: 53.694
steps: 974975, episodes: 39000, mean episode variance: 0.07299217227939517, agent episode variance: [0.02041331488173455, 0.017425104055553673, 0.01301465936563909, 0.022139093976467846], time: 53.695
Running avgs for agent 0: q_loss: 0.04614359140396118, p_loss: 1.2336560487747192, mean_rew: -0.1684629713488408, variance: 0.0816532595269382, mean_q: -1.274227499961853, std_q: 1.9570870399475098
Running avgs for agent 1: q_loss: 0.04958554729819298, p_loss: 0.7329567670822144, mean_rew: -0.16518886261098376, variance: 0.06970041622221469, mean_q: -0.781370222568512, std_q: 2.011234998703003
Running avgs for agent 2: q_loss: 0.013091850094497204, p_loss: 1.6918247938156128, mean_rew: -0.32818339658030093, variance: 0.05205863746255636, mean_q: -1.7613415718078613, std_q: 2.3673672676086426
Running avgs for agent 3: q_loss: 0.01803397201001644, p_loss: 2.3439533710479736, mean_rew: -0.33096561547570175, variance: 0.08855637590587138, mean_q: -2.4026639461517334, std_q: 2.654391050338745

steps: 999975, episodes: 40000, mean episode reward: -23.428153960830304, agent episode reward: [-3.329608056848303, -2.600915475949878, -8.459029972411576, -9.038600455620545], time: 53.698
steps: 999975, episodes: 40000, mean episode variance: 0.06612606169655919, agent episode variance: [0.017290958481840788, 0.017633896575309334, 0.014630081408657134, 0.01657112523075193], time: 53.698
Running avgs for agent 0: q_loss: 0.043037764728069305, p_loss: 1.1628453731536865, mean_rew: -0.16643959213191267, variance: 0.06916383392736315, mean_q: -1.2019191980361938, std_q: 1.913049340248108
Running avgs for agent 1: q_loss: 0.050953224301338196, p_loss: 0.6400044560432434, mean_rew: -0.16170438165426249, variance: 0.07053558630123734, mean_q: -0.6879324913024902, std_q: 1.982898235321045
Running avgs for agent 2: q_loss: 0.016218986362218857, p_loss: 1.7342220544815063, mean_rew: -0.3286867495871563, variance: 0.058520325634628535, mean_q: -1.8064982891082764, std_q: 2.3229165077209473
Running avgs for agent 3: q_loss: 0.014135757461190224, p_loss: 2.3728110790252686, mean_rew: -0.33406167000692955, variance: 0.06628450092300772, mean_q: -2.4319028854370117, std_q: 2.6733131408691406

steps: 1024975, episodes: 41000, mean episode reward: -23.56639532652334, agent episode reward: [-3.361220128772332, -2.5955327252705884, -8.69756784311627, -8.912074629364152], time: 53.844
steps: 1024975, episodes: 41000, mean episode variance: 0.06977804824779742, agent episode variance: [0.016337501376401633, 0.01840009610913694, 0.01362224694690667, 0.02141820381535217], time: 53.844
Running avgs for agent 0: q_loss: 0.03980424627661705, p_loss: 1.075790524482727, mean_rew: -0.16437103375924939, variance: 0.06535000550560653, mean_q: -1.1074076890945435, std_q: 1.871694564819336
Running avgs for agent 1: q_loss: 0.05233825743198395, p_loss: 0.53373122215271, mean_rew: -0.16007340969768102, variance: 0.07360038443654776, mean_q: -0.5731082558631897, std_q: 1.9234533309936523
Running avgs for agent 2: q_loss: 0.013613495975732803, p_loss: 1.6949946880340576, mean_rew: -0.3186831466868132, variance: 0.05448898778762668, mean_q: -1.7670166492462158, std_q: 2.212228536605835
Running avgs for agent 3: q_loss: 0.01564275659620762, p_loss: 2.343712329864502, mean_rew: -0.32523057714138015, variance: 0.08567281526140869, mean_q: -2.396162748336792, std_q: 2.561047315597534

steps: 1049975, episodes: 42000, mean episode reward: -23.407737822245156, agent episode reward: [-3.140630618640729, -2.340116891649248, -9.566345132674803, -8.360645179280375], time: 54.14
steps: 1049975, episodes: 42000, mean episode variance: 0.03833100066590123, agent episode variance: [0.010897882433841004, 0.012288421422243118, 0.007810413873521611, 0.007334282936295494], time: 54.141
Running avgs for agent 0: q_loss: 0.031411051750183105, p_loss: 0.9750847816467285, mean_rew: -0.16160006438570604, variance: 0.04359152973536402, mean_q: -0.9923245906829834, std_q: 1.649032711982727
Running avgs for agent 1: q_loss: 0.03974978253245354, p_loss: 0.39552611112594604, mean_rew: -0.15373753811803095, variance: 0.04915368568897247, mean_q: -0.4188697040081024, std_q: 1.7137378454208374
Running avgs for agent 2: q_loss: 0.01070726104080677, p_loss: 1.5995571613311768, mean_rew: -0.30507193230375207, variance: 0.031241655494086445, mean_q: -1.6577612161636353, std_q: 1.8743103742599487
Running avgs for agent 3: q_loss: 0.010001481510698795, p_loss: 2.223344326019287, mean_rew: -0.3064093325159993, variance: 0.029337131745181978, mean_q: -2.258277416229248, std_q: 1.9182908535003662

steps: 1074975, episodes: 43000, mean episode reward: -24.031866234293098, agent episode reward: [-2.964303704464063, -2.7427929679629535, -10.265422348270361, -8.059347213595723], time: 54.246
steps: 1074975, episodes: 43000, mean episode variance: 0.042706535482313485, agent episode variance: [0.011528237886726857, 0.011247157339937985, 0.012064071504399181, 0.007867068751249462], time: 54.247
Running avgs for agent 0: q_loss: 0.03169136866927147, p_loss: 0.8949037790298462, mean_rew: -0.15826456166490152, variance: 0.04611295154690743, mean_q: -0.9079462289810181, std_q: 1.6448097229003906
Running avgs for agent 1: q_loss: 0.03740290552377701, p_loss: 0.29197046160697937, mean_rew: -0.1484633557718681, variance: 0.04498862935975194, mean_q: -0.30810263752937317, std_q: 1.7024822235107422
Running avgs for agent 2: q_loss: 0.012199071235954762, p_loss: 1.577221393585205, mean_rew: -0.30432575386408695, variance: 0.048256286017596725, mean_q: -1.6361966133117676, std_q: 1.8577191829681396
Running avgs for agent 3: q_loss: 0.00946718268096447, p_loss: 2.2120823860168457, mean_rew: -0.30511005969427635, variance: 0.031468275004997846, mean_q: -2.241278648376465, std_q: 1.8622514009475708

steps: 1099975, episodes: 44000, mean episode reward: -25.12632717405902, agent episode reward: [-3.5084161354031895, -3.1485928079546457, -10.699343338201892, -7.769974892499295], time: 54.077
steps: 1099975, episodes: 44000, mean episode variance: 0.0425799994666595, agent episode variance: [0.010945002558641136, 0.010709484226303176, 0.012575863792095333, 0.008349648889619856], time: 54.078
Running avgs for agent 0: q_loss: 0.03225002810359001, p_loss: 0.8390704989433289, mean_rew: -0.1565549943138984, variance: 0.04378001023456454, mean_q: -0.852871298789978, std_q: 1.6460561752319336
Running avgs for agent 1: q_loss: 0.03668130561709404, p_loss: 0.20151682198047638, mean_rew: -0.14500397691663675, variance: 0.042837936905212703, mean_q: -0.2152559459209442, std_q: 1.6915805339813232
Running avgs for agent 2: q_loss: 0.014307931996881962, p_loss: 1.6067895889282227, mean_rew: -0.3098325287286328, variance: 0.050303455168381334, mean_q: -1.6597392559051514, std_q: 1.8450937271118164
Running avgs for agent 3: q_loss: 0.008401835337281227, p_loss: 2.2192745208740234, mean_rew: -0.3057248805781357, variance: 0.033398595558479426, mean_q: -2.2432641983032227, std_q: 1.866170048713684

steps: 1124975, episodes: 45000, mean episode reward: -24.68237411398347, agent episode reward: [-3.7412114143510697, -3.177437670481118, -10.049763458394152, -7.713961570757132], time: 54.184
steps: 1124975, episodes: 45000, mean episode variance: 0.045325049199163914, agent episode variance: [0.011904065480921417, 0.010964889984112233, 0.013897242518607527, 0.008558851215522736], time: 54.185
Running avgs for agent 0: q_loss: 0.03200342878699303, p_loss: 0.8315805792808533, mean_rew: -0.15529820173467948, variance: 0.04761626192368567, mean_q: -0.84559166431427, std_q: 1.6402963399887085
Running avgs for agent 1: q_loss: 0.03745679557323456, p_loss: 0.1755586415529251, mean_rew: -0.1439845059332895, variance: 0.04385955993644893, mean_q: -0.18850171566009521, std_q: 1.6907893419265747
Running avgs for agent 2: q_loss: 0.01613328605890274, p_loss: 1.6548594236373901, mean_rew: -0.31237844412845595, variance: 0.05558897007443011, mean_q: -1.7099891901016235, std_q: 1.7915090322494507
Running avgs for agent 3: q_loss: 0.008282603695988655, p_loss: 2.2173688411712646, mean_rew: -0.3055667856936045, variance: 0.034235404862090944, mean_q: -2.2448983192443848, std_q: 1.8617348670959473

steps: 1149975, episodes: 46000, mean episode reward: -24.73788257329064, agent episode reward: [-3.259091891525707, -2.7959763087577394, -10.94535067480322, -7.737463698203972], time: 54.501
steps: 1149975, episodes: 46000, mean episode variance: 0.042190986182074994, agent episode variance: [0.011180400806013496, 0.01110708649782464, 0.010835625229869039, 0.009067873648367822], time: 54.502
Running avgs for agent 0: q_loss: 0.03150419518351555, p_loss: 0.8351874947547913, mean_rew: -0.15238240607186324, variance: 0.04472160322405398, mean_q: -0.8489066362380981, std_q: 1.6074397563934326
Running avgs for agent 1: q_loss: 0.038235437124967575, p_loss: 0.18186266720294952, mean_rew: -0.14265602323609614, variance: 0.04442834599129856, mean_q: -0.1939794421195984, std_q: 1.6889835596084595
Running avgs for agent 2: q_loss: 0.016420520842075348, p_loss: 1.6884887218475342, mean_rew: -0.3152576590999602, variance: 0.043342500919476154, mean_q: -1.7580111026763916, std_q: 1.7223811149597168
Running avgs for agent 3: q_loss: 0.008840593509376049, p_loss: 2.204620122909546, mean_rew: -0.30783928928620935, variance: 0.03627149459347129, mean_q: -2.238416910171509, std_q: 1.8934974670410156

steps: 1174975, episodes: 47000, mean episode reward: -23.97163111067925, agent episode reward: [-3.0359454687543805, -2.633443402511265, -10.227785250808502, -8.074456988605107], time: 54.56
steps: 1174975, episodes: 47000, mean episode variance: 0.046893007684033365, agent episode variance: [0.011017782734241336, 0.011700139613822103, 0.010771921394392849, 0.013403163941577078], time: 54.561
Running avgs for agent 0: q_loss: 0.031057631596922874, p_loss: 0.8397257924079895, mean_rew: -0.15341620735040856, variance: 0.044071130936965346, mean_q: -0.852931022644043, std_q: 1.6051770448684692
Running avgs for agent 1: q_loss: 0.03904097154736519, p_loss: 0.1776944100856781, mean_rew: -0.14056892297366214, variance: 0.04680055845528841, mean_q: -0.18852883577346802, std_q: 1.6887624263763428
Running avgs for agent 2: q_loss: 0.017035681754350662, p_loss: 1.714665412902832, mean_rew: -0.31961464676641194, variance: 0.043087685577571394, mean_q: -1.7952349185943604, std_q: 1.7241803407669067
Running avgs for agent 3: q_loss: 0.009961115196347237, p_loss: 2.167793035507202, mean_rew: -0.30875246833161074, variance: 0.05361265576630831, mean_q: -2.207287311553955, std_q: 1.908499836921692

steps: 1199975, episodes: 48000, mean episode reward: -23.724584561847347, agent episode reward: [-3.3228417558517065, -2.6625729111357312, -9.826444952587051, -7.912724942272855], time: 54.177
steps: 1199975, episodes: 48000, mean episode variance: 0.04168555184872821, agent episode variance: [0.01064095902722329, 0.01035554627655074, 0.00925883685052395, 0.011430209694430232], time: 54.177
Running avgs for agent 0: q_loss: 0.030232058838009834, p_loss: 0.8089759349822998, mean_rew: -0.15092413296165136, variance: 0.04256383610889316, mean_q: -0.819375216960907, std_q: 1.5892117023468018
Running avgs for agent 1: q_loss: 0.038565605878829956, p_loss: 0.14985473453998566, mean_rew: -0.13865166429947998, variance: 0.04142218510620296, mean_q: -0.15993870794773102, std_q: 1.6835007667541504
Running avgs for agent 2: q_loss: 0.014386632479727268, p_loss: 1.7883354425430298, mean_rew: -0.32160631447942456, variance: 0.0370353474020958, mean_q: -1.8425090312957764, std_q: 1.7134487628936768
Running avgs for agent 3: q_loss: 0.01058998517692089, p_loss: 2.1317710876464844, mean_rew: -0.3096167374139644, variance: 0.04572083877772093, mean_q: -2.1726953983306885, std_q: 1.9268856048583984

steps: 1224975, episodes: 49000, mean episode reward: -22.456626095424955, agent episode reward: [-2.9696997091540003, -2.565556785903892, -8.881976133961711, -8.039393466405356], time: 56.213
steps: 1224975, episodes: 49000, mean episode variance: 0.04693413254269399, agent episode variance: [0.011843820712529122, 0.010617568657267838, 0.010521193647990004, 0.013951549524907023], time: 56.214
Running avgs for agent 0: q_loss: 0.03046419471502304, p_loss: 0.8027698397636414, mean_rew: -0.14981070206892858, variance: 0.04737528285011649, mean_q: -0.8142951130867004, std_q: 1.584233045578003
Running avgs for agent 1: q_loss: 0.03806973993778229, p_loss: 0.14545181393623352, mean_rew: -0.13615522010836098, variance: 0.04247027462907135, mean_q: -0.1543334722518921, std_q: 1.6890301704406738
Running avgs for agent 2: q_loss: 0.01278793253004551, p_loss: 1.9396262168884277, mean_rew: -0.32448654805954524, variance: 0.042084774591960014, mean_q: -1.9753198623657227, std_q: 1.702908992767334
Running avgs for agent 3: q_loss: 0.011036098934710026, p_loss: 2.082169771194458, mean_rew: -0.3080569267783818, variance: 0.05580619809962809, mean_q: -2.123192310333252, std_q: 1.907759189605713

steps: 1249975, episodes: 50000, mean episode reward: -22.623064025588157, agent episode reward: [-3.0657544481068917, -2.3327176103635585, -8.594557161067685, -8.630034806050023], time: 61.847
steps: 1249975, episodes: 50000, mean episode variance: 0.046675538751063866, agent episode variance: [0.01172042855178006, 0.010929612582549453, 0.01179943282622844, 0.012226064790505916], time: 61.848
Running avgs for agent 0: q_loss: 0.030287262052297592, p_loss: 0.7962639927864075, mean_rew: -0.14855925204045498, variance: 0.04688171420712024, mean_q: -0.807819664478302, std_q: 1.5847392082214355
Running avgs for agent 1: q_loss: 0.038405243307352066, p_loss: 0.16815215349197388, mean_rew: -0.13541865241852388, variance: 0.04371845033019781, mean_q: -0.1765824556350708, std_q: 1.6862529516220093
Running avgs for agent 2: q_loss: 0.013612945564091206, p_loss: 1.9872920513153076, mean_rew: -0.32571720968909257, variance: 0.04719773130491376, mean_q: -2.041506052017212, std_q: 1.6682794094085693
Running avgs for agent 3: q_loss: 0.011713817715644836, p_loss: 2.0590782165527344, mean_rew: -0.3100364722115518, variance: 0.048904259162023664, mean_q: -2.1015262603759766, std_q: 1.919803261756897

steps: 1274975, episodes: 51000, mean episode reward: -22.90038182020676, agent episode reward: [-3.0710295616803442, -2.590725762048589, -8.44465623559656, -8.793970260881268], time: 61.693
steps: 1274975, episodes: 51000, mean episode variance: 0.047250428816769274, agent episode variance: [0.011680629995185881, 0.011191204238217324, 0.012692685764282941, 0.011685908819083124], time: 61.693
Running avgs for agent 0: q_loss: 0.03208167850971222, p_loss: 0.7691048383712769, mean_rew: -0.14659123731374726, variance: 0.046722519980743525, mean_q: -0.7812521457672119, std_q: 1.6017619371414185
Running avgs for agent 1: q_loss: 0.038370102643966675, p_loss: 0.17825119197368622, mean_rew: -0.13319785281826274, variance: 0.044764816952869296, mean_q: -0.18640480935573578, std_q: 1.682215929031372
Running avgs for agent 2: q_loss: 0.01477722730487585, p_loss: 1.93721342086792, mean_rew: -0.32747885887505673, variance: 0.050770743057131765, mean_q: -2.0237972736358643, std_q: 1.6621142625808716
Running avgs for agent 3: q_loss: 0.011812787503004074, p_loss: 2.044832468032837, mean_rew: -0.31162944951100013, variance: 0.0467436352763325, mean_q: -2.0876758098602295, std_q: 1.8994282484054565

steps: 1299975, episodes: 52000, mean episode reward: -22.68776244557782, agent episode reward: [-2.8852576013574724, -2.723682027016861, -8.745806784176079, -8.333016033027407], time: 62.509
steps: 1299975, episodes: 52000, mean episode variance: 0.050832085271831605, agent episode variance: [0.014941866165027023, 0.012262011897284538, 0.012948778267484157, 0.010679428942035883], time: 62.51
Running avgs for agent 0: q_loss: 0.03582381457090378, p_loss: 0.71437668800354, mean_rew: -0.14495395041050316, variance: 0.05976746466010809, mean_q: -0.7255710363388062, std_q: 1.5895063877105713
Running avgs for agent 1: q_loss: 0.03913616016507149, p_loss: 0.1791285276412964, mean_rew: -0.13234298428878868, variance: 0.04904804758913815, mean_q: -0.18727514147758484, std_q: 1.685556173324585
Running avgs for agent 2: q_loss: 0.015292934142053127, p_loss: 1.8489890098571777, mean_rew: -0.32984174582876735, variance: 0.05179511306993663, mean_q: -1.9570235013961792, std_q: 1.7027573585510254
Running avgs for agent 3: q_loss: 0.012148485518991947, p_loss: 2.036428689956665, mean_rew: -0.31230600166674527, variance: 0.04271771576814353, mean_q: -2.078810930252075, std_q: 1.8790360689163208

steps: 1324975, episodes: 53000, mean episode reward: -23.519495086500037, agent episode reward: [-3.257097227565402, -2.692914671436045, -9.095926354146041, -8.473556833352548], time: 55.897
steps: 1324975, episodes: 53000, mean episode variance: 0.04779836976202205, agent episode variance: [0.012439011008944362, 0.011141865651588888, 0.011885986633598805, 0.012331506467889995], time: 55.898
Running avgs for agent 0: q_loss: 0.035340480506420135, p_loss: 0.6638592481613159, mean_rew: -0.14268486235018354, variance: 0.04975604403577745, mean_q: -0.6735063791275024, std_q: 1.591954231262207
Running avgs for agent 1: q_loss: 0.03893771022558212, p_loss: 0.16764365136623383, mean_rew: -0.12967131439770982, variance: 0.04456746260635555, mean_q: -0.1767667979001999, std_q: 1.6634098291397095
Running avgs for agent 2: q_loss: 0.016445182263851166, p_loss: 1.74711275100708, mean_rew: -0.3296661072082594, variance: 0.04754394653439522, mean_q: -1.8618674278259277, std_q: 1.7614880800247192
Running avgs for agent 3: q_loss: 0.012293647974729538, p_loss: 2.035721778869629, mean_rew: -0.3137075621187683, variance: 0.04932602587155998, mean_q: -2.0786919593811035, std_q: 1.8406318426132202

steps: 1349975, episodes: 54000, mean episode reward: -23.73594586612308, agent episode reward: [-2.6188385316162326, -2.5002688418468004, -10.22484127086443, -8.39199722179562], time: 55.216
steps: 1349975, episodes: 54000, mean episode variance: 0.04440441143838689, agent episode variance: [0.012402081601787358, 0.010807687716092914, 0.010016856478061527, 0.011177785642445088], time: 55.216
Running avgs for agent 0: q_loss: 0.03515730798244476, p_loss: 0.6373595595359802, mean_rew: -0.1425466540022699, variance: 0.04960832640714943, mean_q: -0.6471304893493652, std_q: 1.5887805223464966
Running avgs for agent 1: q_loss: 0.03822821378707886, p_loss: 0.15791834890842438, mean_rew: -0.12866726593178884, variance: 0.043230750864371656, mean_q: -0.16794215142726898, std_q: 1.6653386354446411
Running avgs for agent 2: q_loss: 0.013923670165240765, p_loss: 1.6554430723190308, mean_rew: -0.3328296969175408, variance: 0.04006742591224611, mean_q: -1.7611387968063354, std_q: 1.80933678150177
Running avgs for agent 3: q_loss: 0.011725988239049911, p_loss: 2.046966791152954, mean_rew: -0.3155583194927414, variance: 0.04471114256978035, mean_q: -2.093834161758423, std_q: 1.8408955335617065

steps: 1374975, episodes: 55000, mean episode reward: -25.773634000345655, agent episode reward: [-3.0493695445579396, -2.6887218002543625, -11.834996591731368, -8.200546063801987], time: 55.136
steps: 1374975, episodes: 55000, mean episode variance: 0.04082808973151259, agent episode variance: [0.011426268701907247, 0.010900180976372213, 0.008941109084524215, 0.009560530968708918], time: 55.136
Running avgs for agent 0: q_loss: 0.03476863354444504, p_loss: 0.589672327041626, mean_rew: -0.1414252879013125, variance: 0.04570507480762899, mean_q: -0.5989353656768799, std_q: 1.5745176076889038
Running avgs for agent 1: q_loss: 0.04043668508529663, p_loss: 0.08255179226398468, mean_rew: -0.1269161461564497, variance: 0.04360072390548885, mean_q: -0.09192457795143127, std_q: 1.6681517362594604
Running avgs for agent 2: q_loss: 0.011165102012455463, p_loss: 1.676355242729187, mean_rew: -0.3366790273626597, variance: 0.03576443633809686, mean_q: -1.7495440244674683, std_q: 1.8234071731567383
Running avgs for agent 3: q_loss: 0.011976765468716621, p_loss: 2.03200626373291, mean_rew: -0.31536461751060235, variance: 0.03824212387483567, mean_q: -2.086859941482544, std_q: 1.8110525608062744

steps: 1399975, episodes: 56000, mean episode reward: -25.394429535370204, agent episode reward: [-3.9342558065521676, -3.3252341110367696, -10.562878961388767, -7.572060656392498], time: 55.191
steps: 1399975, episodes: 56000, mean episode variance: 0.04611980034445878, agent episode variance: [0.010891236230032518, 0.011259872192051261, 0.011931975597632118, 0.012036716324742883], time: 55.192
Running avgs for agent 0: q_loss: 0.03426802158355713, p_loss: 0.5647628307342529, mean_rew: -0.13898906626259228, variance: 0.04356494492013007, mean_q: -0.5726901292800903, std_q: 1.5606563091278076
Running avgs for agent 1: q_loss: 0.040636636316776276, p_loss: 0.00146866322029382, mean_rew: -0.12523861833428146, variance: 0.045039488768205044, mean_q: -0.010585717856884003, std_q: 1.670128583908081
Running avgs for agent 2: q_loss: 0.010943517088890076, p_loss: 1.808801293373108, mean_rew: -0.34126162068141414, variance: 0.04772790239052847, mean_q: -1.8584308624267578, std_q: 1.7873700857162476
Running avgs for agent 3: q_loss: 0.01330168079584837, p_loss: 1.9965156316757202, mean_rew: -0.31755646520943115, variance: 0.04814686529897153, mean_q: -2.063645124435425, std_q: 1.8300532102584839

steps: 1424975, episodes: 57000, mean episode reward: -24.315894580251218, agent episode reward: [-3.8336664316203977, -3.756080867475105, -9.319770870175114, -7.406376410980598], time: 54.754
steps: 1424975, episodes: 57000, mean episode variance: 0.048678599891485645, agent episode variance: [0.010329756675753742, 0.010627098369179293, 0.016301375056151302, 0.01142036979040131], time: 54.755
Running avgs for agent 0: q_loss: 0.032113999128341675, p_loss: 0.5835121870040894, mean_rew: -0.13963996636281537, variance: 0.04131902670301497, mean_q: -0.5909018516540527, std_q: 1.5636974573135376
Running avgs for agent 1: q_loss: 0.039823632687330246, p_loss: -0.029528042301535606, mean_rew: -0.12491565858192671, variance: 0.04250839347671717, mean_q: 0.021214740350842476, std_q: 1.6718132495880127
Running avgs for agent 2: q_loss: 0.011509096249938011, p_loss: 1.896097183227539, mean_rew: -0.34326142382139235, variance: 0.06520550022460521, mean_q: -1.9659355878829956, std_q: 1.7453019618988037
Running avgs for agent 3: q_loss: 0.012800931930541992, p_loss: 1.9130791425704956, mean_rew: -0.3167544213303384, variance: 0.04568147916160524, mean_q: -1.9809318780899048, std_q: 1.8533116579055786

steps: 1449975, episodes: 58000, mean episode reward: -23.548511274084795, agent episode reward: [-3.7856558990003704, -3.530076360168978, -8.535959180245435, -7.696819834670012], time: 55.402
steps: 1449975, episodes: 58000, mean episode variance: 0.05186973791336641, agent episode variance: [0.009410218873526902, 0.010975767092313618, 0.020263576647266746, 0.011220175300259143], time: 55.403
Running avgs for agent 0: q_loss: 0.030748605728149414, p_loss: 0.5897561311721802, mean_rew: -0.14080388304305558, variance: 0.037640875494107606, mean_q: -0.5973724126815796, std_q: 1.5858592987060547
Running avgs for agent 1: q_loss: 0.03939453884959221, p_loss: -0.01826326735317707, mean_rew: -0.12398858534745902, variance: 0.04390306836925447, mean_q: 0.010338298045098782, std_q: 1.6629836559295654
Running avgs for agent 2: q_loss: 0.013420186936855316, p_loss: 1.8965016603469849, mean_rew: -0.34633152037110154, variance: 0.08105430658906698, mean_q: -1.990154504776001, std_q: 1.77828049659729
Running avgs for agent 3: q_loss: 0.013454730622470379, p_loss: 1.8677613735198975, mean_rew: -0.3181378729978588, variance: 0.04488070120103657, mean_q: -1.9288368225097656, std_q: 1.899839997291565

steps: 1474975, episodes: 59000, mean episode reward: -22.721100728587935, agent episode reward: [-3.474457834918673, -3.3352596542276007, -7.9152960412767746, -7.9960871981648864], time: 57.461
steps: 1474975, episodes: 59000, mean episode variance: 0.049141123618930575, agent episode variance: [0.01034378912532702, 0.010610804778989404, 0.017719484551809727, 0.010467045162804424], time: 57.461
Running avgs for agent 0: q_loss: 0.030652891844511032, p_loss: 0.5792366862297058, mean_rew: -0.13825357951568368, variance: 0.04137515650130808, mean_q: -0.5865931510925293, std_q: 1.5876784324645996
Running avgs for agent 1: q_loss: 0.0384952612221241, p_loss: 0.007533903233706951, mean_rew: -0.1248398805342212, variance: 0.042443219115957616, mean_q: -0.01618817448616028, std_q: 1.6492642164230347
Running avgs for agent 2: q_loss: 0.014967257156968117, p_loss: 1.8177375793457031, mean_rew: -0.3468628016782963, variance: 0.07087793820723891, mean_q: -1.9294573068618774, std_q: 1.8242275714874268
Running avgs for agent 3: q_loss: 0.014432215131819248, p_loss: 1.836618423461914, mean_rew: -0.31852870345005635, variance: 0.041868180651217696, mean_q: -1.8959969282150269, std_q: 1.8863074779510498

steps: 1499975, episodes: 60000, mean episode reward: -23.564309830536242, agent episode reward: [-3.694592293409637, -3.8990512200940866, -7.191779945694723, -8.778886371337794], time: 56.817
steps: 1499975, episodes: 60000, mean episode variance: 0.0637365257279016, agent episode variance: [0.012144312312360853, 0.010430621433537453, 0.02575016933120787, 0.01541142265079543], time: 56.817
Running avgs for agent 0: q_loss: 0.0316801555454731, p_loss: 0.5880389213562012, mean_rew: -0.13916030777060434, variance: 0.04857724924944341, mean_q: -0.595312237739563, std_q: 1.6163040399551392
Running avgs for agent 1: q_loss: 0.038203634321689606, p_loss: 0.014884437434375286, mean_rew: -0.1232080640999535, variance: 0.041722485734149814, mean_q: -0.02311968244612217, std_q: 1.6360185146331787
Running avgs for agent 2: q_loss: 0.017480529844760895, p_loss: 1.7174125909805298, mean_rew: -0.34607815666936004, variance: 0.10300067732483148, mean_q: -1.8324347734451294, std_q: 1.8596384525299072
Running avgs for agent 3: q_loss: 0.016631340608000755, p_loss: 1.8066456317901611, mean_rew: -0.3192030295153762, variance: 0.06164569060318172, mean_q: -1.869327187538147, std_q: 1.8686916828155518

...Finished total of 60001 episodes... Now freezing policy and running for 10000 more episodes to get cvar estimation
steps: 24975, episodes: 1000, mean episode reward: -23.57684645624242, agent episode reward: [-3.713529815392697, -3.6129249107180126, -7.327644704515579, -8.922747025616133], time: 37.113
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 37.113
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -23.570195087925217, agent episode reward: [-3.8513074396286058, -3.6175527378731602, -7.28652642249535, -8.814808487928103], time: 49.835
steps: 49975, episodes: 2000, mean episode variance: 0.10340811622655019, agent episode variance: [0.0011044602794572712, 0.014799385590478779, 0.007198440012056381, 0.08030583034455777], time: 49.836
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14993070830520702, variance: 0.004526476555152751, cvar: 0.8040043711662292, v: 0.23311489820480347, mean_q: -0.5666691660881042, std_q: 1.6165424585342407
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14474698563004507, variance: 0.060653219633109746, cvar: 1.3007721900939941, v: 0.8082805871963501, mean_q: 0.018362415954470634, std_q: 1.601818323135376
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.28996424608762456, variance: 0.02950180332809992, cvar: -0.5812228918075562, v: -0.6255090832710266, mean_q: -1.6128100156784058, std_q: 1.9137085676193237
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35649643244250295, variance: 0.32912227511405945, cvar: -0.5074955224990845, v: -0.7370191216468811, mean_q: -1.7082748413085938, std_q: 1.9325871467590332

steps: 74975, episodes: 3000, mean episode reward: -23.957719667550496, agent episode reward: [-3.8718616655709743, -3.697941898299299, -7.354254097808184, -9.03366200587204], time: 50.385
steps: 74975, episodes: 3000, mean episode variance: 0.10663999677984976, agent episode variance: [0.0010154778198339044, 0.015257914735935628, 0.007402583554619923, 0.08296402066946029], time: 50.386
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15285802689262665, variance: 0.004061911279335618, cvar: 0.7533133029937744, v: 0.20334625244140625, mean_q: -0.583682119846344, std_q: 1.6212868690490723
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14568025038953125, variance: 0.06103165894374251, cvar: 1.2900325059890747, v: 0.8045864105224609, mean_q: 0.015019187703728676, std_q: 1.5916836261749268
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2917118597796979, variance: 0.02961033421847969, cvar: -0.6926542520523071, v: -0.7204075455665588, mean_q: -1.6233928203582764, std_q: 1.9451924562454224
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35571857378673644, variance: 0.3318560719490051, cvar: -0.6134401559829712, v: -0.851651668548584, mean_q: -1.7022910118103027, std_q: 1.9237016439437866

steps: 99975, episodes: 4000, mean episode reward: -23.896210622475017, agent episode reward: [-3.8516360531411955, -3.839598495669132, -7.413239719129379, -8.791736354535312], time: 52.191
steps: 99975, episodes: 4000, mean episode variance: 0.10331704988656566, agent episode variance: [0.000503162631066516, 0.013931829357519746, 0.006831039227778092, 0.0820510186702013], time: 52.191
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1534328583002018, variance: 0.002012650524266064, cvar: 0.7480932474136353, v: 0.2050556093454361, mean_q: -0.590255081653595, std_q: 1.6406009197235107
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1468781237283057, variance: 0.055727317430078985, cvar: 1.292651653289795, v: 0.8053657412528992, mean_q: 0.010520312935113907, std_q: 1.6065349578857422
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2932756515714507, variance: 0.02732415691111237, cvar: -0.6928783655166626, v: -0.7207400798797607, mean_q: -1.6288608312606812, std_q: 1.9454067945480347
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35417323537724227, variance: 0.32820406556129456, cvar: -0.6220837831497192, v: -0.8523361086845398, mean_q: -1.7003469467163086, std_q: 1.9153621196746826

steps: 124975, episodes: 5000, mean episode reward: -23.739190168753506, agent episode reward: [-3.7359347520748205, -3.8248747421002536, -7.341948809381808, -8.836431865196625], time: 51.251
steps: 124975, episodes: 5000, mean episode variance: 0.10341939468681813, agent episode variance: [0.0006613279804587364, 0.014890213852748274, 0.0059994505513459445, 0.08186840230226516], time: 51.251
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1530734052856422, variance: 0.0026453119218349456, cvar: 0.7457206845283508, v: 0.2062099426984787, mean_q: -0.5901601910591125, std_q: 1.6319737434387207
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1479158399048685, variance: 0.0595608554109931, cvar: 1.2926397323608398, v: 0.8068256974220276, mean_q: 0.005089972168207169, std_q: 1.6162631511688232
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2945438395840245, variance: 0.023997802205383778, cvar: -0.693577229976654, v: -0.7215556502342224, mean_q: -1.635014533996582, std_q: 1.9473140239715576
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3563195115135786, variance: 0.32747361063957214, cvar: -0.6294187307357788, v: -0.8524518013000488, mean_q: -1.7121903896331787, std_q: 1.9386775493621826

steps: 149975, episodes: 6000, mean episode reward: -24.493835576753703, agent episode reward: [-4.083333052599973, -3.7992165220007523, -7.451058125073863, -9.160227877079112], time: 51.663
steps: 149975, episodes: 6000, mean episode variance: 0.10505293428525328, agent episode variance: [0.0007863736771978437, 0.01584941943315789, 0.005703050022944808, 0.08271409115195275], time: 51.664
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15301436467679463, variance: 0.003145494708791375, cvar: 0.762849748134613, v: 0.2087053805589676, mean_q: -0.5921276807785034, std_q: 1.6534591913223267
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14896259965955538, variance: 0.06339767773263157, cvar: 1.2796838283538818, v: 0.8050680756568909, mean_q: -0.003937523812055588, std_q: 1.6250133514404297
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29493895559678335, variance: 0.02281220009177923, cvar: -0.6939758658409119, v: -0.7220515012741089, mean_q: -1.6401870250701904, std_q: 1.9642750024795532
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35543253185285484, variance: 0.3308563530445099, cvar: -0.6323906779289246, v: -0.8537225723266602, mean_q: -1.7061249017715454, std_q: 1.9275307655334473

steps: 174975, episodes: 7000, mean episode reward: -23.553360772098365, agent episode reward: [-3.734262748738764, -3.724184347161722, -7.417998707692958, -8.676914968504915], time: 51.752
steps: 174975, episodes: 7000, mean episode variance: 0.10549796916474588, agent episode variance: [0.0004884763222653419, 0.01634087889827788, 0.0058977951146662235, 0.08277081882953644], time: 51.753
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15504719017286878, variance: 0.0019539052890613674, cvar: 0.7511798143386841, v: 0.21268680691719055, mean_q: -0.6045454144477844, std_q: 1.668363094329834
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1494800766657818, variance: 0.06536351559311152, cvar: 1.2821542024612427, v: 0.8055861592292786, mean_q: -0.009398643858730793, std_q: 1.6334638595581055
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2942504985973799, variance: 0.023591180458664894, cvar: -0.693517804145813, v: -0.7218856811523438, mean_q: -1.6367695331573486, std_q: 1.9590786695480347
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35720163428878177, variance: 0.3310832679271698, cvar: -0.624559223651886, v: -0.8543267250061035, mean_q: -1.7098721265792847, std_q: 1.9182969331741333

steps: 199975, episodes: 8000, mean episode reward: -23.39310422580878, agent episode reward: [-3.8958304315504284, -3.485850076079729, -7.310544751308806, -8.700878966869816], time: 51.581
steps: 199975, episodes: 8000, mean episode variance: 0.10529570336453617, agent episode variance: [0.0007249055448919535, 0.015373515316285193, 0.005924840907566249, 0.08327244159579276], time: 51.581
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15304410539353422, variance: 0.002899622179567814, cvar: 0.7437517642974854, v: 0.2093869298696518, mean_q: -0.5920446515083313, std_q: 1.640304446220398
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1479186633683986, variance: 0.06149406126514077, cvar: 1.2962051630020142, v: 0.8066708445549011, mean_q: -0.0006295657367445529, std_q: 1.6250869035720825
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2940725257861541, variance: 0.023699363630264998, cvar: -0.6930791139602661, v: -0.7214397192001343, mean_q: -1.6349477767944336, std_q: 1.9532053470611572
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3550325574449264, variance: 0.33308976888656616, cvar: -0.6268352270126343, v: -0.8545304536819458, mean_q: -1.6992422342300415, std_q: 1.9103412628173828

steps: 224975, episodes: 9000, mean episode reward: -23.73608424555408, agent episode reward: [-4.098095731144011, -3.7161149067495236, -7.252820917265409, -8.669052690395128], time: 52.979
steps: 224975, episodes: 9000, mean episode variance: 0.10626447041449137, agent episode variance: [0.0007777844727970659, 0.0170148752219975, 0.006306356630055234, 0.08216545408964157], time: 52.979
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15503897533944974, variance: 0.0031111378911882637, cvar: 0.7367653250694275, v: 0.20830483734607697, mean_q: -0.5983352065086365, std_q: 1.641377329826355
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14874180880734275, variance: 0.06805950088799, cvar: 1.2935527563095093, v: 0.8077554702758789, mean_q: -0.00399521691724658, std_q: 1.6328874826431274
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2955475285623918, variance: 0.025225426520220934, cvar: -0.6932923793792725, v: -0.7212159037590027, mean_q: -1.6423089504241943, std_q: 1.9667963981628418
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3558246218016827, variance: 0.32866179943084717, cvar: -0.6225649118423462, v: -0.8534570932388306, mean_q: -1.7036511898040771, std_q: 1.9083778858184814

steps: 249975, episodes: 10000, mean episode reward: -23.725046705707225, agent episode reward: [-3.9671342695829623, -3.5606421711212226, -7.287012322364595, -8.910257942638443], time: 51.765
steps: 249975, episodes: 10000, mean episode variance: 0.1044442652198486, agent episode variance: [0.0004652003422379494, 0.01572758423211053, 0.006778339556418359, 0.08147314108908177], time: 51.766
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15434942936169752, variance: 0.0018608013689517975, cvar: 0.7328317761421204, v: 0.21087627112865448, mean_q: -0.5993338823318481, std_q: 1.6533056497573853
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14856192470777269, variance: 0.06291033692844213, cvar: 1.2927329540252686, v: 0.8082345128059387, mean_q: 0.0031117640901356936, std_q: 1.6265050172805786
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29303708052022553, variance: 0.027113358225673435, cvar: -0.693284273147583, v: -0.7215498685836792, mean_q: -1.6303538084030151, std_q: 1.9446213245391846
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3539010201984446, variance: 0.3258925676345825, cvar: -0.6229936480522156, v: -0.854659914970398, mean_q: -1.6903213262557983, std_q: 1.8815829753875732

steps: 274975, episodes: 11000, mean episode reward: -23.68797106266992, agent episode reward: [-3.7786424872832574, -3.8092970528728447, -7.280929215882315, -8.819102306631503], time: 51.776
steps: 274975, episodes: 11000, mean episode variance: 0.10543245185399427, agent episode variance: [0.0006338427769951523, 0.016201708592940123, 0.006016109129879624, 0.08258079135417938], time: 51.777
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15517853291284195, variance: 0.002535371107980609, cvar: 0.7456509470939636, v: 0.21052370965480804, mean_q: -0.5994459390640259, std_q: 1.6525770425796509
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14728844880561762, variance: 0.06480683437176049, cvar: 1.2963955402374268, v: 0.807727038860321, mean_q: 0.004005545750260353, std_q: 1.6292649507522583
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29551321257533564, variance: 0.024064436519518496, cvar: -0.6935014724731445, v: -0.7216938138008118, mean_q: -1.6408002376556396, std_q: 1.9633855819702148
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35436903348554905, variance: 0.33032315969467163, cvar: -0.6245027780532837, v: -0.8549462556838989, mean_q: -1.6945881843566895, std_q: 1.9029985666275024

steps: 299975, episodes: 12000, mean episode reward: -24.01463714772441, agent episode reward: [-3.9892580462276475, -3.956295707471784, -7.119110162737263, -8.949973231287716], time: 51.579
steps: 299975, episodes: 12000, mean episode variance: 0.10338610611855983, agent episode variance: [0.00048030126001685856, 0.016405091310385613, 0.006050962632987648, 0.08044975091516972], time: 51.58
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15504581152765215, variance: 0.0019212050400674342, cvar: 0.7441739439964294, v: 0.20840054750442505, mean_q: -0.5980218052864075, std_q: 1.6479003429412842
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14811254934889487, variance: 0.06562036524154245, cvar: 1.2993214130401611, v: 0.81003737449646, mean_q: -0.0014233837136998773, std_q: 1.6353236436843872
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29511479788990325, variance: 0.024203850531950594, cvar: -0.69302898645401, v: -0.7213221192359924, mean_q: -1.6383785009384155, std_q: 1.947778344154358
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35514746994683105, variance: 0.32179901003837585, cvar: -0.6250476241111755, v: -0.8548706769943237, mean_q: -1.6980924606323242, std_q: 1.894832968711853

steps: 324975, episodes: 13000, mean episode reward: -24.30363297340097, agent episode reward: [-4.060729253105205, -3.7519153674105046, -7.46939593540773, -9.021592417477528], time: 52.541
steps: 324975, episodes: 13000, mean episode variance: 0.10402296229125932, agent episode variance: [0.0009215396656654775, 0.016686885116621853, 0.006048450519330799, 0.08036608698964119], time: 52.541
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15635049886134983, variance: 0.00368615866266191, cvar: 0.7407757639884949, v: 0.21111051738262177, mean_q: -0.6071915030479431, std_q: 1.6621801853179932
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14778559903245897, variance: 0.06674754046648741, cvar: 1.2975873947143555, v: 0.809386134147644, mean_q: 0.001962893409654498, std_q: 1.6267309188842773
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.293005811292879, variance: 0.024193802077323197, cvar: -0.6933294534683228, v: -0.7215867638587952, mean_q: -1.6301605701446533, std_q: 1.9448440074920654
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3540742698782139, variance: 0.3214643895626068, cvar: -0.621017336845398, v: -0.8545079827308655, mean_q: -1.6934248208999634, std_q: 1.888696551322937

steps: 349975, episodes: 14000, mean episode reward: -23.79776633765923, agent episode reward: [-3.7625744605900473, -3.6785261263433657, -7.356265307648044, -9.00040044307777], time: 51.874
steps: 349975, episodes: 14000, mean episode variance: 0.10558177651651203, agent episode variance: [0.0007391254086978734, 0.017017696546390654, 0.005768042943906039, 0.08205691161751748], time: 51.875
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1556033384974386, variance: 0.0029565016347914935, cvar: 0.7482204437255859, v: 0.21020670235157013, mean_q: -0.603392481803894, std_q: 1.6630747318267822
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14884457518616961, variance: 0.06807078618556262, cvar: 1.2901864051818848, v: 0.8081696629524231, mean_q: -0.005631847772747278, std_q: 1.6359225511550903
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2915938532808344, variance: 0.023072171775624158, cvar: -0.6931542158126831, v: -0.7210504412651062, mean_q: -1.6230629682540894, std_q: 1.930413842201233
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35549666159026017, variance: 0.3282276391983032, cvar: -0.6251524686813354, v: -0.8545653820037842, mean_q: -1.7006490230560303, std_q: 1.9024592638015747

steps: 374975, episodes: 15000, mean episode reward: -23.70811317478474, agent episode reward: [-3.620894747932951, -3.606341741025648, -7.541641067854016, -8.939235617972122], time: 52.009
steps: 374975, episodes: 15000, mean episode variance: 0.10471588720125147, agent episode variance: [0.0007263003271073102, 0.016301844503730536, 0.006605378437088802, 0.08108236393332481], time: 52.009
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1541656091592888, variance: 0.002905201308429241, cvar: 0.7587888240814209, v: 0.21302103996276855, mean_q: -0.5945391058921814, std_q: 1.6560016870498657
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14846621631577012, variance: 0.06520737801492214, cvar: 1.305643081665039, v: 0.8104385137557983, mean_q: -0.00021926515910308808, std_q: 1.6362913846969604
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29256063755845185, variance: 0.02642151374835521, cvar: -0.6933576464653015, v: -0.7216217517852783, mean_q: -1.6268404722213745, std_q: 1.9315906763076782
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35718085538026595, variance: 0.3243294656276703, cvar: -0.6204342842102051, v: -0.8541513681411743, mean_q: -1.7114217281341553, std_q: 1.9257510900497437

steps: 399975, episodes: 16000, mean episode reward: -23.65458743153333, agent episode reward: [-4.020866803986543, -3.545817867817079, -7.195335726594962, -8.892567033134743], time: 51.339
steps: 399975, episodes: 16000, mean episode variance: 0.10439915162185207, agent episode variance: [0.0010264920978806913, 0.01740198268601671, 0.005706901191268116, 0.08026377564668655], time: 51.339
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1556955352129461, variance: 0.004105968391522765, cvar: 0.745771586894989, v: 0.2062961906194687, mean_q: -0.6020984053611755, std_q: 1.6561782360076904
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14947157200278297, variance: 0.06960793074406683, cvar: 1.2918840646743774, v: 0.8085689544677734, mean_q: -0.005185691639780998, std_q: 1.6327250003814697
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2932716005996135, variance: 0.022827604765072465, cvar: -0.6937946677207947, v: -0.721678614616394, mean_q: -1.6299498081207275, std_q: 1.937564492225647
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35566990668155063, variance: 0.3210550844669342, cvar: -0.6192454695701599, v: -0.8551048636436462, mean_q: -1.70151686668396, std_q: 1.9042637348175049

steps: 424975, episodes: 17000, mean episode reward: -24.098638986542138, agent episode reward: [-4.019163024990929, -3.672608717875072, -7.375894236587495, -9.030973007088638], time: 51.31
steps: 424975, episodes: 17000, mean episode variance: 0.1035962791161146, agent episode variance: [0.0005526659756433219, 0.016546163260936737, 0.005301828716881573, 0.08119562116265297], time: 51.31
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15553638840966857, variance: 0.0022106639025732877, cvar: 0.7431287169456482, v: 0.20941391587257385, mean_q: -0.602361798286438, std_q: 1.6566486358642578
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14912230225911763, variance: 0.06618465304374695, cvar: 1.2959462404251099, v: 0.8095511198043823, mean_q: -0.005038104020059109, std_q: 1.6423957347869873
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2926337538767201, variance: 0.021207314867526292, cvar: -0.6934637427330017, v: -0.7214577794075012, mean_q: -1.6283739805221558, std_q: 1.9365873336791992
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35591916542956015, variance: 0.32478246092796326, cvar: -0.6200196146965027, v: -0.855053722858429, mean_q: -1.7056649923324585, std_q: 1.9177337884902954

steps: 449975, episodes: 18000, mean episode reward: -23.769486520590508, agent episode reward: [-3.5781441438943657, -3.3286987420184024, -7.633555729818421, -9.22908790485932], time: 51.38
steps: 449975, episodes: 18000, mean episode variance: 0.10394982194248586, agent episode variance: [0.000786331751383841, 0.016486110880039633, 0.005445650906302035, 0.08123172840476037], time: 51.38
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15505639312016078, variance: 0.003145327005535364, cvar: 0.7483662962913513, v: 0.2095056176185608, mean_q: -0.6021294593811035, std_q: 1.6651301383972168
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1478204804717331, variance: 0.06594444352015853, cvar: 1.3031799793243408, v: 0.8088918924331665, mean_q: 0.0006940376479178667, std_q: 1.6302155256271362
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2959986267534208, variance: 0.02178260362520814, cvar: -0.6937280297279358, v: -0.7220661640167236, mean_q: -1.64350163936615, std_q: 1.9571359157562256
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.356869802399239, variance: 0.3249269425868988, cvar: -0.6258013844490051, v: -0.8551551699638367, mean_q: -1.7089987993240356, std_q: 1.920428991317749

steps: 474975, episodes: 19000, mean episode reward: -23.747269920701708, agent episode reward: [-3.900656137345088, -3.6598946577182803, -7.4966155354749375, -8.690103590163401], time: 51.315
steps: 474975, episodes: 19000, mean episode variance: 0.10435607919679023, agent episode variance: [0.0006460724151693284, 0.017007841669023038, 0.005758154080016538, 0.08094401103258132], time: 51.315
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15519540621272376, variance: 0.0025842896606773137, cvar: 0.753219485282898, v: 0.211199551820755, mean_q: -0.6048758029937744, std_q: 1.6647841930389404
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14724223012398863, variance: 0.06803136667609215, cvar: 1.3001794815063477, v: 0.8096683621406555, mean_q: 0.0008723101927898824, std_q: 1.6349575519561768
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2931085797389447, variance: 0.023032616320066153, cvar: -0.6934443712234497, v: -0.721572995185852, mean_q: -1.630604863166809, std_q: 1.937082290649414
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3560999748774577, variance: 0.3237760365009308, cvar: -0.6201200485229492, v: -0.854863703250885, mean_q: -1.7038860321044922, std_q: 1.9061640501022339

steps: 499975, episodes: 20000, mean episode reward: -23.946394184681484, agent episode reward: [-4.074790677230221, -3.826018751388435, -7.389609914396633, -8.655974841666195], time: 51.429
steps: 499975, episodes: 20000, mean episode variance: 0.10342841405584477, agent episode variance: [0.0006069647027179599, 0.016505491575226187, 0.005584882371826097, 0.08073107540607452], time: 51.43
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15531981965389202, variance: 0.0024278588108718395, cvar: 0.747206449508667, v: 0.2103436291217804, mean_q: -0.6014594435691833, std_q: 1.6594706773757935
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14769055966915698, variance: 0.06602196630090475, cvar: 1.3035112619400024, v: 0.8097777366638184, mean_q: 0.0004423756035976112, std_q: 1.6319822072982788
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29451211895689633, variance: 0.022339529487304388, cvar: -0.6932601928710938, v: -0.7215802669525146, mean_q: -1.6372172832489014, std_q: 1.9466464519500732
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3559466952665475, variance: 0.32292431592941284, cvar: -0.6162735819816589, v: -0.8545047640800476, mean_q: -1.7047771215438843, std_q: 1.9133875370025635

steps: 524975, episodes: 21000, mean episode reward: -23.344362955886186, agent episode reward: [-3.675941081952952, -3.760959925052726, -7.185404549006226, -8.722057399874284], time: 51.341
steps: 524975, episodes: 21000, mean episode variance: 0.10467624474433251, agent episode variance: [0.0010258632302284241, 0.017263837135862558, 0.005748460890492424, 0.0806380834877491], time: 51.342
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1554446559062328, variance: 0.0041034529209136965, cvar: 0.7458560466766357, v: 0.20819376409053802, mean_q: -0.6039681434631348, std_q: 1.6610546112060547
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14759687240621108, variance: 0.06905534854345023, cvar: 1.294992446899414, v: 0.8102037906646729, mean_q: 0.000904459971934557, std_q: 1.6335726976394653
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2952362367412863, variance: 0.022993843561969696, cvar: -0.6937519907951355, v: -0.7216840982437134, mean_q: -1.6392936706542969, std_q: 1.9506381750106812
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35445787758477426, variance: 0.32255232334136963, cvar: -0.6229345798492432, v: -0.8550050258636475, mean_q: -1.6970359086990356, std_q: 1.9005478620529175

steps: 549975, episodes: 22000, mean episode reward: -24.17280188403158, agent episode reward: [-3.971728452951159, -3.736803439145419, -7.404261627063173, -9.060008364871827], time: 51.428
steps: 549975, episodes: 22000, mean episode variance: 0.10294255253509618, agent episode variance: [0.0007833482632413506, 0.016264322305563836, 0.005400111603317782, 0.08049477036297321], time: 51.428
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15549650515645527, variance: 0.0031333930529654024, cvar: 0.7657793164253235, v: 0.2097265124320984, mean_q: -0.6017451286315918, std_q: 1.6584025621414185
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14635984323254156, variance: 0.06505728922225534, cvar: 1.3068811893463135, v: 0.8107173442840576, mean_q: 0.009482764638960361, std_q: 1.6212657690048218
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29511094420383016, variance: 0.021600446413271128, cvar: -0.6933585405349731, v: -0.7213693857192993, mean_q: -1.6395002603530884, std_q: 1.9474083185195923
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3549058139239787, variance: 0.3219790756702423, cvar: -0.624253511428833, v: -0.854873776435852, mean_q: -1.6993002891540527, std_q: 1.9023264646530151

steps: 574975, episodes: 23000, mean episode reward: -23.8123166896834, agent episode reward: [-3.9665474244137666, -3.744325813643939, -7.155887534902459, -8.945555916723237], time: 51.515
steps: 574975, episodes: 23000, mean episode variance: 0.10412729161023163, agent episode variance: [0.0005155681949108839, 0.01725004335399717, 0.00511776250670664, 0.08124391755461693], time: 51.515
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1557644644370816, variance: 0.0020622727796435355, cvar: 0.7477831840515137, v: 0.20983447134494781, mean_q: -0.6079277396202087, std_q: 1.6731693744659424
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14838676800557873, variance: 0.06900017341598869, cvar: 1.3014357089996338, v: 0.8095849752426147, mean_q: -0.004506474360823631, std_q: 1.6384247541427612
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2954442021826951, variance: 0.02047105002682656, cvar: -0.6937635540962219, v: -0.7219428420066833, mean_q: -1.6393063068389893, std_q: 1.9415644407272339
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3570229713598263, variance: 0.3249756693840027, cvar: -0.6294095516204834, v: -0.8558359146118164, mean_q: -1.7109545469284058, std_q: 1.9262999296188354

steps: 599975, episodes: 24000, mean episode reward: -23.890714871237062, agent episode reward: [-3.8280563297959707, -3.7564602922323878, -7.271714182878392, -9.034484066330315], time: 51.582
steps: 599975, episodes: 24000, mean episode variance: 0.1049211862430675, agent episode variance: [0.0006156112677417696, 0.01779604662337806, 0.005611517352983355, 0.0808980109989643], time: 51.583
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15514563264418177, variance: 0.0024624450709670783, cvar: 0.7417317032814026, v: 0.21289706230163574, mean_q: -0.6038723587989807, std_q: 1.661767601966858
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1481774500959261, variance: 0.07118418649351224, cvar: 1.296355962753296, v: 0.8094127178192139, mean_q: -0.0038799401372671127, std_q: 1.6368080377578735
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2931048403041892, variance: 0.02244606941193342, cvar: -0.6938313245773315, v: -0.7220414876937866, mean_q: -1.6309148073196411, std_q: 1.9377371072769165
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3567731315750031, variance: 0.32359203696250916, cvar: -0.6249117255210876, v: -0.8547341227531433, mean_q: -1.7103817462921143, std_q: 1.9247760772705078

steps: 624975, episodes: 25000, mean episode reward: -23.920632850967813, agent episode reward: [-3.819245088559387, -3.648783072832679, -7.640790241515471, -8.811814448060277], time: 51.406
steps: 624975, episodes: 25000, mean episode variance: 0.10448212698055431, agent episode variance: [0.0008846414685249329, 0.017194358298555017, 0.005370759060140699, 0.08103236815333366], time: 51.407
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15541198722665872, variance: 0.0035385658740997314, cvar: 0.7429293394088745, v: 0.20937810838222504, mean_q: -0.6073559522628784, std_q: 1.668478012084961
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14850502962192988, variance: 0.06877743319422007, cvar: 1.2982302904129028, v: 0.8079367876052856, mean_q: -0.0024308953434228897, std_q: 1.6332697868347168
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2952991119105106, variance: 0.021483036240562796, cvar: -0.6933803558349609, v: -0.7215770483016968, mean_q: -1.63899564743042, std_q: 1.945388674736023
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35653732220286954, variance: 0.32412946224212646, cvar: -0.6218886375427246, v: -0.8542628884315491, mean_q: -1.7083691358566284, std_q: 1.927148461341858

steps: 649975, episodes: 26000, mean episode reward: -23.54426325486385, agent episode reward: [-3.8363366912703754, -3.685990647280957, -7.070297116127361, -8.95163880018516], time: 51.372
steps: 649975, episodes: 26000, mean episode variance: 0.10485225794650614, agent episode variance: [0.0009324117146898061, 0.017517168363090606, 0.005375222315313294, 0.08102745555341244], time: 51.373
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15451511540803156, variance: 0.0037296468587592244, cvar: 0.7443772554397583, v: 0.21174825727939606, mean_q: -0.6020767092704773, std_q: 1.6595271825790405
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14902888384808727, variance: 0.07006867345236242, cvar: 1.2909653186798096, v: 0.8101474642753601, mean_q: -0.004000226967036724, std_q: 1.6297821998596191
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29465070038532803, variance: 0.021500889261253178, cvar: -0.6933260560035706, v: -0.7213813662528992, mean_q: -1.6368982791900635, std_q: 1.9484856128692627
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3566833392253894, variance: 0.32410985231399536, cvar: -0.6257700324058533, v: -0.8547496795654297, mean_q: -1.7107237577438354, std_q: 1.9244706630706787

steps: 674975, episodes: 27000, mean episode reward: -24.07094304285537, agent episode reward: [-3.7049584369400517, -3.8711984427663237, -7.424125560978826, -9.070660602170166], time: 51.454
steps: 674975, episodes: 27000, mean episode variance: 0.10435551762231626, agent episode variance: [0.0006722347652539611, 0.01702191748144105, 0.005665055722696707, 0.08099630965292454], time: 51.454
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15568064598232292, variance: 0.0026889390610158443, cvar: 0.7490047812461853, v: 0.21019701659679413, mean_q: -0.6062616109848022, std_q: 1.6598689556121826
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.148074482573227, variance: 0.0680876699257642, cvar: 1.2921457290649414, v: 0.8072057962417603, mean_q: -0.004688202403485775, std_q: 1.643627643585205
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29671601973720485, variance: 0.022660222890786827, cvar: -0.6933126449584961, v: -0.7212932109832764, mean_q: -1.6457990407943726, std_q: 1.9605575799942017
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.355696987833437, variance: 0.32398521900177, cvar: -0.6241860389709473, v: -0.8541059494018555, mean_q: -1.7035084962844849, std_q: 1.9115800857543945

steps: 699975, episodes: 28000, mean episode reward: -23.859100750531443, agent episode reward: [-3.8999629406518435, -3.5637424045076163, -7.438358170994255, -8.957037234377731], time: 51.465
steps: 699975, episodes: 28000, mean episode variance: 0.10455306886369363, agent episode variance: [0.0008745312001556158, 0.017095763199962675, 0.006617586288135499, 0.07996518817543984], time: 51.466
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15408734253413525, variance: 0.0034981248006224633, cvar: 0.7486748695373535, v: 0.21049344539642334, mean_q: -0.5963805913925171, std_q: 1.6457291841506958
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1483722533069577, variance: 0.0683830527998507, cvar: 1.29935884475708, v: 0.8090270757675171, mean_q: -0.005467116367071867, std_q: 1.6430678367614746
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2968564052867521, variance: 0.026470345152541996, cvar: -0.6929560303688049, v: -0.7210841178894043, mean_q: -1.6461924314498901, std_q: 1.9624775648117065
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3567070153352826, variance: 0.3198607563972473, cvar: -0.623078465461731, v: -0.854881227016449, mean_q: -1.710019826889038, std_q: 1.9238299131393433

steps: 724975, episodes: 29000, mean episode reward: -23.964831612304017, agent episode reward: [-3.870140682985306, -3.492556731796325, -7.68009892183457, -8.922035275687817], time: 51.477
steps: 724975, episodes: 29000, mean episode variance: 0.1056777444863692, agent episode variance: [0.0005764642944559455, 0.01781431505223736, 0.005771808229852468, 0.08151515690982342], time: 51.478
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15424000096023924, variance: 0.002305857177823782, cvar: 0.745041012763977, v: 0.21140208840370178, mean_q: -0.6015210747718811, std_q: 1.6572321653366089
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.149054092362975, variance: 0.07125726020894944, cvar: 1.2903393507003784, v: 0.8099488615989685, mean_q: -0.0057033030316233635, std_q: 1.6381288766860962
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2945647898004523, variance: 0.02308723291940987, cvar: -0.6937651634216309, v: -0.7215486764907837, mean_q: -1.6354635953903198, std_q: 1.9382073879241943
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3562223032070678, variance: 0.3260606527328491, cvar: -0.6274374723434448, v: -0.8553212881088257, mean_q: -1.706030011177063, std_q: 1.9166513681411743

steps: 749975, episodes: 30000, mean episode reward: -24.136711969851095, agent episode reward: [-3.9854250187546203, -3.91904392747413, -7.46629270028322, -8.76595032333912], time: 51.495
steps: 749975, episodes: 30000, mean episode variance: 0.10468291086098179, agent episode variance: [0.0006131488939281553, 0.01683236480690539, 0.0064157423784490675, 0.08082165478169918], time: 51.495
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15679242420407719, variance: 0.0024525955757126214, cvar: 0.7335470914840698, v: 0.2107335925102234, mean_q: -0.6128867268562317, std_q: 1.66879141330719
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1481570983786139, variance: 0.06732945922762155, cvar: 1.2989518642425537, v: 0.8090283274650574, mean_q: -0.0015477464767172933, std_q: 1.6332066059112549
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29303208508396605, variance: 0.02566296951379627, cvar: -0.6930875182151794, v: -0.7213553190231323, mean_q: -1.6283024549484253, std_q: 1.9313971996307373
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35643456665164625, variance: 0.32328662276268005, cvar: -0.6226153373718262, v: -0.8540621995925903, mean_q: -1.708889126777649, std_q: 1.9286227226257324

steps: 774975, episodes: 31000, mean episode reward: -23.82512678531342, agent episode reward: [-3.965152160626244, -3.6468534140896818, -7.318669672047818, -8.894451538549673], time: 51.52
steps: 774975, episodes: 31000, mean episode variance: 0.10356986915436574, agent episode variance: [0.0007296885079704225, 0.0162232304783538, 0.005603334813145921, 0.0810136153548956], time: 51.521
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15522504576009624, variance: 0.00291875403188169, cvar: 0.7439380884170532, v: 0.2128438651561737, mean_q: -0.6042414307594299, std_q: 1.661632776260376
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14792818872602725, variance: 0.0648929219134152, cvar: 1.2966667413711548, v: 0.8093212842941284, mean_q: -0.001509609748609364, std_q: 1.6299834251403809
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29411468585324996, variance: 0.022413339252583684, cvar: -0.6933576464653015, v: -0.7217528820037842, mean_q: -1.6331441402435303, std_q: 1.9362412691116333
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3563409277868911, variance: 0.32405444979667664, cvar: -0.6245365738868713, v: -0.854751706123352, mean_q: -1.7056633234024048, std_q: 1.9196938276290894

steps: 799975, episodes: 32000, mean episode reward: -23.81493843750624, agent episode reward: [-4.015644085984101, -3.712937524493726, -7.2876587173943586, -8.798698109634056], time: 52.042
steps: 799975, episodes: 32000, mean episode variance: 0.10424528275651392, agent episode variance: [0.0006434287740848958, 0.016267280550673602, 0.006565703703905456, 0.08076886972784995], time: 52.043
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15540867851329454, variance: 0.0025737150963395833, cvar: 0.7375547289848328, v: 0.21116207540035248, mean_q: -0.6019271016120911, std_q: 1.6543283462524414
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.147979384643775, variance: 0.06506912220269441, cvar: 1.2955361604690552, v: 0.8094459772109985, mean_q: 0.003960027825087309, std_q: 1.6196386814117432
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2945073379607834, variance: 0.026262814815621822, cvar: -0.6934613585472107, v: -0.7213439345359802, mean_q: -1.6364154815673828, std_q: 1.9478553533554077
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3573531760219116, variance: 0.32307547330856323, cvar: -0.6254128813743591, v: -0.8553791642189026, mean_q: -1.7130379676818848, std_q: 1.9296997785568237

steps: 824975, episodes: 33000, mean episode reward: -23.96455158122719, agent episode reward: [-4.174993799703426, -3.7020325344325564, -7.147531073551573, -8.939994173539636], time: 51.501
steps: 824975, episodes: 33000, mean episode variance: 0.10274823914002627, agent episode variance: [0.0005076097645796835, 0.016167124758474528, 0.005442760219331831, 0.08063074439764023], time: 51.501
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15573932294836854, variance: 0.002030439058318734, cvar: 0.7426811456680298, v: 0.21064360439777374, mean_q: -0.6032475829124451, std_q: 1.6605544090270996
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1477910342252027, variance: 0.06466849903389811, cvar: 1.3031936883926392, v: 0.8095330595970154, mean_q: -0.00021638644102495164, std_q: 1.6259859800338745
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2957984924896407, variance: 0.021771040877327324, cvar: -0.6935290694236755, v: -0.7220864295959473, mean_q: -1.6420717239379883, std_q: 1.9547590017318726
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35710313030908697, variance: 0.32252296805381775, cvar: -0.6241686940193176, v: -0.8548014760017395, mean_q: -1.708272933959961, std_q: 1.9193503856658936

steps: 849975, episodes: 34000, mean episode reward: -23.39039240909019, agent episode reward: [-3.8273042053971396, -3.6555095278054113, -7.250794696790681, -8.656783979096955], time: 51.373
steps: 849975, episodes: 34000, mean episode variance: 0.10376546059176325, agent episode variance: [0.0007490499713458121, 0.01637669826997444, 0.005602583484724164, 0.08103712886571884], time: 51.373
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15616962641354598, variance: 0.0029961998853832482, cvar: 0.7342215180397034, v: 0.21001501381397247, mean_q: -0.6086927056312561, std_q: 1.6662522554397583
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14808116227737833, variance: 0.06550679307989776, cvar: 1.288882851600647, v: 0.8091470003128052, mean_q: 0.00024660729104653, std_q: 1.6257551908493042
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2945001155242661, variance: 0.022410333938896656, cvar: -0.6935898065567017, v: -0.7215179800987244, mean_q: -1.636479377746582, std_q: 1.945602297782898
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3566395049519683, variance: 0.3241485357284546, cvar: -0.6224210858345032, v: -0.8543660640716553, mean_q: -1.7096261978149414, std_q: 1.9307211637496948

steps: 874975, episodes: 35000, mean episode reward: -23.572846162625932, agent episode reward: [-3.6277507438631766, -3.472384829062339, -7.556491049793691, -8.916219539906727], time: 51.44
steps: 874975, episodes: 35000, mean episode variance: 0.10351815423509106, agent episode variance: [0.0003058633739128709, 0.016818336856085806, 0.005853129949420691, 0.08054082405567169], time: 51.441
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1556358687232335, variance: 0.0012234534956514836, cvar: 0.7475274801254272, v: 0.21137118339538574, mean_q: -0.6050595045089722, std_q: 1.6661118268966675
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14868105616773228, variance: 0.06727334742434322, cvar: 1.2866100072860718, v: 0.8084178566932678, mean_q: 0.0002932228962890804, std_q: 1.6245461702346802
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.294068432605908, variance: 0.023412519797682763, cvar: -0.6933963298797607, v: -0.7214267253875732, mean_q: -1.6328651905059814, std_q: 1.935976266860962
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3555668076767218, variance: 0.32216331362724304, cvar: -0.6244914531707764, v: -0.8544838428497314, mean_q: -1.702551007270813, std_q: 1.9126678705215454

steps: 899975, episodes: 36000, mean episode reward: -23.952564853482706, agent episode reward: [-3.9831967718199257, -3.844085381387758, -7.432188052525227, -8.69309464774979], time: 51.655
steps: 899975, episodes: 36000, mean episode variance: 0.10283282805234194, agent episode variance: [0.0004124429803341627, 0.01567033179383725, 0.005824777773581445, 0.08092527550458908], time: 51.655
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15603879870975212, variance: 0.0016497719213366508, cvar: 0.7405677437782288, v: 0.21086980402469635, mean_q: -0.6071622967720032, std_q: 1.6613703966140747
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1474564422495509, variance: 0.062681327175349, cvar: 1.295596957206726, v: 0.8083924055099487, mean_q: 0.003729424672201276, std_q: 1.62235426902771
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2956253705768987, variance: 0.02329911109432578, cvar: -0.6935371160507202, v: -0.7216797471046448, mean_q: -1.6416316032409668, std_q: 1.9515490531921387
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35662242391996407, variance: 0.3237011134624481, cvar: -0.6290576457977295, v: -0.8550581336021423, mean_q: -1.7075942754745483, std_q: 1.9136464595794678

steps: 924975, episodes: 37000, mean episode reward: -23.40585507725808, agent episode reward: [-3.9444300791105724, -3.5515483792868667, -7.33942736575101, -8.570449253109633], time: 51.409
steps: 924975, episodes: 37000, mean episode variance: 0.10455736274714582, agent episode variance: [0.0007128174775280059, 0.017198252223897724, 0.0055584168450441215, 0.08108787620067597], time: 51.409
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15588888912072915, variance: 0.0028512699101120235, cvar: 0.7323048114776611, v: 0.20992441475391388, mean_q: -0.6059883832931519, std_q: 1.6555002927780151
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1480125369109554, variance: 0.0687930088955909, cvar: 1.2976371049880981, v: 0.8086538314819336, mean_q: 0.002955303993076086, std_q: 1.617013931274414
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.293926235383861, variance: 0.022233667380176486, cvar: -0.6937052011489868, v: -0.7217274308204651, mean_q: -1.6356035470962524, std_q: 1.9471871852874756
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3550791850209755, variance: 0.3243514895439148, cvar: -0.6232143640518188, v: -0.8549371361732483, mean_q: -1.7040425539016724, std_q: 1.9201514720916748

steps: 949975, episodes: 38000, mean episode reward: -23.698117563997542, agent episode reward: [-3.7202804435552754, -3.862940064795038, -7.201134382060355, -8.913762673586874], time: 51.424
steps: 949975, episodes: 38000, mean episode variance: 0.10246885477122851, agent episode variance: [0.0008570499829947949, 0.015165687036467717, 0.005511391066480428, 0.08093472668528556], time: 51.424
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15600108522151285, variance: 0.0034281999319791794, cvar: 0.7454321980476379, v: 0.21172094345092773, mean_q: -0.6071370840072632, std_q: 1.6726741790771484
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14800035526840963, variance: 0.060662748145870866, cvar: 1.2811940908432007, v: 0.8072094917297363, mean_q: 0.003264479339122772, std_q: 1.6159236431121826
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29483203251792256, variance: 0.022045564265921712, cvar: -0.6931268572807312, v: -0.7213310599327087, mean_q: -1.6413185596466064, std_q: 1.9583176374435425
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35629245124504594, variance: 0.3237389028072357, cvar: -0.6237896680831909, v: -0.8540802597999573, mean_q: -1.7077478170394897, std_q: 1.9251619577407837

steps: 974975, episodes: 39000, mean episode reward: -23.400032475698573, agent episode reward: [-3.7214465888338246, -3.65311257451065, -7.32000627490782, -8.705467037446278], time: 51.522
steps: 974975, episodes: 39000, mean episode variance: 0.10287523652624804, agent episode variance: [0.0007707675746642053, 0.016466351776733064, 0.005963899444090202, 0.07967421773076057], time: 51.522
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1550198719282317, variance: 0.0030830702986568213, cvar: 0.7429506778717041, v: 0.21004743874073029, mean_q: -0.5991796851158142, std_q: 1.6479558944702148
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14744279140306296, variance: 0.06586540710693226, cvar: 1.288973093032837, v: 0.8110867738723755, mean_q: 0.0043166400864720345, std_q: 1.6185886859893799
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2955985377291079, variance: 0.023855597776360808, cvar: -0.6929544806480408, v: -0.7213267683982849, mean_q: -1.6434645652770996, std_q: 1.9607257843017578
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3549231203400824, variance: 0.31869688630104065, cvar: -0.6245564818382263, v: -0.8542525768280029, mean_q: -1.7018719911575317, std_q: 1.916253924369812

steps: 999975, episodes: 40000, mean episode reward: -24.228482208449307, agent episode reward: [-3.9587247525940894, -3.8046881971260715, -7.425051548943506, -9.040017709785639], time: 51.457
steps: 999975, episodes: 40000, mean episode variance: 0.1058309181877412, agent episode variance: [0.0008982738316990435, 0.017050320672336965, 0.005319633261766285, 0.0825626904219389], time: 51.458
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1545812954876222, variance: 0.003593095326796174, cvar: 0.7456462979316711, v: 0.21032068133354187, mean_q: -0.5998772382736206, std_q: 1.6527374982833862
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1483886646394485, variance: 0.06820128268934786, cvar: 1.2874704599380493, v: 0.8085410594940186, mean_q: 0.0010909747797995806, std_q: 1.621830940246582
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29486526147463393, variance: 0.02127853304706514, cvar: -0.693223774433136, v: -0.721285343170166, mean_q: -1.6385279893875122, std_q: 1.9468529224395752
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3554707387284444, variance: 0.3302507698535919, cvar: -0.6264782547950745, v: -0.8556474447250366, mean_q: -1.705078125, std_q: 1.9219125509262085

steps: 1024975, episodes: 41000, mean episode reward: -23.4138697089627, agent episode reward: [-3.576287213929591, -3.4417668601041704, -7.314868085773658, -9.080947549155276], time: 51.668
steps: 1024975, episodes: 41000, mean episode variance: 0.102820432457258, agent episode variance: [0.0006987115691881627, 0.016074499570182525, 0.005310718715190887, 0.08073650260269642], time: 51.669
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15302371219106772, variance: 0.002794846276752651, cvar: 0.7493317723274231, v: 0.21107517182826996, mean_q: -0.5919743776321411, std_q: 1.6407502889633179
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14704827769094686, variance: 0.0642979982807301, cvar: 1.305660605430603, v: 0.8106196522712708, mean_q: 0.0024474882520735264, std_q: 1.6303718090057373
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29379906657891974, variance: 0.02124287486076355, cvar: -0.6932110786437988, v: -0.721305787563324, mean_q: -1.6333141326904297, std_q: 1.9335529804229736
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3553949147783857, variance: 0.3229460120201111, cvar: -0.6230998039245605, v: -0.8549726605415344, mean_q: -1.7034907341003418, std_q: 1.9191768169403076

steps: 1049975, episodes: 42000, mean episode reward: -23.941606645085297, agent episode reward: [-4.053882884247891, -3.6465626579053176, -7.520477163439347, -8.72068393949274], time: 51.677
steps: 1049975, episodes: 42000, mean episode variance: 0.10471683889022097, agent episode variance: [0.001057246830780059, 0.017065835396759214, 0.0054922410715371375, 0.08110151559114456], time: 51.677
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15437103180679862, variance: 0.004228987323120236, cvar: 0.7481421828269958, v: 0.21108098328113556, mean_q: -0.5996217727661133, std_q: 1.64986252784729
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14739468001286132, variance: 0.06826334158703685, cvar: 1.2938567399978638, v: 0.8089669942855835, mean_q: 0.004322715103626251, std_q: 1.615691900253296
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29479730366365015, variance: 0.02196896428614855, cvar: -0.693544328212738, v: -0.7215062379837036, mean_q: -1.6375895738601685, std_q: 1.9468108415603638
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.354586629569503, variance: 0.32440605759620667, cvar: -0.6264576315879822, v: -0.8544646501541138, mean_q: -1.6974244117736816, std_q: 1.9011164903640747

steps: 1074975, episodes: 43000, mean episode reward: -24.182638154703245, agent episode reward: [-4.039080399706315, -4.217776606354187, -7.003651874673491, -8.922129273969254], time: 51.962
steps: 1074975, episodes: 43000, mean episode variance: 0.1032890056190081, agent episode variance: [0.0007318808012641967, 0.016032358293887227, 0.0059379842174239455, 0.08058678230643272], time: 51.962
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.156148077524957, variance: 0.0029275232050567867, cvar: 0.7305323481559753, v: 0.20841237902641296, mean_q: -0.606376588344574, std_q: 1.6549559831619263
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14801159106720124, variance: 0.06412943317554891, cvar: 1.2876091003417969, v: 0.8096919655799866, mean_q: 0.0033789407461881638, std_q: 1.615829348564148
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2955348149935548, variance: 0.023751936869695782, cvar: -0.6934374570846558, v: -0.7216577529907227, mean_q: -1.6394870281219482, std_q: 1.9429899454116821
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35618185351528336, variance: 0.32234713435173035, cvar: -0.6261314749717712, v: -0.8547871112823486, mean_q: -1.7086279392242432, std_q: 1.925572395324707

steps: 1099975, episodes: 44000, mean episode reward: -23.70070554286521, agent episode reward: [-3.698594894518764, -3.4890271874224625, -7.538557213125719, -8.974526247798268], time: 51.817
steps: 1099975, episodes: 44000, mean episode variance: 0.10580471346620471, agent episode variance: [0.0008204772358294576, 0.01765585291059688, 0.005458917640848085, 0.08186946567893029], time: 51.818
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15447995268372353, variance: 0.0032819089433178306, cvar: 0.7410654425621033, v: 0.21071027219295502, mean_q: -0.5999182462692261, std_q: 1.6530522108078003
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14810242368798723, variance: 0.07062341164238752, cvar: 1.293389916419983, v: 0.8084673285484314, mean_q: -0.0011483007110655308, std_q: 1.6290490627288818
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2933375643087387, variance: 0.02183567056339234, cvar: -0.6933802366256714, v: -0.7215644717216492, mean_q: -1.6298449039459229, std_q: 1.9317630529403687
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35406597553768926, variance: 0.3274778723716736, cvar: -0.624565601348877, v: -0.8554663062095642, mean_q: -1.697723627090454, std_q: 1.9077672958374023

steps: 1124975, episodes: 45000, mean episode reward: -24.15333359396864, agent episode reward: [-3.8892264036494, -3.6767612318225704, -7.453443548120714, -9.133902410375956], time: 51.685
steps: 1124975, episodes: 45000, mean episode variance: 0.1030849138123449, agent episode variance: [0.00047496089478954673, 0.016614549512043594, 0.005657035302603617, 0.08033836810290813], time: 51.685
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15641892489144502, variance: 0.001899843579158187, cvar: 0.7365192770957947, v: 0.2103056013584137, mean_q: -0.6088325381278992, std_q: 1.666235089302063
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1480344963559909, variance: 0.06645819804817438, cvar: 1.2931087017059326, v: 0.8080692291259766, mean_q: 0.002515744185075164, std_q: 1.617922067642212
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29410814505124966, variance: 0.022628141210414468, cvar: -0.6936651468276978, v: -0.7217656373977661, mean_q: -1.6334950923919678, std_q: 1.9358083009719849
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3560670225775235, variance: 0.3213534951210022, cvar: -0.625633955001831, v: -0.8551527261734009, mean_q: -1.7084131240844727, std_q: 1.9289746284484863

steps: 1149975, episodes: 46000, mean episode reward: -23.8138444037899, agent episode reward: [-4.0301239663306285, -3.7705122568800515, -7.150310977316512, -8.86289720326271], time: 51.75
steps: 1149975, episodes: 46000, mean episode variance: 0.10387184199481271, agent episode variance: [0.0006975424322299659, 0.016489044548477976, 0.005785739429993555, 0.08089951558411121], time: 51.75
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1558616384186059, variance: 0.0027901697289198636, cvar: 0.7465735673904419, v: 0.20704762637615204, mean_q: -0.6047523021697998, std_q: 1.6582287549972534
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14708628634439902, variance: 0.0659561781939119, cvar: 1.2961891889572144, v: 0.8091819882392883, mean_q: 0.0071105933748185635, std_q: 1.6165443658828735
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2953287064721156, variance: 0.02314295771997422, cvar: -0.6934545040130615, v: -0.7215625047683716, mean_q: -1.6382791996002197, std_q: 1.9395548105239868
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35550365047854676, variance: 0.32359808683395386, cvar: -0.6297510266304016, v: -0.8555312752723694, mean_q: -1.7052834033966064, std_q: 1.9219969511032104

steps: 1174975, episodes: 47000, mean episode reward: -23.628769043389415, agent episode reward: [-3.722086398634004, -3.5327721793508826, -7.464421152681282, -8.909489312723245], time: 51.798
steps: 1174975, episodes: 47000, mean episode variance: 0.10402691293996759, agent episode variance: [0.0007005110546015203, 0.01673908449499868, 0.005192676497623325, 0.08139464089274406], time: 51.798
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1544561489891228, variance: 0.002802044218406081, cvar: 0.7362824082374573, v: 0.20880989730358124, mean_q: -0.598220944404602, std_q: 1.6448789834976196
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14767925280102828, variance: 0.06695633797999472, cvar: 1.294893741607666, v: 0.8076268434524536, mean_q: 0.0024605703074485064, std_q: 1.6240185499191284
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2953529675795959, variance: 0.0207707059904933, cvar: -0.693533182144165, v: -0.7214736938476562, mean_q: -1.639509916305542, std_q: 1.9429678916931152
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3552160758452618, variance: 0.3255785405635834, cvar: -0.6293097138404846, v: -0.855562150478363, mean_q: -1.702532172203064, std_q: 1.910823941230774

steps: 1199975, episodes: 48000, mean episode reward: -23.83757011530408, agent episode reward: [-3.7569123175372696, -3.6906049834885257, -7.438249269070893, -8.95180354520739], time: 51.976
steps: 1199975, episodes: 48000, mean episode variance: 0.1033434040346183, agent episode variance: [0.0007682183338329196, 0.015611254801042378, 0.006263982797507197, 0.0806999481022358], time: 51.977
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1553565009813588, variance: 0.0030728733353316785, cvar: 0.7324870824813843, v: 0.20700545608997345, mean_q: -0.6023064255714417, std_q: 1.648956537246704
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14720589992292943, variance: 0.06244501920416951, cvar: 1.2896465063095093, v: 0.807513415813446, mean_q: 0.003143757814541459, std_q: 1.6216789484024048
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29491029522619144, variance: 0.025055931190028788, cvar: -0.693500280380249, v: -0.7214481234550476, mean_q: -1.6341322660446167, std_q: 1.92988920211792
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35708647235223245, variance: 0.32279980182647705, cvar: -0.6246206164360046, v: -0.8551046252250671, mean_q: -1.7121970653533936, std_q: 1.93194580078125

steps: 1224975, episodes: 49000, mean episode reward: -23.664572660215942, agent episode reward: [-3.886855224732451, -3.97138678420061, -7.321650057913537, -8.484680593369344], time: 51.722
steps: 1224975, episodes: 49000, mean episode variance: 0.1049842847834807, agent episode variance: [0.0008081817636266351, 0.016383308134507387, 0.006363227682420984, 0.08142956720292568], time: 51.723
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15593108461652852, variance: 0.0032327270545065402, cvar: 0.7413756251335144, v: 0.2090551108121872, mean_q: -0.6050004959106445, std_q: 1.6543229818344116
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14804541318990108, variance: 0.06553323253802955, cvar: 1.2958427667617798, v: 0.8095420598983765, mean_q: 0.0011142303701490164, std_q: 1.6181361675262451
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2964798758595567, variance: 0.025452910729683937, cvar: -0.6936275362968445, v: -0.7214592695236206, mean_q: -1.642821192741394, std_q: 1.946126937866211
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35635637496279976, variance: 0.3257182538509369, cvar: -0.6273913383483887, v: -0.8549734950065613, mean_q: -1.7092394828796387, std_q: 1.9320094585418701

steps: 1249975, episodes: 50000, mean episode reward: -23.97881127388317, agent episode reward: [-3.744607978051389, -3.4735692836959515, -7.633432219861877, -9.127201792273954], time: 51.798
steps: 1249975, episodes: 50000, mean episode variance: 0.10420218758517877, agent episode variance: [0.0009291796293109656, 0.016644622851163148, 0.005464358903933316, 0.08116402620077133], time: 51.798
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15389835636104826, variance: 0.0037167185172438623, cvar: 0.752906858921051, v: 0.20899701118469238, mean_q: -0.5987554788589478, std_q: 1.6543728113174438
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14796461890598225, variance: 0.06657849140465259, cvar: 1.2981362342834473, v: 0.8079195618629456, mean_q: 0.00270353932864964, std_q: 1.6216567754745483
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29511519106958806, variance: 0.021857435615733264, cvar: -0.6936820149421692, v: -0.7215783596038818, mean_q: -1.640191912651062, std_q: 1.9501100778579712
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35591364255781976, variance: 0.32465609908103943, cvar: -0.6259726881980896, v: -0.8545603156089783, mean_q: -1.7050946950912476, std_q: 1.918527603149414

steps: 1274975, episodes: 51000, mean episode reward: -23.86414344911232, agent episode reward: [-3.874154288563478, -3.79628156910008, -7.2421656358519995, -8.951541955596761], time: 51.861
steps: 1274975, episodes: 51000, mean episode variance: 0.10415431128372438, agent episode variance: [0.0009457154357805848, 0.016391975215636193, 0.005652667089598253, 0.08116395354270935], time: 51.861
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15460057565591925, variance: 0.003782861743122339, cvar: 0.744614839553833, v: 0.21093620359897614, mean_q: -0.5988832712173462, std_q: 1.6510661840438843
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14889714971750645, variance: 0.06556790086254477, cvar: 1.285279393196106, v: 0.8073681592941284, mean_q: -0.0021364581771194935, std_q: 1.6265826225280762
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2949563868985448, variance: 0.022610668358393014, cvar: -0.6934330463409424, v: -0.7214112281799316, mean_q: -1.637634038925171, std_q: 1.9418540000915527
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.356612712597301, variance: 0.32465583086013794, cvar: -0.6262155771255493, v: -0.8553086519241333, mean_q: -1.7110514640808105, std_q: 1.9269450902938843

steps: 1299975, episodes: 52000, mean episode reward: -23.64836902979936, agent episode reward: [-3.793722133390397, -3.8461682444075715, -7.276148834354508, -8.732329817646882], time: 51.929
steps: 1299975, episodes: 52000, mean episode variance: 0.10409583734418265, agent episode variance: [0.0005700909039005638, 0.016859860715456306, 0.0053303122303914275, 0.08133557349443436], time: 51.93
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15415100384437283, variance: 0.002280363615602255, cvar: 0.7385261654853821, v: 0.2089804857969284, mean_q: -0.5992557406425476, std_q: 1.6471810340881348
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14864322848256883, variance: 0.06743944286182522, cvar: 1.2905131578445435, v: 0.8070927858352661, mean_q: -0.0023790535051375628, std_q: 1.623257040977478
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2944750125200604, variance: 0.02132124892156571, cvar: -0.693469762802124, v: -0.7212148308753967, mean_q: -1.6327682733535767, std_q: 1.9296693801879883
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3553492713910017, variance: 0.3253422975540161, cvar: -0.6255192756652832, v: -0.8543019890785217, mean_q: -1.7027606964111328, std_q: 1.9147495031356812

steps: 1324975, episodes: 53000, mean episode reward: -24.0342309779801, agent episode reward: [-3.9079086292043255, -3.663860108969069, -7.607014750364388, -8.855447489442314], time: 51.966
steps: 1324975, episodes: 53000, mean episode variance: 0.10311425272468477, agent episode variance: [0.0007094376487657428, 0.015869211932644248, 0.005493941761553288, 0.0810416613817215], time: 51.967
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15490856066748665, variance: 0.002837750595062971, cvar: 0.7549459338188171, v: 0.20901170372962952, mean_q: -0.6039363145828247, std_q: 1.659242868423462
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14808657151647533, variance: 0.06347684773057699, cvar: 1.2928311824798584, v: 0.8082748055458069, mean_q: 0.001991858473047614, std_q: 1.6250442266464233
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2945370622725244, variance: 0.02197576704621315, cvar: -0.6934065222740173, v: -0.7215059995651245, mean_q: -1.6366058588027954, std_q: 1.9441041946411133
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3563978852660872, variance: 0.3241666257381439, cvar: -0.6241323351860046, v: -0.8550193905830383, mean_q: -1.7110981941223145, std_q: 1.9343663454055786

steps: 1349975, episodes: 54000, mean episode reward: -23.751659175894368, agent episode reward: [-3.9877869903034946, -3.595768482503735, -7.224461915865171, -8.943641787221965], time: 51.965
steps: 1349975, episodes: 54000, mean episode variance: 0.1032390454094857, agent episode variance: [0.0006160799884237348, 0.0159530036249198, 0.005899132148362696, 0.08077082964777947], time: 51.966
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15427024026678338, variance: 0.0024643199536949394, cvar: 0.7504527568817139, v: 0.20967981219291687, mean_q: -0.5997627973556519, std_q: 1.650614857673645
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.146888864299122, variance: 0.0638120144996792, cvar: 1.3033814430236816, v: 0.8103631138801575, mean_q: 0.006479405332356691, std_q: 1.617407202720642
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29472225889150594, variance: 0.023596528593450784, cvar: -0.6935553550720215, v: -0.7213985323905945, mean_q: -1.6377514600753784, std_q: 1.9480189085006714
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35607074885975437, variance: 0.3230833113193512, cvar: -0.6219293475151062, v: -0.8539767861366272, mean_q: -1.7070313692092896, std_q: 1.9207451343536377

steps: 1374975, episodes: 55000, mean episode reward: -23.937098269865384, agent episode reward: [-4.034412479973108, -4.035437538988206, -7.12390532895343, -8.74334292195064], time: 52.162
steps: 1374975, episodes: 55000, mean episode variance: 0.10438428569701501, agent episode variance: [0.0008016729950904846, 0.016146736406255514, 0.006001005724770948, 0.08143487057089806], time: 52.163
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15494931960838282, variance: 0.0032066919803619386, cvar: 0.7334305047988892, v: 0.20988723635673523, mean_q: -0.6030596494674683, std_q: 1.6529790163040161
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.147460723928332, variance: 0.06458694562502205, cvar: 1.2891931533813477, v: 0.8079590797424316, mean_q: 0.001001085271127522, std_q: 1.6248012781143188
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2942311428408048, variance: 0.024004022899083793, cvar: -0.6930914521217346, v: -0.7210648059844971, mean_q: -1.6333930492401123, std_q: 1.9392716884613037
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35615461389597436, variance: 0.3257395029067993, cvar: -0.6266037821769714, v: -0.8548996448516846, mean_q: -1.707545518875122, std_q: 1.926348090171814

steps: 1399975, episodes: 56000, mean episode reward: -23.83072050569678, agent episode reward: [-3.974332936023293, -3.7149590327292845, -7.221335046366757, -8.92009349057745], time: 52.224
steps: 1399975, episodes: 56000, mean episode variance: 0.10310560724465176, agent episode variance: [0.0009052053189370781, 0.016345340107567607, 0.005161929700756446, 0.08069313211739063], time: 52.224
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15604889929395044, variance: 0.0036208212757483124, cvar: 0.7247717380523682, v: 0.20902718603610992, mean_q: -0.6080321669578552, std_q: 1.6479198932647705
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14849456791912713, variance: 0.06538136043027043, cvar: 1.2906999588012695, v: 0.8079954981803894, mean_q: -0.0021249377168715, std_q: 1.6231162548065186
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2940883646845867, variance: 0.020647718803025784, cvar: -0.6931174397468567, v: -0.7210513949394226, mean_q: -1.6346949338912964, std_q: 1.9363173246383667
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35489184956245196, variance: 0.3227725327014923, cvar: -0.6243566870689392, v: -0.8545693755149841, mean_q: -1.7001748085021973, std_q: 1.90692138671875

steps: 1424975, episodes: 57000, mean episode reward: -23.85213334309511, agent episode reward: [-3.935178707916715, -3.742843527149721, -7.252528812823906, -8.921582295204773], time: 52.134
steps: 1424975, episodes: 57000, mean episode variance: 0.1015461171651259, agent episode variance: [0.000684874222613871, 0.015610976777039468, 0.005178952575661242, 0.08007131358981133], time: 52.134
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15469799699182732, variance: 0.002739496890455484, cvar: 0.7295171618461609, v: 0.2076234221458435, mean_q: -0.600039005279541, std_q: 1.6398197412490845
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14860252742174956, variance: 0.06244390710815787, cvar: 1.2958381175994873, v: 0.8066976070404053, mean_q: 0.0015175262233242393, std_q: 1.6230049133300781
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2950382589228719, variance: 0.02071581030264497, cvar: -0.69303297996521, v: -0.7213255763053894, mean_q: -1.6387890577316284, std_q: 1.9493200778961182
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35438410047651403, variance: 0.32028529047966003, cvar: -0.627054750919342, v: -0.8543691635131836, mean_q: -1.6988637447357178, std_q: 1.9047949314117432

steps: 1449975, episodes: 58000, mean episode reward: -23.863843930589574, agent episode reward: [-4.02820327216577, -3.814159443660622, -7.289294129729124, -8.732187085034054], time: 52.067
steps: 1449975, episodes: 58000, mean episode variance: 0.10442287816572934, agent episode variance: [0.0008398981662467122, 0.016462257464416326, 0.005893311041407287, 0.08122741149365902], time: 52.068
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1551005403126236, variance: 0.003359592664986849, cvar: 0.7256466150283813, v: 0.2089555710554123, mean_q: -0.6017217040061951, std_q: 1.6415444612503052
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14732902567245085, variance: 0.0658490298576653, cvar: 1.2988882064819336, v: 0.8077872395515442, mean_q: 0.004560860339552164, std_q: 1.6221098899841309
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29268491814325015, variance: 0.02357324416562915, cvar: -0.6930361390113831, v: -0.7207610011100769, mean_q: -1.6270908117294312, std_q: 1.931386947631836
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35601388579582033, variance: 0.32490965723991394, cvar: -0.6245085597038269, v: -0.8540787100791931, mean_q: -1.7083688974380493, std_q: 1.9276301860809326

steps: 1474975, episodes: 59000, mean episode reward: -23.60536978914948, agent episode reward: [-3.7730457320872137, -3.6664121470868896, -7.477844930361261, -8.688066979614113], time: 52.114
steps: 1474975, episodes: 59000, mean episode variance: 0.10503810697607696, agent episode variance: [0.0007374549531377852, 0.01656754538603127, 0.006480282881762832, 0.08125282375514507], time: 52.115
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15660478918144377, variance: 0.0029498198125511407, cvar: 0.7264756560325623, v: 0.20923009514808655, mean_q: -0.6046369671821594, std_q: 1.6473487615585327
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14938692694076294, variance: 0.06627018154412508, cvar: 1.290479302406311, v: 0.8085062503814697, mean_q: -0.0032202647998929024, std_q: 1.6273239850997925
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2944612646303132, variance: 0.02592113152705133, cvar: -0.6927302479743958, v: -0.7208845019340515, mean_q: -1.6354291439056396, std_q: 1.9417355060577393
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3547208169080365, variance: 0.325011283159256, cvar: -0.626675546169281, v: -0.8541029691696167, mean_q: -1.700904130935669, std_q: 1.9104079008102417

steps: 1499975, episodes: 60000, mean episode reward: -23.88158323877875, agent episode reward: [-3.7043987833131227, -3.4793837848607905, -7.671660932073456, -9.026139738531377], time: 52.398
steps: 1499975, episodes: 60000, mean episode variance: 0.10388442870182917, agent episode variance: [0.0006774897463619709, 0.016546174119226636, 0.006121058308053762, 0.0805397065281868], time: 52.399
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15558567931807488, variance: 0.0027099589854478836, cvar: 0.7366201281547546, v: 0.20761936902999878, mean_q: -0.6053746342658997, std_q: 1.653895616531372
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14899742748047085, variance: 0.06618469647690654, cvar: 1.2960247993469238, v: 0.80860835313797, mean_q: -0.0017723707715049386, std_q: 1.626720666885376
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29460173968354003, variance: 0.024484233232215047, cvar: -0.6931360363960266, v: -0.721168041229248, mean_q: -1.6357115507125854, std_q: 1.9380725622177124
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3552145820526339, variance: 0.3221588134765625, cvar: -0.62217116355896, v: -0.8541302680969238, mean_q: -1.703031301498413, std_q: 1.9188220500946045

steps: 1524975, episodes: 61000, mean episode reward: -23.729562093836755, agent episode reward: [-3.8822321151974344, -3.589363541262339, -7.333745997565589, -8.92422043981139], time: 52.188
steps: 1524975, episodes: 61000, mean episode variance: 0.10276408296730369, agent episode variance: [0.0006777720097452402, 0.015553979523479939, 0.005998227980919182, 0.08053410345315934], time: 52.189
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15530721813081363, variance: 0.0027110880389809608, cvar: 0.7219780683517456, v: 0.20596420764923096, mean_q: -0.6026338338851929, std_q: 1.639768123626709
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14796728346572907, variance: 0.062215918093919756, cvar: 1.2840543985366821, v: 0.8069849014282227, mean_q: 0.0022989376448094845, std_q: 1.6144907474517822
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29488074617792537, variance: 0.023992911923676728, cvar: -0.6931635737419128, v: -0.7213477492332458, mean_q: -1.636886715888977, std_q: 1.9416230916976929
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35423119568472033, variance: 0.32213640213012695, cvar: -0.6279641389846802, v: -0.8545350432395935, mean_q: -1.6981946229934692, std_q: 1.9077167510986328

steps: 1549975, episodes: 62000, mean episode reward: -23.43359485782531, agent episode reward: [-3.8138262583945317, -3.7748859171889366, -7.218105364424326, -8.626777317817517], time: 4105.011
steps: 1549975, episodes: 62000, mean episode variance: 0.1031028271720279, agent episode variance: [0.0007990832459181547, 0.0157470559803769, 0.005645392427453771, 0.08091129551827908], time: 4105.011
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15436461264991733, variance: 0.0031963329836726187, cvar: 0.7346276044845581, v: 0.20837683975696564, mean_q: -0.5999913811683655, std_q: 1.647899866104126
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14812391799498553, variance: 0.0629882239215076, cvar: 1.2945424318313599, v: 0.80797278881073, mean_q: 0.004016094841063023, std_q: 1.6207566261291504
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2952068027544997, variance: 0.022581569709815084, cvar: -0.6932175159454346, v: -0.7215333580970764, mean_q: -1.6387946605682373, std_q: 1.9476395845413208
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3561722193206126, variance: 0.3236452043056488, cvar: -0.6288449168205261, v: -0.8550879955291748, mean_q: -1.7088000774383545, std_q: 1.9206023216247559

steps: 1574975, episodes: 63000, mean episode reward: -23.336874929922125, agent episode reward: [-3.9136366806558027, -3.6101173983151993, -7.083636995631093, -8.729483855320028], time: 52.947
steps: 1574975, episodes: 63000, mean episode variance: 0.10422938864026218, agent episode variance: [0.0009207445862703025, 0.01677530524833128, 0.005839316165074706, 0.0806940226405859], time: 52.947
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1549832664758675, variance: 0.00368297834508121, cvar: 0.7446821331977844, v: 0.2083255499601364, mean_q: -0.6016423106193542, std_q: 1.646833062171936
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14788831758404095, variance: 0.06710122099332512, cvar: 1.2904462814331055, v: 0.8078349828720093, mean_q: 0.000861352076753974, std_q: 1.6292502880096436
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.295768274454103, variance: 0.023357264660298825, cvar: -0.6927343606948853, v: -0.720645546913147, mean_q: -1.6440448760986328, std_q: 1.9607516527175903
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3538459988185515, variance: 0.32277604937553406, cvar: -0.6217462420463562, v: -0.8541103601455688, mean_q: -1.6966733932495117, std_q: 1.9055331945419312

steps: 1599975, episodes: 64000, mean episode reward: -23.995629649119593, agent episode reward: [-3.94762690491805, -3.924587159727538, -7.2281153290290545, -8.895300255444953], time: 52.398
steps: 1599975, episodes: 64000, mean episode variance: 0.10241694201901555, agent episode variance: [0.0005128053885418922, 0.015263308464549482, 0.005685818392550572, 0.08095500977337361], time: 52.398
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15458102090875825, variance: 0.002051221554167569, cvar: 0.7338880896568298, v: 0.20829737186431885, mean_q: -0.6017968654632568, std_q: 1.6488088369369507
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14851535548788183, variance: 0.06105323385819793, cvar: 1.2834445238113403, v: 0.8075682520866394, mean_q: 0.0022749905474483967, std_q: 1.6165918111801147
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2931582283628409, variance: 0.02274327357020229, cvar: -0.6928949356079102, v: -0.7209122776985168, mean_q: -1.627468228340149, std_q: 1.9221025705337524
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35268016600177243, variance: 0.3238200545310974, cvar: -0.6210309863090515, v: -0.8529135584831238, mean_q: -1.6901261806488037, std_q: 1.8987724781036377

steps: 1624975, episodes: 65000, mean episode reward: -23.707248848130373, agent episode reward: [-3.931305655093627, -3.716262759089538, -7.158261912603958, -8.90141852134325], time: 53.214
steps: 1624975, episodes: 65000, mean episode variance: 0.10265927768545226, agent episode variance: [0.000627643265062943, 0.01563337743212469, 0.005336298123933375, 0.08106195886433125], time: 53.214
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15475720225443598, variance: 0.002510573060251772, cvar: 0.7383337020874023, v: 0.20837284624576569, mean_q: -0.5967605710029602, std_q: 1.6439363956451416
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1489860803381124, variance: 0.06253350972849876, cvar: 1.2801300287246704, v: 0.8079946041107178, mean_q: 0.0011318821925669909, std_q: 1.6194334030151367
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29353637280113337, variance: 0.0213451924957335, cvar: -0.693473219871521, v: -0.7214807271957397, mean_q: -1.6336390972137451, std_q: 1.9446072578430176
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35590683168212345, variance: 0.3242478370666504, cvar: -0.6226505637168884, v: -0.8550599217414856, mean_q: -1.7065566778182983, std_q: 1.9194488525390625

steps: 1649975, episodes: 66000, mean episode reward: -23.742538659457836, agent episode reward: [-3.909476858533753, -3.6342531443410624, -7.350300148713387, -8.848508507869632], time: 55.233
steps: 1649975, episodes: 66000, mean episode variance: 0.10353238610201515, agent episode variance: [0.0006424395120702684, 0.016429043010342866, 0.0058477685994002965, 0.08061313498020171], time: 55.233
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1553749147438931, variance: 0.0025697580482810737, cvar: 0.7284825444221497, v: 0.20873185992240906, mean_q: -0.6037806272506714, std_q: 1.6598278284072876
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14830152957349152, variance: 0.06571617204137147, cvar: 1.3068879842758179, v: 0.8074142932891846, mean_q: 0.006043767090886831, std_q: 1.621212124824524
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2922260037895255, variance: 0.023391074397601186, cvar: -0.6934466361999512, v: -0.7213560938835144, mean_q: -1.6258512735366821, std_q: 1.927366852760315
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3551409437047188, variance: 0.32245251536369324, cvar: -0.6261832118034363, v: -0.8551169633865356, mean_q: -1.7037601470947266, std_q: 1.9169989824295044

steps: 1674975, episodes: 67000, mean episode reward: -24.298784018064822, agent episode reward: [-3.9676586197940105, -3.6498464015609353, -7.392901073943984, -9.288377922765891], time: 60.365
steps: 1674975, episodes: 67000, mean episode variance: 0.10263964669173584, agent episode variance: [0.000909686484374106, 0.015873185192933306, 0.005722675873665139, 0.08013409914076328], time: 60.366
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1568284205177773, variance: 0.003638745937496424, cvar: 0.7261280417442322, v: 0.20800381898880005, mean_q: -0.6094325184822083, std_q: 1.6531304121017456
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14857236234000396, variance: 0.06349274077173322, cvar: 1.2933512926101685, v: 0.8083639740943909, mean_q: 0.0008437734795734286, std_q: 1.6218769550323486
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29534610237156667, variance: 0.022890703494660557, cvar: -0.6935552358627319, v: -0.7217184901237488, mean_q: -1.639191746711731, std_q: 1.946542739868164
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35442548144443026, variance: 0.32053637504577637, cvar: -0.623042643070221, v: -0.8547177910804749, mean_q: -1.700631856918335, std_q: 1.9132601022720337

steps: 1699975, episodes: 68000, mean episode reward: -23.397975419815758, agent episode reward: [-3.8240461384028213, -3.5508046593481133, -7.243180774109625, -8.779943847955197], time: 52.457
steps: 1699975, episodes: 68000, mean episode variance: 0.10259266402525827, agent episode variance: [0.0010812599076889455, 0.015393026048317552, 0.006064789410680532, 0.08005358865857125], time: 52.458
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15582083374598688, variance: 0.004325039630755782, cvar: 0.7334045767784119, v: 0.20879922807216644, mean_q: -0.6030918955802917, std_q: 1.6479215621948242
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14842197798552556, variance: 0.06157210419327021, cvar: 1.3088178634643555, v: 0.8071067333221436, mean_q: 0.003100108355283737, std_q: 1.627016305923462
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.29354268821409424, variance: 0.02425915764272213, cvar: -0.6934189200401306, v: -0.7214713096618652, mean_q: -1.630874514579773, std_q: 1.931506872177124
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3547036594293648, variance: 0.3202143609523773, cvar: -0.623507022857666, v: -0.8538968563079834, mean_q: -1.701831579208374, std_q: 1.9155210256576538
/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)


steps: 1724975, episodes: 69000, mean episode reward: -23.48022389930387, agent episode reward: [-4.005648625639137, -3.6405690895075615, -7.1069187886555465, -8.727087395501625], time: 52.236
steps: 1724975, episodes: 69000, mean episode variance: 0.10436268027313053, agent episode variance: [0.0008119412343949079, 0.016785788365639747, 0.00619048426579684, 0.08057446640729904], time: 52.236
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15508270332559715, variance: 0.0032477649375796318, cvar: 0.7393639087677002, v: 0.20732909440994263, mean_q: -0.6050206422805786, std_q: 1.651840090751648
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14699316413022837, variance: 0.06714315346255899, cvar: 1.2944597005844116, v: 0.8068177700042725, mean_q: 0.0010809627128764987, std_q: 1.6264821290969849
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2938533084167007, variance: 0.02476193706318736, cvar: -0.6932509541511536, v: -0.7216031551361084, mean_q: -1.6322375535964966, std_q: 1.9330055713653564
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35415251261836206, variance: 0.3222978413105011, cvar: -0.6265712976455688, v: -0.854730486869812, mean_q: -1.7011396884918213, std_q: 1.9164479970932007

steps: 1749975, episodes: 70000, mean episode reward: -23.803382399801944, agent episode reward: [-4.022572006777536, -3.6185238472555747, -7.295572554773701, -8.866713990995128], time: 52.187
steps: 1749975, episodes: 70000, mean episode variance: 0.10208442115061916, agent episode variance: [0.0007744290512055158, 0.015271229466423392, 0.005429127458715812, 0.08060963517427444], time: 52.188
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.15648084722032946, variance: 0.0030977162048220633, cvar: 0.7220803499221802, v: 0.20606887340545654, mean_q: -0.6043722033500671, std_q: 1.637275218963623
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14772018176883023, variance: 0.06108491786569357, cvar: 1.2915512323379517, v: 0.8070060014724731, mean_q: 0.005947174038738012, std_q: 1.6169990301132202
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2923787366923807, variance: 0.021716509834863246, cvar: -0.6931536197662354, v: -0.7212948203086853, mean_q: -1.6262139081954956, std_q: 1.924238681793213
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35270492686434546, variance: 0.3224385380744934, cvar: -0.6235742568969727, v: -0.8542369604110718, mean_q: -1.6906198263168335, std_q: 1.8942636251449585

...Finished total of 70001 episodes... Now freezing policy and running for 1000 more episodes
steps: 24975, episodes: 1000, mean episode reward: -23.661116829429112, agent episode reward: [-3.776284988865696, -3.777402830056365, -7.094497112172831, -9.012931898334216], time: 37.344
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 37.345
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -23.89358976227623, agent episode reward: [-3.67901342931608, -3.849221829655553, -7.272133496010385, -9.09322100729421], time: 48.362
steps: 49975, episodes: 2000, mean episode variance: 0.10244148715841583, agent episode variance: [0.000162892856169492, 0.016301446322351695, 0.006748917101183906, 0.07922823087871075], time: 48.362
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.14964963518253677, variance: 0.0006675936728257869, cvar: 0.6877213716506958, v: 0.19487425684928894, mean_q: -0.5798386335372925, std_q: 1.6036003828048706
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.1511705471457312, variance: 0.06680920623914628, cvar: 1.2968173027038574, v: 0.8040313720703125, mean_q: -0.019972149282693863, std_q: 1.6461111307144165
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.282561142531525, variance: 0.027659496316327484, cvar: -0.6911528706550598, v: -0.721826434135437, mean_q: -1.5776777267456055, std_q: 1.8419201374053955
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.35982659405948936, variance: 0.3247058391571045, cvar: -0.6260091066360474, v: -0.8573917150497437, mean_q: -1.7252377271652222, std_q: 1.9469242095947266

...Finished total of 2001 episodes with the fixed policy.
