WARNING: Logging before flag parsing goes to stderr.
W0826 16:30:54.936873 4503573952 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0826 16:30:54.937070 4503573952 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-26 16:30:54.937393: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0826 16:30:54.939174 4503573952 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0826 16:30:54.941018 4503573952 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0826 16:30:54.941118 4503573952 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0826 16:30:54.941185 4503573952 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0826 16:30:55.368455 4503573952 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0826 16:30:55.533687 4503573952 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0826 16:30:55.540028 4503573952 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0826 16:30:56.000890 4503573952 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation True
adversary agent:  1.9845386743545532
adversary agent:  1.9845386743545532
good agent:  -0.3292904943227768
good agent:  -0.3292904943227768
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -69.66727705306211, agent episode reward: [-7.679205547731572, -7.407142864054256, -28.27984910061505, -26.301079540661235], time: 33.26
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 33.26
steps: 24975, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0, 0.0], time: 33.261
Running avgs for agent 0: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -64.42497922417786, agent episode reward: [-13.38838060751076, -12.932773543521536, -21.252916841619278, -16.850908231526294], time: 68.761
steps: 49975, episodes: 2000, mean episode variance: 0.37703300366317855, agent episode variance: [0.07116584273939952, 0.03837320861592889, 0.16030493848770858, 0.10718901382014155], time: 68.762
steps: 49975, episodes: 2000, mean episode cvar: 0.7652276199702174, agent episode cvar: [0.40597154223918913, 0.4063258647918701, -0.022945785152725877, -0.024124001908116043], time: 68.762
Running avgs for agent 0: q_loss: 0.22215120494365692, u_loss: 0.3730677366256714, p_loss: -0.15833750367164612, mean_rew: -0.38983670386232816, variance: 0.29166328991557183, cvar: 1.6638178825378418, v: 1.1525194644927979, mean_q: 0.12390332669019699, std_q: 0.9042524695396423, lamda: 1.0026379823684692
Running avgs for agent 1: q_loss: 0.37646913528442383, u_loss: 0.34664386510849, p_loss: -0.26382938027381897, mean_rew: -0.37886009580529345, variance: 0.15726724842593806, cvar: 1.6652699708938599, v: 1.1645976305007935, mean_q: 0.2353629469871521, std_q: 0.8104838132858276, lamda: 1.0046309232711792
Running avgs for agent 2: q_loss: 0.05353649705648422, u_loss: 0.5698978900909424, p_loss: 1.877573013305664, mean_rew: -1.1531414136728606, variance: 0.6569874528184777, cvar: -0.09404008835554123, v: -0.1652149260044098, mean_q: -1.9213017225265503, std_q: 1.0660213232040405, lamda: 1.0000554323196411
Running avgs for agent 3: q_loss: 0.07975663989782333, u_loss: 0.3832446336746216, p_loss: 1.896319031715393, mean_rew: -1.0078414901504977, variance: 0.43929923696779327, cvar: -0.09886886179447174, v: -0.16744554042816162, mean_q: -1.9433623552322388, std_q: 0.8653696775436401, lamda: 1.0003658533096313

steps: 74975, episodes: 3000, mean episode reward: -29.37319683406642, agent episode reward: [-7.222731080901409, -7.868213070485329, -7.1683008448637855, -7.1139518378158995], time: 65.817
steps: 74975, episodes: 3000, mean episode variance: 0.9823744022101164, agent episode variance: [0.27016961865127087, 0.19504191105812788, 0.30772000720351933, 0.2094428652971983], time: 65.818
steps: 74975, episodes: 3000, mean episode cvar: 0.8047856251001358, agent episode cvar: [0.48977187418937684, 0.4906739687919617, -0.08555081203579902, -0.09010940584540367], time: 65.819
Running avgs for agent 0: q_loss: 0.24453015625476837, u_loss: 1.0268349647521973, p_loss: 0.6056455373764038, mean_rew: -0.4007952245449115, variance: 1.0806784746050835, cvar: 1.9590874910354614, v: 1.027572512626648, mean_q: -0.6946437954902649, std_q: 1.6855334043502808, lamda: 1.0147861242294312
Running avgs for agent 1: q_loss: 0.22813951969146729, u_loss: 1.0812478065490723, p_loss: 0.45446375012397766, mean_rew: -0.3935276334803325, variance: 0.7801676442325115, cvar: 1.962695837020874, v: 1.0899516344070435, mean_q: -0.5320185422897339, std_q: 1.5014305114746094, lamda: 1.0168721675872803
Running avgs for agent 2: q_loss: 0.006192585453391075, u_loss: 2.080752372741699, p_loss: 2.4125468730926514, mean_rew: -0.8583293900488825, variance: 1.2308801412582397, cvar: -0.3422032594680786, v: -0.4159184694290161, mean_q: -2.5593481063842773, std_q: 2.3469789028167725, lamda: 1.0008312463760376
Running avgs for agent 3: q_loss: 0.006151561625301838, u_loss: 0.7432706356048584, p_loss: 2.191861867904663, mean_rew: -0.7557420320913749, variance: 0.8377714611887932, cvar: -0.36043763160705566, v: -0.4174787104129791, mean_q: -2.324268102645874, std_q: 1.9093972444534302, lamda: 1.0008381605148315

steps: 99975, episodes: 4000, mean episode reward: -26.16733131473311, agent episode reward: [-5.884703651728502, -6.158564932837385, -6.970379833195975, -7.153682896971246], time: 68.493
steps: 99975, episodes: 4000, mean episode variance: 1.1884210697654636, agent episode variance: [0.38152979017049077, 0.2729435940347612, 0.3141504884790629, 0.21979719708114862], time: 68.494
steps: 99975, episodes: 4000, mean episode cvar: 0.8159588126838208, agent episode cvar: [0.4925810631513596, 0.5026631687879562, -0.08784638479351997, -0.0914390344619751], time: 68.494
Running avgs for agent 0: q_loss: 0.30192631483078003, u_loss: 2.558399200439453, p_loss: 1.0627639293670654, mean_rew: -0.35952059949710496, variance: 1.526119160681963, cvar: 1.9703242778778076, v: 0.6772371530532837, mean_q: -1.2128874063491821, std_q: 2.2840054035186768, lamda: 1.0311251878738403
Running avgs for agent 1: q_loss: 0.2635582685470581, u_loss: 3.3259823322296143, p_loss: 1.0414237976074219, mean_rew: -0.3582551672539372, variance: 1.0917743761390448, cvar: 2.010652780532837, v: 0.7324187159538269, mean_q: -1.1809945106506348, std_q: 2.082827568054199, lamda: 1.0322651863098145
Running avgs for agent 2: q_loss: 0.007702428847551346, u_loss: 5.29887580871582, p_loss: 2.7269887924194336, mean_rew: -0.6904632858158813, variance: 1.2566019539162516, cvar: -0.35138553380966187, v: -0.39378008246421814, mean_q: -2.9329922199249268, std_q: 3.427192211151123, lamda: 1.0010387897491455
Running avgs for agent 3: q_loss: 0.007677051238715649, u_loss: 1.98081374168396, p_loss: 2.4169371128082275, mean_rew: -0.6187213314815183, variance: 0.8791887883245945, cvar: -0.36575615406036377, v: -0.4039931893348694, mean_q: -2.5944807529449463, std_q: 2.7277348041534424, lamda: 1.0011227130889893

steps: 124975, episodes: 5000, mean episode reward: -24.33457004272523, agent episode reward: [-5.220889304089313, -5.50097992671131, -6.696493570550527, -6.916207241374079], time: 68.018
steps: 124975, episodes: 5000, mean episode variance: 1.66912342325598, agent episode variance: [0.6157326610982418, 0.4288755414150655, 0.38827240582555533, 0.23624281491711735], time: 68.018
steps: 124975, episodes: 5000, mean episode cvar: 0.7633995195031166, agent episode cvar: [0.46885950708389285, 0.4770409491062164, -0.09032380527257919, -0.09217713141441346], time: 68.019
Running avgs for agent 0: q_loss: 0.386447012424469, u_loss: 5.365514755249023, p_loss: 1.1233232021331787, mean_rew: -0.32778420694541843, variance: 2.462930644392967, cvar: 1.8754379749298096, v: 0.3595705032348633, mean_q: -1.297025442123413, std_q: 2.645293951034546, lamda: 1.0502666234970093
Running avgs for agent 1: q_loss: 0.3672006130218506, u_loss: 5.910332202911377, p_loss: 1.2224160432815552, mean_rew: -0.329081924754168, variance: 1.715502165660262, cvar: 1.9081637859344482, v: 0.3302750587463379, mean_q: -1.396673321723938, std_q: 2.4511661529541016, lamda: 1.049486517906189
Running avgs for agent 2: q_loss: 0.01141344290226698, u_loss: 7.696195125579834, p_loss: 2.7309157848358154, mean_rew: -0.5958661791279426, variance: 1.5530896233022213, cvar: -0.361295223236084, v: -0.4018465578556061, mean_q: -2.955338478088379, std_q: 4.0631279945373535, lamda: 1.0013127326965332
Running avgs for agent 3: q_loss: 0.011547224596142769, u_loss: 4.2126851081848145, p_loss: 2.404195547103882, mean_rew: -0.5436434556425347, variance: 0.9449712596684694, cvar: -0.3687085509300232, v: -0.41392409801483154, mean_q: -2.590989828109741, std_q: 3.1600351333618164, lamda: 1.0014432668685913

steps: 149975, episodes: 6000, mean episode reward: -24.689873964163425, agent episode reward: [-5.439322919815391, -5.4275934671774, -6.837194130419514, -6.985763446751118], time: 68.215
steps: 149975, episodes: 6000, mean episode variance: 1.9895609907014296, agent episode variance: [0.745930387020111, 0.6222064037621021, 0.4024777902811766, 0.21894640963803977], time: 68.215
steps: 149975, episodes: 6000, mean episode cvar: 0.7462734740674496, agent episode cvar: [0.46673007142543793, 0.4627708021402359, -0.09108173018693924, -0.09214566931128503], time: 68.216
Running avgs for agent 0: q_loss: 0.278187096118927, u_loss: 6.846092700958252, p_loss: 1.0398508310317993, mean_rew: -0.30689218991492634, variance: 2.9837214946746826, cvar: 1.8669203519821167, v: 0.19707217812538147, mean_q: -1.2168229818344116, std_q: 2.8616673946380615, lamda: 1.067895770072937
Running avgs for agent 1: q_loss: 0.30943605303764343, u_loss: 9.579587936401367, p_loss: 1.1055935621261597, mean_rew: -0.31058595012138523, variance: 2.4888256150484085, cvar: 1.8510832786560059, v: 0.10828806459903717, mean_q: -1.282317876815796, std_q: 2.6740975379943848, lamda: 1.0684101581573486
Running avgs for agent 2: q_loss: 0.017940618097782135, u_loss: 11.198114395141602, p_loss: 2.642150402069092, mean_rew: -0.536497399220257, variance: 1.6099111611247063, cvar: -0.3643268942832947, v: -0.4385288953781128, mean_q: -2.86179256439209, std_q: 4.374291896820068, lamda: 1.0021374225616455
Running avgs for agent 3: q_loss: 0.010369688272476196, u_loss: 7.3035430908203125, p_loss: 2.3152530193328857, mean_rew: -0.49603487526798995, variance: 0.8757856385521591, cvar: -0.36858266592025757, v: -0.41849246621131897, mean_q: -2.4956421852111816, std_q: 3.369738817214966, lamda: 1.0017597675323486

steps: 174975, episodes: 7000, mean episode reward: -24.593425169976438, agent episode reward: [-5.383252986050635, -5.462768096499465, -6.853414265647476, -6.893989821778861], time: 70.147
steps: 174975, episodes: 7000, mean episode variance: 2.1678918264359237, agent episode variance: [0.76169226513803, 0.7302554938793182, 0.4467648159116507, 0.22917925150692464], time: 70.147
steps: 174975, episodes: 7000, mean episode cvar: 0.7248260344266891, agent episode cvar: [0.46398536694049836, 0.4441232531070709, -0.09158409902453422, -0.0916984865963459], time: 70.148
Running avgs for agent 0: q_loss: 0.23829497396945953, u_loss: 10.4547700881958, p_loss: 0.9432686567306519, mean_rew: -0.29153977336662446, variance: 3.046769142150879, cvar: 1.8559415340423584, v: 0.06956444680690765, mean_q: -1.1190255880355835, std_q: 2.948654890060425, lamda: 1.08113431930542
Running avgs for agent 1: q_loss: 0.3549002408981323, u_loss: 11.844463348388672, p_loss: 0.9523418545722961, mean_rew: -0.2961229913355236, variance: 2.9210219383239746, cvar: 1.7764930725097656, v: 0.031766075640916824, mean_q: -1.1196540594100952, std_q: 2.7495975494384766, lamda: 1.085232138633728
Running avgs for agent 2: q_loss: 0.01750524900853634, u_loss: 14.146686553955078, p_loss: 2.5192642211914062, mean_rew: -0.49680884721564944, variance: 1.7870592636466027, cvar: -0.3663364052772522, v: -0.4619574248790741, mean_q: -2.728696823120117, std_q: 4.502891540527344, lamda: 1.0032037496566772
Running avgs for agent 3: q_loss: 0.008855434134602547, u_loss: 10.106954574584961, p_loss: 2.204622507095337, mean_rew: -0.46467218866691495, variance: 0.9167170060276986, cvar: -0.3667939305305481, v: -0.41220328211784363, mean_q: -2.3696837425231934, std_q: 3.393584966659546, lamda: 1.0019869804382324

steps: 199975, episodes: 8000, mean episode reward: -25.11799192711006, agent episode reward: [-5.530563787635457, -5.272600130702228, -7.292234706657434, -7.022593302114943], time: 64.299
steps: 199975, episodes: 8000, mean episode variance: 2.4138727585971353, agent episode variance: [0.8285531569719314, 0.8213861334323883, 0.520840075135231, 0.24309339305758476], time: 64.3
steps: 199975, episodes: 8000, mean episode cvar: 0.7303508659005165, agent episode cvar: [0.4539209122657776, 0.4613880262374878, -0.09261863267421723, -0.09233943992853165], time: 64.3
Running avgs for agent 0: q_loss: 0.3492963910102844, u_loss: 12.841586112976074, p_loss: 0.7918649315834045, mean_rew: -0.2841296761614705, variance: 3.3142127990722656, cvar: 1.8156836032867432, v: 0.06653686612844467, mean_q: -0.9536428451538086, std_q: 2.9830448627471924, lamda: 1.0971356630325317
Running avgs for agent 1: q_loss: 0.3363043963909149, u_loss: 15.838438987731934, p_loss: 0.7244196534156799, mean_rew: -0.2844359662786143, variance: 3.2855446338653564, cvar: 1.8455519676208496, v: 0.10808583348989487, mean_q: -0.8768070340156555, std_q: 2.701512098312378, lamda: 1.1028598546981812
Running avgs for agent 2: q_loss: 0.015612998977303505, u_loss: 25.61014175415039, p_loss: 2.409024953842163, mean_rew: -0.4687204763537286, variance: 2.083360300540924, cvar: -0.37047454714775085, v: -0.47915172576904297, mean_q: -2.600635528564453, std_q: 4.5088043212890625, lamda: 1.0041499137878418
Running avgs for agent 3: q_loss: 0.012700725346803665, u_loss: 11.285299301147461, p_loss: 2.0783047676086426, mean_rew: -0.43612410985388644, variance: 0.972373572230339, cvar: -0.36935776472091675, v: -0.4160614609718323, mean_q: -2.2243261337280273, std_q: 3.3178465366363525, lamda: 1.0023208856582642

steps: 224975, episodes: 9000, mean episode reward: -25.31929588078134, agent episode reward: [-5.6431589509269395, -5.401009045037697, -7.263094279136716, -7.012033605679986], time: 64.087
steps: 224975, episodes: 9000, mean episode variance: 2.281952822783962, agent episode variance: [0.8016155645251274, 0.7654683832228184, 0.48939919435977935, 0.22546968067623674], time: 64.088
steps: 224975, episodes: 9000, mean episode cvar: 0.7248222840428352, agent episode cvar: [0.45240142464637756, 0.4593492730855942, -0.0931302926838398, -0.0937981210052967], time: 64.088
Running avgs for agent 0: q_loss: 0.29175978899002075, u_loss: 14.997217178344727, p_loss: 0.585869312286377, mean_rew: -0.27639159322549106, variance: 3.2064623832702637, cvar: 1.8096057176589966, v: 0.1725948303937912, mean_q: -0.734336256980896, std_q: 2.9029324054718018, lamda: 1.113913893699646
Running avgs for agent 1: q_loss: 0.33434587717056274, u_loss: 17.03917121887207, p_loss: 0.5528178811073303, mean_rew: -0.27596577421998053, variance: 3.061873435974121, cvar: 1.8373969793319702, v: 0.1928643435239792, mean_q: -0.6902466416358948, std_q: 2.6058571338653564, lamda: 1.1186318397521973
Running avgs for agent 2: q_loss: 0.015985233709216118, u_loss: 24.334896087646484, p_loss: 2.313654899597168, mean_rew: -0.44709281634351167, variance: 1.9575967774391174, cvar: -0.37252119183540344, v: -0.49296143651008606, mean_q: -2.4872241020202637, std_q: 4.386068344116211, lamda: 1.0050575733184814
Running avgs for agent 3: q_loss: 0.013878784142434597, u_loss: 12.063514709472656, p_loss: 2.005560874938965, mean_rew: -0.4193013396474877, variance: 0.901878722704947, cvar: -0.3751924932003021, v: -0.43074139952659607, mean_q: -2.1389589309692383, std_q: 3.2578587532043457, lamda: 1.003170371055603

steps: 249975, episodes: 10000, mean episode reward: -25.013722728908625, agent episode reward: [-5.587257463790275, -5.619440354452494, -7.07858505367622, -6.72843985698964], time: 63.832
steps: 249975, episodes: 10000, mean episode variance: 2.249681519869715, agent episode variance: [0.7321452633440495, 0.6810965619087219, 0.5953513700067997, 0.2410883246101439], time: 63.832
steps: 249975, episodes: 10000, mean episode cvar: 0.72258467233181, agent episode cvar: [0.4539198338985443, 0.4537276533842087, -0.09167232686281204, -0.09339048808813095], time: 63.833
Running avgs for agent 0: q_loss: 0.2505837082862854, u_loss: 15.132061958312988, p_loss: 0.41471433639526367, mean_rew: -0.26869121157109715, variance: 2.9285809993743896, cvar: 1.8156793117523193, v: 0.27480918169021606, mean_q: -0.5464011430740356, std_q: 2.7795844078063965, lamda: 1.127361536026001
Running avgs for agent 1: q_loss: 0.24044279754161835, u_loss: 17.42837905883789, p_loss: 0.43490567803382874, mean_rew: -0.27053863863906713, variance: 2.724386215209961, cvar: 1.814910650253296, v: 0.2823468744754791, mean_q: -0.5597889423370361, std_q: 2.5874996185302734, lamda: 1.133333444595337
Running avgs for agent 2: q_loss: 0.018505893647670746, u_loss: 25.079877853393555, p_loss: 2.2373270988464355, mean_rew: -0.4306743643983763, variance: 2.381405480027199, cvar: -0.3666892945766449, v: -0.5046873092651367, mean_q: -2.397413969039917, std_q: 4.31582498550415, lamda: 1.0067073106765747
Running avgs for agent 3: q_loss: 0.01489634346216917, u_loss: 13.573525428771973, p_loss: 1.9257862567901611, mean_rew: -0.40227355956083616, variance: 0.9643532984405756, cvar: -0.37356194853782654, v: -0.4383853077888489, mean_q: -2.046433687210083, std_q: 3.1859233379364014, lamda: 1.0039281845092773

steps: 274975, episodes: 11000, mean episode reward: -25.583113199860342, agent episode reward: [-5.610045731675538, -5.596623536322108, -7.188531826783299, -7.1879121050793975], time: 64.233
steps: 274975, episodes: 11000, mean episode variance: 2.1464868435263633, agent episode variance: [0.674705324947834, 0.5891227699518203, 0.6723815153241157, 0.21027723330259324], time: 64.234
steps: 274975, episodes: 11000, mean episode cvar: 0.7163443264365196, agent episode cvar: [0.45288425862789156, 0.4516224329471588, -0.09059663152694702, -0.09756573361158372], time: 64.234
Running avgs for agent 0: q_loss: 0.23552173376083374, u_loss: 15.744470596313477, p_loss: 0.28050854802131653, mean_rew: -0.2660432611069286, variance: 2.6988213062286377, cvar: 1.8115371465682983, v: 0.3823006749153137, mean_q: -0.40159285068511963, std_q: 2.66607403755188, lamda: 1.140691876411438
Running avgs for agent 1: q_loss: 0.23169595003128052, u_loss: 19.528987884521484, p_loss: 0.2945401072502136, mean_rew: -0.26686682156611835, variance: 2.3564913272857666, cvar: 1.8064897060394287, v: 0.4192456007003784, mean_q: -0.4088403880596161, std_q: 2.5150160789489746, lamda: 1.1458208560943604
Running avgs for agent 2: q_loss: 0.013521354645490646, u_loss: 35.50688934326172, p_loss: 2.1312551498413086, mean_rew: -0.41630309238574187, variance: 2.689526061296463, cvar: -0.3623865246772766, v: -0.4890033006668091, mean_q: -2.279019594192505, std_q: 4.160275459289551, lamda: 1.0081069469451904
Running avgs for agent 3: q_loss: 0.01746433600783348, u_loss: 17.713211059570312, p_loss: 1.8838673830032349, mean_rew: -0.39067984820341206, variance: 0.8411089332103729, cvar: -0.3902629315853119, v: -0.452452689409256, mean_q: -1.995216727256775, std_q: 3.1473422050476074, lamda: 1.0059502124786377

steps: 299975, episodes: 12000, mean episode reward: -25.16574746942187, agent episode reward: [-5.528031938964348, -5.421144136349597, -6.897156616236014, -7.319414777871911], time: 64.442
steps: 299975, episodes: 12000, mean episode variance: 2.0161374517083166, agent episode variance: [0.5962510854005814, 0.5031315054222941, 0.6958239580988884, 0.22093090278655292], time: 64.443
steps: 299975, episodes: 12000, mean episode cvar: 0.7129029302299023, agent episode cvar: [0.4499192777872086, 0.4492538242340088, -0.09089117524027825, -0.09537899655103683], time: 64.443
Running avgs for agent 0: q_loss: 0.20389917492866516, u_loss: 18.282615661621094, p_loss: 0.17453372478485107, mean_rew: -0.2625035118581556, variance: 2.3850042819976807, cvar: 1.7996772527694702, v: 0.4881470799446106, mean_q: -0.28209546208381653, std_q: 2.613050937652588, lamda: 1.1532866954803467
Running avgs for agent 1: q_loss: 0.23519787192344666, u_loss: 18.248456954956055, p_loss: 0.16871589422225952, mean_rew: -0.26277254487484497, variance: 2.0125260216891765, cvar: 1.7970154285430908, v: 0.5320524573326111, mean_q: -0.26990368962287903, std_q: 2.4008870124816895, lamda: 1.1586238145828247
Running avgs for agent 2: q_loss: 0.011605205945670605, u_loss: 32.74884033203125, p_loss: 2.065157413482666, mean_rew: -0.4051421824466713, variance: 2.7832958323955537, cvar: -0.36356469988822937, v: -0.47861871123313904, mean_q: -2.2019145488739014, std_q: 4.0771894454956055, lamda: 1.0089786052703857
Running avgs for agent 3: q_loss: 0.00925720576196909, u_loss: 15.669954299926758, p_loss: 1.8256950378417969, mean_rew: -0.3822575850396392, variance: 0.8837236111462117, cvar: -0.3815159499645233, v: -0.45751330256462097, mean_q: -1.9274531602859497, std_q: 3.036165952682495, lamda: 1.006846308708191

steps: 324975, episodes: 13000, mean episode reward: -25.255256367313113, agent episode reward: [-5.489467087666537, -5.578879483185243, -6.763597848803939, -7.423311947657393], time: 64.139
steps: 324975, episodes: 13000, mean episode variance: 1.9427551974914967, agent episode variance: [0.5105373714715242, 0.45062558159977195, 0.7572151498198509, 0.22437709460034966], time: 64.139
steps: 324975, episodes: 13000, mean episode cvar: 0.7279889798164367, agent episode cvar: [0.4538824075460434, 0.45857179307937623, -0.09027215704321861, -0.09419306376576424], time: 64.14
Running avgs for agent 0: q_loss: 0.2008204460144043, u_loss: 15.77298641204834, p_loss: 0.06460559368133545, mean_rew: -0.26005441123188644, variance: 2.042149543762207, cvar: 1.815529704093933, v: 0.5932583808898926, mean_q: -0.1647942215204239, std_q: 2.536215305328369, lamda: 1.1665029525756836
Running avgs for agent 1: q_loss: 0.24608413875102997, u_loss: 19.2105655670166, p_loss: 0.09274730086326599, mean_rew: -0.25894357597796136, variance: 1.8025022745132446, cvar: 1.834287166595459, v: 0.5466575026512146, mean_q: -0.18355225026607513, std_q: 2.330176830291748, lamda: 1.1733437776565552
Running avgs for agent 2: q_loss: 0.011494973674416542, u_loss: 36.03773498535156, p_loss: 2.016571044921875, mean_rew: -0.39639935537940935, variance: 3.0288605992794038, cvar: -0.36108866333961487, v: -0.47337302565574646, mean_q: -2.143739700317383, std_q: 3.986421823501587, lamda: 1.0096991062164307
Running avgs for agent 3: q_loss: 0.00885466206818819, u_loss: 19.676477432250977, p_loss: 1.7751619815826416, mean_rew: -0.37583989447852356, variance: 0.8975083784013986, cvar: -0.3767722547054291, v: -0.4488101899623871, mean_q: -1.8693017959594727, std_q: 2.988748073577881, lamda: 1.007401466369629

steps: 349975, episodes: 14000, mean episode reward: -25.270646142120373, agent episode reward: [-5.604181757394068, -5.522293709296619, -7.02503909857956, -7.119131576850125], time: 64.154
steps: 349975, episodes: 14000, mean episode variance: 1.800733067350462, agent episode variance: [0.4121529584825039, 0.36736204466596245, 0.7775222408026456, 0.24369582339935006], time: 64.154
steps: 349975, episodes: 14000, mean episode cvar: 0.7230388071238995, agent episode cvar: [0.4529037975072861, 0.45478290700912477, -0.08993248498439789, -0.09471541240811349], time: 64.155
Running avgs for agent 0: q_loss: 0.16898302733898163, u_loss: 19.520917892456055, p_loss: -0.03527132421731949, mean_rew: -0.2568592776827362, variance: 1.6486118339300155, cvar: 1.811615228652954, v: 0.7010993361473083, mean_q: -0.06125316768884659, std_q: 2.4884033203125, lamda: 1.1778862476348877
Running avgs for agent 1: q_loss: 0.21040180325508118, u_loss: 18.400402069091797, p_loss: 0.021717872470617294, mean_rew: -0.25638225055134645, variance: 1.4694481786638498, cvar: 1.81913161277771, v: 0.6093273758888245, mean_q: -0.10828907787799835, std_q: 2.284742832183838, lamda: 1.1864444017410278
Running avgs for agent 2: q_loss: 0.010717215947806835, u_loss: 37.92405319213867, p_loss: 1.9568668603897095, mean_rew: -0.3856076462405367, variance: 3.1100889632105826, cvar: -0.35972991585731506, v: -0.47276079654693604, mean_q: -2.072878837585449, std_q: 3.8167552947998047, lamda: 1.0105053186416626
Running avgs for agent 3: q_loss: 0.008408879861235619, u_loss: 18.739574432373047, p_loss: 1.7218559980392456, mean_rew: -0.37018629123864605, variance: 0.9747832935974002, cvar: -0.3788616359233856, v: -0.4345269203186035, mean_q: -1.808666706085205, std_q: 2.9552621841430664, lamda: 1.0078377723693848

steps: 374975, episodes: 15000, mean episode reward: -25.599772676217764, agent episode reward: [-5.773403761441674, -5.384542769455097, -7.140052247846757, -7.301773897474233], time: 64.711
steps: 374975, episodes: 15000, mean episode variance: 1.6561083372980356, agent episode variance: [0.3414616194963455, 0.27085198514908554, 0.7995111973285675, 0.24428353532403707], time: 64.711
steps: 374975, episodes: 15000, mean episode cvar: 0.7218182266056538, agent episode cvar: [0.45397302496433256, 0.4532446895837784, -0.0919813649058342, -0.093418123036623], time: 64.712
Running avgs for agent 0: q_loss: 0.1581474244594574, u_loss: 21.571855545043945, p_loss: -0.12348570674657822, mean_rew: -0.2536398205898438, variance: 1.365846477985382, cvar: 1.8158921003341675, v: 0.7717989683151245, mean_q: 0.03239436447620392, std_q: 2.4166781902313232, lamda: 1.1886358261108398
Running avgs for agent 1: q_loss: 0.20272336900234222, u_loss: 20.62405776977539, p_loss: -0.04955499246716499, mean_rew: -0.25387652560179264, variance: 1.0834079405963422, cvar: 1.812978744506836, v: 0.6837825775146484, mean_q: -0.03367346525192261, std_q: 2.22898530960083, lamda: 1.197862982749939
Running avgs for agent 2: q_loss: 0.013365760445594788, u_loss: 34.02859878540039, p_loss: 1.9107779264450073, mean_rew: -0.3788270609428253, variance: 3.198045015335083, cvar: -0.3679254651069641, v: -0.4490785598754883, mean_q: -2.0190367698669434, std_q: 3.796802282333374, lamda: 1.0112287998199463
Running avgs for agent 3: q_loss: 0.009036554023623466, u_loss: 15.843324661254883, p_loss: 1.6598753929138184, mean_rew: -0.36251934642934996, variance: 0.9771341412961483, cvar: -0.3736724853515625, v: -0.4248035252094269, mean_q: -1.7409892082214355, std_q: 2.8408119678497314, lamda: 1.0084158182144165

steps: 399975, episodes: 16000, mean episode reward: -24.86566347220743, agent episode reward: [-5.857535194989505, -5.221189326330773, -6.780852643409227, -7.006086307477927], time: 64.812
steps: 399975, episodes: 16000, mean episode variance: 1.4684228670047597, agent episode variance: [0.24435949835181237, 0.17338709006458522, 0.8009273728728294, 0.2497489057155326], time: 64.813
steps: 399975, episodes: 16000, mean episode cvar: 0.7240082358717919, agent episode cvar: [0.45280663561820983, 0.45626317596435545, -0.09190718448162079, -0.09315439122915269], time: 64.813
Running avgs for agent 0: q_loss: 0.14904339611530304, u_loss: 18.187156677246094, p_loss: -0.19401122629642487, mean_rew: -0.25361207623212845, variance: 0.9774379934072495, cvar: 1.8112266063690186, v: 0.8552688956260681, mean_q: 0.10833679884672165, std_q: 2.3729264736175537, lamda: 1.1981431245803833
Running avgs for agent 1: q_loss: 0.1884426772594452, u_loss: 20.997318267822266, p_loss: -0.12751725316047668, mean_rew: -0.24988709984932145, variance: 0.6935483602583409, cvar: 1.8250527381896973, v: 0.7199612855911255, mean_q: 0.05029469355940819, std_q: 2.1395702362060547, lamda: 1.2115520238876343
Running avgs for agent 2: q_loss: 0.011286691762506962, u_loss: 32.03781509399414, p_loss: 1.8561393022537231, mean_rew: -0.37461096812636013, variance: 3.203709363937378, cvar: -0.36762872338294983, v: -0.4165526032447815, mean_q: -1.9589035511016846, std_q: 3.712876319885254, lamda: 1.0118225812911987
Running avgs for agent 3: q_loss: 0.010440162383019924, u_loss: 11.626108169555664, p_loss: 1.631636142730713, mean_rew: -0.3585389476486979, variance: 0.9989956228621304, cvar: -0.37261757254600525, v: -0.435972660779953, mean_q: -1.7088700532913208, std_q: 2.7916510105133057, lamda: 1.0094469785690308

steps: 424975, episodes: 17000, mean episode reward: -25.22953008177149, agent episode reward: [-5.667619212754138, -5.067948694559179, -7.039708506654423, -7.454253667803749], time: 64.537
steps: 424975, episodes: 17000, mean episode variance: 1.2608426774665713, agent episode variance: [0.1597929392531514, 0.11355833563953638, 0.7528666558265686, 0.23462474674731493], time: 64.538
steps: 424975, episodes: 17000, mean episode cvar: 0.7238017179369927, agent episode cvar: [0.45388087511062625, 0.4558578751087189, -0.09160962355136872, -0.09432740873098373], time: 64.538
Running avgs for agent 0: q_loss: 0.1234651654958725, u_loss: 16.428531646728516, p_loss: -0.2819337546825409, mean_rew: -0.25160758171804576, variance: 0.6391717570126056, cvar: 1.8155235052108765, v: 0.9475134015083313, mean_q: 0.20225653052330017, std_q: 2.3195295333862305, lamda: 1.2066338062286377
Running avgs for agent 1: q_loss: 0.168417826294899, u_loss: 17.859926223754883, p_loss: -0.19289354979991913, mean_rew: -0.24905286416149286, variance: 0.4542333425581455, cvar: 1.8234314918518066, v: 0.7684105634689331, mean_q: 0.11648053675889969, std_q: 2.1282482147216797, lamda: 1.2222371101379395
Running avgs for agent 2: q_loss: 0.008423976600170135, u_loss: 28.957691192626953, p_loss: 1.7835713624954224, mean_rew: -0.3653541735611367, variance: 3.0114667415618896, cvar: -0.36643847823143005, v: -0.3896929621696472, mean_q: -1.878852128982544, std_q: 3.5734379291534424, lamda: 1.0123264789581299
Running avgs for agent 3: q_loss: 0.00766612496227026, u_loss: 9.898096084594727, p_loss: 1.5900213718414307, mean_rew: -0.3547462014494406, variance: 0.9384989869892597, cvar: -0.3773096203804016, v: -0.42015698552131653, mean_q: -1.6640028953552246, std_q: 2.748883008956909, lamda: 1.010007619857788

steps: 449975, episodes: 18000, mean episode reward: -25.36427522324734, agent episode reward: [-5.380002184386512, -5.115542783614821, -6.807472351250728, -8.061257903995276], time: 64.24
steps: 449975, episodes: 18000, mean episode variance: 1.2101746615320443, agent episode variance: [0.09443731333315372, 0.07899517080187797, 0.7845454355478286, 0.25219674184918406], time: 64.241
steps: 449975, episodes: 18000, mean episode cvar: 0.7200984861254692, agent episode cvar: [0.45308472895622254, 0.4537319794893265, -0.09215540960431098, -0.09456281271576882], time: 64.241
Running avgs for agent 0: q_loss: 0.13632863759994507, u_loss: 19.036903381347656, p_loss: -0.36518463492393494, mean_rew: -0.2498283769117751, variance: 0.3777492533326149, cvar: 1.8123388290405273, v: 1.018408179283142, mean_q: 0.2889516055583954, std_q: 2.2748472690582275, lamda: 1.2148808240890503
Running avgs for agent 1: q_loss: 0.136066734790802, u_loss: 17.224437713623047, p_loss: -0.2892683744430542, mean_rew: -0.24654534629116837, variance: 0.3159806832075119, cvar: 1.8149279356002808, v: 0.8710938692092896, mean_q: 0.21667779982089996, std_q: 2.086552381515503, lamda: 1.231621265411377
Running avgs for agent 2: q_loss: 0.013103626668453217, u_loss: 32.30049514770508, p_loss: 1.7278450727462769, mean_rew: -0.36043032691416105, variance: 3.1381819248199463, cvar: -0.3686216473579407, v: -0.37535127997398376, mean_q: -1.8182965517044067, std_q: 3.559480905532837, lamda: 1.0126616954803467
Running avgs for agent 3: q_loss: 0.007939329370856285, u_loss: 9.339824676513672, p_loss: 1.5324769020080566, mean_rew: -0.34936476759624085, variance: 1.0087869673967362, cvar: -0.37825125455856323, v: -0.40773284435272217, mean_q: -1.602941870689392, std_q: 2.663318157196045, lamda: 1.0104572772979736

steps: 474975, episodes: 19000, mean episode reward: -25.167120903483294, agent episode reward: [-5.3662104446856755, -5.254130469550182, -6.932471007554028, -7.61430898169341], time: 64.378
steps: 474975, episodes: 19000, mean episode variance: 1.1750586059466004, agent episode variance: [0.06623930089175702, 0.04356401707977056, 0.7981621021032334, 0.2670931858718395], time: 64.379
steps: 474975, episodes: 19000, mean episode cvar: 0.7278715892136097, agent episode cvar: [0.4587297739982605, 0.4552356106042862, -0.09223301115632057, -0.09386078423261643], time: 64.379
Running avgs for agent 0: q_loss: 0.12092507630586624, u_loss: 17.927839279174805, p_loss: -0.4319193661212921, mean_rew: -0.24815711458994164, variance: 0.26495720356702807, cvar: 1.8349190950393677, v: 1.0953420400619507, mean_q: 0.3572796881198883, std_q: 2.2126269340515137, lamda: 1.2235454320907593
Running avgs for agent 1: q_loss: 0.14263805747032166, u_loss: 16.541566848754883, p_loss: -0.3791704773902893, mean_rew: -0.24355814079598648, variance: 0.17425606831908225, cvar: 1.8209424018859863, v: 0.9472814798355103, mean_q: 0.3081148862838745, std_q: 2.0053906440734863, lamda: 1.240421175956726
Running avgs for agent 2: q_loss: 0.009460489265620708, u_loss: 44.67727279663086, p_loss: 1.6899070739746094, mean_rew: -0.35754131115555376, variance: 3.192648410797119, cvar: -0.3689320385456085, v: -0.3773668706417084, mean_q: -1.778122067451477, std_q: 3.5217196941375732, lamda: 1.0134146213531494
Running avgs for agent 3: q_loss: 0.009041374549269676, u_loss: 9.146649360656738, p_loss: 1.5134021043777466, mean_rew: -0.3494994547676544, variance: 1.068372743487358, cvar: -0.37544316053390503, v: -0.3960830867290497, mean_q: -1.580202341079712, std_q: 2.616924524307251, lamda: 1.0110139846801758

steps: 499975, episodes: 20000, mean episode reward: -25.34707780719003, agent episode reward: [-5.594803041654405, -5.339731801028329, -7.116648375523352, -7.29589458898394], time: 64.576
steps: 499975, episodes: 20000, mean episode variance: 1.0599806812182069, agent episode variance: [0.019793086223304272, 0.023325729593634607, 0.7157314349412918, 0.3011304304599762], time: 64.577
steps: 499975, episodes: 20000, mean episode cvar: 0.7283724277019501, agent episode cvar: [0.45730147922039033, 0.4572333570718765, -0.09225512021780014, -0.09390728837251663], time: 64.577
Running avgs for agent 0: q_loss: 0.1288924664258957, u_loss: 16.776029586791992, p_loss: -0.49384570121765137, mean_rew: -0.24595114426457973, variance: 0.07917234489321709, cvar: 1.8292057514190674, v: 1.126763939857483, mean_q: 0.4235925078392029, std_q: 2.113264799118042, lamda: 1.230954885482788
Running avgs for agent 1: q_loss: 0.13616988062858582, u_loss: 17.662473678588867, p_loss: -0.46451324224472046, mean_rew: -0.24120806159541155, variance: 0.09330291837453843, cvar: 1.8289333581924438, v: 1.0191665887832642, mean_q: 0.39485132694244385, std_q: 1.934531569480896, lamda: 1.2495211362838745
Running avgs for agent 2: q_loss: 0.01312775444239378, u_loss: 27.182939529418945, p_loss: 1.6413791179656982, mean_rew: -0.35243018843769414, variance: 2.8629257678985596, cvar: -0.3690204918384552, v: -0.3785293400287628, mean_q: -1.7243475914001465, std_q: 3.419375419616699, lamda: 1.014548897743225
Running avgs for agent 3: q_loss: 0.01195533387362957, u_loss: 9.8746919631958, p_loss: 1.4836260080337524, mean_rew: -0.3465804014359727, variance: 1.2045217218399047, cvar: -0.37562915682792664, v: -0.38952839374542236, mean_q: -1.5497831106185913, std_q: 2.5756139755249023, lamda: 1.0119242668151855

steps: 524975, episodes: 21000, mean episode reward: -25.601881810217886, agent episode reward: [-5.6471673300762415, -4.860846708952779, -7.468064235428474, -7.625803535760392], time: 64.26
steps: 524975, episodes: 21000, mean episode variance: 1.0714706595167518, agent episode variance: [0.00513226255774498, 0.022457402653992178, 0.7077642436176539, 0.33611675068736074], time: 64.26
steps: 524975, episodes: 21000, mean episode cvar: 0.7275443317890167, agent episode cvar: [0.4582047139406204, 0.45741775465011597, -0.09320647901296615, -0.0948716577887535], time: 64.261
Running avgs for agent 0: q_loss: 0.1137346476316452, u_loss: 17.191905975341797, p_loss: -0.5252156257629395, mean_rew: -0.2470789940143659, variance: 0.02052905023097992, cvar: 1.832818865776062, v: 1.1557157039642334, mean_q: 0.45627567172050476, std_q: 2.108346939086914, lamda: 1.2387136220932007
Running avgs for agent 1: q_loss: 0.10847207903862, u_loss: 14.72745132446289, p_loss: -0.5302591323852539, mean_rew: -0.240697783786028, variance: 0.08982961061596871, cvar: 1.829671025276184, v: 1.118087887763977, mean_q: 0.4631110727787018, std_q: 1.9617215394973755, lamda: 1.2575432062149048
Running avgs for agent 2: q_loss: 0.012765489518642426, u_loss: 36.694190979003906, p_loss: 1.6048249006271362, mean_rew: -0.3505994473749113, variance: 2.8310569744706156, cvar: -0.3728259205818176, v: -0.3803071975708008, mean_q: -1.6855764389038086, std_q: 3.35429048538208, lamda: 1.0162580013275146
Running avgs for agent 3: q_loss: 0.007683196105062962, u_loss: 8.623320579528809, p_loss: 1.454738736152649, mean_rew: -0.3435074615336499, variance: 1.344467043876648, cvar: -0.379486620426178, v: -0.387801855802536, mean_q: -1.5138636827468872, std_q: 2.5216331481933594, lamda: 1.0132269859313965

steps: 549975, episodes: 22000, mean episode reward: -25.165080675903504, agent episode reward: [-5.370985896700909, -4.662548923558693, -6.91534269787855, -8.216203157765353], time: 64.883
steps: 549975, episodes: 22000, mean episode variance: 0.9902173832766712, agent episode variance: [0.004886040210723877, 0.008976974442601204, 0.6370340704917907, 0.33932029813155534], time: 64.884
steps: 549975, episodes: 22000, mean episode cvar: 0.7317002364099026, agent episode cvar: [0.4600905749797821, 0.4610412304401398, -0.09367532747983932, -0.09575624153017998], time: 64.884
Running avgs for agent 0: q_loss: 0.10520564019680023, u_loss: 12.316699028015137, p_loss: -0.5373099446296692, mean_rew: -0.24384443939121253, variance: 0.019544160842895508, cvar: 1.840362310409546, v: 1.1400963068008423, mean_q: 0.4714961051940918, std_q: 2.060699701309204, lamda: 1.2471983432769775
Running avgs for agent 1: q_loss: 0.10843940824270248, u_loss: 16.562929153442383, p_loss: -0.5926918983459473, mean_rew: -0.2371806994026089, variance: 0.03590789777040482, cvar: 1.8441649675369263, v: 1.2067749500274658, mean_q: 0.526582658290863, std_q: 1.9496879577636719, lamda: 1.2640007734298706
Running avgs for agent 2: q_loss: 0.01274238619953394, u_loss: 35.354190826416016, p_loss: 1.5770864486694336, mean_rew: -0.3478921641704911, variance: 2.548136281967163, cvar: -0.37470129132270813, v: -0.38264545798301697, mean_q: -1.6537995338439941, std_q: 3.2941620349884033, lamda: 1.0183358192443848
Running avgs for agent 3: q_loss: 0.007042211946099997, u_loss: 8.256585121154785, p_loss: 1.4472194910049438, mean_rew: -0.344099435633269, variance: 1.35728120803833, cvar: -0.3830249607563019, v: -0.3936942219734192, mean_q: -1.504865288734436, std_q: 2.507317066192627, lamda: 1.013899803161621

steps: 574975, episodes: 23000, mean episode reward: -25.653022451187788, agent episode reward: [-5.303839769039188, -5.1343601861325805, -6.978422279310908, -8.23640021670511], time: 64.674
steps: 574975, episodes: 23000, mean episode variance: 0.9524326266273856, agent episode variance: [0.0005204408168792725, 0.0024451617896556854, 0.5916142461821436, 0.35785277783870695], time: 64.674
steps: 574975, episodes: 23000, mean episode cvar: 0.7337507425844669, agent episode cvar: [0.4630779641866684, 0.4611270046234131, -0.09328896194696426, -0.09716526427865028], time: 64.675
Running avgs for agent 0: q_loss: 0.10970558226108551, u_loss: 13.35143756866455, p_loss: -0.5016412138938904, mean_rew: -0.24216456430896144, variance: 0.00208176326751709, cvar: 1.8523119688034058, v: 1.1062169075012207, mean_q: 0.4373839497566223, std_q: 2.059300184249878, lamda: 1.2550972700119019
Running avgs for agent 1: q_loss: 0.10499026626348495, u_loss: 12.041695594787598, p_loss: -0.632801353931427, mean_rew: -0.23533994246286813, variance: 0.009780647158622742, cvar: 1.8445080518722534, v: 1.2557282447814941, mean_q: 0.5699187517166138, std_q: 1.8891725540161133, lamda: 1.2706900835037231
Running avgs for agent 2: q_loss: 0.016164829954504967, u_loss: 30.21451187133789, p_loss: 1.546264410018921, mean_rew: -0.34396194630415755, variance: 2.366456985473633, cvar: -0.3731558620929718, v: -0.38183799386024475, mean_q: -1.6180799007415771, std_q: 3.205350637435913, lamda: 1.0205570459365845
Running avgs for agent 3: q_loss: 0.01325633842498064, u_loss: 8.398301124572754, p_loss: 1.4257062673568726, mean_rew: -0.3434072992344664, variance: 1.4314111471176147, cvar: -0.38866108655929565, v: -0.4054451882839203, mean_q: -1.4807473421096802, std_q: 2.486001491546631, lamda: 1.0160183906555176

steps: 599975, episodes: 24000, mean episode reward: -25.695843399599465, agent episode reward: [-5.248949745139729, -5.596909389068056, -6.9023553984456685, -7.94762886694601], time: 67.449
steps: 599975, episodes: 24000, mean episode variance: 0.9847116493582726, agent episode variance: [0.0, 0.0005080050230026245, 0.6359651321172715, 0.3482385122179985], time: 67.449
steps: 599975, episodes: 24000, mean episode cvar: 0.7325709057450295, agent episode cvar: [0.4612991898059845, 0.4626451642513275, -0.09469216763973236, -0.0966812806725502], time: 67.45
Running avgs for agent 0: q_loss: 0.12573598325252533, u_loss: 18.360275268554688, p_loss: -0.47316840291023254, mean_rew: -0.24152078390487522, variance: 0.0, cvar: 1.8451967239379883, v: 1.0809471607208252, mean_q: 0.41408002376556396, std_q: 2.059023141860962, lamda: 1.2644000053405762
Running avgs for agent 1: q_loss: 0.09325864911079407, u_loss: 10.899740219116211, p_loss: -0.6488524079322815, mean_rew: -0.23468985106312823, variance: 0.002032020092010498, cvar: 1.8505806922912598, v: 1.251620888710022, mean_q: 0.5893138647079468, std_q: 1.8684968948364258, lamda: 1.2781915664672852
Running avgs for agent 2: q_loss: 0.01024610735476017, u_loss: 34.99543762207031, p_loss: 1.526990532875061, mean_rew: -0.34241872459710215, variance: 2.543860673904419, cvar: -0.3787686824798584, v: -0.38664937019348145, mean_q: -1.595907211303711, std_q: 3.169687032699585, lamda: 1.02293860912323
Running avgs for agent 3: q_loss: 0.009219729341566563, u_loss: 8.828425407409668, p_loss: 1.3876521587371826, mean_rew: -0.3415624367584551, variance: 1.3929541110992432, cvar: -0.3867250978946686, v: -0.39823517203330994, mean_q: -1.4434196949005127, std_q: 2.449432134628296, lamda: 1.0178453922271729

steps: 624975, episodes: 25000, mean episode reward: -25.680666451512103, agent episode reward: [-5.581860468044738, -5.13704076880956, -7.095332111954175, -7.866433102703631], time: 69.434
steps: 624975, episodes: 25000, mean episode variance: 1.0082025248408317, agent episode variance: [3.786015510559082e-05, 0.0, 0.6616372945904732, 0.346527370095253], time: 69.434
steps: 624975, episodes: 25000, mean episode cvar: 0.7327354151010513, agent episode cvar: [0.46181222081184387, 0.46320762526988984, -0.09553270789980889, -0.09675172308087349], time: 69.435
Running avgs for agent 0: q_loss: 0.14797315001487732, u_loss: 10.63949966430664, p_loss: -0.49415189027786255, mean_rew: -0.2402004098131677, variance: 0.00015144062042236328, cvar: 1.8472487926483154, v: 1.086390495300293, mean_q: 0.438416451215744, std_q: 1.994089126586914, lamda: 1.278062343597412
Running avgs for agent 1: q_loss: 0.10641249269247055, u_loss: 15.740107536315918, p_loss: -0.6647352576255798, mean_rew: -0.23332138048288867, variance: 0.0, cvar: 1.852830410003662, v: 1.24672532081604, mean_q: 0.6070998311042786, std_q: 1.8926513195037842, lamda: 1.2851389646530151
Running avgs for agent 2: q_loss: 0.010776404291391373, u_loss: 31.343276977539062, p_loss: 1.5061113834381104, mean_rew: -0.34004870022606043, variance: 2.6465489864349365, cvar: -0.38213083148002625, v: -0.3916671872138977, mean_q: -1.5745434761047363, std_q: 3.203233480453491, lamda: 1.0255056619644165
Running avgs for agent 3: q_loss: 0.015240329317748547, u_loss: 6.807480335235596, p_loss: 1.3684762716293335, mean_rew: -0.34173129908577243, variance: 1.3861093521118164, cvar: -0.3870069086551666, v: -0.39995700120925903, mean_q: -1.426268219947815, std_q: 2.3935904502868652, lamda: 1.020363450050354

steps: 649975, episodes: 26000, mean episode reward: -25.739612986684726, agent episode reward: [-5.631287696132355, -4.926142465676696, -7.495889409653468, -7.686293415222205], time: 71.458
steps: 649975, episodes: 26000, mean episode variance: 0.9581087512373925, agent episode variance: [0.0, 0.0, 0.6140322675108909, 0.34407648372650146], time: 71.458
steps: 649975, episodes: 26000, mean episode cvar: 0.7323915655910969, agent episode cvar: [0.4609588857889175, 0.4653752723932266, -0.09756824600696563, -0.09637434658408164], time: 71.459
Running avgs for agent 0: q_loss: 0.1042119488120079, u_loss: 11.7664794921875, p_loss: -0.4876743257045746, mean_rew: -0.24015131751055724, variance: 0.0, cvar: 1.843835473060608, v: 1.0943456888198853, mean_q: 0.43162763118743896, std_q: 2.0179824829101562, lamda: 1.2874619960784912
Running avgs for agent 1: q_loss: 0.10127423703670502, u_loss: 15.503410339355469, p_loss: -0.6420654654502869, mean_rew: -0.2325558692230621, variance: 0.0, cvar: 1.8615010976791382, v: 1.2088557481765747, mean_q: 0.5857199430465698, std_q: 1.8812061548233032, lamda: 1.293590784072876
Running avgs for agent 2: q_loss: 0.016434453427791595, u_loss: 27.084308624267578, p_loss: 1.4615410566329956, mean_rew: -0.33708514768356884, variance: 2.4561288356781006, cvar: -0.39027297496795654, v: -0.40171167254447937, mean_q: -1.5276881456375122, std_q: 3.1023809909820557, lamda: 1.0308459997177124
Running avgs for agent 3: q_loss: 0.008252515457570553, u_loss: 9.2322998046875, p_loss: 1.3425084352493286, mean_rew: -0.3393116213778554, variance: 1.3763059377670288, cvar: -0.3854973614215851, v: -0.39312082529067993, mean_q: -1.401029348373413, std_q: 2.3524298667907715, lamda: 1.0222218036651611

steps: 674975, episodes: 27000, mean episode reward: -25.72960158893617, agent episode reward: [-5.103232961705839, -4.549797267477035, -7.855501251846112, -8.221070107907181], time: 64.755
steps: 674975, episodes: 27000, mean episode variance: 0.9452984362244606, agent episode variance: [6.586247682571411e-05, 0.0, 0.5818354731798172, 0.3633971005678177], time: 64.756
steps: 674975, episodes: 27000, mean episode cvar: 0.7318415004014969, agent episode cvar: [0.4638734620809555, 0.46368782508373263, -0.09799831622838974, -0.09772147053480149], time: 64.757
Running avgs for agent 0: q_loss: 0.12399891763925552, u_loss: 11.614923477172852, p_loss: -0.4837137460708618, mean_rew: -0.2381639257621552, variance: 0.00026344990730285645, cvar: 1.8554939031600952, v: 1.1115118265151978, mean_q: 0.4276001453399658, std_q: 1.9695820808410645, lamda: 1.29701566696167
Running avgs for agent 1: q_loss: 0.1033242866396904, u_loss: 13.967816352844238, p_loss: -0.6554134488105774, mean_rew: -0.23147778437639552, variance: 0.0, cvar: 1.8547512292861938, v: 1.2318389415740967, mean_q: 0.6003099083900452, std_q: 1.840844750404358, lamda: 1.301855206489563
Running avgs for agent 2: q_loss: 0.016637569293379784, u_loss: 26.798791885375977, p_loss: 1.4220999479293823, mean_rew: -0.3360967949975931, variance: 2.3273417949676514, cvar: -0.39199328422546387, v: -0.40202638506889343, mean_q: -1.48779296875, std_q: 3.0452890396118164, lamda: 1.0360512733459473
Running avgs for agent 3: q_loss: 0.010901860892772675, u_loss: 7.608929634094238, p_loss: 1.336740493774414, mean_rew: -0.34007161004540065, variance: 1.4535883665084839, cvar: -0.3908858597278595, v: -0.4001480042934418, mean_q: -1.392449140548706, std_q: 2.315965414047241, lamda: 1.0237761735916138

steps: 699975, episodes: 28000, mean episode reward: -24.84594981566798, agent episode reward: [-4.4503810860487265, -4.405664146963183, -7.655944398222458, -8.33396018443361], time: 66.207
steps: 699975, episodes: 28000, mean episode variance: 0.9824865688681602, agent episode variance: [0.0, 0.00014148518443107605, 0.6043933310210705, 0.3779517526626587], time: 66.208
steps: 699975, episodes: 28000, mean episode cvar: 0.725836353033781, agent episode cvar: [0.4598753170967102, 0.4649454902410507, -0.09880738401412964, -0.10017707028985023], time: 66.208
Running avgs for agent 0: q_loss: 0.10637674480676651, u_loss: 9.509062767028809, p_loss: -0.5153791904449463, mean_rew: -0.23766359346581328, variance: 0.0, cvar: 1.8395012617111206, v: 1.1840200424194336, mean_q: 0.45896095037460327, std_q: 1.9847530126571655, lamda: 1.3075326681137085
Running avgs for agent 1: q_loss: 0.0985560268163681, u_loss: 12.217130661010742, p_loss: -0.6505311131477356, mean_rew: -0.2305555783544223, variance: 0.0005659407377243042, cvar: 1.8597819805145264, v: 1.2180699110031128, mean_q: 0.5964052677154541, std_q: 1.851633906364441, lamda: 1.3111618757247925
Running avgs for agent 2: q_loss: 0.02044636942446232, u_loss: 28.88754653930664, p_loss: 1.3851239681243896, mean_rew: -0.3347581317135079, variance: 2.4175732135772705, cvar: -0.3952295482158661, v: -0.40712395310401917, mean_q: -1.448477029800415, std_q: 2.9610493183135986, lamda: 1.0436420440673828
Running avgs for agent 3: q_loss: 0.012580261565744877, u_loss: 8.213743209838867, p_loss: 1.3094552755355835, mean_rew: -0.3385635340899906, variance: 1.5118069648742676, cvar: -0.40070828795433044, v: -0.4122677147388458, mean_q: -1.3626210689544678, std_q: 2.2885637283325195, lamda: 1.0268771648406982

steps: 724975, episodes: 29000, mean episode reward: -24.68150925624749, agent episode reward: [-4.278001807910314, -4.5192717145462415, -7.431888597426372, -8.452347136364565], time: 67.046
steps: 724975, episodes: 29000, mean episode variance: 1.0421711960434914, agent episode variance: [0.0, 0.0, 0.6349200055003166, 0.40725119054317477], time: 67.047
steps: 724975, episodes: 29000, mean episode cvar: 0.7226392028331756, agent episode cvar: [0.46011348044872286, 0.46428097641468047, -0.09848091864585877, -0.10327433538436889], time: 67.047
Running avgs for agent 0: q_loss: 0.09369299560785294, u_loss: 10.649343490600586, p_loss: -0.5826627612113953, mean_rew: -0.23365937194569408, variance: 0.0, cvar: 1.840453863143921, v: 1.2383333444595337, mean_q: 0.5286033153533936, std_q: 1.9143110513687134, lamda: 1.3154866695404053
Running avgs for agent 1: q_loss: 0.08927206695079803, u_loss: 13.72485065460205, p_loss: -0.6661808490753174, mean_rew: -0.22809977797128742, variance: 0.0, cvar: 1.8571237325668335, v: 1.2373148202896118, mean_q: 0.6144219636917114, std_q: 1.824401617050171, lamda: 1.3207619190216064
Running avgs for agent 2: q_loss: 0.014966814778745174, u_loss: 24.839427947998047, p_loss: 1.3639453649520874, mean_rew: -0.3341748766726841, variance: 2.539680242538452, cvar: -0.39392367005348206, v: -0.4020097851753235, mean_q: -1.4246667623519897, std_q: 2.885446071624756, lamda: 1.047018051147461
Running avgs for agent 3: q_loss: 0.013678794726729393, u_loss: 6.1349687576293945, p_loss: 1.2807387113571167, mean_rew: -0.3394654401313692, variance: 1.6290045976638794, cvar: -0.4130973517894745, v: -0.4231094717979431, mean_q: -1.3329172134399414, std_q: 2.2885429859161377, lamda: 1.0300889015197754

steps: 749975, episodes: 30000, mean episode reward: -25.285959682736568, agent episode reward: [-4.2100577248389115, -4.299344366626721, -7.38655540381507, -9.390002187455863], time: 65.415
steps: 749975, episodes: 30000, mean episode variance: 1.0610086811184882, agent episode variance: [0.0, 0.0, 0.6207658823728561, 0.44024279874563216], time: 65.416
steps: 749975, episodes: 30000, mean episode cvar: 0.7269076572060585, agent episode cvar: [0.4670306580066681, 0.46523804223537446, -0.09868230080604554, -0.1066787422299385], time: 65.416
Running avgs for agent 0: q_loss: 0.13274319469928741, u_loss: 10.154953002929688, p_loss: -0.5906466841697693, mean_rew: -0.23087424437531318, variance: 0.0, cvar: 1.8681225776672363, v: 1.2683093547821045, mean_q: 0.5357844233512878, std_q: 1.8793249130249023, lamda: 1.3240431547164917
Running avgs for agent 1: q_loss: 0.10347544401884079, u_loss: 11.324603080749512, p_loss: -0.673321008682251, mean_rew: -0.22750212542680323, variance: 0.0, cvar: 1.8609521389007568, v: 1.2691705226898193, mean_q: 0.6222520470619202, std_q: 1.8901286125183105, lamda: 1.3294732570648193
Running avgs for agent 2: q_loss: 0.016334131360054016, u_loss: 26.973421096801758, p_loss: 1.332878828048706, mean_rew: -0.330661812993123, variance: 2.4830634593963623, cvar: -0.3947291970252991, v: -0.4051233232021332, mean_q: -1.3915185928344727, std_q: 2.794452667236328, lamda: 1.0514106750488281
Running avgs for agent 3: q_loss: 0.018743908032774925, u_loss: 7.881779193878174, p_loss: 1.2306958436965942, mean_rew: -0.33934795801869144, variance: 1.760971188545227, cvar: -0.4267149567604065, v: -0.4364794194698334, mean_q: -1.2789638042449951, std_q: 2.2075674533843994, lamda: 1.0373376607894897

steps: 774975, episodes: 31000, mean episode reward: -25.647195935623277, agent episode reward: [-4.275892470637836, -3.8609025250553732, -7.7806530063122175, -9.72974793361785], time: 64.961
steps: 774975, episodes: 31000, mean episode variance: 1.0747478015720844, agent episode variance: [0.0, 0.0, 0.6208475903570652, 0.4539002112150192], time: 64.961
steps: 774975, episodes: 31000, mean episode cvar: 0.722712755382061, agent episode cvar: [0.46194665336608887, 0.466107900261879, -0.09867024075984955, -0.10667155748605728], time: 64.962
Running avgs for agent 0: q_loss: 0.14970165491104126, u_loss: 7.998176097869873, p_loss: -0.5958238244056702, mean_rew: -0.23180135796692172, variance: 0.0, cvar: 1.8477866649627686, v: 1.3140815496444702, mean_q: 0.5329714417457581, std_q: 1.9221233129501343, lamda: 1.3392651081085205
Running avgs for agent 1: q_loss: 0.09403100609779358, u_loss: 15.205538749694824, p_loss: -0.6731261014938354, mean_rew: -0.22330456067740886, variance: 0.0, cvar: 1.864431619644165, v: 1.2784439325332642, mean_q: 0.6229252219200134, std_q: 1.867855429649353, lamda: 1.3391797542572021
Running avgs for agent 2: q_loss: 0.014679590240120888, u_loss: 28.999128341674805, p_loss: 1.3207772970199585, mean_rew: -0.32986808995136946, variance: 2.4833900928497314, cvar: -0.3946809768676758, v: -0.40457141399383545, mean_q: -1.3782800436019897, std_q: 2.7805511951446533, lamda: 1.0542646646499634
Running avgs for agent 3: q_loss: 0.016214316710829735, u_loss: 6.170468807220459, p_loss: 1.1996744871139526, mean_rew: -0.34010269427553275, variance: 1.815600872039795, cvar: -0.42668622732162476, v: -0.43507975339889526, mean_q: -1.2461528778076172, std_q: 2.1690938472747803, lamda: 1.043460488319397

steps: 799975, episodes: 32000, mean episode reward: -26.981759835252692, agent episode reward: [-5.043605283370547, -4.237643571222093, -7.671264792164811, -10.029246188495236], time: 64.847
steps: 799975, episodes: 32000, mean episode variance: 1.066421414911747, agent episode variance: [0.0, 0.0, 0.5971416987776756, 0.4692797161340713], time: 64.848
steps: 799975, episodes: 32000, mean episode cvar: 0.7238933337330818, agent episode cvar: [0.46420601844787596, 0.46586195230484007, -0.1000283374786377, -0.10614629954099655], time: 64.848
Running avgs for agent 0: q_loss: 0.15773645043373108, u_loss: 8.429388046264648, p_loss: -0.6291212439537048, mean_rew: -0.2293622815067298, variance: 0.0, cvar: 1.8568239212036133, v: 1.4056727886199951, mean_q: 0.5533818006515503, std_q: 1.9624766111373901, lamda: 1.350637435913086
Running avgs for agent 1: q_loss: 0.09004835784435272, u_loss: 10.511905670166016, p_loss: -0.7059183120727539, mean_rew: -0.22162011593913175, variance: 0.0, cvar: 1.8634477853775024, v: 1.3735414743423462, mean_q: 0.652429461479187, std_q: 1.8730615377426147, lamda: 1.348667025566101
Running avgs for agent 2: q_loss: 0.02172723039984703, u_loss: 19.803890228271484, p_loss: 1.3215304613113403, mean_rew: -0.33139059703188284, variance: 2.3885669708251953, cvar: -0.4001133441925049, v: -0.40946635603904724, mean_q: -1.37654709815979, std_q: 2.7669546604156494, lamda: 1.0614545345306396
Running avgs for agent 3: q_loss: 0.018322434276342392, u_loss: 6.4141082763671875, p_loss: 1.1929844617843628, mean_rew: -0.3435484114691792, variance: 1.8771188259124756, cvar: -0.4245851933956146, v: -0.4366673529148102, mean_q: -1.239727258682251, std_q: 2.1198792457580566, lamda: 1.0459487438201904

steps: 824975, episodes: 33000, mean episode reward: -26.15829017003406, agent episode reward: [-4.154023375785744, -4.064562138953024, -7.839790575572358, -10.099914079722934], time: 64.978
steps: 824975, episodes: 33000, mean episode variance: 1.1213011655807494, agent episode variance: [0.0, 0.0, 0.630067640542984, 0.4912335250377655], time: 64.979
steps: 824975, episodes: 33000, mean episode cvar: 0.7258970961272716, agent episode cvar: [0.46397941756248473, 0.4660996314287186, -0.09956032758951187, -0.10462162527441979], time: 64.979
Running avgs for agent 0: q_loss: 0.11919374018907547, u_loss: 8.023724555969238, p_loss: -0.6434183120727539, mean_rew: -0.2273836705243316, variance: 0.0, cvar: 1.8559176921844482, v: 1.400561809539795, mean_q: 0.574532151222229, std_q: 1.949655532836914, lamda: 1.3617849349975586
Running avgs for agent 1: q_loss: 0.0969940572977066, u_loss: 12.450745582580566, p_loss: -0.7301269769668579, mean_rew: -0.22041517048750797, variance: 0.0, cvar: 1.86439847946167, v: 1.416548490524292, mean_q: 0.6745675206184387, std_q: 1.881921410560608, lamda: 1.3581664562225342
Running avgs for agent 2: q_loss: 0.023458153009414673, u_loss: 21.94680404663086, p_loss: 1.3108460903167725, mean_rew: -0.330631360637459, variance: 2.520270586013794, cvar: -0.3982413411140442, v: -0.40730682015419006, mean_q: -1.3641997575759888, std_q: 2.779374122619629, lamda: 1.0666643381118774
Running avgs for agent 3: q_loss: 0.013431222178041935, u_loss: 5.8503265380859375, p_loss: 1.189382553100586, mean_rew: -0.3434235036168633, variance: 1.9649341106414795, cvar: -0.41848650574684143, v: -0.42449840903282166, mean_q: -1.2376892566680908, std_q: 2.0933594703674316, lamda: 1.04682457447052

steps: 849975, episodes: 34000, mean episode reward: -25.50414303883434, agent episode reward: [-4.791676580620797, -4.318093509322942, -7.889032403448437, -8.505340545442165], time: 65.168
steps: 849975, episodes: 34000, mean episode variance: 1.1139798560738563, agent episode variance: [0.0, 0.0, 0.5986415361762046, 0.5153383198976517], time: 65.168
steps: 849975, episodes: 34000, mean episode cvar: 0.7346358036100864, agent episode cvar: [0.4671328283548355, 0.4707991622686386, -0.09904682159423828, -0.10424936541914939], time: 65.169
Running avgs for agent 0: q_loss: 0.15731389820575714, u_loss: 7.165937423706055, p_loss: -0.70541912317276, mean_rew: -0.22659233105001803, variance: 0.0, cvar: 1.8685312271118164, v: 1.4556695222854614, mean_q: 0.6418058276176453, std_q: 1.8986408710479736, lamda: 1.374059796333313
Running avgs for agent 1: q_loss: 0.09201770275831223, u_loss: 8.756681442260742, p_loss: -0.7336627244949341, mean_rew: -0.2184895125666834, variance: 0.0, cvar: 1.8831965923309326, v: 1.4291719198226929, mean_q: 0.6752334237098694, std_q: 1.8022091388702393, lamda: 1.3672232627868652
Running avgs for agent 2: q_loss: 0.014136074110865593, u_loss: 26.908992767333984, p_loss: 1.299635887145996, mean_rew: -0.3295678375296889, variance: 2.3945658206939697, cvar: -0.3961872458457947, v: -0.4055059254169464, mean_q: -1.353488802909851, std_q: 2.7408931255340576, lamda: 1.0717713832855225
Running avgs for agent 3: q_loss: 0.013583041727542877, u_loss: 4.648879051208496, p_loss: 1.1986018419265747, mean_rew: -0.3447640271714645, variance: 2.0613532066345215, cvar: -0.4169974625110626, v: -0.4226316511631012, mean_q: -1.2440630197525024, std_q: 2.0713305473327637, lamda: 1.0485154390335083

steps: 874975, episodes: 35000, mean episode reward: -24.90492550036256, agent episode reward: [-4.678617773272182, -4.309173980757916, -8.027838597875514, -7.889295148456946], time: 65.202
steps: 874975, episodes: 35000, mean episode variance: 1.1186233392953873, agent episode variance: [0.0, 0.0, 0.5530956041812897, 0.5655277351140976], time: 65.202
steps: 874975, episodes: 35000, mean episode cvar: 0.733624354481697, agent episode cvar: [0.4679876997470856, 0.4701080417633057, -0.09818721386790276, -0.1062841731607914], time: 65.203
Running avgs for agent 0: q_loss: 0.1479419767856598, u_loss: 7.917314529418945, p_loss: -0.7727538347244263, mean_rew: -0.22636343052179128, variance: 0.0, cvar: 1.871950626373291, v: 1.525644302368164, mean_q: 0.7124605774879456, std_q: 1.8972517251968384, lamda: 1.3844759464263916
Running avgs for agent 1: q_loss: 0.09902186691761017, u_loss: 10.736495971679688, p_loss: -0.7336517572402954, mean_rew: -0.21812210629854056, variance: 0.0, cvar: 1.88043212890625, v: 1.4478086233139038, mean_q: 0.6760725378990173, std_q: 1.810445785522461, lamda: 1.3784860372543335
Running avgs for agent 2: q_loss: 0.012745426036417484, u_loss: 16.875078201293945, p_loss: 1.2895666360855103, mean_rew: -0.3281149047023249, variance: 2.2123823165893555, cvar: -0.3927488625049591, v: -0.3999192416667938, mean_q: -1.3411327600479126, std_q: 2.652557134628296, lamda: 1.075027346611023
Running avgs for agent 3: q_loss: 0.015205294825136662, u_loss: 7.584709167480469, p_loss: 1.1914985179901123, mean_rew: -0.34346664117089787, variance: 2.262111186981201, cvar: -0.4251367151737213, v: -0.4319182336330414, mean_q: -1.2355687618255615, std_q: 2.0606801509857178, lamda: 1.0524648427963257

steps: 899975, episodes: 36000, mean episode reward: -24.99730131640673, agent episode reward: [-4.3658828380606725, -4.180795439085288, -7.842873365320701, -8.60774967394007], time: 65.484
steps: 899975, episodes: 36000, mean episode variance: 1.1575733201503753, agent episode variance: [0.0, 0.0, 0.5672447191476822, 0.5903286010026931], time: 65.485
steps: 899975, episodes: 36000, mean episode cvar: 0.7284516457617283, agent episode cvar: [0.46566689157485963, 0.46971596705913543, -0.09823957347869873, -0.10869163939356805], time: 65.485
Running avgs for agent 0: q_loss: 0.10822052508592606, u_loss: 6.780252456665039, p_loss: -0.8258452415466309, mean_rew: -0.22449802648396075, variance: 0.0, cvar: 1.862667441368103, v: 1.570886254310608, mean_q: 0.7722132802009583, std_q: 1.8210229873657227, lamda: 1.3914839029312134
Running avgs for agent 1: q_loss: 0.10158687084913254, u_loss: 10.592469215393066, p_loss: -0.7316015958786011, mean_rew: -0.21557131006124744, variance: 0.0, cvar: 1.87886381149292, v: 1.4326725006103516, mean_q: 0.6762228012084961, std_q: 1.8035460710525513, lamda: 1.3908393383026123
Running avgs for agent 2: q_loss: 0.013403676450252533, u_loss: 18.973297119140625, p_loss: 1.2868883609771729, mean_rew: -0.32774189349935784, variance: 2.268979072570801, cvar: -0.39295831322669983, v: -0.4005502760410309, mean_q: -1.339903712272644, std_q: 2.688368082046509, lamda: 1.0774070024490356
Running avgs for agent 3: q_loss: 0.018374599516391754, u_loss: 5.483678817749023, p_loss: 1.1677031517028809, mean_rew: -0.34311949584308676, variance: 2.361314535140991, cvar: -0.434766560792923, v: -0.44762781262397766, mean_q: -1.21242356300354, std_q: 2.057157278060913, lamda: 1.0553345680236816

steps: 924975, episodes: 37000, mean episode reward: -24.852298437954932, agent episode reward: [-4.516868836306483, -3.648915526457766, -7.940748701356305, -8.745765373834374], time: 65.453
steps: 924975, episodes: 37000, mean episode variance: 1.182734603226185, agent episode variance: [0.0, 0.0, 0.5511676827073098, 0.6315669205188751], time: 65.453
steps: 924975, episodes: 37000, mean episode cvar: 0.7223658901751041, agent episode cvar: [0.4630973525047302, 0.46941084480285644, -0.0980563697218895, -0.11208593741059303], time: 65.454
Running avgs for agent 0: q_loss: 0.17925754189491272, u_loss: 6.405570030212402, p_loss: -0.8612603545188904, mean_rew: -0.22315795802194813, variance: 0.0, cvar: 1.8523893356323242, v: 1.5765800476074219, mean_q: 0.8155484795570374, std_q: 1.8416836261749268, lamda: 1.3988027572631836
Running avgs for agent 1: q_loss: 0.13793295621871948, u_loss: 9.438072204589844, p_loss: -0.7483220100402832, mean_rew: -0.21433372091808814, variance: 0.0, cvar: 1.8776434659957886, v: 1.4305036067962646, mean_q: 0.694823145866394, std_q: 1.779239535331726, lamda: 1.403340458869934
Running avgs for agent 2: q_loss: 0.013500132597982883, u_loss: 25.19023895263672, p_loss: 1.2887656688690186, mean_rew: -0.32862383332099515, variance: 2.2046709060668945, cvar: -0.3922255039215088, v: -0.4036139249801636, mean_q: -1.3416574001312256, std_q: 2.6599109172821045, lamda: 1.0804678201675415
Running avgs for agent 3: q_loss: 0.028865322470664978, u_loss: 5.030037879943848, p_loss: 1.130895972251892, mean_rew: -0.3451603254301331, variance: 2.5262675285339355, cvar: -0.44834375381469727, v: -0.4598817825317383, mean_q: -1.1744723320007324, std_q: 2.027106523513794, lamda: 1.0590084791183472

steps: 949975, episodes: 38000, mean episode reward: -26.943876414030417, agent episode reward: [-6.096218804449782, -3.4603645570620865, -8.198225344412219, -9.189067708106329], time: 65.711
steps: 949975, episodes: 38000, mean episode variance: 1.1607472532391547, agent episode variance: [0.0, 0.0, 0.521479422390461, 0.6392678308486939], time: 65.711
steps: 949975, episodes: 38000, mean episode cvar: 0.7728235822021962, agent episode cvar: [0.5174673705101013, 0.4696824102401733, -0.10020925572514534, -0.11411694282293319], time: 65.712
Running avgs for agent 0: q_loss: 1.9817900657653809, u_loss: 6.840044021606445, p_loss: -0.4815472364425659, mean_rew: -0.22270865098867762, variance: 0.0, cvar: 2.0698695182800293, v: 1.5399056673049927, mean_q: 0.39164116978645325, std_q: 2.1327826976776123, lamda: 1.4304600954055786
Running avgs for agent 1: q_loss: 0.15254627168178558, u_loss: 13.175820350646973, p_loss: -0.7557085752487183, mean_rew: -0.21207720633881924, variance: 0.0, cvar: 1.8787297010421753, v: 1.4038889408111572, mean_q: 0.7025198936462402, std_q: 1.7564473152160645, lamda: 1.4207192659378052
Running avgs for agent 2: q_loss: 0.01726463995873928, u_loss: 13.021281242370605, p_loss: 1.2787079811096191, mean_rew: -0.32794366815482107, variance: 2.0859177112579346, cvar: -0.4008370339870453, v: -0.4098879396915436, mean_q: -1.329515814781189, std_q: 2.6375014781951904, lamda: 1.086206316947937
Running avgs for agent 3: q_loss: 0.0261998288333416, u_loss: 4.546347618103027, p_loss: 1.083984613418579, mean_rew: -0.3444550599878493, variance: 2.5570712089538574, cvar: -0.45646777749061584, v: -0.4643853008747101, mean_q: -1.1272042989730835, std_q: 1.9642753601074219, lamda: 1.0650426149368286

steps: 974975, episodes: 39000, mean episode reward: -29.892652552858724, agent episode reward: [-7.677022995535806, -4.684486260385569, -7.837264722776333, -9.693878574161015], time: 65.092
steps: 974975, episodes: 39000, mean episode variance: 1.1277567420303822, agent episode variance: [0.0019067271649837493, 0.0, 0.4945758348107338, 0.6312741800546646], time: 65.093
steps: 974975, episodes: 39000, mean episode cvar: 0.7050224699378014, agent episode cvar: [0.4576629822850227, 0.45946008050441745, -0.10097099220752716, -0.11112960064411163], time: 65.094
Running avgs for agent 0: q_loss: 4.890834331512451, u_loss: 5.708108425140381, p_loss: 0.07894561439752579, mean_rew: -0.22385909968393491, variance: 0.007626908659934997, cvar: 1.830651879310608, v: 0.9099809527397156, mean_q: -0.17745144665241241, std_q: 1.9284307956695557, lamda: 1.4632689952850342
Running avgs for agent 1: q_loss: 0.2038620263338089, u_loss: 11.350032806396484, p_loss: -0.8323608040809631, mean_rew: -0.21051810947003946, variance: 0.0, cvar: 1.8378403186798096, v: 1.4966585636138916, mean_q: 0.7817553877830505, std_q: 1.77012038230896, lamda: 1.4346297979354858
Running avgs for agent 2: q_loss: 0.021558230742812157, u_loss: 13.55160903930664, p_loss: 1.2615708112716675, mean_rew: -0.32696666862664003, variance: 1.9783034324645996, cvar: -0.4038839638233185, v: -0.4167250096797943, mean_q: -1.3094738721847534, std_q: 2.6191225051879883, lamda: 1.0955179929733276
Running avgs for agent 3: q_loss: 0.022563042119145393, u_loss: 5.390201091766357, p_loss: 1.0656777620315552, mean_rew: -0.34487369829954195, variance: 2.525096893310547, cvar: -0.44451844692230225, v: -0.45305168628692627, mean_q: -1.11329984664917, std_q: 1.9052765369415283, lamda: 1.06846284866333

steps: 999975, episodes: 40000, mean episode reward: -33.68322831278042, agent episode reward: [-8.702360116963705, -5.6309141157360285, -8.23237398055468, -11.117580099526007], time: 65.134
steps: 999975, episodes: 40000, mean episode variance: 1.1237050522640348, agent episode variance: [0.014022567875683307, 0.0, 0.4871031436920166, 0.6225793406963348], time: 65.134
steps: 999975, episodes: 40000, mean episode cvar: 1.0437680745720863, agent episode cvar: [0.7470088647603988, 0.5077003022432327, -0.10100373414158821, -0.10993735828995704], time: 65.135
Running avgs for agent 0: q_loss: 25.444549560546875, u_loss: 5.817240238189697, p_loss: 0.2689739167690277, mean_rew: -0.2266480511139299, variance: 0.05609027150273323, cvar: 2.9880354404449463, v: 0.4540552496910095, mean_q: -0.3225184381008148, std_q: 1.8189198970794678, lamda: 1.4896447658538818
Running avgs for agent 1: q_loss: 2.0302908420562744, u_loss: 9.630780220031738, p_loss: -0.7231971025466919, mean_rew: -0.2109282888222581, variance: 0.0, cvar: 2.030801296234131, v: 1.5421996116638184, mean_q: 0.6450847387313843, std_q: 1.8200052976608276, lamda: 1.4594993591308594
Running avgs for agent 2: q_loss: 0.016160305589437485, u_loss: 12.112837791442871, p_loss: 1.2518266439437866, mean_rew: -0.32782209763978043, variance: 1.9484126567840576, cvar: -0.4040149450302124, v: -0.4142492115497589, mean_q: -1.2995811700820923, std_q: 2.612452507019043, lamda: 1.1015559434890747
Running avgs for agent 3: q_loss: 0.02234850637614727, u_loss: 6.527598857879639, p_loss: 1.0684205293655396, mean_rew: -0.34754244654792105, variance: 2.4903173446655273, cvar: -0.43974944949150085, v: -0.4453531503677368, mean_q: -1.118956208229065, std_q: 1.8699439764022827, lamda: 1.0712820291519165

steps: 1024975, episodes: 41000, mean episode reward: -35.97084366961207, agent episode reward: [-8.845798122218476, -5.088717074046232, -8.533449114576618, -13.502879358770745], time: 65.253
steps: 1024975, episodes: 41000, mean episode variance: 1.1491630667895079, agent episode variance: [0.04958325545489788, 0.0, 0.47348352932929993, 0.6260962820053101], time: 65.253
steps: 1024975, episodes: 41000, mean episode cvar: 0.9749099880158901, agent episode cvar: [0.6428483700156212, 0.5399401922225953, -0.10017698720097541, -0.10770158702135087], time: 65.254
Running avgs for agent 0: q_loss: 12.312390327453613, u_loss: 5.5528059005737305, p_loss: 0.4597693681716919, mean_rew: -0.22991444254376803, variance: 0.19833302181959153, cvar: 2.5713934898376465, v: 0.258949875831604, mean_q: -0.5010803937911987, std_q: 1.6316176652908325, lamda: 1.519884467124939
Running avgs for agent 1: q_loss: 1.9493783712387085, u_loss: 11.506906509399414, p_loss: -0.7296324372291565, mean_rew: -0.21006560335566332, variance: 0.0, cvar: 2.1597607135772705, v: 1.4661623239517212, mean_q: 0.6591625213623047, std_q: 1.7757811546325684, lamda: 1.491591453552246
Running avgs for agent 2: q_loss: 0.017040995880961418, u_loss: 15.9547700881958, p_loss: 1.196463942527771, mean_rew: -0.3170230250841911, variance: 1.8939341306686401, cvar: -0.4007079601287842, v: -0.41399022936820984, mean_q: -1.2325589656829834, std_q: 2.4999210834503174, lamda: 1.1066632270812988
Running avgs for agent 3: q_loss: 0.01957080326974392, u_loss: 4.843769550323486, p_loss: 1.0558100938796997, mean_rew: -0.3419762652717753, variance: 2.504384994506836, cvar: -0.43080633878707886, v: -0.43720972537994385, mean_q: -1.099124550819397, std_q: 1.766615867614746, lamda: 1.0764867067337036

steps: 1049975, episodes: 42000, mean episode reward: -36.48829450325064, agent episode reward: [-10.119010141784507, -5.687863462257034, -8.125088086285327, -12.556332812923769], time: 65.64
steps: 1049975, episodes: 42000, mean episode variance: 1.0482364013306797, agent episode variance: [0.054479367028921845, 0.0, 0.37376126265525816, 0.6199957716464997], time: 65.64
steps: 1049975, episodes: 42000, mean episode cvar: 0.9802268067300319, agent episode cvar: [0.6241776964068413, 0.5606391413211822, -0.09927033486962318, -0.10531969612836838], time: 65.641
Running avgs for agent 0: q_loss: 13.512564659118652, u_loss: 2.676032066345215, p_loss: 0.3159213662147522, mean_rew: -0.22748216518016795, variance: 0.21791746811568738, cvar: 2.496710777282715, v: 0.47689223289489746, mean_q: -0.33849313855171204, std_q: 1.297437310218811, lamda: 1.543968915939331
Running avgs for agent 1: q_loss: 4.556687355041504, u_loss: 2.8839385509490967, p_loss: -0.7200605869293213, mean_rew: -0.20472596956949268, variance: 0.0, cvar: 2.242556571960449, v: 1.2661867141723633, mean_q: 0.6934857964515686, std_q: 1.3600295782089233, lamda: 1.5200027227401733
Running avgs for agent 2: q_loss: 0.011633678339421749, u_loss: 3.693389654159546, p_loss: 1.0702918767929077, mean_rew: -0.2976001557741796, variance: 1.4950449466705322, cvar: -0.3970813453197479, v: -0.4072244167327881, mean_q: -1.084031581878662, std_q: 1.8044196367263794, lamda: 1.1104892492294312
Running avgs for agent 3: q_loss: 0.01102353259921074, u_loss: 1.1009482145309448, p_loss: 1.0564106702804565, mean_rew: -0.3313269317763396, variance: 2.479982852935791, cvar: -0.42127880454063416, v: -0.4285992681980133, mean_q: -1.0758718252182007, std_q: 1.560530662536621, lamda: 1.0802451372146606

steps: 1074975, episodes: 43000, mean episode reward: -33.9769160616902, agent episode reward: [-10.543018744787437, -5.751600407300755, -8.068939584476574, -9.613357325125431], time: 65.479
steps: 1074975, episodes: 43000, mean episode variance: 1.002735150007531, agent episode variance: [0.04334134498052299, 0.0, 0.372161363363266, 0.5872324416637421], time: 65.48
steps: 1074975, episodes: 43000, mean episode cvar: 0.8171298405528069, agent episode cvar: [0.5015733103752136, 0.5213322050571442, -0.09918619990348816, -0.10658947497606278], time: 65.481
Running avgs for agent 0: q_loss: 3.174708604812622, u_loss: 1.3469170331954956, p_loss: -0.059789638966321945, mean_rew: -0.2269202007609556, variance: 0.17336537992209197, cvar: 2.006293296813965, v: 0.7261224389076233, mean_q: 0.040635548532009125, std_q: 1.0059535503387451, lamda: 1.5610849857330322
Running avgs for agent 1: q_loss: 1.8318077325820923, u_loss: 1.4278349876403809, p_loss: -0.7164762616157532, mean_rew: -0.20053574061498972, variance: 0.0, cvar: 2.0853285789489746, v: 1.1596012115478516, mean_q: 0.7058663368225098, std_q: 1.2374353408813477, lamda: 1.547804594039917
Running avgs for agent 2: q_loss: 0.01107024122029543, u_loss: 0.8589831590652466, p_loss: 1.0563545227050781, mean_rew: -0.29591726299104815, variance: 1.4886455535888672, cvar: -0.3967447876930237, v: -0.4024362862110138, mean_q: -1.0713725090026855, std_q: 1.737840175628662, lamda: 1.1124433279037476
Running avgs for agent 3: q_loss: 0.012545042671263218, u_loss: 0.9267667531967163, p_loss: 1.1011784076690674, mean_rew: -0.33317461065465365, variance: 2.3489296436309814, cvar: -0.4263579249382019, v: -0.4335470199584961, mean_q: -1.115960717201233, std_q: 1.6155568361282349, lamda: 1.0830509662628174

steps: 1099975, episodes: 44000, mean episode reward: -34.8192451160302, agent episode reward: [-11.551421722962639, -6.393662156942603, -8.239840924077393, -8.634320312047567], time: 65.427
steps: 1099975, episodes: 44000, mean episode variance: 0.9660425860360264, agent episode variance: [0.023522358365356922, 0.0, 0.3757697125673294, 0.5667505151033402], time: 65.428
steps: 1099975, episodes: 44000, mean episode cvar: 0.8801827559173108, agent episode cvar: [0.5725188989639283, 0.5151029936075211, -0.09760517001152039, -0.10983396664261819], time: 65.429
Running avgs for agent 0: q_loss: 7.523422718048096, u_loss: 1.3092122077941895, p_loss: -0.06805021315813065, mean_rew: -0.23220469771748442, variance: 0.09408943346142769, cvar: 2.2900757789611816, v: 0.7559553384780884, mean_q: 0.043947361409664154, std_q: 1.0407932996749878, lamda: 1.577059268951416
Running avgs for agent 1: q_loss: 1.126683235168457, u_loss: 1.2348941564559937, p_loss: -0.7509375214576721, mean_rew: -0.19977954897623276, variance: 0.0, cvar: 2.0604121685028076, v: 1.1346997022628784, mean_q: 0.7387852668762207, std_q: 1.1106401681900024, lamda: 1.5681909322738647
Running avgs for agent 2: q_loss: 0.010486175306141376, u_loss: 0.825239896774292, p_loss: 1.0532538890838623, mean_rew: -0.2970719402088079, variance: 1.503078818321228, cvar: -0.39042067527770996, v: -0.3994610607624054, mean_q: -1.0730148553848267, std_q: 1.7327321767807007, lamda: 1.1154632568359375
Running avgs for agent 3: q_loss: 0.017192088067531586, u_loss: 0.935718834400177, p_loss: 1.1045299768447876, mean_rew: -0.3357715562858546, variance: 2.2670018672943115, cvar: -0.4393358826637268, v: -0.4514214098453522, mean_q: -1.1229438781738281, std_q: 1.6853903532028198, lamda: 1.0871819257736206

steps: 1124975, episodes: 45000, mean episode reward: -33.50122409165268, agent episode reward: [-10.523473116804373, -5.901221760841295, -8.507649704952962, -8.568879509054048], time: 65.645
steps: 1124975, episodes: 45000, mean episode variance: 0.9760729785002769, agent episode variance: [0.023109977845102547, 0.0, 0.3812114119529724, 0.5717515887022019], time: 65.646
steps: 1124975, episodes: 45000, mean episode cvar: 0.8129277618527413, agent episode cvar: [0.5419304169416428, 0.482700292468071, -0.09779677739739417, -0.11390617015957832], time: 65.646
Running avgs for agent 0: q_loss: 4.742076396942139, u_loss: 1.3223432302474976, p_loss: -0.0021222438663244247, mean_rew: -0.2383451551004661, variance: 0.09243991138041019, cvar: 2.167721748352051, v: 0.6686983108520508, mean_q: -0.02356773614883423, std_q: 1.0901957750320435, lamda: 1.590417504310608
Running avgs for agent 1: q_loss: 0.4676326811313629, u_loss: 1.2112544775009155, p_loss: -0.7673700451850891, mean_rew: -0.20052382452806905, variance: 0.0, cvar: 1.9308011531829834, v: 1.2835732698440552, mean_q: 0.7493892908096313, std_q: 1.14895761013031, lamda: 1.5823346376419067
Running avgs for agent 2: q_loss: 0.010786464437842369, u_loss: 0.7568308711051941, p_loss: 1.0504662990570068, mean_rew: -0.29860272720483977, variance: 1.5248457193374634, cvar: -0.3911871016025543, v: -0.4008345901966095, mean_q: -1.0724338293075562, std_q: 1.720761775970459, lamda: 1.1173709630966187
Running avgs for agent 3: q_loss: 0.021957987919449806, u_loss: 1.0028706789016724, p_loss: 1.0638688802719116, mean_rew: -0.3364115126448694, variance: 2.287006378173828, cvar: -0.45562469959259033, v: -0.46874329447746277, mean_q: -1.083009958267212, std_q: 1.6872280836105347, lamda: 1.088144302368164

steps: 1149975, episodes: 46000, mean episode reward: -32.54858463401508, agent episode reward: [-8.99674035288957, -5.249222932950248, -9.216103861865799, -9.08651748630947], time: 65.761
steps: 1149975, episodes: 46000, mean episode variance: 1.0178983802255244, agent episode variance: [0.034952582424506544, 0.0, 0.39712043607234954, 0.5858253617286682], time: 65.762
steps: 1149975, episodes: 46000, mean episode cvar: 0.8367313705980778, agent episode cvar: [0.5762998893260955, 0.4772927631139755, -0.1000391783118248, -0.11682210353016853], time: 65.762
Running avgs for agent 0: q_loss: 7.685533046722412, u_loss: 1.3069711923599243, p_loss: 0.13391967117786407, mean_rew: -0.24203906324611085, variance: 0.13981032969802618, cvar: 2.30519962310791, v: 0.6207981109619141, mean_q: -0.16072915494441986, std_q: 1.1301178932189941, lamda: 1.6052021980285645
Running avgs for agent 1: q_loss: 0.4293268322944641, u_loss: 1.1906094551086426, p_loss: -0.7922872304916382, mean_rew: -0.19854834905621477, variance: 0.0, cvar: 1.9091711044311523, v: 1.3335736989974976, mean_q: 0.7772199511528015, std_q: 1.2039439678192139, lamda: 1.5955744981765747
Running avgs for agent 2: q_loss: 0.012113476172089577, u_loss: 0.6887809038162231, p_loss: 1.044411540031433, mean_rew: -0.29976263491901944, variance: 1.5884817838668823, cvar: -0.400156706571579, v: -0.4078991115093231, mean_q: -1.0660083293914795, std_q: 1.7055904865264893, lamda: 1.1196562051773071
Running avgs for agent 3: q_loss: 0.028323929756879807, u_loss: 1.0493336915969849, p_loss: 1.0095796585083008, mean_rew: -0.3381204913920791, variance: 2.343301296234131, cvar: -0.46728840470314026, v: -0.48618391156196594, mean_q: -1.0266674757003784, std_q: 1.6120219230651855, lamda: 1.0897619724273682

steps: 1174975, episodes: 47000, mean episode reward: -33.52866282003856, agent episode reward: [-8.853912043719811, -6.277318602341298, -8.891376157023668, -9.506056016953785], time: 68.828
steps: 1174975, episodes: 47000, mean episode variance: 1.1095032418016344, agent episode variance: [0.06980444195307792, 0.0, 0.41939766240119936, 0.6203011374473572], time: 68.829
steps: 1174975, episodes: 47000, mean episode cvar: 0.8238053181767464, agent episode cvar: [0.5534201076030731, 0.4935515022277832, -0.10179345473647118, -0.12137283691763878], time: 68.83
Running avgs for agent 0: q_loss: 5.60904598236084, u_loss: 1.297944188117981, p_loss: 0.0501551479101181, mean_rew: -0.2455829370755395, variance: 0.27921776781231167, cvar: 2.2136805057525635, v: 0.656829833984375, mean_q: -0.08082884550094604, std_q: 1.0973368883132935, lamda: 1.6214052438735962
Running avgs for agent 1: q_loss: 0.9302851557731628, u_loss: 1.1973103284835815, p_loss: -0.9847540259361267, mean_rew: -0.199471538486236, variance: 0.0, cvar: 1.9742059707641602, v: 1.453991174697876, mean_q: 0.9675112366676331, std_q: 1.190606713294983, lamda: 1.6124919652938843
Running avgs for agent 2: q_loss: 0.014099448919296265, u_loss: 0.7404356598854065, p_loss: 1.0366194248199463, mean_rew: -0.30287304071032406, variance: 1.6775907278060913, cvar: -0.40717384219169617, v: -0.4160029888153076, mean_q: -1.0561261177062988, std_q: 1.6929389238357544, lamda: 1.1252553462982178
Running avgs for agent 3: q_loss: 0.03257189318537712, u_loss: 0.9916064739227295, p_loss: 0.9581313133239746, mean_rew: -0.3403068482604809, variance: 2.4812045097351074, cvar: -0.4854913353919983, v: -0.49330341815948486, mean_q: -0.9729386568069458, std_q: 1.5349303483963013, lamda: 1.0941765308380127

steps: 1199975, episodes: 48000, mean episode reward: -35.85108219757936, agent episode reward: [-8.78401629387557, -8.508585777862768, -8.729142293991137, -9.829337831849884], time: 2788.854
steps: 1199975, episodes: 48000, mean episode variance: 1.131254387576133, agent episode variance: [0.05335904153063893, 0.0, 0.44010903799533846, 0.6377863080501557], time: 2788.855
steps: 1199975, episodes: 48000, mean episode cvar: 0.8448316610455513, agent episode cvar: [0.5693579570055008, 0.5000932351350784, -0.10229871046543121, -0.1223208206295967], time: 2788.855
Running avgs for agent 0: q_loss: 4.7974090576171875, u_loss: 1.2984977960586548, p_loss: -0.0004022617358714342, mean_rew: -0.24936906851699792, variance: 0.21343616612255573, cvar: 2.2774319648742676, v: 0.6777690649032593, mean_q: -0.027904411777853966, std_q: 1.097549319267273, lamda: 1.6396374702453613
Running avgs for agent 1: q_loss: 1.4231271743774414, u_loss: 1.1612021923065186, p_loss: -1.0800838470458984, mean_rew: -0.20343289511545548, variance: 0.0, cvar: 2.000372886657715, v: 1.5373297929763794, mean_q: 1.0649172067642212, std_q: 1.140029788017273, lamda: 1.6301409006118774
Running avgs for agent 2: q_loss: 0.016914978623390198, u_loss: 0.662885844707489, p_loss: 1.0282562971115112, mean_rew: -0.3049448980632646, variance: 1.7604361772537231, cvar: -0.40919485688209534, v: -0.42644834518432617, mean_q: -1.0455420017242432, std_q: 1.676217794418335, lamda: 1.127027153968811
Running avgs for agent 3: q_loss: 0.0348624587059021, u_loss: 1.0373774766921997, p_loss: 0.9301936030387878, mean_rew: -0.343264318080705, variance: 2.551145076751709, cvar: -0.4892832636833191, v: -0.5033887624740601, mean_q: -0.9448440074920654, std_q: 1.4582490921020508, lamda: 1.097751259803772

steps: 1224975, episodes: 49000, mean episode reward: -35.51949487040746, agent episode reward: [-8.241206708860005, -8.80844195143713, -8.563670569410434, -9.906175640699894], time: 68.654
steps: 1224975, episodes: 49000, mean episode variance: 1.1469554447215051, agent episode variance: [0.047703796891495585, 0.0, 0.45510648906230927, 0.6441451587677002], time: 68.654
steps: 1224975, episodes: 49000, mean episode cvar: 1.079462606370449, agent episode cvar: [0.6510696601867676, 0.6559961017370224, -0.10496335765719414, -0.12263979789614678], time: 68.655
Running avgs for agent 0: q_loss: 15.363273620605469, u_loss: 1.4082322120666504, p_loss: 0.1705566942691803, mean_rew: -0.25194100512665046, variance: 0.19081518756598234, cvar: 2.604278564453125, v: 0.6019205451011658, mean_q: -0.20010049641132355, std_q: 1.207910418510437, lamda: 1.659220576286316
Running avgs for agent 1: q_loss: 17.374649047851562, u_loss: 1.0691096782684326, p_loss: -0.9601998329162598, mean_rew: -0.20549457626065085, variance: 0.0, cvar: 2.6239843368530273, v: 1.4651695489883423, mean_q: 0.9422674775123596, std_q: 1.0404547452926636, lamda: 1.653271198272705
Running avgs for agent 2: q_loss: 0.01771567575633526, u_loss: 0.760558009147644, p_loss: 1.0134748220443726, mean_rew: -0.30638786248964833, variance: 1.8204258680343628, cvar: -0.41985341906547546, v: -0.4301225543022156, mean_q: -1.028913974761963, std_q: 1.6570976972579956, lamda: 1.1272478103637695
Running avgs for agent 3: q_loss: 0.032870471477508545, u_loss: 0.8657588958740234, p_loss: 0.9135517477989197, mean_rew: -0.3463858714020838, variance: 2.57658052444458, cvar: -0.4905591905117035, v: -0.49847665429115295, mean_q: -0.930120587348938, std_q: 1.3997257947921753, lamda: 1.1056833267211914

steps: 1249975, episodes: 50000, mean episode reward: -40.88918940509625, agent episode reward: [-8.147534855801512, -13.087974173500058, -9.479300961387498, -10.174379414407184], time: 65.907
steps: 1249975, episodes: 50000, mean episode variance: 1.2061375570595265, agent episode variance: [0.11378390923142433, 0.0, 0.45074556410312655, 0.6416080837249756], time: 65.908
steps: 1249975, episodes: 50000, mean episode cvar: 1.044304775595665, agent episode cvar: [0.6240453605651856, 0.6430515531301498, -0.10293300262093544, -0.11985913547873497], time: 65.908
Running avgs for agent 0: q_loss: 13.014913558959961, u_loss: 1.7394623756408691, p_loss: 0.6354265213012695, mean_rew: -0.25554289941920333, variance: 0.4551356369256973, cvar: 2.4961814880371094, v: 0.4494141638278961, mean_q: -0.6840054988861084, std_q: 1.4012629985809326, lamda: 1.6795927286148071
Running avgs for agent 1: q_loss: 13.741854667663574, u_loss: 1.0790834426879883, p_loss: -0.7097273468971252, mean_rew: -0.21173949134539863, variance: 0.0, cvar: 2.5722060203552246, v: 1.1146188974380493, mean_q: 0.6837025284767151, std_q: 0.9911181926727295, lamda: 1.6839094161987305
Running avgs for agent 2: q_loss: 0.016606083139777184, u_loss: 0.7021067142486572, p_loss: 1.0078575611114502, mean_rew: -0.30801249325263375, variance: 1.8029823303222656, cvar: -0.411732017993927, v: -0.42192909121513367, mean_q: -1.0269837379455566, std_q: 1.6401439905166626, lamda: 1.1311169862747192
Running avgs for agent 3: q_loss: 0.02791627123951912, u_loss: 0.7402680516242981, p_loss: 0.9215579628944397, mean_rew: -0.3498926471177469, variance: 2.566432476043701, cvar: -0.4794365465641022, v: -0.4886377453804016, mean_q: -0.9422089457511902, std_q: 1.3720998764038086, lamda: 1.1082241535186768

steps: 1274975, episodes: 51000, mean episode reward: -41.249035261076344, agent episode reward: [-6.608639698852663, -14.046285628467372, -9.706939712956405, -10.887170220799904], time: 66.242
steps: 1274975, episodes: 51000, mean episode variance: 1.3096440562456846, agent episode variance: [0.20991778080165385, 0.0, 0.45577825713157655, 0.6439480183124542], time: 66.243
steps: 1274975, episodes: 51000, mean episode cvar: 0.913063321441412, agent episode cvar: [0.5296930041313171, 0.605343425989151, -0.10480728185176849, -0.11716582682728767], time: 66.243
Running avgs for agent 0: q_loss: 3.0410375595092773, u_loss: 1.7492268085479736, p_loss: 0.1450098156929016, mean_rew: -0.2564221537861239, variance: 0.8396711232066154, cvar: 2.118772029876709, v: 0.6165674924850464, mean_q: -0.21098709106445312, std_q: 1.2840908765792847, lamda: 1.6969249248504639
Running avgs for agent 1: q_loss: 10.689138412475586, u_loss: 1.9984960556030273, p_loss: -0.42810389399528503, mean_rew: -0.22038171773733406, variance: 0.0, cvar: 2.4213736057281494, v: 0.7438722252845764, mean_q: 0.3973124623298645, std_q: 1.1196538209915161, lamda: 1.7064584493637085
Running avgs for agent 2: q_loss: 0.018007487058639526, u_loss: 0.7712914943695068, p_loss: 1.0040748119354248, mean_rew: -0.3103784624214067, variance: 1.8231130838394165, cvar: -0.419229120016098, v: -0.425787091255188, mean_q: -1.0259658098220825, std_q: 1.6349639892578125, lamda: 1.1354714632034302
Running avgs for agent 3: q_loss: 0.02334754541516304, u_loss: 0.6790211200714111, p_loss: 0.9488953948020935, mean_rew: -0.35327090775430825, variance: 2.575792074203491, cvar: -0.4686633050441742, v: -0.47958284616470337, mean_q: -0.9742017984390259, std_q: 1.3925583362579346, lamda: 1.1099040508270264

steps: 1299975, episodes: 52000, mean episode reward: -44.365622122341165, agent episode reward: [-7.88083198982052, -16.702423445365685, -10.060874368088697, -9.72149231906626], time: 67.743
steps: 1299975, episodes: 52000, mean episode variance: 1.4731269556879998, agent episode variance: [0.3471774086356163, 0.0, 0.4837598147392273, 0.6421897323131561], time: 67.743
steps: 1299975, episodes: 52000, mean episode cvar: 0.8057309481799603, agent episode cvar: [0.4984230480194092, 0.5272991578578949, -0.10672308120131492, -0.1132681764960289], time: 67.744
Running avgs for agent 0: q_loss: 1.7008713483810425, u_loss: 1.7032568454742432, p_loss: -0.0911712720990181, mean_rew: -0.25878371325002436, variance: 1.3887096345424652, cvar: 1.9936922788619995, v: 0.8489339351654053, mean_q: 0.029271870851516724, std_q: 1.193408489227295, lamda: 1.710663080215454
Running avgs for agent 1: q_loss: 5.118982315063477, u_loss: 2.8404178619384766, p_loss: 0.030758343636989594, mean_rew: -0.22885002608458838, variance: 0.0, cvar: 2.109196662902832, v: 0.31163284182548523, mean_q: -0.06337527185678482, std_q: 1.2719743251800537, lamda: 1.7297449111938477
Running avgs for agent 2: q_loss: 0.020986447110772133, u_loss: 0.7979040741920471, p_loss: 0.9890753626823425, mean_rew: -0.3129285983192273, variance: 1.9350392818450928, cvar: -0.42689234018325806, v: -0.43434423208236694, mean_q: -1.0110669136047363, std_q: 1.613771677017212, lamda: 1.1407074928283691
Running avgs for agent 3: q_loss: 0.016579519957304, u_loss: 0.6685972809791565, p_loss: 0.9945765137672424, mean_rew: -0.35648311199802785, variance: 2.568758726119995, cvar: -0.4530726969242096, v: -0.45778888463974, mean_q: -1.026638388633728, std_q: 1.432370662689209, lamda: 1.112955927848816

steps: 1324975, episodes: 53000, mean episode reward: -42.05054489542717, agent episode reward: [-8.006993977110243, -15.250987397055207, -10.19143677211255, -8.601126749149172], time: 66.086
steps: 1324975, episodes: 53000, mean episode variance: 1.4885287986658513, agent episode variance: [0.3430512406229973, 0.00538852658495307, 0.5070388123989106, 0.6330502190589905], time: 66.086
steps: 1324975, episodes: 53000, mean episode cvar: 0.7596424361169338, agent episode cvar: [0.4856322673559189, 0.4911893230676651, -0.10773936972022057, -0.1094397845864296], time: 66.087
Running avgs for agent 0: q_loss: 1.0426901578903198, u_loss: 1.4749631881713867, p_loss: -0.22532321512699127, mean_rew: -0.2611965133584661, variance: 1.3722049624919892, cvar: 1.9425290822982788, v: 0.9400710463523865, mean_q: 0.18096481263637543, std_q: 1.133020281791687, lamda: 1.723933458328247
Running avgs for agent 1: q_loss: 1.5763705968856812, u_loss: 3.0922939777374268, p_loss: -0.1433156579732895, mean_rew: -0.24059397163508528, variance: 0.02155410633981228, cvar: 1.9647573232650757, v: 0.5337019562721252, mean_q: 0.10830502957105637, std_q: 1.3396536111831665, lamda: 1.750187635421753
Running avgs for agent 2: q_loss: 0.022807305678725243, u_loss: 0.9053896069526672, p_loss: 0.9807946085929871, mean_rew: -0.3163003345391382, variance: 2.0281553268432617, cvar: -0.43095746636390686, v: -0.4453720450401306, mean_q: -1.001065969467163, std_q: 1.5978599786758423, lamda: 1.1439502239227295
Running avgs for agent 3: q_loss: 0.012673856690526009, u_loss: 0.6340121030807495, p_loss: 1.0408127307891846, mean_rew: -0.35863614415849293, variance: 2.532200813293457, cvar: -0.437759131193161, v: -0.44296887516975403, mean_q: -1.0824321508407593, std_q: 1.4950124025344849, lamda: 1.115490198135376

steps: 1349975, episodes: 54000, mean episode reward: -39.01197796930676, agent episode reward: [-6.913670014376893, -13.599085150702644, -10.165598939039521, -8.333623865187702], time: 66.083
steps: 1349975, episodes: 54000, mean episode variance: 1.3909175434969365, agent episode variance: [0.21183580776304006, 0.014828187827020884, 0.5238941791057586, 0.640359368801117], time: 66.083
steps: 1349975, episodes: 54000, mean episode cvar: 0.7594878619611263, agent episode cvar: [0.4822886509895325, 0.4934540430307388, -0.11032082912325859, -0.10593400293588638], time: 66.084
Running avgs for agent 0: q_loss: 0.6499868035316467, u_loss: 1.3946599960327148, p_loss: -0.3342077434062958, mean_rew: -0.261729838959094, variance: 0.8473432310521603, cvar: 1.9291545152664185, v: 0.9924212098121643, mean_q: 0.28687456250190735, std_q: 1.1376783847808838, lamda: 1.7368119955062866
Running avgs for agent 1: q_loss: 1.3825825452804565, u_loss: 3.6703929901123047, p_loss: -0.17868348956108093, mean_rew: -0.24887034817918405, variance: 0.059312751308083535, cvar: 1.973816156387329, v: 0.722075879573822, mean_q: 0.13361337780952454, std_q: 1.4478250741958618, lamda: 1.7691246271133423
Running avgs for agent 2: q_loss: 0.024212954565882683, u_loss: 0.8597226738929749, p_loss: 0.959017813205719, mean_rew: -0.3175980666755859, variance: 2.0955770015716553, cvar: -0.44128331542015076, v: -0.45102590322494507, mean_q: -0.9768549203872681, std_q: 1.5561420917510986, lamda: 1.1445958614349365
Running avgs for agent 3: q_loss: 0.009625262580811977, u_loss: 0.6476765275001526, p_loss: 1.0850111246109009, mean_rew: -0.35792892445745145, variance: 2.5614376068115234, cvar: -0.423736035823822, v: -0.428028404712677, mean_q: -1.1294803619384766, std_q: 1.5440541505813599, lamda: 1.1176525354385376

steps: 1374975, episodes: 55000, mean episode reward: -34.42601812885122, agent episode reward: [-5.722881997675444, -10.433861685504887, -9.846199507189413, -8.423074938481474], time: 66.023
steps: 1374975, episodes: 55000, mean episode variance: 1.2869568038154393, agent episode variance: [0.08322393041849137, 0.01879688663966954, 0.5275717730522156, 0.6573642137050628], time: 66.023
steps: 1374975, episodes: 55000, mean episode cvar: 0.772378204792738, agent episode cvar: [0.48459330928325656, 0.5044286440610886, -0.1107346163392067, -0.10590913221240043], time: 66.024
Running avgs for agent 0: q_loss: 0.5514531135559082, u_loss: 1.4266327619552612, p_loss: -0.3953501880168915, mean_rew: -0.2634881301628379, variance: 0.3328957216739655, cvar: 1.9383732080459595, v: 1.0397446155548096, mean_q: 0.34246259927749634, std_q: 1.1969820261001587, lamda: 1.750184178352356
Running avgs for agent 1: q_loss: 1.9682279825210571, u_loss: 3.278904676437378, p_loss: -0.24170827865600586, mean_rew: -0.25659327232278983, variance: 0.07518754655867815, cvar: 2.017714500427246, v: 0.7907955646514893, mean_q: 0.19882331788539886, std_q: 1.454468846321106, lamda: 1.7871965169906616
Running avgs for agent 2: q_loss: 0.024913357570767403, u_loss: 0.9476791024208069, p_loss: 0.9605401158332825, mean_rew: -0.32283570651324583, variance: 2.1102871894836426, cvar: -0.4429384469985962, v: -0.45863044261932373, mean_q: -0.9780622720718384, std_q: 1.551889419555664, lamda: 1.148136019706726
Running avgs for agent 3: q_loss: 0.00972337182611227, u_loss: 0.8076558113098145, p_loss: 1.137487530708313, mean_rew: -0.3611751424807211, variance: 2.6294569969177246, cvar: -0.4236365258693695, v: -0.42927831411361694, mean_q: -1.1810448169708252, std_q: 1.6178010702133179, lamda: 1.1207414865493774

steps: 1399975, episodes: 56000, mean episode reward: -32.1909318509909, agent episode reward: [-4.696470749956312, -9.17298301657652, -10.04460917626716, -8.276868908190908], time: 65.991
steps: 1399975, episodes: 56000, mean episode variance: 1.2168483483130113, agent episode variance: [0.01791201001498848, 0.009707294337451457, 0.5122085111141205, 0.6770205328464508], time: 65.991
steps: 1399975, episodes: 56000, mean episode cvar: 0.7484790340960026, agent episode cvar: [0.476860916018486, 0.49042753779888154, -0.10950007155537606, -0.10930934816598892], time: 65.992
Running avgs for agent 0: q_loss: 0.3301331698894501, u_loss: 1.3931504487991333, p_loss: -0.39138859510421753, mean_rew: -0.2631683756361583, variance: 0.07164804005995393, cvar: 1.9074437618255615, v: 1.113860845565796, mean_q: 0.31554991006851196, std_q: 1.2651081085205078, lamda: 1.7613420486450195
Running avgs for agent 1: q_loss: 1.2231731414794922, u_loss: 2.8614442348480225, p_loss: -0.34488382935523987, mean_rew: -0.2595362716208681, variance: 0.03882917734980583, cvar: 1.9617100954055786, v: 0.9653021097183228, mean_q: 0.29755473136901855, std_q: 1.4088943004608154, lamda: 1.804689645767212
Running avgs for agent 2: q_loss: 0.023456063121557236, u_loss: 0.9406829476356506, p_loss: 0.9547001123428345, mean_rew: -0.32455518472859274, variance: 2.0488340854644775, cvar: -0.43800029158592224, v: -0.4576590061187744, mean_q: -0.974683940410614, std_q: 1.5141689777374268, lamda: 1.1499077081680298
Running avgs for agent 3: q_loss: 0.011908173561096191, u_loss: 0.9206026196479797, p_loss: 1.135654091835022, mean_rew: -0.3617664475754275, variance: 2.7080819606781006, cvar: -0.43723738193511963, v: -0.4434182643890381, mean_q: -1.173869252204895, std_q: 1.6636972427368164, lamda: 1.1231704950332642

steps: 1424975, episodes: 57000, mean episode reward: -32.49426640441112, agent episode reward: [-4.9182872792908645, -9.382419298815993, -10.443906933730045, -7.74965289257422], time: 66.111
steps: 1424975, episodes: 57000, mean episode variance: 1.189865685660392, agent episode variance: [0.002363364640623331, 0.0029470336586236955, 0.5057908346652985, 0.6787644526958465], time: 66.112
steps: 1424975, episodes: 57000, mean episode cvar: 0.7597252043485642, agent episode cvar: [0.48088139545917513, 0.5021935386657714, -0.10928852075338363, -0.11406120902299881], time: 66.112
Running avgs for agent 0: q_loss: 0.2979038953781128, u_loss: 1.423595905303955, p_loss: -0.42709141969680786, mean_rew: -0.2610388488114612, variance: 0.009453458562493324, cvar: 1.9235256910324097, v: 1.3043327331542969, mean_q: 0.32936546206474304, std_q: 1.3403615951538086, lamda: 1.7726950645446777
Running avgs for agent 1: q_loss: 1.3722692728042603, u_loss: 2.86014461517334, p_loss: -0.44335293769836426, mean_rew: -0.2641301499319784, variance: 0.011788134634494782, cvar: 2.0087740421295166, v: 0.9834879040718079, mean_q: 0.39528223872184753, std_q: 1.3669450283050537, lamda: 1.8206050395965576
Running avgs for agent 2: q_loss: 0.01867992803454399, u_loss: 0.8540183305740356, p_loss: 0.9757324457168579, mean_rew: -0.32946320143016034, variance: 2.023163318634033, cvar: -0.437154084444046, v: -0.44388091564178467, mean_q: -0.9991273880004883, std_q: 1.528905987739563, lamda: 1.1486363410949707
Running avgs for agent 3: q_loss: 0.014859235845506191, u_loss: 0.9151120781898499, p_loss: 1.1126093864440918, mean_rew: -0.36321503837511915, variance: 2.715057611465454, cvar: -0.4562448561191559, v: -0.46174243092536926, mean_q: -1.1416047811508179, std_q: 1.6751563549041748, lamda: 1.1264381408691406

steps: 1449975, episodes: 58000, mean episode reward: -31.45520771839791, agent episode reward: [-4.661536488771356, -8.669728467476547, -9.185785561031313, -8.938157201118699], time: 66.248
steps: 1449975, episodes: 58000, mean episode variance: 1.181200683876872, agent episode variance: [0.0, 0.0005859204977750779, 0.5032015136480331, 0.6774132497310639], time: 66.248
steps: 1449975, episodes: 58000, mean episode cvar: 0.7385704501271247, agent episode cvar: [0.4879153372049332, 0.4719221707582474, -0.10738271135091781, -0.11388434648513794], time: 66.249
Running avgs for agent 0: q_loss: 0.3310866057872772, u_loss: 1.4377959966659546, p_loss: -0.487911194562912, mean_rew: -0.2597332896010054, variance: 0.0, cvar: 1.9516613483428955, v: 1.4911296367645264, mean_q: 0.36862489581108093, std_q: 1.4211652278900146, lamda: 1.781767725944519
Running avgs for agent 1: q_loss: 0.4798230528831482, u_loss: 2.0215227603912354, p_loss: -0.7552038431167603, mean_rew: -0.268196392523942, variance: 0.0023436819911003114, cvar: 1.8876887559890747, v: 1.391419529914856, mean_q: 0.7092717289924622, std_q: 1.3487858772277832, lamda: 1.8334033489227295
Running avgs for agent 2: q_loss: 0.016649043187499046, u_loss: 0.9311622977256775, p_loss: 0.993282675743103, mean_rew: -0.3311565428047523, variance: 2.0128061771392822, cvar: -0.42953088879585266, v: -0.4421829581260681, mean_q: -1.0169062614440918, std_q: 1.534732699394226, lamda: 1.1499580144882202
Running avgs for agent 3: q_loss: 0.015771077945828438, u_loss: 0.9484521746635437, p_loss: 1.0755399465560913, mean_rew: -0.363088784669015, variance: 2.70965313911438, cvar: -0.4555373787879944, v: -0.4616854190826416, mean_q: -1.1014409065246582, std_q: 1.6345136165618896, lamda: 1.1306238174438477

steps: 1474975, episodes: 59000, mean episode reward: -32.61535007034432, agent episode reward: [-5.653240843463049, -8.411763712058155, -8.906376189305842, -9.64396932551728], time: 66.288
steps: 1474975, episodes: 59000, mean episode variance: 1.1890116798877717, agent episode variance: [0.0, 0.0, 0.5106619052886963, 0.6783497745990753], time: 66.289
steps: 1474975, episodes: 59000, mean episode cvar: 0.7366155778169632, agent episode cvar: [0.4891768119335175, 0.4729359784126282, -0.10846568965911865, -0.11703152287006378], time: 66.289
Running avgs for agent 0: q_loss: 0.21365748345851898, u_loss: 1.427451252937317, p_loss: -0.5363844037055969, mean_rew: -0.2603549237411281, variance: 0.0, cvar: 1.956707239151001, v: 1.6314460039138794, mean_q: 0.3958585858345032, std_q: 1.4738174676895142, lamda: 1.7899320125579834
Running avgs for agent 1: q_loss: 0.29547393321990967, u_loss: 1.8199868202209473, p_loss: -0.9253459572792053, mean_rew: -0.2715280716540186, variance: 0.0, cvar: 1.8917438983917236, v: 1.593895673751831, mean_q: 0.8725440502166748, std_q: 1.343294620513916, lamda: 1.8429608345031738
Running avgs for agent 2: q_loss: 0.015523012727499008, u_loss: 1.0264133214950562, p_loss: 1.0165302753448486, mean_rew: -0.3340646536061359, variance: 2.04264760017395, cvar: -0.4338627755641937, v: -0.4443541169166565, mean_q: -1.0384857654571533, std_q: 1.577823281288147, lamda: 1.1507872343063354
Running avgs for agent 3: q_loss: 0.019382888451218605, u_loss: 0.9920107126235962, p_loss: 1.0393929481506348, mean_rew: -0.3635873731513686, variance: 2.7133989334106445, cvar: -0.4681260883808136, v: -0.4757232666015625, mean_q: -1.065277338027954, std_q: 1.5866972208023071, lamda: 1.132383942604065

steps: 1499975, episodes: 60000, mean episode reward: -31.729265443984573, agent episode reward: [-5.335712335625416, -7.098557994462047, -9.088505192027638, -10.20648992186947], time: 66.214
steps: 1499975, episodes: 60000, mean episode variance: 1.2102104659080506, agent episode variance: [0.0, 0.0, 0.5204684767723083, 0.6897419891357421], time: 66.215
steps: 1499975, episodes: 60000, mean episode cvar: 0.7403884689509869, agent episode cvar: [0.4964849772453308, 0.4748883811235428, -0.11005986896157265, -0.12092502045631409], time: 66.215
Running avgs for agent 0: q_loss: 0.15368828177452087, u_loss: 1.4580607414245605, p_loss: -0.5128490924835205, mean_rew: -0.25869032035888956, variance: 0.0, cvar: 1.9859399795532227, v: 1.694641351699829, mean_q: 0.34283775091171265, std_q: 1.5496493577957153, lamda: 1.7943617105484009
Running avgs for agent 1: q_loss: 0.21818923950195312, u_loss: 1.74208664894104, p_loss: -0.9954959750175476, mean_rew: -0.2741636960924141, variance: 0.0, cvar: 1.8995535373687744, v: 1.7275199890136719, mean_q: 0.9316855669021606, std_q: 1.3029533624649048, lamda: 1.84861159324646
Running avgs for agent 2: q_loss: 0.01694813184440136, u_loss: 1.107007622718811, p_loss: 1.0147032737731934, mean_rew: -0.33487739082792983, variance: 2.081874132156372, cvar: -0.44023948907852173, v: -0.44827452301979065, mean_q: -1.0341635942459106, std_q: 1.5998544692993164, lamda: 1.1516309976577759
Running avgs for agent 3: q_loss: 0.022988805547356606, u_loss: 0.9924212098121643, p_loss: 1.0104022026062012, mean_rew: -0.3665615872741674, variance: 2.758967876434326, cvar: -0.48370006680488586, v: -0.4934535026550293, mean_q: -1.030889868736267, std_q: 1.5416278839111328, lamda: 1.1375304460525513

steps: 1524975, episodes: 61000, mean episode reward: -31.660705025871813, agent episode reward: [-4.593171983119075, -6.571403166110387, -9.591856183897065, -10.904273692745281], time: 66.111
steps: 1524975, episodes: 61000, mean episode variance: 1.2427095361948013, agent episode variance: [0.0, 0.0, 0.540851303935051, 0.7018582322597504], time: 66.111
steps: 1524975, episodes: 61000, mean episode cvar: 0.7467226028442383, agent episode cvar: [0.503003041267395, 0.4797222766876221, -0.11178999826312065, -0.12421271684765815], time: 66.112
Running avgs for agent 0: q_loss: 0.14328619837760925, u_loss: 1.65052330493927, p_loss: -0.49356135725975037, mean_rew: -0.2602936252826957, variance: 0.0, cvar: 2.012012004852295, v: 1.732414960861206, mean_q: 0.2882918417453766, std_q: 1.6588852405548096, lamda: 1.797372817993164
Running avgs for agent 1: q_loss: 0.13932643830776215, u_loss: 1.6158331632614136, p_loss: -0.9976826310157776, mean_rew: -0.2743857644126674, variance: 0.0, cvar: 1.9188891649246216, v: 1.7953754663467407, mean_q: 0.9143322706222534, std_q: 1.2920501232147217, lamda: 1.8519848585128784
Running avgs for agent 2: q_loss: 0.019522840157151222, u_loss: 1.2704293727874756, p_loss: 1.0075032711029053, mean_rew: -0.3368832860863868, variance: 2.163405179977417, cvar: -0.4471599757671356, v: -0.4601036310195923, mean_q: -1.0232667922973633, std_q: 1.5941846370697021, lamda: 1.1529959440231323
Running avgs for agent 3: q_loss: 0.027747489511966705, u_loss: 0.9802953004837036, p_loss: 0.9816100001335144, mean_rew: -0.3704896847981289, variance: 2.8074328899383545, cvar: -0.496850848197937, v: -0.5063711404800415, mean_q: -0.9987397193908691, std_q: 1.4972299337387085, lamda: 1.1394023895263672

steps: 1549975, episodes: 62000, mean episode reward: -30.923299592958323, agent episode reward: [-4.403181021978439, -5.404487068876538, -10.211424822714164, -10.904206679389183], time: 66.513
steps: 1549975, episodes: 62000, mean episode variance: 1.2974878199100495, agent episode variance: [0.0, 0.0, 0.5684815211296081, 0.7290062987804413], time: 66.514
steps: 1549975, episodes: 62000, mean episode cvar: 0.7528528035879135, agent episode cvar: [0.5067257550954819, 0.4865786858797073, -0.11332453572750091, -0.12712710165977478], time: 66.514
Running avgs for agent 0: q_loss: 0.15612301230430603, u_loss: 1.7877519130706787, p_loss: -0.47588568925857544, mean_rew: -0.25926031510036396, variance: 0.0, cvar: 2.026902914047241, v: 1.7274456024169922, mean_q: 0.25047582387924194, std_q: 1.7161474227905273, lamda: 1.8011428117752075
Running avgs for agent 1: q_loss: 0.11729057878255844, u_loss: 1.8459573984146118, p_loss: -0.8563241362571716, mean_rew: -0.2773269779025433, variance: 0.0, cvar: 1.9463146924972534, v: 1.7836112976074219, mean_q: 0.7423205375671387, std_q: 1.3751952648162842, lamda: 1.8543063402175903
Running avgs for agent 2: q_loss: 0.020190127193927765, u_loss: 1.3476513624191284, p_loss: 1.0002071857452393, mean_rew: -0.3397450498586669, variance: 2.273926019668579, cvar: -0.4532981216907501, v: -0.4623911380767822, mean_q: -1.015720009803772, std_q: 1.571608066558838, lamda: 1.1549878120422363
Running avgs for agent 3: q_loss: 0.031464364379644394, u_loss: 1.1135913133621216, p_loss: 0.9572201371192932, mean_rew: -0.37409741065953417, variance: 2.916025161743164, cvar: -0.5085084438323975, v: -0.523751974105835, mean_q: -0.9720794558525085, std_q: 1.4333276748657227, lamda: 1.143667221069336

steps: 1574975, episodes: 63000, mean episode reward: -30.476472284845087, agent episode reward: [-4.759016358327257, -4.301164876889266, -10.296099549033418, -11.12019150059515], time: 66.439
steps: 1574975, episodes: 63000, mean episode variance: 1.3549795668125153, agent episode variance: [0.0, 0.0, 0.6109953348636628, 0.7439842319488525], time: 66.44
steps: 1574975, episodes: 63000, mean episode cvar: 0.75396202480793, agent episode cvar: [0.5049030326604843, 0.4899533957242966, -0.1136221459209919, -0.127272257655859], time: 66.44
Running avgs for agent 0: q_loss: 0.1600310355424881, u_loss: 1.8365728855133057, p_loss: -0.4784313440322876, mean_rew: -0.2592914458159293, variance: 0.0, cvar: 2.0196120738983154, v: 1.6674855947494507, mean_q: 0.27034443616867065, std_q: 1.6907374858856201, lamda: 1.8056509494781494
Running avgs for agent 1: q_loss: 0.12283065915107727, u_loss: 1.5504367351531982, p_loss: -0.7430981397628784, mean_rew: -0.2763992816322427, variance: 0.0, cvar: 1.9598134756088257, v: 1.7671326398849487, mean_q: 0.6021568179130554, std_q: 1.468289852142334, lamda: 1.8568415641784668
Running avgs for agent 2: q_loss: 0.021462703123688698, u_loss: 1.3438091278076172, p_loss: 1.0006617307662964, mean_rew: -0.34348075684673035, variance: 2.443981409072876, cvar: -0.4544885754585266, v: -0.4658886194229126, mean_q: -1.0172561407089233, std_q: 1.568868637084961, lamda: 1.1585522890090942
Running avgs for agent 3: q_loss: 0.03270107880234718, u_loss: 1.0293046236038208, p_loss: 0.9299106597900391, mean_rew: -0.37529318585925453, variance: 2.9759371280670166, cvar: -0.5090889930725098, v: -0.525330126285553, mean_q: -0.9436969757080078, std_q: 1.3711992502212524, lamda: 1.145297884941101

steps: 1599975, episodes: 64000, mean episode reward: -30.149440484595456, agent episode reward: [-4.906040305681456, -4.207864070356021, -10.209512669039377, -10.826023439518606], time: 66.206
steps: 1599975, episodes: 64000, mean episode variance: 1.4090906405448913, agent episode variance: [0.0, 0.0, 0.6634653012752533, 0.745625339269638], time: 66.207
steps: 1599975, episodes: 64000, mean episode cvar: 0.7500582871735096, agent episode cvar: [0.5045373678207398, 0.48805436980724337, -0.1148265486061573, -0.12770690184831618], time: 66.207
Running avgs for agent 0: q_loss: 0.1605362743139267, u_loss: 1.8020212650299072, p_loss: -0.4078715741634369, mean_rew: -0.25674689427400105, variance: 0.0, cvar: 2.0181496143341064, v: 1.5845019817352295, mean_q: 0.21702703833580017, std_q: 1.663900375366211, lamda: 1.8108261823654175
Running avgs for agent 1: q_loss: 0.13881011307239532, u_loss: 1.8280847072601318, p_loss: -0.728147566318512, mean_rew: -0.27552015726710144, variance: 0.0, cvar: 1.95221745967865, v: 1.7811700105667114, mean_q: 0.578283429145813, std_q: 1.5478447675704956, lamda: 1.8596042394638062
Running avgs for agent 2: q_loss: 0.021741347387433052, u_loss: 1.3628185987472534, p_loss: 1.0016690492630005, mean_rew: -0.3479407770200208, variance: 2.6538612842559814, cvar: -0.45930618047714233, v: -0.46981364488601685, mean_q: -1.0176211595535278, std_q: 1.5584369897842407, lamda: 1.1610008478164673
Running avgs for agent 3: q_loss: 0.03183756396174431, u_loss: 1.0567829608917236, p_loss: 0.927690327167511, mean_rew: -0.3776794454849987, variance: 2.98250150680542, cvar: -0.5108276009559631, v: -0.5316221714019775, mean_q: -0.940761923789978, std_q: 1.3366175889968872, lamda: 1.1418205499649048

steps: 1624975, episodes: 65000, mean episode reward: -29.943766135652183, agent episode reward: [-5.149128444796501, -4.252011702800904, -9.949017601261525, -10.593608386793253], time: 67.653
steps: 1624975, episodes: 65000, mean episode variance: 1.4644819858074187, agent episode variance: [0.0, 0.0, 0.7161289627552032, 0.7483530230522156], time: 67.654
steps: 1624975, episodes: 65000, mean episode cvar: 0.7282491007447243, agent episode cvar: [0.501832100868225, 0.4698827491998672, -0.11522657886147498, -0.1282391704618931], time: 67.654
Running avgs for agent 0: q_loss: 0.195982426404953, u_loss: 1.6678929328918457, p_loss: -0.314689576625824, mean_rew: -0.2567046354561794, variance: 0.0, cvar: 2.0073282718658447, v: 1.4785654544830322, mean_q: 0.15286892652511597, std_q: 1.5923571586608887, lamda: 1.8178012371063232
Running avgs for agent 1: q_loss: 0.147302508354187, u_loss: 1.4988757371902466, p_loss: -0.7722828388214111, mean_rew: -0.2733137378865002, variance: 0.0, cvar: 1.8795310258865356, v: 1.7689428329467773, mean_q: 0.6477289795875549, std_q: 1.568528652191162, lamda: 1.863795280456543
Running avgs for agent 2: q_loss: 0.021985545754432678, u_loss: 1.2727653980255127, p_loss: 1.0008857250213623, mean_rew: -0.3507967387300721, variance: 2.864515781402588, cvar: -0.4609062969684601, v: -0.4745367467403412, mean_q: -1.0155397653579712, std_q: 1.551092267036438, lamda: 1.1620774269104004
Running avgs for agent 3: q_loss: 0.029425708577036858, u_loss: 0.9572673439979553, p_loss: 0.9440622329711914, mean_rew: -0.38328307381913135, variance: 2.9934120178222656, cvar: -0.5129566788673401, v: -0.5305030345916748, mean_q: -0.9569512009620667, std_q: 1.347844123840332, lamda: 1.1396162509918213

steps: 1649975, episodes: 66000, mean episode reward: -30.00468323118592, agent episode reward: [-4.985764109381495, -4.71664036892098, -10.07571084892982, -10.226567903953622], time: 66.937
steps: 1649975, episodes: 66000, mean episode variance: 1.5090689175128937, agent episode variance: [0.0, 0.0, 0.7626530444622039, 0.7464158730506897], time: 66.938
steps: 1649975, episodes: 66000, mean episode cvar: 0.7364425618052483, agent episode cvar: [0.49581118953228, 0.486113770365715, -0.11666286566853523, -0.1288195324242115], time: 66.938
Running avgs for agent 0: q_loss: 0.1565389335155487, u_loss: 1.467983365058899, p_loss: -0.27872326970100403, mean_rew: -0.2557895073140244, variance: 0.0, cvar: 1.983244776725769, v: 1.3829797506332397, mean_q: 0.14566224813461304, std_q: 1.4924577474594116, lamda: 1.8298239707946777
Running avgs for agent 1: q_loss: 0.5440377593040466, u_loss: 1.4947682619094849, p_loss: -0.7438674569129944, mean_rew: -0.27232774774913865, variance: 0.0, cvar: 1.9444550275802612, v: 1.7295811176300049, mean_q: 0.6171708703041077, std_q: 1.5603852272033691, lamda: 1.874072790145874
Running avgs for agent 2: q_loss: 0.022322475910186768, u_loss: 1.1688752174377441, p_loss: 0.9966207146644592, mean_rew: -0.35278892808603723, variance: 3.050612211227417, cvar: -0.4666514992713928, v: -0.4807763695716858, mean_q: -1.010021448135376, std_q: 1.5414109230041504, lamda: 1.1597602367401123
Running avgs for agent 3: q_loss: 0.026542069390416145, u_loss: 1.0249634981155396, p_loss: 0.9453084468841553, mean_rew: -0.3833104790053155, variance: 2.985663652420044, cvar: -0.515278160572052, v: -0.5275488495826721, mean_q: -0.9584836363792419, std_q: 1.3308219909667969, lamda: 1.1398870944976807

steps: 1674975, episodes: 67000, mean episode reward: -33.77179886456991, agent episode reward: [-3.854507529252162, -9.481153852125853, -10.451805842236032, -9.984331640955865], time: 66.187
steps: 1674975, episodes: 67000, mean episode variance: 1.5267056803703307, agent episode variance: [0.0, 0.0, 0.7894553425312042, 0.7372503378391266], time: 66.188
steps: 1674975, episodes: 67000, mean episode cvar: 0.7819088552892208, agent episode cvar: [0.495813955783844, 0.5301245405673981, -0.11630165988206863, -0.12772798117995263], time: 66.188
Running avgs for agent 0: q_loss: 0.20959892868995667, u_loss: 1.2755613327026367, p_loss: -0.20241068303585052, mean_rew: -0.2552896538363786, variance: 0.0, cvar: 1.9832558631896973, v: 1.3256406784057617, mean_q: 0.08123672753572464, std_q: 1.4521223306655884, lamda: 1.8405592441558838
Running avgs for agent 1: q_loss: 5.317271709442139, u_loss: 1.4800145626068115, p_loss: -0.6674628257751465, mean_rew: -0.27625161472473897, variance: 0.0, cvar: 2.120497941970825, v: 1.5685800313949585, mean_q: 0.5455843210220337, std_q: 1.491283655166626, lamda: 1.904515266418457
Running avgs for agent 2: q_loss: 0.02243228629231453, u_loss: 1.0949879884719849, p_loss: 0.9971898794174194, mean_rew: -0.355166531680158, variance: 3.1578211784362793, cvar: -0.4652066230773926, v: -0.4841318428516388, mean_q: -1.0096042156219482, std_q: 1.5323362350463867, lamda: 1.156360387802124
Running avgs for agent 3: q_loss: 0.0234368909150362, u_loss: 0.9858508110046387, p_loss: 0.9576286673545837, mean_rew: -0.3852018332042693, variance: 2.9490015506744385, cvar: -0.5109119415283203, v: -0.5208972692489624, mean_q: -0.9737165570259094, std_q: 1.3528778553009033, lamda: 1.142436146736145

steps: 1699975, episodes: 68000, mean episode reward: -33.63646302115688, agent episode reward: [-3.635201977190407, -9.919908411790004, -10.257607549024442, -9.82374508315203], time: 66.804
steps: 1699975, episodes: 68000, mean episode variance: 1.50874139046669, agent episode variance: [0.0, 0.0, 0.775691246509552, 0.7330501439571381], time: 66.805
steps: 1699975, episodes: 68000, mean episode cvar: 0.8899840522408485, agent episode cvar: [0.490619132399559, 0.6427526932954788, -0.1178888507783413, -0.125498922675848], time: 66.805
Running avgs for agent 0: q_loss: 0.14260870218276978, u_loss: 1.2760368585586548, p_loss: -0.17974017560482025, mean_rew: -0.2545690016932646, variance: 0.0, cvar: 1.9624764919281006, v: 1.272375226020813, mean_q: 0.05616099387407303, std_q: 1.4494507312774658, lamda: 1.851057529449463
Running avgs for agent 1: q_loss: 27.731430053710938, u_loss: 1.3840693235397339, p_loss: -0.6505116820335388, mean_rew: -0.28112913984576526, variance: 0.0, cvar: 2.5710108280181885, v: 1.40095853805542, mean_q: 0.5823448300361633, std_q: 1.353401780128479, lamda: 1.9423651695251465
Running avgs for agent 2: q_loss: 0.02017037384212017, u_loss: 1.0039026737213135, p_loss: 1.0019898414611816, mean_rew: -0.3579140826490796, variance: 3.102764844894409, cvar: -0.4715553820133209, v: -0.48094677925109863, mean_q: -1.0158257484436035, std_q: 1.5231237411499023, lamda: 1.1559115648269653
Running avgs for agent 3: q_loss: 0.019968640059232712, u_loss: 0.9777985215187073, p_loss: 0.979076087474823, mean_rew: -0.38853691173864546, variance: 2.9322006702423096, cvar: -0.5019956827163696, v: -0.5101598501205444, mean_q: -1.0036088228225708, std_q: 1.397040605545044, lamda: 1.1461132764816284

steps: 1724975, episodes: 69000, mean episode reward: -33.67672993494039, agent episode reward: [-3.81473538436111, -10.0818699544222, -10.724400152912123, -9.055724443244953], time: 66.816
steps: 1724975, episodes: 69000, mean episode variance: 1.5039902818202973, agent episode variance: [0.0, 0.0, 0.7807125113010407, 0.7232777705192566], time: 66.817
steps: 1724975, episodes: 69000, mean episode cvar: 0.9642352638840676, agent episode cvar: [0.4912040145397186, 0.7127392530441284, -0.11652265068888665, -0.12318535301089287], time: 66.818
Running avgs for agent 0: q_loss: 0.22934813797473907, u_loss: 1.160495400428772, p_loss: -0.16654759645462036, mean_rew: -0.25440883548670656, variance: 0.0, cvar: 1.9648160934448242, v: 1.2573423385620117, mean_q: 0.039122216403484344, std_q: 1.4347976446151733, lamda: 1.8646020889282227
Running avgs for agent 1: q_loss: 31.395906448364258, u_loss: 1.697556495666504, p_loss: -0.36959192156791687, mean_rew: -0.2859934075142862, variance: 0.0, cvar: 2.850956916809082, v: 1.4200448989868164, mean_q: 0.2963663935661316, std_q: 1.4388900995254517, lamda: 1.973375678062439
Running avgs for agent 2: q_loss: 0.018875904381275177, u_loss: 1.0040186643600464, p_loss: 1.0134931802749634, mean_rew: -0.3620944831579817, variance: 3.122850179672241, cvar: -0.4660906195640564, v: -0.4761632978916168, mean_q: -1.033111572265625, std_q: 1.5368045568466187, lamda: 1.159532904624939
Running avgs for agent 3: q_loss: 0.01717451401054859, u_loss: 0.8992392420768738, p_loss: 0.9853214025497437, mean_rew: -0.3884462560101786, variance: 2.893111228942871, cvar: -0.4927414059638977, v: -0.4976634383201599, mean_q: -1.0138734579086304, std_q: 1.4104756116867065, lamda: 1.1492633819580078

steps: 1749975, episodes: 70000, mean episode reward: -30.849289688815944, agent episode reward: [-3.1428234865035636, -7.8552912761890585, -10.782479288473255, -9.06869563765007], time: 66.385
steps: 1749975, episodes: 70000, mean episode variance: 1.5403728244304657, agent episode variance: [0.0, 0.0, 0.8172741527557373, 0.7230986716747284], time: 66.386
steps: 1749975, episodes: 70000, mean episode cvar: 0.799687543451786, agent episode cvar: [0.48732665276527404, 0.5508801056146622, -0.11715605309605598, -0.1213631618320942], time: 66.386
Running avgs for agent 0: q_loss: 0.16148388385772705, u_loss: 1.1225587129592896, p_loss: -0.2082964926958084, mean_rew: -0.2533480221402776, variance: 0.0, cvar: 1.949306607246399, v: 1.2565158605575562, mean_q: 0.07936319708824158, std_q: 1.429205298423767, lamda: 1.877801775932312
Running avgs for agent 1: q_loss: 5.386640548706055, u_loss: 1.6905089616775513, p_loss: -0.29008710384368896, mean_rew: -0.28941820234085636, variance: 0.0, cvar: 2.2035205364227295, v: 1.2630239725112915, mean_q: 0.21711251139640808, std_q: 1.3504101037979126, lamda: 1.9935710430145264
Running avgs for agent 2: q_loss: 0.019503174349665642, u_loss: 1.1529403924942017, p_loss: 1.012350082397461, mean_rew: -0.36428315273444023, variance: 3.2690963745117188, cvar: -0.46862420439720154, v: -0.47889986634254456, mean_q: -1.0343869924545288, std_q: 1.5267726182937622, lamda: 1.159937858581543
Running avgs for agent 3: q_loss: 0.017445504665374756, u_loss: 0.8489764332771301, p_loss: 1.0064030885696411, mean_rew: -0.3889819041759105, variance: 2.892394542694092, cvar: -0.4854526221752167, v: -0.4989182949066162, mean_q: -1.036437749862671, std_q: 1.4393831491470337, lamda: 1.1506807804107666

steps: 1774975, episodes: 71000, mean episode reward: -30.28169755992928, agent episode reward: [-2.929443207172095, -7.137818319706205, -10.748791772114362, -9.465644260936617], time: 66.425
steps: 1774975, episodes: 71000, mean episode variance: 1.6064422438144683, agent episode variance: [0.0, 6.705522537231445e-05, 0.8692514355182648, 0.7371237530708313], time: 66.426
steps: 1774975, episodes: 71000, mean episode cvar: 0.8894833514392376, agent episode cvar: [0.49189704823493957, 0.6396582436561584, -0.11922607892751694, -0.12284586152434349], time: 66.426
Running avgs for agent 0: q_loss: 0.18490645289421082, u_loss: 1.1236313581466675, p_loss: -0.22541439533233643, mean_rew: -0.2528409536205482, variance: 0.0, cvar: 1.967588186264038, v: 1.273559331893921, mean_q: 0.09186814725399017, std_q: 1.437383770942688, lamda: 1.8896299600601196
Running avgs for agent 1: q_loss: 14.62123966217041, u_loss: 1.6359106302261353, p_loss: -0.31572225689888, mean_rew: -0.29351707985312125, variance: 0.0002682209014892578, cvar: 2.5586330890655518, v: 1.1332252025604248, mean_q: 0.2612866461277008, std_q: 1.3237231969833374, lamda: 2.0136685371398926
Running avgs for agent 2: q_loss: 0.021022027358412743, u_loss: 1.164643406867981, p_loss: 1.0084292888641357, mean_rew: -0.3683202599049198, variance: 3.47700572013855, cvar: -0.47690433263778687, v: -0.48680031299591064, mean_q: -1.0289306640625, std_q: 1.5289063453674316, lamda: 1.1612216234207153
Running avgs for agent 3: q_loss: 0.018488094210624695, u_loss: 0.8701733350753784, p_loss: 1.0092476606369019, mean_rew: -0.388381146706841, variance: 2.9484951496124268, cvar: -0.49138346314430237, v: -0.49992823600769043, mean_q: -1.0376055240631104, std_q: 1.4650295972824097, lamda: 1.1513488292694092

steps: 1799975, episodes: 72000, mean episode reward: -27.824092517307673, agent episode reward: [-2.4937949245827222, -5.690675746301542, -10.486900101899373, -9.152721744524037], time: 66.597
steps: 1799975, episodes: 72000, mean episode variance: 1.6647172660827636, agent episode variance: [0.0, 0.0, 0.9083853015899658, 0.7563319644927978], time: 66.597
steps: 1799975, episodes: 72000, mean episode cvar: 0.8724295506179333, agent episode cvar: [0.49565201699733735, 0.6211239205598831, -0.12087742125988006, -0.12346896567940711], time: 66.599
Running avgs for agent 0: q_loss: 0.2159307897090912, u_loss: 1.1265230178833008, p_loss: -0.23461687564849854, mean_rew: -0.24974155683813812, variance: 0.0, cvar: 1.9826080799102783, v: 1.276940941810608, mean_q: 0.09688518196344376, std_q: 1.4489595890045166, lamda: 1.9036375284194946
Running avgs for agent 1: q_loss: 17.01578712463379, u_loss: 1.5796440839767456, p_loss: -0.07623272389173508, mean_rew: -0.29750598933017164, variance: 0.0, cvar: 2.4844956398010254, v: 1.1212241649627686, mean_q: 0.012173446826636791, std_q: 1.3738296031951904, lamda: 2.0367965698242188
Running avgs for agent 2: q_loss: 0.023118287324905396, u_loss: 1.3539202213287354, p_loss: 1.001802921295166, mean_rew: -0.3709806183599293, variance: 3.6335413455963135, cvar: -0.48350971937179565, v: -0.49637317657470703, mean_q: -1.0189963579177856, std_q: 1.5092582702636719, lamda: 1.1627113819122314
Running avgs for agent 3: q_loss: 0.01967676915228367, u_loss: 0.9343311786651611, p_loss: 1.0074726343154907, mean_rew: -0.3878613214204598, variance: 3.0253279209136963, cvar: -0.4938758611679077, v: -0.503663957118988, mean_q: -1.033516526222229, std_q: 1.462011694908142, lamda: 1.1521663665771484

steps: 1824975, episodes: 73000, mean episode reward: -28.622386302702463, agent episode reward: [-2.555492690347409, -6.881697148611747, -10.031946427451523, -9.153250036291784], time: 66.492
steps: 1824975, episodes: 73000, mean episode variance: 1.669991133928299, agent episode variance: [0.0, 0.0, 0.9037848608493805, 0.7662062730789184], time: 66.493
steps: 1824975, episodes: 73000, mean episode cvar: 0.8499806326031685, agent episode cvar: [0.4923991596698761, 0.6019847515821457, -0.12101604595780373, -0.12338723269104958], time: 66.493
Running avgs for agent 0: q_loss: 0.18495050072669983, u_loss: 1.158105492591858, p_loss: -0.3105887472629547, mean_rew: -0.24740159824972438, variance: 0.0, cvar: 1.9695966243743896, v: 1.3221769332885742, mean_q: 0.1712738573551178, std_q: 1.4564040899276733, lamda: 1.9160199165344238
Running avgs for agent 1: q_loss: 11.932511329650879, u_loss: 1.7053229808807373, p_loss: 0.22887709736824036, mean_rew: -0.29975183245471093, variance: 0.0, cvar: 2.4079389572143555, v: 1.006777048110962, mean_q: -0.29863885045051575, std_q: 1.3809291124343872, lamda: 2.0581352710723877
Running avgs for agent 2: q_loss: 0.02237665466964245, u_loss: 1.2389273643493652, p_loss: 0.9980937242507935, mean_rew: -0.3742099320243656, variance: 3.6151397228240967, cvar: -0.4840642213821411, v: -0.5000883340835571, mean_q: -1.0151289701461792, std_q: 1.4891273975372314, lamda: 1.1633142232894897
Running avgs for agent 3: q_loss: 0.01962251588702202, u_loss: 0.9637126922607422, p_loss: 1.0048556327819824, mean_rew: -0.38603050313003506, variance: 3.0648252964019775, cvar: -0.4935489296913147, v: -0.5032001733779907, mean_q: -1.028185248374939, std_q: 1.4526668787002563, lamda: 1.1525532007217407

steps: 1849975, episodes: 74000, mean episode reward: -29.567226461127873, agent episode reward: [-2.9823864661991166, -6.871005353979216, -10.405891298448477, -9.30794334250106], time: 66.488
steps: 1849975, episodes: 74000, mean episode variance: 1.6557004047483206, agent episode variance: [0.0, 0.0007814365476369858, 0.8859865891933442, 0.7689323790073395], time: 66.488
steps: 1849975, episodes: 74000, mean episode cvar: 0.73860458740592, agent episode cvar: [0.49147078704833985, 0.4927330466508865, -0.12073990535736084, -0.12485934093594551], time: 66.489
Running avgs for agent 0: q_loss: 0.15817488729953766, u_loss: 1.1229976415634155, p_loss: -0.41216331720352173, mean_rew: -0.24552471353345504, variance: 0.0, cvar: 1.9658831357955933, v: 1.3895336389541626, mean_q: 0.26957324147224426, std_q: 1.4488571882247925, lamda: 1.928432822227478
Running avgs for agent 1: q_loss: 1.5622323751449585, u_loss: 1.8474223613739014, p_loss: 0.21903285384178162, mean_rew: -0.3016474823066708, variance: 0.0031257461905479433, cvar: 1.9709322452545166, v: 0.8823070526123047, mean_q: -0.3008784353733063, std_q: 1.422366738319397, lamda: 2.073458433151245
Running avgs for agent 2: q_loss: 0.020768625661730766, u_loss: 1.2638177871704102, p_loss: 0.9968689680099487, mean_rew: -0.3756944207339394, variance: 3.5439462661743164, cvar: -0.4829596281051636, v: -0.49466192722320557, mean_q: -1.0183405876159668, std_q: 1.4886001348495483, lamda: 1.1629548072814941
Running avgs for agent 3: q_loss: 0.019821785390377045, u_loss: 1.1068239212036133, p_loss: 1.0018489360809326, mean_rew: -0.38606689460580934, variance: 3.0757296085357666, cvar: -0.4994373619556427, v: -0.5071443915367126, mean_q: -1.0235148668289185, std_q: 1.4394701719284058, lamda: 1.1559138298034668

steps: 1874975, episodes: 75000, mean episode reward: -30.418516025772064, agent episode reward: [-2.8783224992257805, -5.695376203828823, -11.65641521257911, -10.188402110138352], time: 66.48
steps: 1874975, episodes: 75000, mean episode variance: 1.7174970971420407, agent episode variance: [0.0, 0.0033509160354733467, 0.9120791277885437, 0.8020670533180236], time: 66.481
steps: 1874975, episodes: 75000, mean episode cvar: 0.7665150837004184, agent episode cvar: [0.492901647567749, 0.5208573927879333, -0.12102058079838753, -0.12622337585687637], time: 66.482
Running avgs for agent 0: q_loss: 0.16570954024791718, u_loss: 1.1440283060073853, p_loss: -0.4730980694293976, mean_rew: -0.24426796571105, variance: 0.0, cvar: 1.9716064929962158, v: 1.4214404821395874, mean_q: 0.33447641134262085, std_q: 1.443050503730774, lamda: 1.9387292861938477
Running avgs for agent 1: q_loss: 3.7786056995391846, u_loss: 1.700482964515686, p_loss: -0.12620550394058228, mean_rew: -0.30347079209771904, variance: 0.013403664141893387, cvar: 2.0834295749664307, v: 0.9225615859031677, mean_q: 0.047849345952272415, std_q: 1.2771612405776978, lamda: 2.0865821838378906
Running avgs for agent 2: q_loss: 0.02097957953810692, u_loss: 1.208459734916687, p_loss: 0.9874035716056824, mean_rew: -0.3780517361012386, variance: 3.6483163833618164, cvar: -0.48408231139183044, v: -0.49322807788848877, mean_q: -1.0113219022750854, std_q: 1.4699586629867554, lamda: 1.1638271808624268
Running avgs for agent 3: q_loss: 0.022296002134680748, u_loss: 1.3161052465438843, p_loss: 1.0011303424835205, mean_rew: -0.3894487995979404, variance: 3.208268165588379, cvar: -0.5048934817314148, v: -0.5141832828521729, mean_q: -1.0189316272735596, std_q: 1.439127802848816, lamda: 1.1611229181289673

steps: 1899975, episodes: 76000, mean episode reward: -30.236181361521172, agent episode reward: [-2.465501287813544, -4.802485429108018, -12.290314275078643, -10.677880369520965], time: 66.747
steps: 1899975, episodes: 76000, mean episode variance: 1.7787163451313972, agent episode variance: [0.0, 0.0009877045750617981, 0.9440504996776581, 0.8336781408786774], time: 66.747
steps: 1899975, episodes: 76000, mean episode cvar: 0.7643546037077904, agent episode cvar: [0.4827432736158371, 0.5319984513521194, -0.12256526380777359, -0.12782185745239258], time: 66.748
Running avgs for agent 0: q_loss: 0.2311270833015442, u_loss: 1.080312728881836, p_loss: -0.5820710062980652, mean_rew: -0.24277659651094613, variance: 0.0, cvar: 1.9309731721878052, v: 1.4036061763763428, mean_q: 0.4610756039619446, std_q: 1.3800116777420044, lamda: 1.9508296251296997
Running avgs for agent 1: q_loss: 3.1185555458068848, u_loss: 1.6796423196792603, p_loss: -0.2994450330734253, mean_rew: -0.3056853645165414, variance: 0.003950818300247193, cvar: 2.1279938220977783, v: 1.0121071338653564, mean_q: 0.23600149154663086, std_q: 1.21562659740448, lamda: 2.1033880710601807
Running avgs for agent 2: q_loss: 0.02238992042839527, u_loss: 1.2524561882019043, p_loss: 0.990082323551178, mean_rew: -0.3828053805469957, variance: 3.7762022018432617, cvar: -0.490261048078537, v: -0.5012184381484985, mean_q: -1.014479398727417, std_q: 1.463916301727295, lamda: 1.1647167205810547
Running avgs for agent 3: q_loss: 0.025570930913090706, u_loss: 1.3167047500610352, p_loss: 0.9842903017997742, mean_rew: -0.3891543776727832, variance: 3.334712505340576, cvar: -0.5112874507904053, v: -0.5278745293617249, mean_q: -0.9984255433082581, std_q: 1.420561671257019, lamda: 1.1619079113006592

steps: 1924975, episodes: 77000, mean episode reward: -31.662739994157167, agent episode reward: [-2.5118631129047015, -5.411818781368798, -12.879208161022921, -10.859849938860746], time: 66.782
steps: 1924975, episodes: 77000, mean episode variance: 1.8892878920733929, agent episode variance: [0.0, 0.0003307771384716034, 1.009172379255295, 0.8797847356796265], time: 66.782
steps: 1924975, episodes: 77000, mean episode cvar: 0.8548116247951985, agent episode cvar: [0.5038082545995712, 0.6063498830795289, -0.12498349344730378, -0.13036301943659784], time: 66.783
Running avgs for agent 0: q_loss: 0.8446862101554871, u_loss: 1.078023076057434, p_loss: -0.4924028515815735, mean_rew: -0.24099364057989422, variance: 0.0, cvar: 2.015233039855957, v: 1.343618392944336, mean_q: 0.3722195327281952, std_q: 1.3491919040679932, lamda: 1.9727994203567505
Running avgs for agent 1: q_loss: 13.705713272094727, u_loss: 1.4221519231796265, p_loss: -0.26767370104789734, mean_rew: -0.30535764156798645, variance: 0.0013231085538864137, cvar: 2.4253993034362793, v: 0.9063677787780762, mean_q: 0.2189909666776657, std_q: 1.2184109687805176, lamda: 2.1241137981414795
Running avgs for agent 2: q_loss: 0.024975284934043884, u_loss: 1.2901620864868164, p_loss: 0.9812929630279541, mean_rew: -0.38796399670017717, variance: 4.036689758300781, cvar: -0.49993398785591125, v: -0.5096807479858398, mean_q: -1.0048174858093262, std_q: 1.4289377927780151, lamda: 1.167807936668396
Running avgs for agent 3: q_loss: 0.028388330712914467, u_loss: 1.3031532764434814, p_loss: 0.9679582118988037, mean_rew: -0.3928692734419444, variance: 3.519138813018799, cvar: -0.5214520692825317, v: -0.5357403755187988, mean_q: -0.9797982573509216, std_q: 1.4025580883026123, lamda: 1.1622602939605713

steps: 1949975, episodes: 78000, mean episode reward: -34.005730652332, agent episode reward: [-2.9539177930345253, -6.260886862507052, -13.634269848403937, -11.156656148386485], time: 66.535
steps: 1949975, episodes: 78000, mean episode variance: 1.9675883738994597, agent episode variance: [0.0, 0.0, 1.0524403934478759, 0.9151479804515839], time: 66.535
steps: 1949975, episodes: 78000, mean episode cvar: 0.8646202596724033, agent episode cvar: [0.5020679317712784, 0.6219813064336777, -0.12737822678685187, -0.13205075174570083], time: 66.536
Running avgs for agent 0: q_loss: 0.6183192133903503, u_loss: 1.0115361213684082, p_loss: -0.40327969193458557, mean_rew: -0.2367276058423161, variance: 0.0, cvar: 2.0082716941833496, v: 1.2238496541976929, mean_q: 0.2820895314216614, std_q: 1.3182612657546997, lamda: 1.996166706085205
Running avgs for agent 1: q_loss: 16.563838958740234, u_loss: 1.4269084930419922, p_loss: -0.22342842817306519, mean_rew: -0.3093507163630067, variance: 0.0, cvar: 2.4879252910614014, v: 0.8582359552383423, mean_q: 0.18539299070835114, std_q: 1.2728888988494873, lamda: 2.1463711261749268
Running avgs for agent 2: q_loss: 0.02898089587688446, u_loss: 1.2827379703521729, p_loss: 0.9671878814697266, mean_rew: -0.3927704583312948, variance: 4.209761619567871, cvar: -0.5095129013061523, v: -0.5198010206222534, mean_q: -0.9876972436904907, std_q: 1.3962665796279907, lamda: 1.1690617799758911
Running avgs for agent 3: q_loss: 0.030063370242714882, u_loss: 1.1609001159667969, p_loss: 0.9554561376571655, mean_rew: -0.3940778666225163, variance: 3.6605920791625977, cvar: -0.528203010559082, v: -0.5426384210586548, mean_q: -0.9657276272773743, std_q: 1.3747082948684692, lamda: 1.1616520881652832

steps: 1974975, episodes: 79000, mean episode reward: -37.363494328366286, agent episode reward: [-3.3354176406015963, -8.79638573214993, -14.306971909121431, -10.92471904649333], time: 66.553
steps: 1974975, episodes: 79000, mean episode variance: 1.960541016817093, agent episode variance: [0.0, 0.0, 1.0468262240886688, 0.913714792728424], time: 66.553
steps: 1974975, episodes: 79000, mean episode cvar: 0.8519735958874226, agent episode cvar: [0.4919812074899673, 0.6216986806392669, -0.12955175957083703, -0.13215453267097474], time: 66.554
Running avgs for agent 0: q_loss: 0.36699676513671875, u_loss: 0.9528195858001709, p_loss: -0.3795955777168274, mean_rew: -0.2356023762425429, variance: 0.0, cvar: 1.9679248332977295, v: 1.1949456930160522, mean_q: 0.2844831049442291, std_q: 1.2770369052886963, lamda: 2.011331081390381
Running avgs for agent 1: q_loss: 20.328022003173828, u_loss: 1.9079455137252808, p_loss: 0.13374511897563934, mean_rew: -0.3120711868470803, variance: 0.0, cvar: 2.486794948577881, v: 0.541143000125885, mean_q: -0.16460376977920532, std_q: 1.3622673749923706, lamda: 2.1708462238311768
Running avgs for agent 2: q_loss: 0.03267190605401993, u_loss: 1.311477780342102, p_loss: 0.9559592008590698, mean_rew: -0.3979140034356002, variance: 4.187304496765137, cvar: -0.5182070136070251, v: -0.5332958698272705, mean_q: -0.9738937020301819, std_q: 1.3633781671524048, lamda: 1.166448950767517
Running avgs for agent 3: q_loss: 0.02904663421213627, u_loss: 0.9933782815933228, p_loss: 0.9483095407485962, mean_rew: -0.395712938666951, variance: 3.6548593044281006, cvar: -0.5286181569099426, v: -0.5416656732559204, mean_q: -0.95794677734375, std_q: 1.338026762008667, lamda: 1.1603914499282837

steps: 1999975, episodes: 80000, mean episode reward: -35.20852904032149, agent episode reward: [-3.888090930552573, -7.981848952352644, -12.800916009624954, -10.537673147791313], time: 66.789
steps: 1999975, episodes: 80000, mean episode variance: 1.9018986763041466, agent episode variance: [0.0, 0.022378885654732585, 0.9886015899181366, 0.8909182007312775], time: 66.789
steps: 1999975, episodes: 80000, mean episode cvar: 0.7384958126246929, agent episode cvar: [0.4952502107620239, 0.5059206199645996, -0.1323285520672798, -0.1303464660346508], time: 66.79
Running avgs for agent 0: q_loss: 0.6923329830169678, u_loss: 0.8969553112983704, p_loss: -0.33117735385894775, mean_rew: -0.2298822417398614, variance: 0.0, cvar: 1.9810010194778442, v: 1.15340256690979, mean_q: 0.24321912229061127, std_q: 1.2175719738006592, lamda: 2.0272598266601562
Running avgs for agent 1: q_loss: 4.557085990905762, u_loss: 2.1702094078063965, p_loss: -0.04688327759504318, mean_rew: -0.31584613600921635, variance: 0.08951554261893034, cvar: 2.0236825942993164, v: 0.6594122052192688, mean_q: 0.008329273201525211, std_q: 1.2622981071472168, lamda: 2.194387435913086
Running avgs for agent 2: q_loss: 0.03348361700773239, u_loss: 1.2421013116836548, p_loss: 0.9446174502372742, mean_rew: -0.40358020990192883, variance: 3.954406261444092, cvar: -0.5293142199516296, v: -0.5385463833808899, mean_q: -0.9627981185913086, std_q: 1.316602349281311, lamda: 1.1633083820343018
Running avgs for agent 3: q_loss: 0.026710765436291695, u_loss: 0.9024896621704102, p_loss: 0.9593110084533691, mean_rew: -0.39634427449130066, variance: 3.5636727809906006, cvar: -0.5213858485221863, v: -0.5399469137191772, mean_q: -0.96987384557724, std_q: 1.3485593795776367, lamda: 1.1597883701324463

...Finished total of 80001 episodes... Now freezing policy and running for 1000 more episodes
steps: 24975, episodes: 1000, mean episode reward: -35.125044128002955, agent episode reward: [-4.4994375475908255, -7.511379042739736, -13.131518186591846, -9.98270935108055], time: 43.579
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 43.58
steps: 24975, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0, 0.0], time: 43.58
Running avgs for agent 0: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -35.044904712666785, agent episode reward: [-4.499991002010969, -7.54490258947233, -13.026171039355466, -9.973840081828023], time: 54.297
steps: 49975, episodes: 2000, mean episode variance: 1.9061926594674587, agent episode variance: [0.0, 0.00514429435133934, 1.0842161836624145, 0.8168321814537048], time: 54.297
steps: 49975, episodes: 2000, mean episode cvar: 0.5751419270336628, agent episode cvar: [0.4856513147354126, 0.3428754587173462, -0.12804607379436492, -0.12533877262473106], time: 54.298
Running avgs for agent 0: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.18050153840068947, variance: 0.0, cvar: 1.990374207496643, v: 1.2455490827560425, mean_q: 0.5313985347747803, std_q: 1.145844578742981, lamda: 2.035926580429077
Running avgs for agent 1: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.30049598233077307, variance: 0.02108317357106287, cvar: 1.4052274227142334, v: -0.0005169383948668838, mean_q: -1.0032141208648682, std_q: 1.4198355674743652, lamda: 2.2040674686431885
Running avgs for agent 2: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.5224909838229963, variance: 4.443508625030518, cvar: -0.5247790217399597, v: -0.5284627079963684, mean_q: -0.9206845760345459, std_q: 1.2560049295425415, lamda: 1.1630133390426636
Running avgs for agent 3: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.39785724098164377, variance: 3.347672700881958, cvar: -0.5136834979057312, v: -0.5181925296783447, mean_q: -0.9016520977020264, std_q: 1.257521629333496, lamda: 1.1584206819534302

...Finished total of 2001 episodes with the fixed policy.
