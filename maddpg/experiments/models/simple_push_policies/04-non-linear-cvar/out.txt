WARNING: Logging before flag parsing goes to stderr.
W0827 00:17:57.685280 4526245312 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0827 00:17:57.685585 4526245312 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-27 00:17:57.686023: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0827 00:17:57.688858 4526245312 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0827 00:17:57.691284 4526245312 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0827 00:17:57.691430 4526245312 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0827 00:17:57.691519 4526245312 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0827 00:17:58.164767 4526245312 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0827 00:17:58.355395 4526245312 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0827 00:17:58.362574 4526245312 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0827 00:17:58.869163 4526245312 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation True
adversary agent:  1.35
adversary agent:  1.35
good agent:  -0.3
good agent:  -0.3
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -72.10206898000152, agent episode reward: [-7.82872936664437, -8.46059866343289, -27.472455751984043, -28.340285197940233], time: 34.963
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 34.963
steps: 24975, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0, 0.0], time: 34.964
Running avgs for agent 0: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -58.250553804810416, agent episode reward: [-12.167609774683864, -8.18544052832498, -18.506489479232002, -19.391014022569568], time: 71.503
steps: 49975, episodes: 2000, mean episode variance: 0.46693806420220063, agent episode variance: [0.11264662911370396, 0.04957222139602527, 0.17232488417066633, 0.13239432952180505], time: 71.503
steps: 49975, episodes: 2000, mean episode cvar: 0.5373388342503458, agent episode cvar: [0.2893896540403366, 0.28381541818380357, -0.02118665997963399, -0.014679577994160354], time: 71.504
Running avgs for agent 0: q_loss: 0.18467630445957184, u_loss: 0.5411049127578735, p_loss: 0.2865709960460663, mean_rew: -0.3795873803270665, variance: 0.46166651276108184, cvar: 1.1860231161117554, v: 0.7361364364624023, mean_q: -0.3239279091358185, std_q: 0.9323239326477051, lamda: 1.0042792558670044
Running avgs for agent 1: q_loss: 0.17005641758441925, u_loss: 0.4178706407546997, p_loss: 0.2265203297138214, mean_rew: -0.326504266250887, variance: 0.20316484178698882, cvar: 1.1631778478622437, v: 0.6980154514312744, mean_q: -0.26202383637428284, std_q: 0.8226034045219421, lamda: 1.003702163696289
Running avgs for agent 2: q_loss: 0.06661096960306168, u_loss: 0.4378483295440674, p_loss: 1.777745008468628, mean_rew: -1.0804296456387652, variance: 0.7062495252896162, cvar: -0.08683057874441147, v: -0.15592588484287262, mean_q: -1.8265039920806885, std_q: 0.9449601769447327, lamda: 1.0003254413604736
Running avgs for agent 3: q_loss: 0.042815133929252625, u_loss: 0.585673451423645, p_loss: 1.7574105262756348, mean_rew: -1.1257455441444892, variance: 0.5425997111549388, cvar: -0.06016220897436142, v: -0.1284644901752472, mean_q: -1.807660460472107, std_q: 1.0430828332901, lamda: 1.000009298324585

steps: 74975, episodes: 3000, mean episode reward: -28.726263407232558, agent episode reward: [-7.0825143393985455, -6.913445726800142, -7.325803065157474, -7.404500275876398], time: 66.955
steps: 74975, episodes: 3000, mean episode variance: 0.9692554458975792, agent episode variance: [0.1559952152594924, 0.10875095281004905, 0.3722924105450511, 0.33221686728298666], time: 66.955
steps: 74975, episodes: 3000, mean episode cvar: 0.5468162283301353, agent episode cvar: [0.3553037543296814, 0.35888580632209777, -0.08627014437317848, -0.08110318794846534], time: 66.956
Running avgs for agent 0: q_loss: 0.21177785098552704, u_loss: 1.41311514377594, p_loss: 1.054229736328125, mean_rew: -0.3791828198128202, variance: 0.6239808610379696, cvar: 1.4212149381637573, v: 0.4976233243942261, mean_q: -1.1504161357879639, std_q: 1.6575113534927368, lamda: 1.0199581384658813
Running avgs for agent 1: q_loss: 0.265625536441803, u_loss: 0.9399955868721008, p_loss: 0.8839134573936462, mean_rew: -0.32678066588432875, variance: 0.4350038112401962, cvar: 1.435543179512024, v: 0.46444422006607056, mean_q: -0.9703285098075867, std_q: 1.457027554512024, lamda: 1.0197621583938599
Running avgs for agent 2: q_loss: 0.011045972816646099, u_loss: 2.34525203704834, p_loss: 2.201845645904541, mean_rew: -0.8048727265127035, variance: 1.489169716835022, cvar: -0.34508058428764343, v: -0.41507408022880554, mean_q: -2.3385424613952637, std_q: 2.0936198234558105, lamda: 1.0012680292129517
Running avgs for agent 3: q_loss: 0.007391346152871847, u_loss: 2.022237777709961, p_loss: 2.2878496646881104, mean_rew: -0.8329064678702596, variance: 1.3288674691319466, cvar: -0.3244127631187439, v: -0.38464537262916565, mean_q: -2.4281582832336426, std_q: 2.2079877853393555, lamda: 1.0006945133209229

steps: 99975, episodes: 4000, mean episode reward: -24.936812637383913, agent episode reward: [-5.499885088928364, -5.232513539327145, -7.175952431232903, -7.0284615778955], time: 67.155
steps: 99975, episodes: 4000, mean episode variance: 1.0547354118600487, agent episode variance: [0.1877049217298627, 0.11421719633042812, 0.3684857602715492, 0.38432753352820875], time: 67.155
steps: 99975, episodes: 4000, mean episode cvar: 0.5577930137217045, agent episode cvar: [0.3574697073698044, 0.36674064755439756, -0.08386014014482499, -0.0825572010576725], time: 67.156
Running avgs for agent 0: q_loss: 0.2806820869445801, u_loss: 3.418342113494873, p_loss: 1.5218312740325928, mean_rew: -0.3425285510742768, variance: 0.7508196869194508, cvar: 1.4298789501190186, v: 0.0742620974779129, mean_q: -1.6695204973220825, std_q: 2.1991870403289795, lamda: 1.040338397026062
Running avgs for agent 1: q_loss: 0.31662634015083313, u_loss: 2.07334041595459, p_loss: 1.3813318014144897, mean_rew: -0.3016329516058951, variance: 0.4568687853217125, cvar: 1.4669626951217651, v: 0.055244218558073044, mean_q: -1.51866614818573, std_q: 1.9826208353042603, lamda: 1.0403857231140137
Running avgs for agent 2: q_loss: 0.008755097165703773, u_loss: 4.591455936431885, p_loss: 2.4947316646575928, mean_rew: -0.6550213145823409, variance: 1.473943041086197, cvar: -0.3354405462741852, v: -0.3929532468318939, mean_q: -2.684854030609131, std_q: 3.033836841583252, lamda: 1.0015672445297241
Running avgs for agent 3: q_loss: 0.007708913646638393, u_loss: 5.637778282165527, p_loss: 2.590639591217041, mean_rew: -0.6728343553541191, variance: 1.537310134112835, cvar: -0.3302287757396698, v: -0.36807796359062195, mean_q: -2.781435251235962, std_q: 3.1969659328460693, lamda: 1.0010349750518799

steps: 124975, episodes: 5000, mean episode reward: -24.247394034100246, agent episode reward: [-4.9159949197953745, -4.6438856632530205, -7.370181616394793, -7.3173318346570575], time: 67.039
steps: 124975, episodes: 5000, mean episode variance: 1.3021368634290993, agent episode variance: [0.26425624890625476, 0.19794488681852818, 0.40464122178405526, 0.43529450592026114], time: 67.039
steps: 124975, episodes: 5000, mean episode cvar: 0.5118502560555935, agent episode cvar: [0.33880359429121015, 0.3419507428109646, -0.08453935742378235, -0.08436472362279893], time: 67.04
Running avgs for agent 0: q_loss: 0.34234708547592163, u_loss: 5.292969703674316, p_loss: 1.6945372819900513, mean_rew: -0.3141212497452711, variance: 1.057024995625019, cvar: 1.3552143573760986, v: -0.262484073638916, mean_q: -1.8745951652526855, std_q: 2.5685794353485107, lamda: 1.0588449239730835
Running avgs for agent 1: q_loss: 0.42053836584091187, u_loss: 4.537766933441162, p_loss: 1.5653384923934937, mean_rew: -0.2771196543752973, variance: 0.7917795472741127, cvar: 1.3678029775619507, v: -0.285153865814209, mean_q: -1.726367473602295, std_q: 2.3275558948516846, lamda: 1.0588562488555908
Running avgs for agent 2: q_loss: 0.012951678596436977, u_loss: 7.65283203125, p_loss: 2.5130438804626465, mean_rew: -0.5723498667031315, variance: 1.618564887136221, cvar: -0.33815744519233704, v: -0.39955419301986694, mean_q: -2.7178492546081543, std_q: 3.6043200492858887, lamda: 1.0020041465759277
Running avgs for agent 3: q_loss: 0.01346820779144764, u_loss: 9.115617752075195, p_loss: 2.615135669708252, mean_rew: -0.5863418530639053, variance: 1.7411780236810446, cvar: -0.33745887875556946, v: -0.3719111680984497, mean_q: -2.822683811187744, std_q: 3.782240152359009, lamda: 1.0014795064926147

steps: 149975, episodes: 6000, mean episode reward: -24.316520381973092, agent episode reward: [-5.50848538063548, -4.701055066247014, -7.0196968055641005, -7.087283129526501], time: 67.565
steps: 149975, episodes: 6000, mean episode variance: 1.8367079446092247, agent episode variance: [0.5093466883823276, 0.41140144744515417, 0.44554068917781114, 0.4704191196039319], time: 67.566
steps: 149975, episodes: 6000, mean episode cvar: 0.48493119049072264, agent episode cvar: [0.3214274589419365, 0.332509043097496, -0.08461624500155449, -0.08438906654715538], time: 67.566
Running avgs for agent 0: q_loss: 0.35596293210983276, u_loss: 7.744585990905762, p_loss: 1.561134696006775, mean_rew: -0.2927587587123796, variance: 2.03738675352931, cvar: 1.2857097387313843, v: -0.35127145051956177, mean_q: -1.7401481866836548, std_q: 2.7058706283569336, lamda: 1.0782876014709473
Running avgs for agent 1: q_loss: 0.4084697961807251, u_loss: 6.497804641723633, p_loss: 1.456404209136963, mean_rew: -0.2625072078582705, variance: 1.6456057897806167, cvar: 1.3300361633300781, v: -0.36725854873657227, mean_q: -1.6128231287002563, std_q: 2.471048355102539, lamda: 1.079122543334961
Running avgs for agent 2: q_loss: 0.015162582509219646, u_loss: 13.745376586914062, p_loss: 2.417587995529175, mean_rew: -0.5195500799564604, variance: 1.7821627567112446, cvar: -0.33846497535705566, v: -0.4012582302093506, mean_q: -2.61584210395813, std_q: 3.8131163120269775, lamda: 1.0024607181549072
Running avgs for agent 3: q_loss: 0.01784585230052471, u_loss: 11.5885591506958, p_loss: 2.5319371223449707, mean_rew: -0.5319436446722944, variance: 1.8816764784157276, cvar: -0.33755627274513245, v: -0.3985629379749298, mean_q: -2.736032485961914, std_q: 4.065393447875977, lamda: 1.0028878450393677

steps: 174975, episodes: 7000, mean episode reward: -24.852171099574488, agent episode reward: [-5.69151750020129, -4.809805088745414, -7.1905403553879195, -7.160308155239866], time: 67.621
steps: 174975, episodes: 7000, mean episode variance: 2.2594591732732954, agent episode variance: [0.6810127315223217, 0.5741225418820978, 0.4976588211245835, 0.5066650787442922], time: 67.621
steps: 174975, episodes: 7000, mean episode cvar: 0.4486653310954571, agent episode cvar: [0.30554790776968005, 0.3131669724881649, -0.0849575492143631, -0.08509199994802474], time: 67.622
Running avgs for agent 0: q_loss: 0.283078134059906, u_loss: 11.41862678527832, p_loss: 1.381824254989624, mean_rew: -0.2823864606866164, variance: 2.7240509260892867, cvar: 1.2221916913986206, v: -0.2927016317844391, mean_q: -1.5516804456710815, std_q: 2.767120361328125, lamda: 1.094893217086792
Running avgs for agent 1: q_loss: 0.40714094042778015, u_loss: 7.76129150390625, p_loss: 1.3012051582336426, mean_rew: -0.2502721665586693, variance: 2.296490167528391, cvar: 1.2526679039001465, v: -0.3693939745426178, mean_q: -1.4462310075759888, std_q: 2.481153726577759, lamda: 1.098770260810852
Running avgs for agent 2: q_loss: 0.010165474377572536, u_loss: 16.06289291381836, p_loss: 2.3111512660980225, mean_rew: -0.48335125197858725, variance: 1.990635284498334, cvar: -0.3398301899433136, v: -0.4013199210166931, mean_q: -2.497161626815796, std_q: 3.9219062328338623, lamda: 1.0029610395431519
Running avgs for agent 3: q_loss: 0.011243319138884544, u_loss: 17.21599006652832, p_loss: 2.407114028930664, mean_rew: -0.4932079172536996, variance: 2.026660314977169, cvar: -0.34036797285079956, v: -0.3987947702407837, mean_q: -2.5989856719970703, std_q: 4.17575216293335, lamda: 1.0034462213516235

steps: 199975, episodes: 8000, mean episode reward: -24.57787201697273, agent episode reward: [-5.6096506833942055, -5.037034435777899, -6.837569596022984, -7.093617301777639], time: 67.296
steps: 199975, episodes: 8000, mean episode variance: 2.5368542390465736, agent episode variance: [0.7454173600375652, 0.6102060738801957, 0.5809890503287315, 0.6002417548000812], time: 67.297
steps: 199975, episodes: 8000, mean episode cvar: 0.4554044829308987, agent episode cvar: [0.3135588683485985, 0.31256026244163515, -0.08539373221993446, -0.08532091563940049], time: 67.297
Running avgs for agent 0: q_loss: 0.24553006887435913, u_loss: 12.858308792114258, p_loss: 1.213984489440918, mean_rew: -0.2755803388947648, variance: 2.9816694259643555, cvar: 1.2542356252670288, v: -0.2393685281276703, mean_q: -1.3725364208221436, std_q: 2.719804286956787, lamda: 1.109681248664856
Running avgs for agent 1: q_loss: 0.25015994906425476, u_loss: 10.132608413696289, p_loss: 1.1443052291870117, mean_rew: -0.2438603922545139, variance: 2.440824270248413, cvar: 1.2502411603927612, v: -0.28124678134918213, mean_q: -1.280246376991272, std_q: 2.4761743545532227, lamda: 1.1136082410812378
Running avgs for agent 2: q_loss: 0.008937706239521503, u_loss: 18.581518173217773, p_loss: 2.20074725151062, mean_rew: -0.4572624886181164, variance: 2.323956251144409, cvar: -0.34157490730285645, v: -0.3936116397380829, mean_q: -2.3705801963806152, std_q: 3.927556037902832, lamda: 1.0031896829605103
Running avgs for agent 3: q_loss: 0.015495802275836468, u_loss: 21.8685359954834, p_loss: 2.30043888092041, mean_rew: -0.46747186962553683, variance: 2.400967019200325, cvar: -0.3412836492061615, v: -0.4086003303527832, mean_q: -2.4778218269348145, std_q: 4.180738925933838, lamda: 1.0044453144073486

steps: 224975, episodes: 9000, mean episode reward: -24.48454702926431, agent episode reward: [-5.369168657618531, -5.040383953770021, -7.088601001086874, -6.986393416788884], time: 67.646
steps: 224975, episodes: 9000, mean episode variance: 2.5411823436915872, agent episode variance: [0.7888990312367677, 0.6152967935800553, 0.549450988933444, 0.5875355299413204], time: 67.647
steps: 224975, episodes: 9000, mean episode cvar: 0.4504988653361797, agent episode cvar: [0.30316694793105126, 0.3191074347794056, -0.08615888723731041, -0.0856166301369667], time: 67.647
Running avgs for agent 0: q_loss: 0.2975299656391144, u_loss: 16.465051651000977, p_loss: 1.042608618736267, mean_rew: -0.2681786837531633, variance: 3.1555962562561035, cvar: 1.21266770362854, v: -0.16974440217018127, mean_q: -1.180831789970398, std_q: 2.6128056049346924, lamda: 1.1248759031295776
Running avgs for agent 1: q_loss: 0.30111274123191833, u_loss: 11.724472045898438, p_loss: 1.0155547857284546, mean_rew: -0.23820748948985965, variance: 2.461187174320221, cvar: 1.2764296531677246, v: -0.2438403218984604, mean_q: -1.140196681022644, std_q: 2.424682855606079, lamda: 1.1284655332565308
Running avgs for agent 2: q_loss: 0.010468046180903912, u_loss: 23.325801849365234, p_loss: 2.08829402923584, mean_rew: -0.43586428472991645, variance: 2.197803955733776, cvar: -0.344635546207428, v: -0.390595406293869, mean_q: -2.2420401573181152, std_q: 3.8469247817993164, lamda: 1.0035861730575562
Running avgs for agent 3: q_loss: 0.01781275123357773, u_loss: 24.404327392578125, p_loss: 2.204465389251709, mean_rew: -0.4442933460554005, variance: 2.350142002105713, cvar: -0.3424665629863739, v: -0.4252638518810272, mean_q: -2.3648741245269775, std_q: 4.10403299331665, lamda: 1.006095290184021

steps: 249975, episodes: 10000, mean episode reward: -23.98535898485047, agent episode reward: [-5.182681419058579, -5.128639313066408, -6.766733891223671, -6.907304361501813], time: 67.585
steps: 249975, episodes: 10000, mean episode variance: 2.4876099498569966, agent episode variance: [0.7651252731084823, 0.6256851523071527, 0.5613242083787918, 0.5354753160625696], time: 67.586
steps: 249975, episodes: 10000, mean episode cvar: 0.4437740450501442, agent episode cvar: [0.30187829542160033, 0.31388965117931367, -0.0869750774204731, -0.08501882413029671], time: 67.586
Running avgs for agent 0: q_loss: 0.18184341490268707, u_loss: 14.901596069335938, p_loss: 0.8859608769416809, mean_rew: -0.2631924556844968, variance: 3.0605008602142334, cvar: 1.2075132131576538, v: -0.08350455015897751, mean_q: -1.0093847513198853, std_q: 2.4983911514282227, lamda: 1.1380584239959717
Running avgs for agent 1: q_loss: 0.3061043620109558, u_loss: 11.89714241027832, p_loss: 0.9103899002075195, mean_rew: -0.23408086605507375, variance: 2.502740609228611, cvar: 1.2555586099624634, v: -0.1951114535331726, mean_q: -1.02056086063385, std_q: 2.3194169998168945, lamda: 1.144138216972351
Running avgs for agent 2: q_loss: 0.007309280335903168, u_loss: 24.009258270263672, p_loss: 2.011303186416626, mean_rew: -0.4214843451415805, variance: 2.2452968335151673, cvar: -0.3479003310203552, v: -0.3796132206916809, mean_q: -2.153635263442993, std_q: 3.826572895050049, lamda: 1.0040841102600098
Running avgs for agent 3: q_loss: 0.01415180042386055, u_loss: 28.16942596435547, p_loss: 2.106942653656006, mean_rew: -0.4256974731666302, variance: 2.1419012642502784, cvar: -0.3400753140449524, v: -0.4284737706184387, mean_q: -2.254307270050049, std_q: 3.9871816635131836, lamda: 1.0073076486587524

steps: 274975, episodes: 11000, mean episode reward: -24.15573822101029, agent episode reward: [-4.835979514998462, -4.949644169496947, -7.246991232104722, -7.123123304410157], time: 67.97
steps: 274975, episodes: 11000, mean episode variance: 2.372884529478848, agent episode variance: [0.7073029100298881, 0.6382542199492455, 0.5303638701587915, 0.4969635293409228], time: 67.971
steps: 274975, episodes: 11000, mean episode cvar: 0.4516968964934349, agent episode cvar: [0.30479067206382754, 0.31810045617818833, -0.08626701784133911, -0.08492721390724182], time: 67.971
Running avgs for agent 0: q_loss: 0.28426215052604675, u_loss: 19.572338104248047, p_loss: 0.7842015027999878, mean_rew: -0.25663085634639937, variance: 2.8292116401195524, cvar: 1.2191627025604248, v: 0.0230186115950346, mean_q: -0.8955711126327515, std_q: 2.441088914871216, lamda: 1.1508080959320068
Running avgs for agent 1: q_loss: 0.2640424370765686, u_loss: 10.897966384887695, p_loss: 0.8130205273628235, mean_rew: -0.23205911183336142, variance: 2.5530171394348145, cvar: 1.2724018096923828, v: -0.14659051597118378, mean_q: -0.9111342430114746, std_q: 2.250331163406372, lamda: 1.1604840755462646
Running avgs for agent 2: q_loss: 0.015455180779099464, u_loss: 25.76176643371582, p_loss: 1.9191125631332397, mean_rew: -0.40447488030182593, variance: 2.121455480635166, cvar: -0.3450680673122406, v: -0.37269341945648193, mean_q: -2.0472183227539062, std_q: 3.6787421703338623, lamda: 1.0051796436309814
Running avgs for agent 3: q_loss: 0.01188291423022747, u_loss: 28.03810691833496, p_loss: 2.027836322784424, mean_rew: -0.4125430605855003, variance: 1.9878541173636912, cvar: -0.3397088348865509, v: -0.42641448974609375, mean_q: -2.164597511291504, std_q: 3.912900447845459, lamda: 1.008689522743225

steps: 299975, episodes: 12000, mean episode reward: -24.29307610551136, agent episode reward: [-5.344460322195582, -5.269878151033507, -6.76234450112239, -6.9163931311598805], time: 67.938
steps: 299975, episodes: 12000, mean episode variance: 2.314073736861348, agent episode variance: [0.7096638640165329, 0.5887411221861839, 0.4987130330502987, 0.5169557176083326], time: 67.939
steps: 299975, episodes: 12000, mean episode cvar: 0.45115277895331385, agent episode cvar: [0.31226430428028107, 0.3110782974362373, -0.0870237230360508, -0.08516609972715378], time: 67.939
Running avgs for agent 0: q_loss: 0.3082602620124817, u_loss: 16.598323822021484, p_loss: 0.6606619954109192, mean_rew: -0.2527417352241705, variance: 2.8386552333831787, cvar: 1.2490572929382324, v: 0.06492166221141815, mean_q: -0.757396399974823, std_q: 2.286665201187134, lamda: 1.1676985025405884
Running avgs for agent 1: q_loss: 0.27007153630256653, u_loss: 12.389565467834473, p_loss: 0.7372811436653137, mean_rew: -0.22844655488984564, variance: 2.354964256286621, cvar: 1.2443132400512695, v: -0.0945439413189888, mean_q: -0.8240346908569336, std_q: 2.163942337036133, lamda: 1.1752055883407593
Running avgs for agent 2: q_loss: 0.01958255097270012, u_loss: 34.71332931518555, p_loss: 1.857010841369629, mean_rew: -0.39386817749300645, variance: 1.9948521322011947, cvar: -0.3480948805809021, v: -0.3735322952270508, mean_q: -1.9764021635055542, std_q: 3.5854010581970215, lamda: 1.0073351860046387
Running avgs for agent 3: q_loss: 0.012759605422616005, u_loss: 34.95647430419922, p_loss: 1.9618523120880127, mean_rew: -0.40126746530948254, variance: 2.0678228704333304, cvar: -0.3406643867492676, v: -0.4245840311050415, mean_q: -2.0877578258514404, std_q: 3.856822967529297, lamda: 1.0098363161087036

steps: 324975, episodes: 13000, mean episode reward: -24.55840466967038, agent episode reward: [-5.208075491090409, -4.983770125695908, -7.303267344093219, -7.0632917087908424], time: 67.98
steps: 324975, episodes: 13000, mean episode variance: 2.2838679858893154, agent episode variance: [0.6628438694477081, 0.5591114231348038, 0.5380100108385086, 0.5239026824682951], time: 67.98
steps: 324975, episodes: 13000, mean episode cvar: 0.4546192610710859, agent episode cvar: [0.3062781679034233, 0.3170567501783371, -0.08410719250142575, -0.08460846450924873], time: 67.981
Running avgs for agent 0: q_loss: 0.21245689690113068, u_loss: 18.45737075805664, p_loss: 0.573010265827179, mean_rew: -0.24965707873771698, variance: 2.6513755321502686, cvar: 1.2251126766204834, v: 0.12374190986156464, mean_q: -0.6618373990058899, std_q: 2.240645408630371, lamda: 1.1824856996536255
Running avgs for agent 1: q_loss: 0.24673667550086975, u_loss: 11.793168067932129, p_loss: 0.6956608295440674, mean_rew: -0.22780355391895557, variance: 2.236445903778076, cvar: 1.2682271003723145, v: -0.06759849190711975, mean_q: -0.7758517265319824, std_q: 2.122560739517212, lamda: 1.1901397705078125
Running avgs for agent 2: q_loss: 0.009947312995791435, u_loss: 29.67544937133789, p_loss: 1.7711344957351685, mean_rew: -0.3878117017899008, variance: 2.1520400433540345, cvar: -0.33642876148223877, v: -0.3514537513256073, mean_q: -1.8823304176330566, std_q: 3.5296738147735596, lamda: 1.0108884572982788
Running avgs for agent 3: q_loss: 0.01504444982856512, u_loss: 30.61286163330078, p_loss: 1.8948829174041748, mean_rew: -0.3910218014363449, variance: 2.0956107298731803, cvar: -0.3384338617324829, v: -0.43324199318885803, mean_q: -2.0130834579467773, std_q: 3.7145800590515137, lamda: 1.0119630098342896

steps: 349975, episodes: 14000, mean episode reward: -24.33120163062351, agent episode reward: [-5.213238178934071, -5.238559853431746, -6.839378987274697, -7.040024610983], time: 68.12
steps: 349975, episodes: 14000, mean episode variance: 2.1395884214937686, agent episode variance: [0.6424838354587555, 0.5091132901906967, 0.49066974261403085, 0.49732155323028565], time: 68.121
steps: 349975, episodes: 14000, mean episode cvar: 0.45379658499360087, agent episode cvar: [0.30692521268129347, 0.316817465364933, -0.08446227446198463, -0.08548381859064103], time: 68.121
Running avgs for agent 0: q_loss: 0.18785196542739868, u_loss: 14.943936347961426, p_loss: 0.5375486016273499, mean_rew: -0.24648313979319642, variance: 2.5699355602264404, cvar: 1.2277008295059204, v: 0.15918247401714325, mean_q: -0.6225681900978088, std_q: 2.2341091632843018, lamda: 1.1937671899795532
Running avgs for agent 1: q_loss: 0.28772056102752686, u_loss: 10.555749893188477, p_loss: 0.670221745967865, mean_rew: -0.22470267927518933, variance: 2.036453160762787, cvar: 1.2672697305679321, v: -0.06770182400941849, mean_q: -0.7435925006866455, std_q: 2.04775071144104, lamda: 1.206966519355774
Running avgs for agent 2: q_loss: 0.00992479920387268, u_loss: 25.395505905151367, p_loss: 1.7184687852859497, mean_rew: -0.3774241705959425, variance: 1.9626789704561234, cvar: -0.33784911036491394, v: -0.3555242419242859, mean_q: -1.8210097551345825, std_q: 3.3751745223999023, lamda: 1.0122555494308472
Running avgs for agent 3: q_loss: 0.013805722817778587, u_loss: 35.49917984008789, p_loss: 1.8493156433105469, mean_rew: -0.3831547918311851, variance: 1.9892862129211426, cvar: -0.34193527698516846, v: -0.4274995028972626, mean_q: -1.9592487812042236, std_q: 3.6794562339782715, lamda: 1.013516902923584

steps: 374975, episodes: 15000, mean episode reward: -24.852347198826802, agent episode reward: [-5.333480113841943, -5.524355298338577, -6.934152097640767, -7.060359689005517], time: 68.195
steps: 374975, episodes: 15000, mean episode variance: 2.0326469092965125, agent episode variance: [0.5970205500721931, 0.48857770383358, 0.4982599045932293, 0.4487887507975101], time: 68.195
steps: 374975, episodes: 15000, mean episode cvar: 0.4541650221645832, agent episode cvar: [0.3133678830862045, 0.3128476129770279, -0.0854655005633831, -0.08658497333526612], time: 68.196
Running avgs for agent 0: q_loss: 0.21478569507598877, u_loss: 15.659871101379395, p_loss: 0.5023361444473267, mean_rew: -0.24324392096725192, variance: 2.3880820274353027, cvar: 1.2534714937210083, v: 0.18352791666984558, mean_q: -0.5822796821594238, std_q: 2.190131664276123, lamda: 1.2073639631271362
Running avgs for agent 1: q_loss: 0.2515154480934143, u_loss: 10.051347732543945, p_loss: 0.6332374215126038, mean_rew: -0.22584279390879497, variance: 1.9543107748031616, cvar: 1.2513903379440308, v: -0.02171343006193638, mean_q: -0.7036303877830505, std_q: 2.0479536056518555, lamda: 1.2224433422088623
Running avgs for agent 2: q_loss: 0.010420860722661018, u_loss: 27.174070358276367, p_loss: 1.6957730054855347, mean_rew: -0.3720552760816642, variance: 1.993039618372917, cvar: -0.3418619930744171, v: -0.34978997707366943, mean_q: -1.7915358543395996, std_q: 3.3855202198028564, lamda: 1.0129361152648926
Running avgs for agent 3: q_loss: 0.010495212860405445, u_loss: 39.760169982910156, p_loss: 1.7949923276901245, mean_rew: -0.3771999737315761, variance: 1.7951550031900405, cvar: -0.3463399112224579, v: -0.4091028571128845, mean_q: -1.8973993062973022, std_q: 3.567229986190796, lamda: 1.014672040939331

steps: 399975, episodes: 16000, mean episode reward: -24.915733541609455, agent episode reward: [-5.351764165315818, -5.219395771585566, -7.011966317528084, -7.332607287179986], time: 67.962
steps: 399975, episodes: 16000, mean episode variance: 1.8530622090511024, agent episode variance: [0.5038870121240616, 0.48715012407302855, 0.434004351221025, 0.42802072163298727], time: 67.963
steps: 399975, episodes: 16000, mean episode cvar: 0.4497492608129978, agent episode cvar: [0.31147045630216597, 0.31025119850039484, -0.08524033892154693, -0.08673205506801605], time: 67.963
Running avgs for agent 0: q_loss: 0.21628338098526, u_loss: 20.10240936279297, p_loss: 0.5084843635559082, mean_rew: -0.2419707600080078, variance: 2.0155480484962465, cvar: 1.2458816766738892, v: 0.16901062428951263, mean_q: -0.5827368497848511, std_q: 2.1471006870269775, lamda: 1.2210570573806763
Running avgs for agent 1: q_loss: 0.30135059356689453, u_loss: 9.911373138427734, p_loss: 0.5876145362854004, mean_rew: -0.2235547120120724, variance: 1.9486004962921142, cvar: 1.2410048246383667, v: -0.02371487021446228, mean_q: -0.6489064693450928, std_q: 1.9431354999542236, lamda: 1.23983633518219
Running avgs for agent 2: q_loss: 0.009810014627873898, u_loss: 43.77090835571289, p_loss: 1.6487692594528198, mean_rew: -0.3649673150339151, variance: 1.7360174048841, cvar: -0.34096136689186096, v: -0.344552606344223, mean_q: -1.7374489307403564, std_q: 3.2592320442199707, lamda: 1.0133378505706787
Running avgs for agent 3: q_loss: 0.014493667520582676, u_loss: 29.362689971923828, p_loss: 1.735283374786377, mean_rew: -0.3711296701000074, variance: 1.712082886531949, cvar: -0.346928209066391, v: -0.38246971368789673, mean_q: -1.8286259174346924, std_q: 3.448793411254883, lamda: 1.0154383182525635

steps: 424975, episodes: 17000, mean episode reward: -24.563402057275745, agent episode reward: [-5.37040084569182, -5.341882843382736, -6.9491595984742185, -6.901958769726968], time: 68.727
steps: 424975, episodes: 17000, mean episode variance: 1.6415812459252774, agent episode variance: [0.46524499616026876, 0.4529875578917563, 0.34647205443680285, 0.3768766374364495], time: 68.728
steps: 424975, episodes: 17000, mean episode cvar: 0.46843772947788237, agent episode cvar: [0.3180161727666855, 0.32285977590084075, -0.08505467054247856, -0.0873835486471653], time: 68.728
Running avgs for agent 0: q_loss: 0.2985974848270416, u_loss: 15.38591194152832, p_loss: 0.5368083119392395, mean_rew: -0.24045167471410248, variance: 1.860979984641075, cvar: 1.2720646858215332, v: 0.14761804044246674, mean_q: -0.6089955568313599, std_q: 2.0901083946228027, lamda: 1.2374966144561768
Running avgs for agent 1: q_loss: 0.23888930678367615, u_loss: 9.251164436340332, p_loss: 0.5721985101699829, mean_rew: -0.2222847410296385, variance: 1.8119502067565918, cvar: 1.2914390563964844, v: -0.028455864638090134, mean_q: -0.6297358274459839, std_q: 1.917102336883545, lamda: 1.255019187927246
Running avgs for agent 2: q_loss: 0.006373874843120575, u_loss: 24.96210479736328, p_loss: 1.6168292760849, mean_rew: -0.35883357419861683, variance: 1.3858882177472114, cvar: -0.3402186930179596, v: -0.34393224120140076, mean_q: -1.7002971172332764, std_q: 3.2025351524353027, lamda: 1.0140548944473267
Running avgs for agent 3: q_loss: 0.010870176367461681, u_loss: 36.264923095703125, p_loss: 1.6899837255477905, mean_rew: -0.36851277004196187, variance: 1.507506549745798, cvar: -0.34953418374061584, v: -0.3611055314540863, mean_q: -1.7780424356460571, std_q: 3.400562047958374, lamda: 1.0163112878799438

steps: 449975, episodes: 18000, mean episode reward: -24.261633323916094, agent episode reward: [-5.235381212026785, -4.970450014423317, -6.895307625860791, -7.160494471605201], time: 67.902
steps: 449975, episodes: 18000, mean episode variance: 1.5748664022237062, agent episode variance: [0.43506517991423604, 0.4437531228363514, 0.3344843885600567, 0.3615637109130621], time: 67.902
steps: 449975, episodes: 18000, mean episode cvar: 0.47053862699866295, agent episode cvar: [0.3310632144510746, 0.31125227504968644, -0.08515765470266343, -0.08661920779943466], time: 67.903
Running avgs for agent 0: q_loss: 0.4379197955131531, u_loss: 15.450006484985352, p_loss: 0.5714792609214783, mean_rew: -0.23712005815910345, variance: 1.7402607196569442, cvar: 1.3242528438568115, v: 0.09341615438461304, mean_q: -0.6420312523841858, std_q: 2.0869810581207275, lamda: 1.2567609548568726
Running avgs for agent 1: q_loss: 0.23915764689445496, u_loss: 9.467375755310059, p_loss: 0.5562120079994202, mean_rew: -0.22337879797446392, variance: 1.775012493133545, cvar: 1.245009183883667, v: 0.003206830471754074, mean_q: -0.6111136674880981, std_q: 1.9095666408538818, lamda: 1.2697317600250244
Running avgs for agent 2: q_loss: 0.009235686622560024, u_loss: 20.77012825012207, p_loss: 1.59389328956604, mean_rew: -0.3548357179378482, variance: 1.3379375542402268, cvar: -0.3406306207180023, v: -0.3454449474811554, mean_q: -1.6732136011123657, std_q: 3.1558499336242676, lamda: 1.0144530534744263
Running avgs for agent 3: q_loss: 0.010319176129996777, u_loss: 30.752981185913086, p_loss: 1.645856499671936, mean_rew: -0.362079297358106, variance: 1.4462548436522484, cvar: -0.34647682309150696, v: -0.3531459867954254, mean_q: -1.73036789894104, std_q: 3.3874599933624268, lamda: 1.0180176496505737

steps: 474975, episodes: 19000, mean episode reward: -25.018991950253906, agent episode reward: [-5.456029067811142, -5.360217166337148, -7.329157216787309, -6.873588499318305], time: 68.498
steps: 474975, episodes: 19000, mean episode variance: 1.4840373431295157, agent episode variance: [0.3976166031956673, 0.40635763184726237, 0.3310765350013971, 0.3489865730851889], time: 68.499
steps: 474975, episodes: 19000, mean episode cvar: 0.46752933087944987, agent episode cvar: [0.32169214463233947, 0.3169300347566605, -0.0851936793923378, -0.08589916911721229], time: 68.499
Running avgs for agent 0: q_loss: 0.4469503164291382, u_loss: 16.277986526489258, p_loss: 0.5249256491661072, mean_rew: -0.23702820896890192, variance: 1.5904662609100342, cvar: 1.2867685556411743, v: 0.19000692665576935, mean_q: -0.598197877407074, std_q: 2.0335092544555664, lamda: 1.277402400970459
Running avgs for agent 1: q_loss: 0.19946372509002686, u_loss: 8.808648109436035, p_loss: 0.5875388979911804, mean_rew: -0.22328367997683052, variance: 1.6254304647445679, cvar: 1.2677202224731445, v: -0.03464733436703682, mean_q: -0.6425431966781616, std_q: 1.9202890396118164, lamda: 1.283014178276062
Running avgs for agent 2: q_loss: 0.010010669939219952, u_loss: 27.038223266601562, p_loss: 1.5816044807434082, mean_rew: -0.3520832345423595, variance: 1.3243061400055884, cvar: -0.3407747149467468, v: -0.3460381031036377, mean_q: -1.657596468925476, std_q: 3.127392530441284, lamda: 1.0154609680175781
Running avgs for agent 3: q_loss: 0.01494219247251749, u_loss: 31.798477172851562, p_loss: 1.6187007427215576, mean_rew: -0.3582405493695019, variance: 1.3959462923407555, cvar: -0.3435966372489929, v: -0.3509323000907898, mean_q: -1.697920560836792, std_q: 3.3242294788360596, lamda: 1.0194400548934937

steps: 499975, episodes: 20000, mean episode reward: -25.98759187301524, agent episode reward: [-6.299511564374529, -5.442497768157792, -7.108640955498488, -7.136941584984432], time: 70.629
steps: 499975, episodes: 20000, mean episode variance: 1.4191371767371892, agent episode variance: [0.3956423966139555, 0.40153749895095825, 0.321850660469383, 0.30010662070289257], time: 70.63
steps: 499975, episodes: 20000, mean episode cvar: 0.46841474866867067, agent episode cvar: [0.31943785107135775, 0.3197044293284416, -0.0852384283542633, -0.08548910337686538], time: 70.63
Running avgs for agent 0: q_loss: 0.4215429127216339, u_loss: 13.678988456726074, p_loss: 0.39222466945648193, mean_rew: -0.23745184938528824, variance: 1.582569586455822, cvar: 1.2777513265609741, v: 0.387665718793869, mean_q: -0.45890888571739197, std_q: 2.014636516571045, lamda: 1.2948793172836304
Running avgs for agent 1: q_loss: 0.23943302035331726, u_loss: 7.876740455627441, p_loss: 0.5849985480308533, mean_rew: -0.22251601617040181, variance: 1.6061499118804932, cvar: 1.2788176536560059, v: -0.044775910675525665, mean_q: -0.6412088871002197, std_q: 1.8986222743988037, lamda: 1.2962132692337036
Running avgs for agent 2: q_loss: 0.007536832243204117, u_loss: 27.56606674194336, p_loss: 1.5623222589492798, mean_rew: -0.3489516670407745, variance: 1.287402641877532, cvar: -0.3409537076950073, v: -0.3456568419933319, mean_q: -1.6338175535202026, std_q: 3.0781216621398926, lamda: 1.016513705253601
Running avgs for agent 3: q_loss: 0.011492560617625713, u_loss: 37.72188186645508, p_loss: 1.5833770036697388, mean_rew: -0.3525015525163705, variance: 1.2004264828115703, cvar: -0.34195640683174133, v: -0.3505532741546631, mean_q: -1.6583110094070435, std_q: 3.231471538543701, lamda: 1.0212721824645996

steps: 524975, episodes: 21000, mean episode reward: -25.439921390816775, agent episode reward: [-5.926543500758469, -5.254951829267818, -7.1485095445121445, -7.109916516278339], time: 77.313
steps: 524975, episodes: 21000, mean episode variance: 1.369615968093276, agent episode variance: [0.3372052418291569, 0.4145668561756611, 0.3221936495900154, 0.29565022049844264], time: 77.314
steps: 524975, episodes: 21000, mean episode cvar: 0.44994316175580024, agent episode cvar: [0.3096333927512169, 0.31136304688453675, -0.08475932332873344, -0.08629395455121994], time: 77.314
Running avgs for agent 0: q_loss: 0.18633145093917847, u_loss: 13.595939636230469, p_loss: 0.2691304683685303, mean_rew: -0.2380023442095603, variance: 1.3488209673166276, cvar: 1.2385337352752686, v: 0.5426994562149048, mean_q: -0.33352160453796387, std_q: 1.9857831001281738, lamda: 1.306801438331604
Running avgs for agent 1: q_loss: 0.2931688129901886, u_loss: 8.515030860900879, p_loss: 0.5215508937835693, mean_rew: -0.21893489380948022, variance: 1.6582674980163574, cvar: 1.2454521656036377, v: 0.0055238427594304085, mean_q: -0.5731863975524902, std_q: 1.8260834217071533, lamda: 1.3136777877807617
Running avgs for agent 2: q_loss: 0.006423945538699627, u_loss: 24.451112747192383, p_loss: 1.5493608713150024, mean_rew: -0.34558448941082487, variance: 1.2887745983600616, cvar: -0.33903729915618896, v: -0.3422492742538452, mean_q: -1.616979718208313, std_q: 3.0467777252197266, lamda: 1.0171594619750977
Running avgs for agent 3: q_loss: 0.011191903613507748, u_loss: 30.993011474609375, p_loss: 1.558067798614502, mean_rew: -0.349356290935842, variance: 1.1826008819937706, cvar: -0.3451758325099945, v: -0.3508174419403076, mean_q: -1.6308521032333374, std_q: 3.1972568035125732, lamda: 1.0232338905334473

steps: 549975, episodes: 22000, mean episode reward: -25.3060860210498, agent episode reward: [-5.971464472003718, -5.463024953686022, -6.826851965000948, -7.044744630359111], time: 69.656
steps: 549975, episodes: 22000, mean episode variance: 1.3496577338948845, agent episode variance: [0.2939982418566942, 0.4167324530482292, 0.34195947351306677, 0.2969675654768944], time: 69.656
steps: 549975, episodes: 22000, mean episode cvar: 0.45144233855605126, agent episode cvar: [0.3105107731223106, 0.312835214138031, -0.08540790569782257, -0.08649574300646781], time: 69.657
Running avgs for agent 0: q_loss: 0.17630495131015778, u_loss: 11.129847526550293, p_loss: 0.14199987053871155, mean_rew: -0.23712490550140883, variance: 1.1759929674267768, cvar: 1.242043137550354, v: 0.6418665647506714, mean_q: -0.19995492696762085, std_q: 1.9348161220550537, lamda: 1.3151987791061401
Running avgs for agent 1: q_loss: 0.21757031977176666, u_loss: 7.036233901977539, p_loss: 0.4738360643386841, mean_rew: -0.2220025644958213, variance: 1.666929841041565, cvar: 1.2513408660888672, v: 0.05880351737141609, mean_q: -0.529649019241333, std_q: 1.8170064687728882, lamda: 1.3276702165603638
Running avgs for agent 2: q_loss: 0.009882097132503986, u_loss: 26.841110229492188, p_loss: 1.5216926336288452, mean_rew: -0.34156248587196386, variance: 1.367837894052267, cvar: -0.34163162112236023, v: -0.34691503643989563, mean_q: -1.5860567092895508, std_q: 2.9510722160339355, lamda: 1.018417477607727
Running avgs for agent 3: q_loss: 0.01419951394200325, u_loss: 31.509620666503906, p_loss: 1.5306257009506226, mean_rew: -0.34627883309159496, variance: 1.1878702619075776, cvar: -0.34598296880722046, v: -0.3555891215801239, mean_q: -1.6014587879180908, std_q: 3.131002902984619, lamda: 1.0250405073165894

steps: 574975, episodes: 23000, mean episode reward: -25.359018105637496, agent episode reward: [-5.305446415423013, -5.6957942298692625, -6.889998639688933, -7.467778820656287], time: 65.664
steps: 574975, episodes: 23000, mean episode variance: 1.2398769326508046, agent episode variance: [0.20335527942329645, 0.39849853083491327, 0.32478004388511184, 0.313243078507483], time: 65.664
steps: 574975, episodes: 23000, mean episode cvar: 0.4515854457914829, agent episode cvar: [0.3103147792816162, 0.31501172584295273, -0.08550295305252076, -0.08823810628056526], time: 65.665
Running avgs for agent 0: q_loss: 0.1342170536518097, u_loss: 11.119986534118652, p_loss: 0.060421425849199295, mean_rew: -0.23767287557216363, variance: 0.8134211176931858, cvar: 1.2412590980529785, v: 0.7397193312644958, mean_q: -0.11368728429079056, std_q: 1.939503788948059, lamda: 1.321707844734192
Running avgs for agent 1: q_loss: 0.1694253832101822, u_loss: 7.085460186004639, p_loss: 0.38531458377838135, mean_rew: -0.22047287711179173, variance: 1.5939942598342896, cvar: 1.2600468397140503, v: 0.2810567021369934, mean_q: -0.44248339533805847, std_q: 1.7944812774658203, lamda: 1.338415503501892
Running avgs for agent 2: q_loss: 0.007662413641810417, u_loss: 15.910128593444824, p_loss: 1.5084432363510132, mean_rew: -0.3391335165270064, variance: 1.2991201877593994, cvar: -0.34201180934906006, v: -0.34614652395248413, mean_q: -1.5715558528900146, std_q: 2.94027042388916, lamda: 1.0196499824523926
Running avgs for agent 3: q_loss: 0.0127869239076972, u_loss: 24.077661514282227, p_loss: 1.495391845703125, mean_rew: -0.34354698212999013, variance: 1.252972314029932, cvar: -0.3529524505138397, v: -0.3609248101711273, mean_q: -1.5653477907180786, std_q: 3.0755953788757324, lamda: 1.0290364027023315

steps: 599975, episodes: 24000, mean episode reward: -24.655702152125865, agent episode reward: [-5.182976667672834, -5.2923395030608225, -7.001498622097063, -7.178887359295145], time: 65.414
steps: 599975, episodes: 24000, mean episode variance: 1.1325382870174945, agent episode variance: [0.14100618855282665, 0.34879175943136215, 0.3298818866312504, 0.3128584524020553], time: 65.415
steps: 599975, episodes: 24000, mean episode cvar: 0.4543233784139156, agent episode cvar: [0.313027264714241, 0.316315096616745, -0.08617741286754609, -0.08884157004952431], time: 65.415
Running avgs for agent 0: q_loss: 0.1216626763343811, u_loss: 8.837939262390137, p_loss: -0.010411585681140423, mean_rew: -0.23494497783846446, variance: 0.5640247542113066, cvar: 1.2521090507507324, v: 0.7896550297737122, mean_q: -0.03822170943021774, std_q: 1.9001091718673706, lamda: 1.327924370765686
Running avgs for agent 1: q_loss: 0.13233233988285065, u_loss: 8.18224048614502, p_loss: 0.32501500844955444, mean_rew: -0.22087474380775465, variance: 1.3951669931411743, cvar: 1.2652604579925537, v: 0.4974750578403473, mean_q: -0.38187190890312195, std_q: 1.7869675159454346, lamda: 1.3466880321502686
Running avgs for agent 2: q_loss: 0.008733652532100677, u_loss: 18.912769317626953, p_loss: 1.4871089458465576, mean_rew: -0.336081742303198, variance: 1.3195276260375977, cvar: -0.3447096645832062, v: -0.35083693265914917, mean_q: -1.547782063484192, std_q: 2.9198596477508545, lamda: 1.021809458732605
Running avgs for agent 3: q_loss: 0.01243035402148962, u_loss: 22.61410903930664, p_loss: 1.4584916830062866, mean_rew: -0.341083819183131, variance: 1.2514338096082211, cvar: -0.3553662598133087, v: -0.3647308051586151, mean_q: -1.5260436534881592, std_q: 3.011359930038452, lamda: 1.0329465866088867

steps: 624975, episodes: 25000, mean episode reward: -24.812870979415916, agent episode reward: [-4.838029473523718, -5.086486730608665, -7.251218529940083, -7.63713624534345], time: 65.441
steps: 624975, episodes: 25000, mean episode variance: 1.046001114293933, agent episode variance: [0.06977872757986188, 0.2889608567021787, 0.3463746433854103, 0.34088688662648203], time: 65.442
steps: 624975, episodes: 25000, mean episode cvar: 0.4569054862558842, agent episode cvar: [0.3123576599359512, 0.31927594769001005, -0.08525420671701431, -0.08947391465306281], time: 65.443
Running avgs for agent 0: q_loss: 0.09912408143281937, u_loss: 9.071788787841797, p_loss: -0.044756341725587845, mean_rew: -0.23490411456849913, variance: 0.2791149103194475, cvar: 1.2494306564331055, v: 0.8293284773826599, mean_q: -0.0023082070983946323, std_q: 1.8715343475341797, lamda: 1.3331130743026733
Running avgs for agent 1: q_loss: 0.12943223118782043, u_loss: 6.416157245635986, p_loss: 0.2741181552410126, mean_rew: -0.22075386964196003, variance: 1.1558433771133423, cvar: 1.2771037817001343, v: 0.6499156951904297, mean_q: -0.3299856185913086, std_q: 1.8030747175216675, lamda: 1.3539555072784424
Running avgs for agent 2: q_loss: 0.006025020498782396, u_loss: 23.783761978149414, p_loss: 1.4794809818267822, mean_rew: -0.336742737313235, variance: 1.3854985735416412, cvar: -0.34101682901382446, v: -0.3441549241542816, mean_q: -1.539405345916748, std_q: 2.8801069259643555, lamda: 1.023032546043396
Running avgs for agent 3: q_loss: 0.018149584531784058, u_loss: 32.5805778503418, p_loss: 1.432217001914978, mean_rew: -0.34052312939047114, variance: 1.3635475465059281, cvar: -0.3578956723213196, v: -0.3703886866569519, mean_q: -1.497538685798645, std_q: 2.9606244564056396, lamda: 1.039278268814087

steps: 649975, episodes: 26000, mean episode reward: -25.101209186977226, agent episode reward: [-4.748385946332384, -5.02808633344972, -7.1699064597433635, -8.154830447451758], time: 65.844
steps: 649975, episodes: 26000, mean episode variance: 1.00490194324404, agent episode variance: [0.03627525646239519, 0.22751170502603055, 0.3732232542037964, 0.36789172755181787], time: 65.844
steps: 649975, episodes: 26000, mean episode cvar: 0.45887457591295244, agent episode cvar: [0.31791082572937013, 0.3165263961553574, -0.08576204046607018, -0.08980060550570489], time: 65.845
Running avgs for agent 0: q_loss: 0.11089983582496643, u_loss: 7.5907721519470215, p_loss: -0.04335956647992134, mean_rew: -0.2316848903309144, variance: 0.14510102584958076, cvar: 1.2716432809829712, v: 0.8210877776145935, mean_q: -0.004610996227711439, std_q: 1.8538841009140015, lamda: 1.3378846645355225
Running avgs for agent 1: q_loss: 0.10419898480176926, u_loss: 5.616085052490234, p_loss: 0.20305627584457397, mean_rew: -0.2198137974112708, variance: 0.9100468158721924, cvar: 1.2661057710647583, v: 0.7745036482810974, mean_q: -0.2554561495780945, std_q: 1.7653759717941284, lamda: 1.3605408668518066
Running avgs for agent 2: q_loss: 0.007494314573705196, u_loss: 21.1755313873291, p_loss: 1.4564566612243652, mean_rew: -0.33255586997900544, variance: 1.492893099784851, cvar: -0.3430481553077698, v: -0.35066840052604675, mean_q: -1.5135483741760254, std_q: 2.844738245010376, lamda: 1.0246907472610474
Running avgs for agent 3: q_loss: 0.011937569826841354, u_loss: 27.336580276489258, p_loss: 1.3924050331115723, mean_rew: -0.33801281950693607, variance: 1.4715669102072715, cvar: -0.35920241475105286, v: -0.368608295917511, mean_q: -1.45601224899292, std_q: 2.8841495513916016, lamda: 1.0429272651672363

steps: 674975, episodes: 27000, mean episode reward: -24.35626229971506, agent episode reward: [-4.611281546749406, -4.670931441784778, -7.190987746022678, -7.883061565158194], time: 65.358
steps: 674975, episodes: 27000, mean episode variance: 0.9822768204603344, agent episode variance: [0.028756665378808974, 0.18323751099593938, 0.3795959339663386, 0.39068671011924744], time: 65.359
steps: 674975, episodes: 27000, mean episode cvar: 0.45746783301234245, agent episode cvar: [0.3150699155330658, 0.31928863978385924, -0.08595600169897079, -0.09093472060561181], time: 65.359
Running avgs for agent 0: q_loss: 0.10132630169391632, u_loss: 9.514469146728516, p_loss: -0.06366729736328125, mean_rew: -0.2304806400241112, variance: 0.1150266615152359, cvar: 1.260279655456543, v: 0.8077276349067688, mean_q: 0.01561825629323721, std_q: 1.8580844402313232, lamda: 1.3438291549682617
Running avgs for agent 1: q_loss: 0.09762021899223328, u_loss: 5.864229202270508, p_loss: 0.16557051241397858, mean_rew: -0.217920154982545, variance: 0.7329500439837575, cvar: 1.277154564857483, v: 0.8254120945930481, mean_q: -0.21699900925159454, std_q: 1.7703731060028076, lamda: 1.365903615951538
Running avgs for agent 2: q_loss: 0.0083179185166955, u_loss: 20.99666404724121, p_loss: 1.438245415687561, mean_rew: -0.3305644301844252, variance: 1.5183837413787842, cvar: -0.34382399916648865, v: -0.35028642416000366, mean_q: -1.4944573640823364, std_q: 2.8390302658081055, lamda: 1.0271157026290894
Running avgs for agent 3: q_loss: 0.013801920227706432, u_loss: 29.85284996032715, p_loss: 1.3679934740066528, mean_rew: -0.3365853602473635, variance: 1.5627468404769898, cvar: -0.36373889446258545, v: -0.37390148639678955, mean_q: -1.4276816844940186, std_q: 2.843735694885254, lamda: 1.0476127862930298

steps: 699975, episodes: 28000, mean episode reward: -24.7637530380511, agent episode reward: [-4.339586315098095, -5.104426665562356, -7.243006879873108, -8.076733177517537], time: 65.434
steps: 699975, episodes: 28000, mean episode variance: 0.9861423782557249, agent episode variance: [0.017608377743512392, 0.14623003717884422, 0.3637136158645153, 0.458590347468853], time: 65.434
steps: 699975, episodes: 28000, mean episode cvar: 0.4589220555126667, agent episode cvar: [0.3183110179901123, 0.3197961131334305, -0.08665190634131431, -0.09253316926956177], time: 65.435
Running avgs for agent 0: q_loss: 0.08511108160018921, u_loss: 7.68941068649292, p_loss: -0.061280541121959686, mean_rew: -0.22904790350445103, variance: 0.07043351097404957, cvar: 1.273244023323059, v: 0.8103553056716919, mean_q: 0.012049142271280289, std_q: 1.8290700912475586, lamda: 1.3492425680160522
Running avgs for agent 1: q_loss: 0.08338966965675354, u_loss: 5.477745056152344, p_loss: 0.14440713822841644, mean_rew: -0.2173478160642132, variance: 0.5849201487153769, cvar: 1.2791844606399536, v: 0.8673968315124512, mean_q: -0.19423916935920715, std_q: 1.7655922174453735, lamda: 1.3714016675949097
Running avgs for agent 2: q_loss: 0.009076299145817757, u_loss: 23.63371467590332, p_loss: 1.409066081047058, mean_rew: -0.3276584803177078, variance: 1.4548544883728027, cvar: -0.34660759568214417, v: -0.354379266500473, mean_q: -1.462795615196228, std_q: 2.761678695678711, lamda: 1.0302788019180298
Running avgs for agent 3: q_loss: 0.014799459837377071, u_loss: 20.840988159179688, p_loss: 1.3475362062454224, mean_rew: -0.3378522589834546, variance: 1.834361389875412, cvar: -0.3701326549053192, v: -0.3770252764225006, mean_q: -1.404250144958496, std_q: 2.8125953674316406, lamda: 1.0529913902282715

steps: 724975, episodes: 29000, mean episode reward: -24.442304137542443, agent episode reward: [-4.158681349645108, -4.508143418771111, -7.535403012467665, -8.240076356658559], time: 65.617
steps: 724975, episodes: 29000, mean episode variance: 0.966897629937157, agent episode variance: [0.008540411841124296, 0.11082035667262971, 0.3437818194180727, 0.5037550420053303], time: 65.618
steps: 724975, episodes: 29000, mean episode cvar: 0.4606438579559326, agent episode cvar: [0.3192166599035263, 0.32188772773742674, -0.08623757475614548, -0.09422295492887497], time: 65.618
Running avgs for agent 0: q_loss: 0.09245438873767853, u_loss: 6.825375556945801, p_loss: -0.04660521820187569, mean_rew: -0.22766762500596396, variance: 0.034161647364497186, cvar: 1.2768666744232178, v: 0.7919346690177917, mean_q: -0.003861113451421261, std_q: 1.8458281755447388, lamda: 1.3550634384155273
Running avgs for agent 1: q_loss: 0.08706018328666687, u_loss: 6.4073710441589355, p_loss: 0.1554635912179947, mean_rew: -0.2178700253402252, variance: 0.44328142669051884, cvar: 1.2875508069992065, v: 0.8622381091117859, mean_q: -0.204313263297081, std_q: 1.7704073190689087, lamda: 1.3761221170425415
Running avgs for agent 2: q_loss: 0.008553449995815754, u_loss: 15.555058479309082, p_loss: 1.40030837059021, mean_rew: -0.3276597302516901, variance: 1.3751273155212402, cvar: -0.34495028853416443, v: -0.3514694273471832, mean_q: -1.4546207189559937, std_q: 2.7610106468200684, lamda: 1.0334010124206543
Running avgs for agent 3: q_loss: 0.01952509395778179, u_loss: 29.192115783691406, p_loss: 1.3261271715164185, mean_rew: -0.33803397083784203, variance: 2.015020168021321, cvar: -0.3768918216228485, v: -0.38818684220314026, mean_q: -1.3810725212097168, std_q: 2.757554769515991, lamda: 1.0591027736663818

steps: 749975, episodes: 30000, mean episode reward: -24.18880088745478, agent episode reward: [-3.8470595380165795, -4.323234365834036, -7.989267316465872, -8.029239667138292], time: 65.514
steps: 749975, episodes: 30000, mean episode variance: 0.9427292095720768, agent episode variance: [0.010520766660571099, 0.10104822261631488, 0.3057177175283432, 0.5254425027668476], time: 65.515
steps: 749975, episodes: 30000, mean episode cvar: 0.4633618941307068, agent episode cvar: [0.32339022409915924, 0.32231408762931824, -0.08689504393935203, -0.09544737365841865], time: 65.515
Running avgs for agent 0: q_loss: 0.10369688272476196, u_loss: 7.862843036651611, p_loss: -0.0249915923923254, mean_rew: -0.22516904754875497, variance: 0.042083066642284395, cvar: 1.2935608625411987, v: 0.7657735347747803, mean_q: -0.024147266522049904, std_q: 1.8445274829864502, lamda: 1.3613938093185425
Running avgs for agent 1: q_loss: 0.08014903217554092, u_loss: 4.90208625793457, p_loss: 0.1682840883731842, mean_rew: -0.21617151459195372, variance: 0.4041928904652595, cvar: 1.2892563343048096, v: 0.8346709609031677, mean_q: -0.21376478672027588, std_q: 1.7713581323623657, lamda: 1.3808069229125977
Running avgs for agent 2: q_loss: 0.010100588202476501, u_loss: 16.496639251708984, p_loss: 1.3903297185897827, mean_rew: -0.32790644510533035, variance: 1.2228708701133728, cvar: -0.3475801646709442, v: -0.3546624779701233, mean_q: -1.4433043003082275, std_q: 2.748878240585327, lamda: 1.0373454093933105
Running avgs for agent 3: q_loss: 0.02321181446313858, u_loss: 27.771127700805664, p_loss: 1.2743842601776123, mean_rew: -0.33617230883506033, variance: 2.1017699241638184, cvar: -0.3817894756793976, v: -0.39693397283554077, mean_q: -1.3291752338409424, std_q: 2.6885697841644287, lamda: 1.0642077922821045

steps: 774975, episodes: 31000, mean episode reward: -24.57103134664572, agent episode reward: [-3.6514867304782452, -3.792063858029548, -8.406952233290644, -8.720528524847285], time: 65.606
steps: 774975, episodes: 31000, mean episode variance: 0.9479783729426563, agent episode variance: [0.011992204084992408, 0.09666857996210455, 0.3113052803874016, 0.5280123085081577], time: 65.606
steps: 774975, episodes: 31000, mean episode cvar: 0.4556071170270443, agent episode cvar: [0.32052693915367125, 0.31951109421253204, -0.08716457670927048, -0.09726633962988854], time: 65.606
Running avgs for agent 0: q_loss: 0.11146875470876694, u_loss: 8.40516185760498, p_loss: 0.0033864928409457207, mean_rew: -0.22213993896441608, variance: 0.04796881633996963, cvar: 1.2821078300476074, v: 0.7237460613250732, mean_q: -0.05060945078730583, std_q: 1.8252177238464355, lamda: 1.3718969821929932
Running avgs for agent 1: q_loss: 0.08841610699892044, u_loss: 4.381811618804932, p_loss: 0.1514039933681488, mean_rew: -0.213321346384337, variance: 0.3866743198484182, cvar: 1.2780444622039795, v: 0.8183603882789612, mean_q: -0.1934559941291809, std_q: 1.7355523109436035, lamda: 1.3866640329360962
Running avgs for agent 2: q_loss: 0.007839363999664783, u_loss: 13.865744590759277, p_loss: 1.371695637702942, mean_rew: -0.32701825108083266, variance: 1.2452211215496063, cvar: -0.34865832328796387, v: -0.355638712644577, mean_q: -1.425714373588562, std_q: 2.71097731590271, lamda: 1.0425291061401367
Running avgs for agent 3: q_loss: 0.020477866753935814, u_loss: 25.877180099487305, p_loss: 1.2311732769012451, mean_rew: -0.3362358402103873, variance: 2.1120493412017822, cvar: -0.38906535506248474, v: -0.4009886682033539, mean_q: -1.2870694398880005, std_q: 2.6250553131103516, lamda: 1.0696580410003662

steps: 799975, episodes: 32000, mean episode reward: -23.768818605234788, agent episode reward: [-3.3508509523385293, -3.503636356084805, -8.544103053086856, -8.370228243724597], time: 65.417
steps: 799975, episodes: 32000, mean episode variance: 0.898740812106058, agent episode variance: [0.0053771269954741, 0.10060511637665331, 0.30638403257727626, 0.4863745361566544], time: 65.418
steps: 799975, episodes: 32000, mean episode cvar: 0.45542120817303655, agent episode cvar: [0.32136344981193543, 0.3200733515024185, -0.08737000378966331, -0.09864558935165406], time: 65.418
Running avgs for agent 0: q_loss: 0.08679495751857758, u_loss: 7.114781379699707, p_loss: 0.0038294291589409113, mean_rew: -0.21866098765312847, variance: 0.0215085079818964, cvar: 1.2854539155960083, v: 0.7200475931167603, mean_q: -0.05199677124619484, std_q: 1.7973954677581787, lamda: 1.3795673847198486
Running avgs for agent 1: q_loss: 0.08383489400148392, u_loss: 5.218616962432861, p_loss: 0.12118499726057053, mean_rew: -0.21142911712193568, variance: 0.40242046550661326, cvar: 1.2802934646606445, v: 0.7874297499656677, mean_q: -0.16195198893547058, std_q: 1.7433953285217285, lamda: 1.3936492204666138
Running avgs for agent 2: q_loss: 0.009421166963875294, u_loss: 13.29633903503418, p_loss: 1.351035237312317, mean_rew: -0.32846126414144294, variance: 1.225536130309105, cvar: -0.349480003118515, v: -0.3571058213710785, mean_q: -1.4058784246444702, std_q: 2.6381497383117676, lamda: 1.0476325750350952
Running avgs for agent 3: q_loss: 0.02404208853840828, u_loss: 30.13623809814453, p_loss: 1.1910821199417114, mean_rew: -0.33646773306866046, variance: 1.945497989654541, cvar: -0.3945823609828949, v: -0.4032377302646637, mean_q: -1.2462265491485596, std_q: 2.571488857269287, lamda: 1.0751490592956543

steps: 824975, episodes: 33000, mean episode reward: -24.521411208454836, agent episode reward: [-3.7306277729469803, -4.324642365669657, -7.5126919908239325, -8.953449079014264], time: 65.343
steps: 824975, episodes: 33000, mean episode variance: 0.8797049202434719, agent episode variance: [0.01904750832542777, 0.0767052486911416, 0.3225780568793416, 0.4613741063475609], time: 65.344
steps: 824975, episodes: 33000, mean episode cvar: 0.457645437836647, agent episode cvar: [0.3241297074556351, 0.31806433606147766, -0.08579048535227776, -0.09875812032818794], time: 65.344
Running avgs for agent 0: q_loss: 0.11721020936965942, u_loss: 6.4892706871032715, p_loss: 0.02750994823873043, mean_rew: -0.21739280746092737, variance: 0.07619003330171108, cvar: 1.2965188026428223, v: 0.7072863578796387, mean_q: -0.07459664344787598, std_q: 1.8152339458465576, lamda: 1.388593316078186
Running avgs for agent 1: q_loss: 0.0775996670126915, u_loss: 3.6978583335876465, p_loss: 0.06916111707687378, mean_rew: -0.2090600044011587, variance: 0.3068209947645664, cvar: 1.2722573280334473, v: 0.7555495500564575, mean_q: -0.10884521156549454, std_q: 1.707677960395813, lamda: 1.4003422260284424
Running avgs for agent 2: q_loss: 0.010174636729061604, u_loss: 18.817319869995117, p_loss: 1.3411067724227905, mean_rew: -0.32748573424629523, variance: 1.2903122275173664, cvar: -0.343161940574646, v: -0.3500021994113922, mean_q: -1.3948932886123657, std_q: 2.613583564758301, lamda: 1.051881194114685
Running avgs for agent 3: q_loss: 0.022955648601055145, u_loss: 22.927448272705078, p_loss: 1.169073462486267, mean_rew: -0.3371838650363453, variance: 1.8454962968826294, cvar: -0.39503246545791626, v: -0.4064996838569641, mean_q: -1.2231303453445435, std_q: 2.5513250827789307, lamda: 1.0809326171875

steps: 849975, episodes: 34000, mean episode reward: -25.422440945701513, agent episode reward: [-4.319815601071854, -4.856838620796883, -6.895017064648268, -9.35076965918451], time: 65.549
steps: 849975, episodes: 34000, mean episode variance: 0.8550495938062668, agent episode variance: [0.013324696220457554, 0.06071955806016922, 0.3325800618603826, 0.44842527766525747], time: 65.55
steps: 849975, episodes: 34000, mean episode cvar: 0.4554887254834175, agent episode cvar: [0.32308177864551546, 0.31735441637039186, -0.08630638322234153, -0.09864108631014824], time: 65.551
Running avgs for agent 0: q_loss: 0.09860599786043167, u_loss: 6.6063995361328125, p_loss: 0.023094715550541878, mean_rew: -0.21526321572370033, variance: 0.053298784881830215, cvar: 1.2923271656036377, v: 0.7188824415206909, mean_q: -0.07006515562534332, std_q: 1.7981773614883423, lamda: 1.3991645574569702
Running avgs for agent 1: q_loss: 0.08487029373645782, u_loss: 6.5272908210754395, p_loss: 0.019233113154768944, mean_rew: -0.20974182227803395, variance: 0.24287823224067687, cvar: 1.2694177627563477, v: 0.7139188051223755, mean_q: -0.05775543302297592, std_q: 1.7204594612121582, lamda: 1.4070574045181274
Running avgs for agent 2: q_loss: 0.014040209352970123, u_loss: 12.49970531463623, p_loss: 1.3277133703231812, mean_rew: -0.32566543877500004, variance: 1.3303203582763672, cvar: -0.3452255129814148, v: -0.3539655804634094, mean_q: -1.377170205116272, std_q: 2.5550146102905273, lamda: 1.055923342704773
Running avgs for agent 3: q_loss: 0.02225019782781601, u_loss: 21.354570388793945, p_loss: 1.148101806640625, mean_rew: -0.3375867598549355, variance: 1.7937010526657104, cvar: -0.39456433057785034, v: -0.4067226052284241, mean_q: -1.201075792312622, std_q: 2.4594671726226807, lamda: 1.0840476751327515

steps: 874975, episodes: 35000, mean episode reward: -24.938963282198372, agent episode reward: [-3.7848081735779195, -4.035666519876165, -7.376444913821919, -9.74204367492237], time: 65.414
steps: 874975, episodes: 35000, mean episode variance: 0.8128357777185738, agent episode variance: [0.006613209642469883, 0.03452679366990924, 0.3164670607149601, 0.4552287136912346], time: 65.415
steps: 874975, episodes: 35000, mean episode cvar: 0.45734651070833204, agent episode cvar: [0.32567346942424774, 0.3188598895072937, -0.08789892959594726, -0.09928791862726212], time: 65.415
Running avgs for agent 0: q_loss: 0.11208900809288025, u_loss: 6.06827449798584, p_loss: 0.048227664083242416, mean_rew: -0.2139550952064057, variance: 0.026452838569879532, cvar: 1.3026938438415527, v: 0.6871721148490906, mean_q: -0.09465086460113525, std_q: 1.7872055768966675, lamda: 1.409056544303894
Running avgs for agent 1: q_loss: 0.09983790665864944, u_loss: 4.37205171585083, p_loss: -0.057532913982868195, mean_rew: -0.20803099265404132, variance: 0.13810717467963696, cvar: 1.2754395008087158, v: 0.7267985939979553, mean_q: 0.017461583018302917, std_q: 1.672692894935608, lamda: 1.4168052673339844
Running avgs for agent 2: q_loss: 0.009651970118284225, u_loss: 16.756378173828125, p_loss: 1.324267864227295, mean_rew: -0.3271402911654607, variance: 1.2658682428598405, cvar: -0.3515956997871399, v: -0.3582704961299896, mean_q: -1.3746742010116577, std_q: 2.5720746517181396, lamda: 1.0586374998092651
Running avgs for agent 3: q_loss: 0.026008365675807, u_loss: 25.885116577148438, p_loss: 1.1493796110153198, mean_rew: -0.34047917325390886, variance: 1.8209148645401, cvar: -0.3971516788005829, v: -0.410642147064209, mean_q: -1.2027865648269653, std_q: 2.465167999267578, lamda: 1.0870046615600586

steps: 899975, episodes: 36000, mean episode reward: -25.135979038863624, agent episode reward: [-3.1133440839102158, -3.712687347308372, -8.319713195336316, -9.990234412308725], time: 65.343
steps: 899975, episodes: 36000, mean episode variance: 0.8396165074035525, agent episode variance: [0.0036248034685850143, 0.018671316035091878, 0.3740827273726463, 0.4432376605272293], time: 65.343
steps: 899975, episodes: 36000, mean episode cvar: 0.45831904605031015, agent episode cvar: [0.32225697207450865, 0.32653028881549834, -0.09026808366179466, -0.10020013117790222], time: 65.344
Running avgs for agent 0: q_loss: 0.09740105271339417, u_loss: 6.4565863609313965, p_loss: 0.046163298189640045, mean_rew: -0.21287786147960508, variance: 0.014499213874340057, cvar: 1.2890278100967407, v: 0.6862817406654358, mean_q: -0.09167595207691193, std_q: 1.7907030582427979, lamda: 1.4204879999160767
Running avgs for agent 1: q_loss: 0.09794274717569351, u_loss: 3.597705125808716, p_loss: 0.001506818807683885, mean_rew: -0.20673979267411038, variance: 0.07468526414036751, cvar: 1.3061212301254272, v: 0.6900083422660828, mean_q: -0.04501869156956673, std_q: 1.681366205215454, lamda: 1.4282890558242798
Running avgs for agent 2: q_loss: 0.011106208898127079, u_loss: 12.126983642578125, p_loss: 1.2901678085327148, mean_rew: -0.3252997388113651, variance: 1.496330976486206, cvar: -0.3610723614692688, v: -0.37024152278900146, mean_q: -1.3386735916137695, std_q: 2.5243890285491943, lamda: 1.0621916055679321
Running avgs for agent 3: q_loss: 0.019325824454426765, u_loss: 20.99353790283203, p_loss: 1.135719656944275, mean_rew: -0.3410656500971058, variance: 1.7729506492614746, cvar: -0.40080052614212036, v: -0.4086846113204956, mean_q: -1.1888196468353271, std_q: 2.4419257640838623, lamda: 1.0905324220657349

steps: 924975, episodes: 37000, mean episode reward: -25.00126475930206, agent episode reward: [-3.6663570718105984, -3.7093241881475287, -7.788440001186349, -9.837143498157584], time: 65.558
steps: 924975, episodes: 37000, mean episode variance: 0.8598211739361287, agent episode variance: [0.0073680316656827925, 0.015093258306384087, 0.40313835090398786, 0.4342215330600738], time: 65.559
steps: 924975, episodes: 37000, mean episode cvar: 0.45788000637292864, agent episode cvar: [0.3223726994991302, 0.3267556540966034, -0.0918951899409294, -0.0993531572818756], time: 65.559
Running avgs for agent 0: q_loss: 0.09465628117322922, u_loss: 5.6416425704956055, p_loss: 0.027895577251911163, mean_rew: -0.21031554317977735, variance: 0.02947212666273117, cvar: 1.2894906997680664, v: 0.6904805898666382, mean_q: -0.07201923429965973, std_q: 1.756473422050476, lamda: 1.4311069250106812
Running avgs for agent 1: q_loss: 0.10972956568002701, u_loss: 4.777517795562744, p_loss: 0.07767225056886673, mean_rew: -0.2045229555109563, variance: 0.060373033225536346, cvar: 1.3070225715637207, v: 0.5720484852790833, mean_q: -0.11841952800750732, std_q: 1.684713363647461, lamda: 1.4431438446044922
Running avgs for agent 2: q_loss: 0.014682278968393803, u_loss: 9.095788955688477, p_loss: 1.2620983123779297, mean_rew: -0.32665026636259176, variance: 1.6125534772872925, cvar: -0.36758074164390564, v: -0.37621840834617615, mean_q: -1.3107695579528809, std_q: 2.509486436843872, lamda: 1.0683046579360962
Running avgs for agent 3: q_loss: 0.01811620220541954, u_loss: 18.442167282104492, p_loss: 1.1257282495498657, mean_rew: -0.3422861890945689, variance: 1.7368860244750977, cvar: -0.39741262793540955, v: -0.4082302153110504, mean_q: -1.1782026290893555, std_q: 2.3606908321380615, lamda: 1.0930026769638062

steps: 949975, episodes: 38000, mean episode reward: -25.74217243602748, agent episode reward: [-3.4579444177556717, -3.9138630414122004, -8.3408488358795, -10.02951614098011], time: 65.406
steps: 949975, episodes: 38000, mean episode variance: 0.943762580435723, agent episode variance: [0.009342627223581076, 0.016473057970404624, 0.4214448760151863, 0.4965020192265511], time: 65.406
steps: 949975, episodes: 38000, mean episode cvar: 0.46081471505761146, agent episode cvar: [0.3254904763698578, 0.32695440006256105, -0.09148868972063065, -0.10014147165417671], time: 65.407
Running avgs for agent 0: q_loss: 0.10542166233062744, u_loss: 6.721951484680176, p_loss: 0.054377906024456024, mean_rew: -0.20872210028767307, variance: 0.037370508894324306, cvar: 1.301961898803711, v: 0.7024345993995667, mean_q: -0.09810511022806168, std_q: 1.77359938621521, lamda: 1.4417314529418945
Running avgs for agent 1: q_loss: 0.12041663378477097, u_loss: 3.2770168781280518, p_loss: 0.0935150682926178, mean_rew: -0.20383239174128517, variance: 0.0658922318816185, cvar: 1.3078174591064453, v: 0.5564848780632019, mean_q: -0.1328119933605194, std_q: 1.6321732997894287, lamda: 1.4565705060958862
Running avgs for agent 2: q_loss: 0.017343910411000252, u_loss: 13.724406242370605, p_loss: 1.217595100402832, mean_rew: -0.3248034499028394, variance: 1.6857795040607453, cvar: -0.36595478653907776, v: -0.37511390447616577, mean_q: -1.2695684432983398, std_q: 2.4541733264923096, lamda: 1.0740429162979126
Running avgs for agent 3: q_loss: 0.020291991531848907, u_loss: 21.71792221069336, p_loss: 1.1273776292800903, mean_rew: -0.3429293792180244, variance: 1.9860080480575562, cvar: -0.40056589245796204, v: -0.40895506739616394, mean_q: -1.1792343854904175, std_q: 2.3984482288360596, lamda: 1.0951743125915527

steps: 974975, episodes: 39000, mean episode reward: -27.124123083676576, agent episode reward: [-3.681316424821569, -3.7290690293320243, -9.674598343356031, -10.039139286166952], time: 65.995
steps: 974975, episodes: 39000, mean episode variance: 0.9403888054341077, agent episode variance: [0.005203283078968525, 0.020940699242055417, 0.40520620776712896, 0.5090386153459548], time: 65.996
steps: 974975, episodes: 39000, mean episode cvar: 0.4607585090994835, agent episode cvar: [0.32413604569435117, 0.32864336717128756, -0.09150638663768769, -0.10051451712846755], time: 65.996
Running avgs for agent 0: q_loss: 0.08854998648166656, u_loss: 5.912356853485107, p_loss: 0.05942320078611374, mean_rew: -0.20775828236257557, variance: 0.0208131323158741, cvar: 1.2965441942214966, v: 0.7089541554450989, mean_q: -0.10455868393182755, std_q: 1.7517204284667969, lamda: 1.4509469270706177
Running avgs for agent 1: q_loss: 0.1412169486284256, u_loss: 3.9781930446624756, p_loss: 0.18245795369148254, mean_rew: -0.2035368855753365, variance: 0.08376279696822167, cvar: 1.3145735263824463, v: 0.4658137857913971, mean_q: -0.22257351875305176, std_q: 1.669973373413086, lamda: 1.472294569015503
Running avgs for agent 2: q_loss: 0.016050415113568306, u_loss: 10.67593765258789, p_loss: 1.1971275806427002, mean_rew: -0.32666393795879883, variance: 1.620824933052063, cvar: -0.36602556705474854, v: -0.37698039412498474, mean_q: -1.2529304027557373, std_q: 2.429530620574951, lamda: 1.0766772031784058
Running avgs for agent 3: q_loss: 0.030970828607678413, u_loss: 21.138120651245117, p_loss: 1.1339558362960815, mean_rew: -0.34599284644125816, variance: 2.0361545085906982, cvar: -0.40205803513526917, v: -0.4193316102027893, mean_q: -1.1841626167297363, std_q: 2.356896162033081, lamda: 1.1001728773117065

steps: 999975, episodes: 40000, mean episode reward: -27.189171531736932, agent episode reward: [-3.442599754699318, -3.858449386001856, -9.99354736622399, -9.894575024811761], time: 65.375
steps: 999975, episodes: 40000, mean episode variance: 0.9458983145663514, agent episode variance: [0.0036573130749166013, 0.01781395382154733, 0.4033901882171631, 0.5210368594527245], time: 65.375
steps: 999975, episodes: 40000, mean episode cvar: 0.456473444879055, agent episode cvar: [0.3224319373369217, 0.32875334823131563, -0.0933746834397316, -0.10133715724945068], time: 65.376
Running avgs for agent 0: q_loss: 0.09091546386480331, u_loss: 7.467623233795166, p_loss: 0.033194251358509064, mean_rew: -0.20461554693950848, variance: 0.014629252299666405, cvar: 1.289727807044983, v: 0.6935433745384216, mean_q: -0.07833974063396454, std_q: 1.7503010034561157, lamda: 1.4598900079727173
Running avgs for agent 1: q_loss: 0.12936019897460938, u_loss: 4.326757431030273, p_loss: 0.2594163417816162, mean_rew: -0.20187516446618942, variance: 0.07125581528618932, cvar: 1.3150134086608887, v: 0.3749487102031708, mean_q: -0.3000940680503845, std_q: 1.6558281183242798, lamda: 1.4902644157409668
Running avgs for agent 2: q_loss: 0.01759203150868416, u_loss: 11.088316917419434, p_loss: 1.173203945159912, mean_rew: -0.3278209551723738, variance: 1.6135607957839966, cvar: -0.37349870800971985, v: -0.3830188512802124, mean_q: -1.2256745100021362, std_q: 2.3189163208007812, lamda: 1.08184814453125
Running avgs for agent 3: q_loss: 0.02119479514658451, u_loss: 15.441109657287598, p_loss: 1.1214345693588257, mean_rew: -0.34639123002306327, variance: 2.0841474533081055, cvar: -0.40534862875938416, v: -0.4181808531284332, mean_q: -1.169752597808838, std_q: 2.3314990997314453, lamda: 1.1042604446411133

steps: 1024975, episodes: 41000, mean episode reward: -25.773421230364892, agent episode reward: [-3.516369753725017, -3.5497284875575277, -9.054855431387365, -9.652467557694981], time: 65.691
steps: 1024975, episodes: 41000, mean episode variance: 0.9189103275686502, agent episode variance: [0.0031260970905423163, 0.02393017626553774, 0.38951279735565186, 0.5023412568569183], time: 65.692
steps: 1024975, episodes: 41000, mean episode cvar: 0.4484666509926319, agent episode cvar: [0.31982208824157715, 0.3237017894983292, -0.09378641182184219, -0.1012708149254322], time: 65.692
Running avgs for agent 0: q_loss: 0.08519796282052994, u_loss: 7.349343299865723, p_loss: -0.03067205101251602, mean_rew: -0.2023110820076881, variance: 0.012504388362169265, cvar: 1.2792882919311523, v: 0.7274824380874634, mean_q: -0.00900272186845541, std_q: 1.6860288381576538, lamda: 1.4694774150848389
Running avgs for agent 1: q_loss: 0.10788952559232712, u_loss: 3.9510302543640137, p_loss: 0.20307523012161255, mean_rew: -0.1973464600753948, variance: 0.09572070506215095, cvar: 1.2948070764541626, v: 0.3685360252857208, mean_q: -0.23155748844146729, std_q: 1.5559126138687134, lamda: 1.50411856174469
Running avgs for agent 2: q_loss: 0.021065590903162956, u_loss: 11.315545082092285, p_loss: 1.1195040941238403, mean_rew: -0.3200455104520597, variance: 1.5580509901046753, cvar: -0.37514564394950867, v: -0.39128637313842773, mean_q: -1.1614573001861572, std_q: 2.2316927909851074, lamda: 1.085710883140564
Running avgs for agent 3: q_loss: 0.022169994190335274, u_loss: 20.43873405456543, p_loss: 1.0796196460723877, mean_rew: -0.3396179511547296, variance: 2.0093650817871094, cvar: -0.4050832688808441, v: -0.4215574264526367, mean_q: -1.1174372434616089, std_q: 2.216482639312744, lamda: 1.1044120788574219

steps: 1049975, episodes: 42000, mean episode reward: -25.433309939622987, agent episode reward: [-3.4272645762671043, -3.8491172347731553, -8.97623148538757, -9.180696643195159], time: 66.697
steps: 1049975, episodes: 42000, mean episode variance: 0.7967670587012544, agent episode variance: [7.456138730049133e-05, 0.003721373082138598, 0.351454667031765, 0.44151645720005034], time: 66.698
steps: 1049975, episodes: 42000, mean episode cvar: 0.4374745972752571, agent episode cvar: [0.3161852649450302, 0.32040304780006407, -0.09702423393726349, -0.1020894815325737], time: 66.698
Running avgs for agent 0: q_loss: 0.06305840611457825, u_loss: 2.028780937194824, p_loss: -0.18949788808822632, mean_rew: -0.19462024497541505, variance: 0.0002982455492019653, cvar: 1.2647409439086914, v: 0.8173409700393677, mean_q: 0.16485433280467987, std_q: 1.405390739440918, lamda: 1.4764958620071411
Running avgs for agent 1: q_loss: 0.08337205648422241, u_loss: 1.7637145519256592, p_loss: 0.069767065346241, mean_rew: -0.19186437945869245, variance: 0.014885492328554392, cvar: 1.2816121578216553, v: 0.47105804085731506, mean_q: -0.08739938586950302, std_q: 1.3846675157546997, lamda: 1.5134196281433105
Running avgs for agent 2: q_loss: 0.018512630835175514, u_loss: 1.776743769645691, p_loss: 0.9965707063674927, mean_rew: -0.3020086789395624, variance: 1.4058187007904053, cvar: -0.3880969285964966, v: -0.39856889843940735, mean_q: -1.0166503190994263, std_q: 1.7371089458465576, lamda: 1.0913223028182983
Running avgs for agent 3: q_loss: 0.019428551197052002, u_loss: 3.9474940299987793, p_loss: 0.9694472551345825, mean_rew: -0.32054066908840556, variance: 1.7660659551620483, cvar: -0.4083579480648041, v: -0.4167371988296509, mean_q: -0.9862831234931946, std_q: 1.6305269002914429, lamda: 1.103171944618225

steps: 1074975, episodes: 43000, mean episode reward: -25.095829235512152, agent episode reward: [-3.8814986071378796, -4.0884836740455786, -9.199594194920362, -7.9262527594083325], time: 65.942
steps: 1074975, episodes: 43000, mean episode variance: 0.7823674873327836, agent episode variance: [0.0, 0.0034075757479295134, 0.35087855756282804, 0.4280813540220261], time: 65.942
steps: 1074975, episodes: 43000, mean episode cvar: 0.44946857732534407, agent episode cvar: [0.32030385065078737, 0.3259630663394928, -0.09709016567468644, -0.09970817399024963], time: 65.943
Running avgs for agent 0: q_loss: 0.057871054857969284, u_loss: 1.2433098554611206, p_loss: -0.24288439750671387, mean_rew: -0.18895835827826193, variance: 0.0, cvar: 1.2812153100967407, v: 0.8448495864868164, mean_q: 0.22452419996261597, std_q: 1.3479549884796143, lamda: 1.4815202951431274
Running avgs for agent 1: q_loss: 0.10750843584537506, u_loss: 1.262618899345398, p_loss: 0.07279898226261139, mean_rew: -0.19037927799176282, variance: 0.013630302991718054, cvar: 1.3038523197174072, v: 0.4485906958580017, mean_q: -0.08537307381629944, std_q: 1.3568345308303833, lamda: 1.524505615234375
Running avgs for agent 2: q_loss: 0.018651673570275307, u_loss: 0.878048837184906, p_loss: 0.9786739945411682, mean_rew: -0.3022560162543929, variance: 1.4035142660140991, cvar: -0.3883606493473053, v: -0.40179285407066345, mean_q: -0.9938029646873474, std_q: 1.6520785093307495, lamda: 1.0923110246658325
Running avgs for agent 3: q_loss: 0.015767766162753105, u_loss: 0.7801281809806824, p_loss: 0.9591700434684753, mean_rew: -0.3178964552893331, variance: 1.7123254537582397, cvar: -0.3988327085971832, v: -0.4045215845108032, mean_q: -0.9810979962348938, std_q: 1.5585592985153198, lamda: 1.1066834926605225

steps: 1099975, episodes: 44000, mean episode reward: -24.64714123098299, agent episode reward: [-3.749323598849841, -4.1172807244553224, -9.034358468594485, -7.746178439083344], time: 65.869
steps: 1099975, episodes: 44000, mean episode variance: 0.7866471292655915, agent episode variance: [0.0, 0.006527662126347423, 0.35940929424762724, 0.42071017289161683], time: 65.87
steps: 1099975, episodes: 44000, mean episode cvar: 0.4532114126086235, agent episode cvar: [0.3195385348796844, 0.32731364798545837, -0.0977297415137291, -0.09591102874279023], time: 65.87
Running avgs for agent 0: q_loss: 0.057405225932598114, u_loss: 1.1744128465652466, p_loss: -0.29599276185035706, mean_rew: -0.18696398626451857, variance: 0.0, cvar: 1.2781540155410767, v: 0.8814521431922913, mean_q: 0.27794191241264343, std_q: 1.3455628156661987, lamda: 1.4866416454315186
Running avgs for agent 1: q_loss: 0.1492353230714798, u_loss: 1.2606457471847534, p_loss: 0.01818259060382843, mean_rew: -0.18838466654093888, variance: 0.026110648505389692, cvar: 1.3092546463012695, v: 0.5126242637634277, mean_q: -0.03135329484939575, std_q: 1.3275961875915527, lamda: 1.5384526252746582
Running avgs for agent 2: q_loss: 0.019309859722852707, u_loss: 0.7787834405899048, p_loss: 0.9738308191299438, mean_rew: -0.30479488264978805, variance: 1.437637209892273, cvar: -0.3909189701080322, v: -0.4053695499897003, mean_q: -0.9879575967788696, std_q: 1.638576626777649, lamda: 1.0943589210510254
Running avgs for agent 3: q_loss: 0.0122899878770113, u_loss: 0.8222953081130981, p_loss: 0.9931507110595703, mean_rew: -0.3203939082637029, variance: 1.6828407049179077, cvar: -0.38364410400390625, v: -0.3885689675807953, mean_q: -1.0204510688781738, std_q: 1.6035268306732178, lamda: 1.1102162599563599

steps: 1124975, episodes: 45000, mean episode reward: -24.86830890727375, agent episode reward: [-4.102137377905387, -4.439803816861647, -8.857049365243551, -7.469318347263163], time: 66.077
steps: 1124975, episodes: 45000, mean episode variance: 0.7822537202890962, agent episode variance: [0.0, 0.0061001002844423054, 0.3661613812446594, 0.4099922387599945], time: 66.078
steps: 1124975, episodes: 45000, mean episode cvar: 0.45762641850113867, agent episode cvar: [0.3214064338207245, 0.32885551154613496, -0.09858729255199432, -0.09404823431372643], time: 66.078
Running avgs for agent 0: q_loss: 0.05685688182711601, u_loss: 1.1664032936096191, p_loss: -0.31358635425567627, mean_rew: -0.18550435037742571, variance: 0.0, cvar: 1.285625696182251, v: 0.897903561592102, mean_q: 0.29671424627304077, std_q: 1.3390343189239502, lamda: 1.4925734996795654
Running avgs for agent 1: q_loss: 0.15855105221271515, u_loss: 1.2488117218017578, p_loss: 0.052210733294487, mean_rew: -0.18732771523027938, variance: 0.024400401137769222, cvar: 1.3154221773147583, v: 0.4936292767524719, mean_q: -0.0631285086274147, std_q: 1.3739389181137085, lamda: 1.555788278579712
Running avgs for agent 2: q_loss: 0.01995931938290596, u_loss: 0.844346821308136, p_loss: 0.9737669229507446, mean_rew: -0.30657187720931206, variance: 1.464645504951477, cvar: -0.39434918761253357, v: -0.4117620289325714, mean_q: -0.9864113926887512, std_q: 1.628443717956543, lamda: 1.0933889150619507
Running avgs for agent 3: q_loss: 0.010615895502269268, u_loss: 0.7166651487350464, p_loss: 1.0126513242721558, mean_rew: -0.3203798058465841, variance: 1.6399688720703125, cvar: -0.37619292736053467, v: -0.38166141510009766, mean_q: -1.0430219173431396, std_q: 1.6162110567092896, lamda: 1.1126068830490112

steps: 1149975, episodes: 46000, mean episode reward: -24.389727289499206, agent episode reward: [-3.6771976930423005, -4.515317577903053, -8.455836925258025, -7.741375093295832], time: 66.169
steps: 1149975, episodes: 46000, mean episode variance: 0.7969103715540842, agent episode variance: [0.0, 0.01356275070924312, 0.3663952294588089, 0.41695239138603213], time: 66.17
steps: 1149975, episodes: 46000, mean episode cvar: 0.46983587417006495, agent episode cvar: [0.32069449150562285, 0.34178115701675416, -0.09921920663118362, -0.09342056772112846], time: 66.17
Running avgs for agent 0: q_loss: 0.056639306247234344, u_loss: 1.071747899055481, p_loss: -0.34624072909355164, mean_rew: -0.18482761261261715, variance: 0.0, cvar: 1.2827779054641724, v: 0.9187690615653992, mean_q: 0.3285110592842102, std_q: 1.3255186080932617, lamda: 1.4982014894485474
Running avgs for agent 1: q_loss: 0.31507694721221924, u_loss: 1.2814364433288574, p_loss: 0.09969743341207504, mean_rew: -0.1861052176398438, variance: 0.05425100283697248, cvar: 1.3671246767044067, v: 0.43316394090652466, mean_q: -0.10994753241539001, std_q: 1.3620109558105469, lamda: 1.5746203660964966
Running avgs for agent 2: q_loss: 0.019878046587109566, u_loss: 0.7553310990333557, p_loss: 0.9641417264938354, mean_rew: -0.30594529982404994, variance: 1.465580940246582, cvar: -0.39687684178352356, v: -0.41433289647102356, mean_q: -0.9764819145202637, std_q: 1.6149892807006836, lamda: 1.092625617980957
Running avgs for agent 3: q_loss: 0.009952365420758724, u_loss: 0.7631938457489014, p_loss: 1.0279985666275024, mean_rew: -0.3204789636854404, variance: 1.6678096055984497, cvar: -0.37368226051330566, v: -0.3796616792678833, mean_q: -1.0598764419555664, std_q: 1.6446260213851929, lamda: 1.1150776147842407

steps: 1174975, episodes: 47000, mean episode reward: -25.573050755846694, agent episode reward: [-3.4608873804713003, -5.502973253296879, -8.510556194592857, -8.09863392748566], time: 66.328
steps: 1174975, episodes: 47000, mean episode variance: 0.8455672378819435, agent episode variance: [0.0, 0.04400855782441795, 0.3674966753721237, 0.4340620046854019], time: 66.329
steps: 1174975, episodes: 47000, mean episode cvar: 0.46474814373254775, agent episode cvar: [0.3212217059135437, 0.3365265756845474, -0.09992904809117317, -0.09307108977437019], time: 66.329
Running avgs for agent 0: q_loss: 0.058995701372623444, u_loss: 1.0525563955307007, p_loss: -0.36402153968811035, mean_rew: -0.18221342189664277, variance: 0.0, cvar: 1.2848867177963257, v: 0.9505643248558044, mean_q: 0.34220775961875916, std_q: 1.3345412015914917, lamda: 1.5054948329925537
Running avgs for agent 1: q_loss: 0.29944518208503723, u_loss: 1.3004919290542603, p_loss: 0.09439973533153534, mean_rew: -0.18706206883067372, variance: 0.1760342312976718, cvar: 1.3461061716079712, v: 0.44166654348373413, mean_q: -0.10626676678657532, std_q: 1.3089685440063477, lamda: 1.5935046672821045
Running avgs for agent 2: q_loss: 0.019374337047338486, u_loss: 0.7720525860786438, p_loss: 0.9622275829315186, mean_rew: -0.3076907710388113, variance: 1.4699866771697998, cvar: -0.39971619844436646, v: -0.4129845201969147, mean_q: -0.9773029685020447, std_q: 1.6194336414337158, lamda: 1.0922224521636963
Running avgs for agent 3: q_loss: 0.01012243889272213, u_loss: 0.7606079578399658, p_loss: 1.0476300716400146, mean_rew: -0.322084557405495, variance: 1.7362480163574219, cvar: -0.3722843527793884, v: -0.37998855113983154, mean_q: -1.078493356704712, std_q: 1.6772546768188477, lamda: 1.116919755935669

steps: 1199975, episodes: 48000, mean episode reward: -25.911820098016754, agent episode reward: [-3.3701215080121183, -4.868575594760989, -9.312153259117938, -8.360969736125707], time: 65.88
steps: 1199975, episodes: 48000, mean episode variance: 0.8876992105357349, agent episode variance: [0.0, 0.06576594353839756, 0.3733429555892944, 0.4485903114080429], time: 65.88
steps: 1199975, episodes: 48000, mean episode cvar: 0.46759142696857453, agent episode cvar: [0.31960934615135195, 0.33938252747058867, -0.09743187692761421, -0.09396856972575188], time: 65.881
Running avgs for agent 0: q_loss: 0.06703715771436691, u_loss: 1.1074578762054443, p_loss: -0.4054807126522064, mean_rew: -0.17978811116745375, variance: 0.0, cvar: 1.2784373760223389, v: 1.0165621042251587, mean_q: 0.38642066717147827, std_q: 1.3410202264785767, lamda: 1.5119695663452148
Running avgs for agent 1: q_loss: 0.4351249933242798, u_loss: 1.257262110710144, p_loss: 0.09280478209257126, mean_rew: -0.1886459472574236, variance: 0.2630637741535902, cvar: 1.3575299978256226, v: 0.39003974199295044, mean_q: -0.10060610622167587, std_q: 1.2700117826461792, lamda: 1.6118320226669312
Running avgs for agent 2: q_loss: 0.014992528595030308, u_loss: 0.8000751733779907, p_loss: 0.9641221761703491, mean_rew: -0.3107945769210239, variance: 1.493371844291687, cvar: -0.38972747325897217, v: -0.3973119556903839, mean_q: -0.9879475235939026, std_q: 1.5983315706253052, lamda: 1.0955640077590942
Running avgs for agent 3: q_loss: 0.011176236905157566, u_loss: 0.7406901717185974, p_loss: 1.0481492280960083, mean_rew: -0.32134666669687056, variance: 1.7943612337112427, cvar: -0.37587428092956543, v: -0.3847304880619049, mean_q: -1.0777981281280518, std_q: 1.6736711263656616, lamda: 1.1178909540176392

steps: 1224975, episodes: 49000, mean episode reward: -26.148224406207206, agent episode reward: [-3.293729445896157, -4.533340605353895, -9.849556245090175, -8.471598109866976], time: 66.275
steps: 1224975, episodes: 49000, mean episode variance: 0.930845394236967, agent episode variance: [0.0, 0.08492838792316616, 0.3772811693549156, 0.46863583695888517], time: 66.275
steps: 1224975, episodes: 49000, mean episode cvar: 0.4758874182403088, agent episode cvar: [0.32005034375190733, 0.34671344339847565, -0.09564510035514831, -0.09523126855492592], time: 66.276
Running avgs for agent 0: q_loss: 0.06870562583208084, u_loss: 1.1078757047653198, p_loss: -0.299071341753006, mean_rew: -0.17788227109273436, variance: 0.0, cvar: 1.2802014350891113, v: 0.8990667462348938, mean_q: 0.2917948067188263, std_q: 1.3324873447418213, lamda: 1.521743655204773
Running avgs for agent 1: q_loss: 0.6105021238327026, u_loss: 1.1858989000320435, p_loss: 0.07346037775278091, mean_rew: -0.18722382448439415, variance: 0.33971355169266465, cvar: 1.386853814125061, v: 0.39198291301727295, mean_q: -0.07912171632051468, std_q: 1.219394326210022, lamda: 1.632572889328003
Running avgs for agent 2: q_loss: 0.011977210640907288, u_loss: 0.7049881815910339, p_loss: 0.9788835644721985, mean_rew: -0.3125128959232193, variance: 1.509124755859375, cvar: -0.38258039951324463, v: -0.38824060559272766, mean_q: -1.008802056312561, std_q: 1.6152397394180298, lamda: 1.0988935232162476
Running avgs for agent 3: q_loss: 0.01187039352953434, u_loss: 0.7458415031433105, p_loss: 1.047367811203003, mean_rew: -0.3235864685432755, variance: 1.8745434284210205, cvar: -0.38092508912086487, v: -0.39098724722862244, mean_q: -1.07651686668396, std_q: 1.693311333656311, lamda: 1.1191835403442383

steps: 1249975, episodes: 50000, mean episode reward: -26.732069379666797, agent episode reward: [-3.367155912549737, -4.770572929239788, -10.31455287624879, -8.27978766162848], time: 66.074
steps: 1249975, episodes: 50000, mean episode variance: 0.9799324337989092, agent episode variance: [0.0, 0.09012274749577046, 0.39335924780368803, 0.4964504384994507], time: 66.074
steps: 1249975, episodes: 50000, mean episode cvar: 0.4720311123728752, agent episode cvar: [0.31695270478725435, 0.34820385873317716, -0.09631622609496117, -0.09680922505259514], time: 66.075
Running avgs for agent 0: q_loss: 0.07078693807125092, u_loss: 1.0048515796661377, p_loss: -0.33480748534202576, mean_rew: -0.17564993630647194, variance: 0.0, cvar: 1.2678108215332031, v: 0.8803719878196716, mean_q: 0.3265879452228546, std_q: 1.2970541715621948, lamda: 1.532455325126648
Running avgs for agent 1: q_loss: 0.6708508133888245, u_loss: 1.1370896100997925, p_loss: 0.10281823575496674, mean_rew: -0.18694056487670008, variance: 0.3604909899830818, cvar: 1.3928154706954956, v: 0.3730500340461731, mean_q: -0.11096757650375366, std_q: 1.2041946649551392, lamda: 1.6521447896957397
Running avgs for agent 2: q_loss: 0.013237575069069862, u_loss: 0.6850327849388123, p_loss: 0.9820898771286011, mean_rew: -0.3144388758675251, variance: 1.573436975479126, cvar: -0.38526490330696106, v: -0.39133360981941223, mean_q: -1.0124856233596802, std_q: 1.6267361640930176, lamda: 1.1039069890975952
Running avgs for agent 3: q_loss: 0.01302233338356018, u_loss: 0.7737994194030762, p_loss: 1.0363242626190186, mean_rew: -0.324833035881054, variance: 1.9858019351959229, cvar: -0.38723689317703247, v: -0.40027564764022827, mean_q: -1.061862587928772, std_q: 1.6819182634353638, lamda: 1.1207102537155151

steps: 1274975, episodes: 51000, mean episode reward: -27.25103311751056, agent episode reward: [-3.2302362327358707, -4.821772249400826, -10.396690586190955, -8.802334049182907], time: 66.475
steps: 1274975, episodes: 51000, mean episode variance: 0.9870301694236696, agent episode variance: [0.0, 0.05087682193145156, 0.40709028363227845, 0.5290630638599396], time: 66.475
steps: 1274975, episodes: 51000, mean episode cvar: 0.45465771222114565, agent episode cvar: [0.31991653370857237, 0.3296068919301033, -0.09677508646249772, -0.09809062695503235], time: 66.476
Running avgs for agent 0: q_loss: 0.0641699880361557, u_loss: 1.0289925336837769, p_loss: -0.30467352271080017, mean_rew: -0.17425439757357863, variance: 0.0, cvar: 1.2796660661697388, v: 0.8739660382270813, mean_q: 0.29409778118133545, std_q: 1.3134320974349976, lamda: 1.5425742864608765
Running avgs for agent 1: q_loss: 0.38810211420059204, u_loss: 1.229088306427002, p_loss: 0.2479657083749771, mean_rew: -0.1872941823398293, variance: 0.20350728772580623, cvar: 1.318427562713623, v: 0.3215840458869934, mean_q: -0.2570655643939972, std_q: 1.3342716693878174, lamda: 1.6667826175689697
Running avgs for agent 2: q_loss: 0.0158739872276783, u_loss: 0.6291318535804749, p_loss: 0.981550395488739, mean_rew: -0.3186131737257092, variance: 1.6283611059188843, cvar: -0.38710033893585205, v: -0.40217435359954834, mean_q: -1.0100177526474, std_q: 1.6123970746994019, lamda: 1.1053098440170288
Running avgs for agent 3: q_loss: 0.012583550997078419, u_loss: 0.7651880979537964, p_loss: 1.030461311340332, mean_rew: -0.3260531121491055, variance: 2.1162524223327637, cvar: -0.39236247539520264, v: -0.40123119950294495, mean_q: -1.053206443786621, std_q: 1.6878818273544312, lamda: 1.1223341226577759

steps: 1299975, episodes: 52000, mean episode reward: -27.608418222759013, agent episode reward: [-3.5506526428554, -4.844951298799289, -10.032682575697057, -9.180131705407272], time: 66.276
steps: 1299975, episodes: 52000, mean episode variance: 1.0391213761847466, agent episode variance: [0.0, 0.09627856507711112, 0.4303571038246155, 0.51248570728302], time: 66.277
steps: 1299975, episodes: 52000, mean episode cvar: 0.4621706373989582, agent episode cvar: [0.32057068169116976, 0.33891653192043303, -0.10052014476060868, -0.0967964314520359], time: 66.278
Running avgs for agent 0: q_loss: 0.07220456004142761, u_loss: 1.0399866104125977, p_loss: -0.3369283080101013, mean_rew: -0.17318880128349687, variance: 0.0, cvar: 1.2822827100753784, v: 0.9307578206062317, mean_q: 0.3133675456047058, std_q: 1.3192660808563232, lamda: 1.5529468059539795
Running avgs for agent 1: q_loss: 1.0727988481521606, u_loss: 1.2143083810806274, p_loss: 0.19157558679580688, mean_rew: -0.18645224534905014, variance: 0.3851142603084445, cvar: 1.3556662797927856, v: 0.3815511465072632, mean_q: -0.19887804985046387, std_q: 1.3125600814819336, lamda: 1.6834145784378052
Running avgs for agent 2: q_loss: 0.01779158227145672, u_loss: 0.638297975063324, p_loss: 0.9780780673027039, mean_rew: -0.32237453714037945, variance: 1.7214285135269165, cvar: -0.40208056569099426, v: -0.41207626461982727, mean_q: -1.0025622844696045, std_q: 1.6157785654067993, lamda: 1.106123924255371
Running avgs for agent 3: q_loss: 0.011155284941196442, u_loss: 0.7360858917236328, p_loss: 1.040452480316162, mean_rew: -0.32799158421966984, variance: 2.049942970275879, cvar: -0.3871857225894928, v: -0.39839911460876465, mean_q: -1.0639199018478394, std_q: 1.6899703741073608, lamda: 1.123010516166687

steps: 1324975, episodes: 53000, mean episode reward: -28.216219227194443, agent episode reward: [-3.3117300409799593, -5.838830598205501, -10.023345033464716, -9.042313554544267], time: 66.345
steps: 1324975, episodes: 53000, mean episode variance: 1.0186814817916603, agent episode variance: [0.0, 0.12386633391492069, 0.44402436089515684, 0.45079078698158265], time: 66.346
steps: 1324975, episodes: 53000, mean episode cvar: 0.425662563174963, agent episode cvar: [0.3115365750789642, 0.3125926167964935, -0.10158955237269401, -0.09687707632780075], time: 66.346
Running avgs for agent 0: q_loss: 0.10965150594711304, u_loss: 1.0114880800247192, p_loss: -0.32044604420661926, mean_rew: -0.17078889841578498, variance: 0.0, cvar: 1.2461462020874023, v: 0.9115116000175476, mean_q: 0.29964929819107056, std_q: 1.32643461227417, lamda: 1.5643057823181152
Running avgs for agent 1: q_loss: 0.6339541673660278, u_loss: 1.1444928646087646, p_loss: 0.06919185072183609, mean_rew: -0.1861280030116788, variance: 0.49546533565968276, cvar: 1.2503706216812134, v: 0.5564795136451721, mean_q: -0.07902181893587112, std_q: 1.246000051498413, lamda: 1.705432415008545
Running avgs for agent 2: q_loss: 0.01875215768814087, u_loss: 0.7440710663795471, p_loss: 0.9736778736114502, mean_rew: -0.32594575830489486, variance: 1.7760974168777466, cvar: -0.4063582122325897, v: -0.4165264070034027, mean_q: -0.9971283078193665, std_q: 1.6082556247711182, lamda: 1.1097887754440308
Running avgs for agent 3: q_loss: 0.00929437018930912, u_loss: 0.7751158475875854, p_loss: 1.0517832040786743, mean_rew: -0.3308531629073251, variance: 1.8031630516052246, cvar: -0.3875083029270172, v: -0.3925883173942566, mean_q: -1.0775340795516968, std_q: 1.690427541732788, lamda: 1.1254947185516357

steps: 1349975, episodes: 54000, mean episode reward: -31.698559598359477, agent episode reward: [-6.038535098564196, -7.604019405966266, -9.974736483949862, -8.081268609879151], time: 66.08
steps: 1349975, episodes: 54000, mean episode variance: 0.9663805296402425, agent episode variance: [0.0, 0.09807688832096756, 0.44877167308330534, 0.41953196823596955], time: 66.08
steps: 1349975, episodes: 54000, mean episode cvar: 0.46766027200222016, agent episode cvar: [0.31748369431495665, 0.34803171092271806, -0.10164050707221031, -0.09621462616324425], time: 66.081
Running avgs for agent 0: q_loss: 0.4891924560070038, u_loss: 1.1614902019500732, p_loss: -0.2636186182498932, mean_rew: -0.16898724359024142, variance: 0.0, cvar: 1.2699347734451294, v: 0.996815025806427, mean_q: 0.23795467615127563, std_q: 1.472784399986267, lamda: 1.5943195819854736
Running avgs for agent 1: q_loss: 1.342123031616211, u_loss: 1.0710909366607666, p_loss: 0.1811673939228058, mean_rew: -0.18750968227492093, variance: 0.39230755328387024, cvar: 1.3921269178390503, v: 0.580395519733429, mean_q: -0.1916440725326538, std_q: 1.187583088874817, lamda: 1.7265900373458862
Running avgs for agent 2: q_loss: 0.017949676141142845, u_loss: 0.6918870806694031, p_loss: 0.9704045653343201, mean_rew: -0.3279523073557403, variance: 1.7950866222381592, cvar: -0.4065620005130768, v: -0.4172482192516327, mean_q: -0.9940513968467712, std_q: 1.5918257236480713, lamda: 1.1119424104690552
Running avgs for agent 3: q_loss: 0.008715429343283176, u_loss: 0.814517617225647, p_loss: 1.05961275100708, mean_rew: -0.33197767573264425, variance: 1.6781278848648071, cvar: -0.38485851883888245, v: -0.3906136751174927, mean_q: -1.0886207818984985, std_q: 1.701818823814392, lamda: 1.1278373003005981

steps: 1374975, episodes: 55000, mean episode reward: -35.15114366242659, agent episode reward: [-8.43608791456738, -9.033598077582088, -9.878823813494593, -7.802633856782533], time: 66.24
steps: 1374975, episodes: 55000, mean episode variance: 0.9957322239466011, agent episode variance: [0.000166513592004776, 0.08428702442720533, 0.4630666407346725, 0.4482120451927185], time: 66.241
steps: 1374975, episodes: 55000, mean episode cvar: 0.621849519520998, agent episode cvar: [0.3275463783740997, 0.4931990326046944, -0.10134399372339249, -0.0975518977344036], time: 66.241
Running avgs for agent 0: q_loss: 1.3843311071395874, u_loss: 1.2658504247665405, p_loss: -0.11333010345697403, mean_rew: -0.17190715189869452, variance: 0.000666054368019104, cvar: 1.3101855516433716, v: 0.8514097929000854, mean_q: 0.05637654289603233, std_q: 1.4714741706848145, lamda: 1.6278924942016602
Running avgs for agent 1: q_loss: 14.941011428833008, u_loss: 1.0092555284500122, p_loss: 0.03788807615637779, mean_rew: -0.192106702881989, variance: 0.3371480977088213, cvar: 1.9727962017059326, v: 0.6756137609481812, mean_q: -0.053430814296007156, std_q: 1.1168075799942017, lamda: 1.7499380111694336
Running avgs for agent 2: q_loss: 0.017254257574677467, u_loss: 0.6833426356315613, p_loss: 0.9756872057914734, mean_rew: -0.33141113099294606, variance: 1.852266550064087, cvar: -0.40537598729133606, v: -0.419404000043869, mean_q: -1.000844120979309, std_q: 1.586141586303711, lamda: 1.1119612455368042
Running avgs for agent 3: q_loss: 0.009553458541631699, u_loss: 0.7475612759590149, p_loss: 1.0671249628067017, mean_rew: -0.33257609145060646, variance: 1.7928482294082642, cvar: -0.39020758867263794, v: -0.39753225445747375, mean_q: -1.0936335325241089, std_q: 1.7104148864746094, lamda: 1.1314970254898071

steps: 1399975, episodes: 56000, mean episode reward: -37.81300901949262, agent episode reward: [-9.319962687394987, -11.601196086845649, -9.220836692164132, -7.671013553087852], time: 66.277
steps: 1399975, episodes: 56000, mean episode variance: 1.0503509359713643, agent episode variance: [0.00030405235290527344, 0.08154500905238092, 0.48668372428417206, 0.4818181502819061], time: 66.278
steps: 1399975, episodes: 56000, mean episode cvar: 0.512815567612648, agent episode cvar: [0.34679019594192506, 0.3674650818109512, -0.10268839928507804, -0.09875131085515022], time: 66.278
Running avgs for agent 0: q_loss: 1.1494016647338867, u_loss: 1.3204326629638672, p_loss: -0.11883509159088135, mean_rew: -0.17530198170396677, variance: 0.0012162094116210938, cvar: 1.3871607780456543, v: 0.7592411041259766, mean_q: 0.05659503862261772, std_q: 1.4031152725219727, lamda: 1.6592191457748413
Running avgs for agent 1: q_loss: 1.8292863368988037, u_loss: 1.0275555849075317, p_loss: 0.3735004961490631, mean_rew: -0.19748868989002757, variance: 0.3261800362095237, cvar: 1.469860315322876, v: 0.6346839666366577, mean_q: -0.3893866539001465, std_q: 1.0806716680526733, lamda: 1.780332088470459
Running avgs for agent 2: q_loss: 0.016134662553668022, u_loss: 0.6699283123016357, p_loss: 0.9771974086761475, mean_rew: -0.3334571573320565, variance: 1.94673490524292, cvar: -0.41075360774993896, v: -0.4183204770088196, mean_q: -1.0015445947647095, std_q: 1.5774694681167603, lamda: 1.1124544143676758
Running avgs for agent 3: q_loss: 0.010008161887526512, u_loss: 0.7626283764839172, p_loss: 1.060975432395935, mean_rew: -0.33297887415424027, variance: 1.9272725582122803, cvar: -0.3950052559375763, v: -0.4007217586040497, mean_q: -1.0842925310134888, std_q: 1.6922476291656494, lamda: 1.1390020847320557

steps: 1424975, episodes: 57000, mean episode reward: -38.88671730237053, agent episode reward: [-9.225253883852135, -12.580365312864268, -9.024815353144938, -8.056282752509187], time: 66.285
steps: 1424975, episodes: 57000, mean episode variance: 1.1222317514456808, agent episode variance: [0.0, 0.11122998857870697, 0.5053505226373672, 0.5056512402296066], time: 66.286
steps: 1424975, episodes: 57000, mean episode cvar: 0.49401003205776217, agent episode cvar: [0.34030959236621855, 0.35513845944404604, -0.10177089759707451, -0.09966712215542793], time: 66.287
Running avgs for agent 0: q_loss: 0.9499924182891846, u_loss: 1.3328073024749756, p_loss: -0.07010296732187271, mean_rew: -0.17964418667570825, variance: 0.0, cvar: 1.3612382411956787, v: 0.6214544773101807, mean_q: 0.029933825135231018, std_q: 1.3124186992645264, lamda: 1.6823123693466187
Running avgs for agent 1: q_loss: 1.5179778337478638, u_loss: 1.0123634338378906, p_loss: 0.48984360694885254, mean_rew: -0.2024286131381325, variance: 0.4449199543148279, cvar: 1.4205538034439087, v: 0.5255587100982666, mean_q: -0.5039128661155701, std_q: 1.0249879360198975, lamda: 1.8064110279083252
Running avgs for agent 2: q_loss: 0.015152940526604652, u_loss: 0.6468268036842346, p_loss: 0.9867989420890808, mean_rew: -0.3352602830601852, variance: 2.021402359008789, cvar: -0.4070836007595062, v: -0.41615334153175354, mean_q: -1.0112135410308838, std_q: 1.586217999458313, lamda: 1.1113359928131104
Running avgs for agent 3: q_loss: 0.01090781856328249, u_loss: 0.7554836273193359, p_loss: 1.0605398416519165, mean_rew: -0.33340509337095126, variance: 2.0226049423217773, cvar: -0.398668497800827, v: -0.4076908528804779, mean_q: -1.0808801651000977, std_q: 1.6966145038604736, lamda: 1.1448607444763184

steps: 1449975, episodes: 58000, mean episode reward: -39.41675067577898, agent episode reward: [-10.487540033033522, -12.295334616350543, -8.75464817587115, -7.879227850523768], time: 64.067
steps: 1449975, episodes: 58000, mean episode variance: 1.2090216851979494, agent episode variance: [0.0004068601354956627, 0.154944715552032, 0.5217167069911957, 0.5319534025192261], time: 64.068
steps: 1449975, episodes: 58000, mean episode cvar: 0.5705776887536049, agent episode cvar: [0.35004661107063295, 0.42464735090732575, -0.10261418217420579, -0.10150209105014801], time: 64.068
Running avgs for agent 0: q_loss: 0.8931485414505005, u_loss: 1.268774151802063, p_loss: -0.06996581703424454, mean_rew: -0.1841665059317811, variance: 0.0016274405419826508, cvar: 1.4001864194869995, v: 0.49966779351234436, mean_q: 0.03332401439547539, std_q: 1.2138545513153076, lamda: 1.7002567052841187
Running avgs for agent 1: q_loss: 7.643138885498047, u_loss: 1.174131155014038, p_loss: 0.528488039970398, mean_rew: -0.20982028136017306, variance: 0.619778862208128, cvar: 1.6985893249511719, v: 0.3599425256252289, mean_q: -0.5431358814239502, std_q: 1.0250837802886963, lamda: 1.8278636932373047
Running avgs for agent 2: q_loss: 0.014354394748806953, u_loss: 0.6683370471000671, p_loss: 1.0072513818740845, mean_rew: -0.338935680530283, variance: 2.086866617202759, cvar: -0.41045671701431274, v: -0.4182048439979553, mean_q: -1.0317405462265015, std_q: 1.6143568754196167, lamda: 1.1123887300491333
Running avgs for agent 3: q_loss: 0.01258042175322771, u_loss: 0.8474076986312866, p_loss: 1.05899178981781, mean_rew: -0.3361249380270713, variance: 2.1278135776519775, cvar: -0.40600836277008057, v: -0.41457507014274597, mean_q: -1.0751467943191528, std_q: 1.69963538646698, lamda: 1.1515514850616455

steps: 1474975, episodes: 59000, mean episode reward: -42.86590085446987, agent episode reward: [-12.67918891341331, -13.672340666558739, -8.662452108776609, -7.851919165721211], time: 63.121
steps: 1474975, episodes: 59000, mean episode variance: 1.276364663194865, agent episode variance: [0.0001884424053132534, 0.1926100213676691, 0.5329978790283203, 0.5505683203935623], time: 63.122
steps: 1474975, episodes: 59000, mean episode cvar: 0.5861712257266045, agent episode cvar: [0.41501390826702117, 0.376313807785511, -0.10263010162115097, -0.10252638870477676], time: 63.122
Running avgs for agent 0: q_loss: 4.980137348175049, u_loss: 1.1974287033081055, p_loss: 0.02700047940015793, mean_rew: -0.1901040451391857, variance: 0.0007537696212530136, cvar: 1.6600555181503296, v: 0.34660786390304565, mean_q: -0.06558633595705032, std_q: 1.2030466794967651, lamda: 1.722731351852417
Running avgs for agent 1: q_loss: 2.5563666820526123, u_loss: 1.4634008407592773, p_loss: 0.8515771627426147, mean_rew: -0.2181537823027843, variance: 0.7704400854706764, cvar: 1.505255103111267, v: 0.30961376428604126, mean_q: -0.8646222352981567, std_q: 1.1315593719482422, lamda: 1.8511284589767456
Running avgs for agent 2: q_loss: 0.014905969612300396, u_loss: 0.5979794859886169, p_loss: 1.0088738203048706, mean_rew: -0.34006537626993655, variance: 2.1319916248321533, cvar: -0.41052040457725525, v: -0.4180554449558258, mean_q: -1.0339980125427246, std_q: 1.6219648122787476, lamda: 1.1147838830947876
Running avgs for agent 3: q_loss: 0.015084270387887955, u_loss: 0.9159514904022217, p_loss: 1.0440316200256348, mean_rew: -0.3360097250784107, variance: 2.202273368835449, cvar: -0.41010552644729614, v: -0.42067229747772217, mean_q: -1.0563104152679443, std_q: 1.6868687868118286, lamda: 1.1563247442245483

steps: 1499975, episodes: 60000, mean episode reward: -39.94593753126444, agent episode reward: [-10.640394758860667, -12.348767447717783, -8.817199815545228, -8.139575509140759], time: 64.77
steps: 1499975, episodes: 60000, mean episode variance: 1.3609767864029854, agent episode variance: [0.006297514749690891, 0.2542556606084108, 0.5356103707551956, 0.5648132402896882], time: 64.77
steps: 1499975, episodes: 60000, mean episode cvar: 0.5774070325791836, agent episode cvar: [0.42134168177843095, 0.36259097945690155, -0.10244257748126984, -0.10408305117487908], time: 64.771
Running avgs for agent 0: q_loss: 5.06887674331665, u_loss: 1.3816264867782593, p_loss: 0.2060781568288803, mean_rew: -0.19699251238882018, variance: 0.025190058998763563, cvar: 1.6853667497634888, v: 0.3270481526851654, mean_q: -0.25953614711761475, std_q: 1.3294570446014404, lamda: 1.7518093585968018
Running avgs for agent 1: q_loss: 2.5641028881073, u_loss: 1.6247714757919312, p_loss: 1.040092945098877, mean_rew: -0.22545874843892222, variance: 1.0170226424336433, cvar: 1.4503638744354248, v: 0.16333319246768951, mean_q: -1.0526819229125977, std_q: 1.1330357789993286, lamda: 1.8733384609222412
Running avgs for agent 2: q_loss: 0.01474409643560648, u_loss: 0.6165767908096313, p_loss: 1.0129832029342651, mean_rew: -0.34143291775745577, variance: 2.142441511154175, cvar: -0.40977030992507935, v: -0.41989919543266296, mean_q: -1.0387450456619263, std_q: 1.6242880821228027, lamda: 1.1149623394012451
Running avgs for agent 3: q_loss: 0.017849018797278404, u_loss: 1.0439670085906982, p_loss: 1.0266920328140259, mean_rew: -0.33682078024639855, variance: 2.2592530250549316, cvar: -0.4163322150707245, v: -0.43196967244148254, mean_q: -1.0363932847976685, std_q: 1.6555898189544678, lamda: 1.1542348861694336

steps: 1524975, episodes: 61000, mean episode reward: -41.181694243742676, agent episode reward: [-11.671925412788584, -12.28993885426525, -8.783739601253941, -8.436090375434896], time: 65.002
steps: 1524975, episodes: 61000, mean episode variance: 1.4934948803223669, agent episode variance: [0.05386050305515528, 0.31404811003431676, 0.5389242562055587, 0.5866620110273362], time: 65.003
steps: 1524975, episodes: 61000, mean episode cvar: 0.8095384192168713, agent episode cvar: [0.5438681876659394, 0.47706676548719407, -0.10404147163033485, -0.10735506230592727], time: 65.003
Running avgs for agent 0: q_loss: 22.666780471801758, u_loss: 1.745311975479126, p_loss: 0.5257112979888916, mean_rew: -0.20160600379211674, variance: 0.21544201222062112, cvar: 2.1754727363586426, v: 0.07555779069662094, mean_q: -0.5815644264221191, std_q: 1.4316426515579224, lamda: 1.7802497148513794
Running avgs for agent 1: q_loss: 22.261432647705078, u_loss: 1.7914193868637085, p_loss: 0.9543136954307556, mean_rew: -0.23283645936996133, variance: 1.256192440137267, cvar: 1.9082671403884888, v: 0.09604190289974213, mean_q: -0.96933913230896, std_q: 1.2452126741409302, lamda: 1.89241361618042
Running avgs for agent 2: q_loss: 0.014702577143907547, u_loss: 0.6236543655395508, p_loss: 1.009645938873291, mean_rew: -0.34229023371916023, variance: 2.1556971073150635, cvar: -0.41616588830947876, v: -0.42171454429626465, mean_q: -1.0341464281082153, std_q: 1.6246379613876343, lamda: 1.1183053255081177
Running avgs for agent 3: q_loss: 0.020631195977330208, u_loss: 1.0462902784347534, p_loss: 0.9993044137954712, mean_rew: -0.33737685961825464, variance: 2.3466479778289795, cvar: -0.42942026257514954, v: -0.43991002440452576, mean_q: -1.0069408416748047, std_q: 1.6240383386611938, lamda: 1.1554539203643799

steps: 1549975, episodes: 62000, mean episode reward: -40.95031812713141, agent episode reward: [-12.794737832308336, -10.573745805658941, -8.851930065335818, -8.729904423828318], time: 65.528
steps: 1549975, episodes: 62000, mean episode variance: 1.6204296956956386, agent episode variance: [0.1574470678344369, 0.32468764602392913, 0.5372219198942184, 0.6010730619430542], time: 65.528
steps: 1549975, episodes: 62000, mean episode cvar: 0.6392151269316674, agent episode cvar: [0.45496924763917923, 0.3956993975043297, -0.1032192533314228, -0.10823426488041878], time: 65.529
Running avgs for agent 0: q_loss: 10.280811309814453, u_loss: 2.196664333343506, p_loss: 0.7802096009254456, mean_rew: -0.2071583347374762, variance: 0.6297882713377476, cvar: 1.8198769092559814, v: -0.09221728146076202, mean_q: -0.8651265501976013, std_q: 1.553146243095398, lamda: 1.8082237243652344
Running avgs for agent 1: q_loss: 7.749680519104004, u_loss: 1.9610894918441772, p_loss: 1.2462780475616455, mean_rew: -0.23856878224455308, variance: 1.2987505840957165, cvar: 1.582797646522522, v: -0.05542188882827759, mean_q: -1.2646287679672241, std_q: 1.3052533864974976, lamda: 1.9097635746002197
Running avgs for agent 2: q_loss: 0.014793796464800835, u_loss: 0.6428021192550659, p_loss: 1.0131328105926514, mean_rew: -0.34398809839665195, variance: 2.1488876342773438, cvar: -0.41287702322006226, v: -0.42395922541618347, mean_q: -1.0374395847320557, std_q: 1.6242554187774658, lamda: 1.120730996131897
Running avgs for agent 3: q_loss: 0.0222419835627079, u_loss: 1.023360252380371, p_loss: 0.9821508526802063, mean_rew: -0.34066346333230607, variance: 2.404292345046997, cvar: -0.4329370856285095, v: -0.4442058503627777, mean_q: -0.9897741675376892, std_q: 1.5996108055114746, lamda: 1.154783010482788

steps: 1574975, episodes: 63000, mean episode reward: -56.917661927062724, agent episode reward: [-30.30118961049668, -8.83769576573311, -8.787025886180363, -8.991750664652566], time: 65.572
steps: 1574975, episodes: 63000, mean episode variance: 2.246581613060087, agent episode variance: [0.7770760574936867, 0.3404707525335252, 0.5307559278011322, 0.5982788752317428], time: 65.573
steps: 1574975, episodes: 63000, mean episode cvar: 0.8045339021980763, agent episode cvar: [0.5701487876474858, 0.44570587211847307, -0.10353001627326011, -0.10779074129462242], time: 65.573
Running avgs for agent 0: q_loss: 25.304855346679688, u_loss: 11.913771629333496, p_loss: 0.7189942002296448, mean_rew: -0.22430498219976575, variance: 3.1083042299747468, cvar: 2.280595302581787, v: 0.029685936868190765, mean_q: -0.7634530067443848, std_q: 1.8397711515426636, lamda: 1.833986520767212
Running avgs for agent 1: q_loss: 15.573370933532715, u_loss: 2.0900256633758545, p_loss: 1.476403832435608, mean_rew: -0.24353835901729565, variance: 1.3618830101341008, cvar: 1.7828233242034912, v: 0.011142768897116184, mean_q: -1.5093923807144165, std_q: 1.4105730056762695, lamda: 1.923507571220398
Running avgs for agent 2: q_loss: 0.014670178294181824, u_loss: 0.618989109992981, p_loss: 1.0172593593597412, mean_rew: -0.34562415294609156, variance: 2.123023748397827, cvar: -0.41412004828453064, v: -0.4285144507884979, mean_q: -1.0408681631088257, std_q: 1.6135659217834473, lamda: 1.1184254884719849
Running avgs for agent 3: q_loss: 0.021631576120853424, u_loss: 0.9371421337127686, p_loss: 0.9685944318771362, mean_rew: -0.34113086334348325, variance: 2.393115520477295, cvar: -0.43116295337677, v: -0.44877490401268005, mean_q: -0.9769718647003174, std_q: 1.5588730573654175, lamda: 1.152051568031311

steps: 1599975, episodes: 64000, mean episode reward: -45.582823875554645, agent episode reward: [-14.861103422620955, -11.993966004986364, -8.894588540228574, -9.833165907718758], time: 65.656
steps: 1599975, episodes: 64000, mean episode variance: 2.801399727452546, agent episode variance: [1.1463260079622268, 0.5255434436686337, 0.5485200524330139, 0.5810102233886719], time: 65.656
steps: 1599975, episodes: 64000, mean episode cvar: 0.5684760127663613, agent episode cvar: [0.37039746329188344, 0.4098991015553474, -0.10466222035884858, -0.1071583317220211], time: 65.657
Running avgs for agent 0: q_loss: 6.045589923858643, u_loss: 11.912576675415039, p_loss: 0.8921420574188232, mean_rew: -0.240386617002824, variance: 4.585304031848907, cvar: 1.4815897941589355, v: -0.041874658316373825, mean_q: -0.9321812987327576, std_q: 2.297553777694702, lamda: 1.858278751373291
Running avgs for agent 1: q_loss: 12.929888725280762, u_loss: 1.7144402265548706, p_loss: 1.255732536315918, mean_rew: -0.2461776785126356, variance: 2.102173774674535, cvar: 1.6395963430404663, v: -0.0331934429705143, mean_q: -1.293768286705017, std_q: 1.2099963426589966, lamda: 1.9399751424789429
Running avgs for agent 2: q_loss: 0.014542576856911182, u_loss: 0.6679127216339111, p_loss: 1.0161032676696777, mean_rew: -0.34784120299271953, variance: 2.194080114364624, cvar: -0.4186488687992096, v: -0.43072861433029175, mean_q: -1.038885235786438, std_q: 1.6052658557891846, lamda: 1.1205329895019531
Running avgs for agent 3: q_loss: 0.018090149387717247, u_loss: 0.9515103101730347, p_loss: 0.9671789407730103, mean_rew: -0.34317105879181836, variance: 2.3240411281585693, cvar: -0.42863330245018005, v: -0.4384547472000122, mean_q: -0.9792885184288025, std_q: 1.5427253246307373, lamda: 1.1512701511383057

steps: 1624975, episodes: 65000, mean episode reward: -46.20740993202965, agent episode reward: [-11.92790680287265, -15.531202198043731, -8.703512198747132, -10.044788732366136], time: 65.63
steps: 1624975, episodes: 65000, mean episode variance: 2.9231553770676255, agent episode variance: [1.1906866611540317, 0.5814481967762113, 0.5550111012458802, 0.5960094178915024], time: 65.63
steps: 1624975, episodes: 65000, mean episode cvar: 0.5350109411776066, agent episode cvar: [0.34311133992671966, 0.40447886872291566, -0.10515188336372376, -0.10742738410830498], time: 65.631
Running avgs for agent 0: q_loss: 2.9554033279418945, u_loss: 9.62844181060791, p_loss: 0.7462360858917236, mean_rew: -0.24858986006395553, variance: 4.762746644616127, cvar: 1.3724453449249268, v: 0.21804766356945038, mean_q: -0.8109251260757446, std_q: 2.4205048084259033, lamda: 1.878214955329895
Running avgs for agent 1: q_loss: 12.178069114685059, u_loss: 2.0168943405151367, p_loss: 1.4118547439575195, mean_rew: -0.2551449685500009, variance: 2.325792787104845, cvar: 1.6179155111312866, v: -0.11408677697181702, mean_q: -1.461913824081421, std_q: 1.2320386171340942, lamda: 1.9571789503097534
Running avgs for agent 2: q_loss: 0.014203840866684914, u_loss: 0.5931259989738464, p_loss: 1.023887038230896, mean_rew: -0.35083490054377475, variance: 2.2200443744659424, cvar: -0.4206075072288513, v: -0.43453431129455566, mean_q: -1.0461063385009766, std_q: 1.6156748533248901, lamda: 1.1200141906738281
Running avgs for agent 3: q_loss: 0.017121490091085434, u_loss: 0.9288943409919739, p_loss: 0.9687446355819702, mean_rew: -0.3452561063123606, variance: 2.384037733078003, cvar: -0.4297095239162445, v: -0.43604543805122375, mean_q: -0.9838619232177734, std_q: 1.5365298986434937, lamda: 1.155476689338684

steps: 1649975, episodes: 66000, mean episode reward: -42.75571432535451, agent episode reward: [-7.705270710070815, -15.735619988239861, -9.019895192713335, -10.294928434330497], time: 65.626
steps: 1649975, episodes: 66000, mean episode variance: 2.962346731621772, agent episode variance: [1.149579044818878, 0.5920328533239663, 0.5686906669139862, 0.6520441665649414], time: 65.627
steps: 1649975, episodes: 66000, mean episode cvar: 0.42138169044256213, agent episode cvar: [0.3023532565832138, 0.332014112830162, -0.10476131519675255, -0.10822436377406121], time: 65.627
Running avgs for agent 0: q_loss: 0.47139984369277954, u_loss: 9.550559043884277, p_loss: 0.4117714464664459, mean_rew: -0.25431271945513845, variance: 4.598316192626953, cvar: 1.2094130516052246, v: 0.5641828179359436, mean_q: -0.49304160475730896, std_q: 2.5262949466705322, lamda: 1.8897804021835327
Running avgs for agent 1: q_loss: 1.1614973545074463, u_loss: 2.519185781478882, p_loss: 1.6347861289978027, mean_rew: -0.26647566798836125, variance: 2.368131413295865, cvar: 1.3280564546585083, v: -0.13080812990665436, mean_q: -1.707139253616333, std_q: 1.3827515840530396, lamda: 1.9736899137496948
Running avgs for agent 2: q_loss: 0.01388339139521122, u_loss: 0.5533044934272766, p_loss: 1.016789197921753, mean_rew: -0.3508525595974662, variance: 2.2747626304626465, cvar: -0.41904526948928833, v: -0.4316844344139099, mean_q: -1.0399624109268188, std_q: 1.5961953401565552, lamda: 1.1181588172912598
Running avgs for agent 3: q_loss: 0.01940569467842579, u_loss: 0.8923085927963257, p_loss: 0.9690404534339905, mean_rew: -0.3474201338247277, variance: 2.6081764698028564, cvar: -0.4328974485397339, v: -0.44460031390190125, mean_q: -0.985272228717804, std_q: 1.5430022478103638, lamda: 1.159396767616272

steps: 1674975, episodes: 67000, mean episode reward: -40.445854977620876, agent episode reward: [-5.877676902537727, -15.308347614080287, -8.992379795948022, -10.26745066505484], time: 65.975
steps: 1674975, episodes: 67000, mean episode variance: 2.8116423149108885, agent episode variance: [0.7113583031892776, 0.8339806547164917, 0.5862286347150802, 0.6800747222900391], time: 65.976
steps: 1674975, episodes: 67000, mean episode cvar: 0.4661554964184761, agent episode cvar: [0.31310183835029604, 0.3705098584294319, -0.10701812365651131, -0.11043807670474053], time: 65.977
Running avgs for agent 0: q_loss: 0.21734534204006195, u_loss: 9.170476913452148, p_loss: 0.17544616758823395, mean_rew: -0.2571104285853705, variance: 2.845432996749878, cvar: 1.2524073123931885, v: 0.8210901021957397, mean_q: -0.2785365879535675, std_q: 2.671215772628784, lamda: 1.8955739736557007
Running avgs for agent 1: q_loss: 5.03387975692749, u_loss: 2.802027940750122, p_loss: 1.6670243740081787, mean_rew: -0.2769770006010412, variance: 3.335922618865967, cvar: 1.482039451599121, v: -0.12780852615833282, mean_q: -1.7456765174865723, std_q: 1.4084583520889282, lamda: 1.9908273220062256
Running avgs for agent 2: q_loss: 0.013868310488760471, u_loss: 0.6342424750328064, p_loss: 1.020677089691162, mean_rew: -0.35415921588896404, variance: 2.344914436340332, cvar: -0.4280725121498108, v: -0.434627503156662, mean_q: -1.0428658723831177, std_q: 1.6094746589660645, lamda: 1.1182504892349243
Running avgs for agent 3: q_loss: 0.022481877356767654, u_loss: 0.8178934454917908, p_loss: 0.9641572833061218, mean_rew: -0.35237303546136595, variance: 2.7202987670898438, cvar: -0.4417523145675659, v: -0.45451995730400085, mean_q: -0.9788191914558411, std_q: 1.533798336982727, lamda: 1.1593893766403198

steps: 1699975, episodes: 68000, mean episode reward: -38.92021981225739, agent episode reward: [-5.846806422471169, -13.639979296195827, -8.952525167647202, -10.480908925943188], time: 66.005
steps: 1699975, episodes: 68000, mean episode variance: 2.7167193535491823, agent episode variance: [0.3609566839262843, 1.073387188732624, 0.597234566450119, 0.685140914440155], time: 66.006
steps: 1699975, episodes: 68000, mean episode cvar: 0.4394864073693752, agent episode cvar: [0.3193071354627609, 0.33938411957025527, -0.10711048501729965, -0.11209436264634133], time: 66.006
Running avgs for agent 0: q_loss: 0.1501210629940033, u_loss: 7.074034214019775, p_loss: 0.0015158948954194784, mean_rew: -0.2567044947290832, variance: 1.4438267357051373, cvar: 1.2772284746170044, v: 0.9539145827293396, mean_q: -0.11713561415672302, std_q: 2.6850881576538086, lamda: 1.8982675075531006
Running avgs for agent 1: q_loss: 1.8872429132461548, u_loss: 3.260810136795044, p_loss: 1.8518821001052856, mean_rew: -0.28664469574280627, variance: 4.293548754930496, cvar: 1.3575365543365479, v: -0.30386626720428467, mean_q: -1.9317131042480469, std_q: 1.443933129310608, lamda: 2.004474401473999
Running avgs for agent 2: q_loss: 0.014051513746380806, u_loss: 0.5441694855690002, p_loss: 1.0148719549179077, mean_rew: -0.35471059772822555, variance: 2.3889381885528564, cvar: -0.42844194173812866, v: -0.436926007270813, mean_q: -1.037136197090149, std_q: 1.592546820640564, lamda: 1.1210060119628906
Running avgs for agent 3: q_loss: 0.024965770542621613, u_loss: 0.8660609722137451, p_loss: 0.9417921304702759, mean_rew: -0.3529078165671979, variance: 2.7405636310577393, cvar: -0.44837743043899536, v: -0.4619694650173187, mean_q: -0.9542356133460999, std_q: 1.5048935413360596, lamda: 1.1592154502868652

steps: 1724975, episodes: 69000, mean episode reward: -38.279884480163396, agent episode reward: [-6.471975831952527, -12.443171430592365, -9.015476352987701, -10.34926086463081], time: 66.245
steps: 1724975, episodes: 69000, mean episode variance: 2.8177473701126874, agent episode variance: [0.1849429105259478, 1.3531146803051233, 0.6040336630344391, 0.6756561162471771], time: 66.246
steps: 1724975, episodes: 69000, mean episode cvar: 0.47303948616981506, agent episode cvar: [0.34654580628871917, 0.3465195298194885, -0.10746840906143189, -0.11255744087696075], time: 66.246
Running avgs for agent 0: q_loss: 0.7832072973251343, u_loss: 9.209097862243652, p_loss: -0.13572491705417633, mean_rew: -0.25914841436663527, variance: 0.7397716421037912, cvar: 1.386183261871338, v: 1.0832868814468384, mean_q: 0.0076780663803219795, std_q: 2.725405693054199, lamda: 1.9061040878295898
Running avgs for agent 1: q_loss: 2.579977512359619, u_loss: 3.552095413208008, p_loss: 1.8351261615753174, mean_rew: -0.2959188540113581, variance: 5.412458721220493, cvar: 1.386078119277954, v: -0.2721996009349823, mean_q: -1.9202220439910889, std_q: 1.508729338645935, lamda: 2.0217103958129883
Running avgs for agent 2: q_loss: 0.013989350758492947, u_loss: 0.5424681305885315, p_loss: 1.0071020126342773, mean_rew: -0.3550234130760382, variance: 2.416134834289551, cvar: -0.42987361550331116, v: -0.43716827034950256, mean_q: -1.0302523374557495, std_q: 1.5750266313552856, lamda: 1.122768521308899
Running avgs for agent 3: q_loss: 0.02717234008014202, u_loss: 0.8127907514572144, p_loss: 0.9246042370796204, mean_rew: -0.35413185509417305, variance: 2.702624559402466, cvar: -0.4502297639846802, v: -0.4681017994880676, mean_q: -0.9357525706291199, std_q: 1.463700532913208, lamda: 1.1590365171432495

steps: 1749975, episodes: 70000, mean episode reward: -34.035635992319115, agent episode reward: [-4.290177805624308, -10.826245099838685, -9.171719531739527, -9.747493555116593], time: 65.964
steps: 1749975, episodes: 70000, mean episode variance: 2.923765458881855, agent episode variance: [0.06956822517514229, 1.6068876488506794, 0.611057130098343, 0.6362524547576904], time: 65.964
steps: 1749975, episodes: 70000, mean episode cvar: 0.4419985062479973, agent episode cvar: [0.3265436338186264, 0.3355167838335037, -0.10669548839330673, -0.11336642301082611], time: 65.965
Running avgs for agent 0: q_loss: 0.1725248247385025, u_loss: 6.789045810699463, p_loss: -0.11544301360845566, mean_rew: -0.2618997025185896, variance: 0.27827290070056915, cvar: 1.3061745166778564, v: 1.1363916397094727, mean_q: -0.01854564994573593, std_q: 2.8362743854522705, lamda: 1.9129037857055664
Running avgs for agent 1: q_loss: 2.2783589363098145, u_loss: 3.226072072982788, p_loss: 1.8679767847061157, mean_rew: -0.3020798860664406, variance: 6.4275505954027174, cvar: 1.3420671224594116, v: -0.35063043236732483, mean_q: -1.9540364742279053, std_q: 1.502944827079773, lamda: 2.0383121967315674
Running avgs for agent 2: q_loss: 0.014054547995328903, u_loss: 0.6120087504386902, p_loss: 1.0150480270385742, mean_rew: -0.35736474173968946, variance: 2.444228410720825, cvar: -0.42678195238113403, v: -0.4395439326763153, mean_q: -1.0392411947250366, std_q: 1.5860623121261597, lamda: 1.1227786540985107
Running avgs for agent 3: q_loss: 0.02481364831328392, u_loss: 0.7836700677871704, p_loss: 0.9185850024223328, mean_rew: -0.3567349027257113, variance: 2.5450098514556885, cvar: -0.45346570014953613, v: -0.466861367225647, mean_q: -0.9314574003219604, std_q: 1.4339520931243896, lamda: 1.1561282873153687

steps: 1774975, episodes: 71000, mean episode reward: -34.64780459828323, agent episode reward: [-4.85017182798643, -11.291138331116752, -9.443745061920593, -9.062749377259454], time: 65.94
steps: 1774975, episodes: 71000, mean episode variance: 2.995316367827356, agent episode variance: [0.012617248497903346, 1.7682409675121307, 0.6127978384494781, 0.6016603133678436], time: 65.941
steps: 1774975, episodes: 71000, mean episode cvar: 0.4581209056079388, agent episode cvar: [0.33394271063804626, 0.34381347239017485, -0.10791766348481178, -0.11171761393547058], time: 65.941
Running avgs for agent 0: q_loss: 0.10574488341808319, u_loss: 8.24507999420166, p_loss: -0.04641438275575638, mean_rew: -0.2612976095286439, variance: 0.050468993991613385, cvar: 1.335770845413208, v: 1.1802223920822144, mean_q: -0.11912574619054794, std_q: 2.913544178009033, lamda: 1.9152235984802246
Running avgs for agent 1: q_loss: 2.6609392166137695, u_loss: 3.0433106422424316, p_loss: 1.8014042377471924, mean_rew: -0.31019399893504224, variance: 7.072963870048523, cvar: 1.3752539157867432, v: -0.3518242835998535, mean_q: -1.8905386924743652, std_q: 1.4767155647277832, lamda: 2.0538010597229004
Running avgs for agent 2: q_loss: 0.013928358443081379, u_loss: 0.5944083333015442, p_loss: 1.0180177688598633, mean_rew: -0.35960146084639116, variance: 2.4511914253234863, cvar: -0.4316706657409668, v: -0.44161006808280945, mean_q: -1.0416747331619263, std_q: 1.5849236249923706, lamda: 1.124040126800537
Running avgs for agent 3: q_loss: 0.019629627466201782, u_loss: 0.6736730337142944, p_loss: 0.9297630786895752, mean_rew: -0.35785562758204686, variance: 2.4066410064697266, cvar: -0.44687044620513916, v: -0.45608916878700256, mean_q: -0.9450736045837402, std_q: 1.4399387836456299, lamda: 1.1570650339126587

steps: 1799975, episodes: 72000, mean episode reward: -34.7468086353158, agent episode reward: [-4.914605392182914, -10.510244124383503, -9.546616714424657, -9.775342404324723], time: 65.987
steps: 1799975, episodes: 72000, mean episode variance: 3.228150980450213, agent episode variance: [0.008657659448683262, 2.0049853168129923, 0.6062153007984161, 0.6082927033901214], time: 65.987
steps: 1799975, episodes: 72000, mean episode cvar: 0.46164305266737937, agent episode cvar: [0.3358654141426086, 0.3422360496520996, -0.10683831813931464, -0.10962009298801421], time: 65.988
Running avgs for agent 0: q_loss: 0.10517694056034088, u_loss: 7.2879862785339355, p_loss: -0.01876525767147541, mean_rew: -0.262857344389894, variance: 0.03463063779473305, cvar: 1.3434616327285767, v: 1.1942018270492554, mean_q: -0.1658569574356079, std_q: 2.966205596923828, lamda: 1.9168119430541992
Running avgs for agent 1: q_loss: 4.260718822479248, u_loss: 2.7376132011413574, p_loss: 1.6129136085510254, mean_rew: -0.31690382516540444, variance: 8.01994126725197, cvar: 1.3689440488815308, v: -0.33308643102645874, mean_q: -1.6954076290130615, std_q: 1.4178928136825562, lamda: 2.071913957595825
Running avgs for agent 2: q_loss: 0.014622722752392292, u_loss: 0.776823878288269, p_loss: 1.015230655670166, mean_rew: -0.35990831070718404, variance: 2.424861431121826, cvar: -0.42735326290130615, v: -0.44121313095092773, mean_q: -1.0393208265304565, std_q: 1.5693368911743164, lamda: 1.1240180730819702
Running avgs for agent 3: q_loss: 0.01728253997862339, u_loss: 0.8359696865081787, p_loss: 0.9599016308784485, mean_rew: -0.35907824623083573, variance: 2.433170795440674, cvar: -0.4384803771972656, v: -0.4518723785877228, mean_q: -0.9767761826515198, std_q: 1.4751667976379395, lamda: 1.1587368249893188

steps: 1824975, episodes: 73000, mean episode reward: -33.34202618797333, agent episode reward: [-4.5596660916558776, -9.059133184639446, -9.692586914468949, -10.03063999720906], time: 66.011
steps: 1824975, episodes: 73000, mean episode variance: 3.3441772687248887, agent episode variance: [0.009337192058563232, 2.070990757327527, 0.6190701760053635, 0.6447791433334351], time: 66.012
steps: 1824975, episodes: 73000, mean episode cvar: 0.4975069258511066, agent episode cvar: [0.3349406603574753, 0.3782549386620522, -0.10636059898138046, -0.10932807418704032], time: 66.012
Running avgs for agent 0: q_loss: 0.15341557562351227, u_loss: 10.232894897460938, p_loss: -0.008570894598960876, mean_rew: -0.26434960030563603, variance: 0.03734876823425293, cvar: 1.3397626876831055, v: 1.15163254737854, mean_q: -0.18844744563102722, std_q: 3.028089761734009, lamda: 1.920166015625
Running avgs for agent 1: q_loss: 6.700157165527344, u_loss: 2.6478676795959473, p_loss: 1.320901870727539, mean_rew: -0.32152196989768883, variance: 8.283963029310108, cvar: 1.5130198001861572, v: -0.21152161061763763, mean_q: -1.4063563346862793, std_q: 1.4346164464950562, lamda: 2.089144468307495
Running avgs for agent 2: q_loss: 0.013334615156054497, u_loss: 0.6828019022941589, p_loss: 1.0204495191574097, mean_rew: -0.36159509410775525, variance: 2.476280689239502, cvar: -0.4254424273967743, v: -0.4334525167942047, mean_q: -1.050701379776001, std_q: 1.5749592781066895, lamda: 1.1229727268218994
Running avgs for agent 3: q_loss: 0.018115190789103508, u_loss: 0.8843796253204346, p_loss: 0.9681415557861328, mean_rew: -0.3590626614605582, variance: 2.5791165828704834, cvar: -0.4373123049736023, v: -0.4528571665287018, mean_q: -0.9867655634880066, std_q: 1.4998096227645874, lamda: 1.1584938764572144

steps: 1849975, episodes: 74000, mean episode reward: -34.83795169145882, agent episode reward: [-6.895295282770567, -8.354531475910305, -8.552925345161183, -11.035199587616757], time: 66.132
steps: 1849975, episodes: 74000, mean episode variance: 3.136018601812422, agent episode variance: [0.006484742157161236, 1.8035095371454954, 0.6312562301158905, 0.6947680923938752], time: 66.133
steps: 1849975, episodes: 74000, mean episode cvar: 0.5034302693009377, agent episode cvar: [0.3330024966001511, 0.38476296746730804, -0.10304267910122872, -0.11129251566529275], time: 66.133
Running avgs for agent 0: q_loss: 0.16475671529769897, u_loss: 10.160137176513672, p_loss: -0.017153197899460793, mean_rew: -0.26427771423386875, variance: 0.025938968628644945, cvar: 1.33201003074646, v: 1.1256287097930908, mean_q: -0.17589502036571503, std_q: 2.974961996078491, lamda: 1.9250408411026
Running avgs for agent 1: q_loss: 7.370068073272705, u_loss: 2.739274024963379, p_loss: 1.242568016052246, mean_rew: -0.32711365449032875, variance: 7.214038148581982, cvar: 1.5390517711639404, v: -0.13810153305530548, mean_q: -1.3374123573303223, std_q: 1.4752323627471924, lamda: 2.104978561401367
Running avgs for agent 2: q_loss: 0.011600128374993801, u_loss: 0.6771434545516968, p_loss: 1.031488060951233, mean_rew: -0.3632334111788275, variance: 2.525024890899658, cvar: -0.4121707081794739, v: -0.4170086681842804, mean_q: -1.071743369102478, std_q: 1.5700397491455078, lamda: 1.1244418621063232
Running avgs for agent 3: q_loss: 0.02152712643146515, u_loss: 0.8592602014541626, p_loss: 0.9664197564125061, mean_rew: -0.36239012832892203, variance: 2.7790725231170654, cvar: -0.4451700448989868, v: -0.4595499336719513, mean_q: -0.9846166968345642, std_q: 1.5171960592269897, lamda: 1.1564992666244507

steps: 1874975, episodes: 75000, mean episode reward: -35.99690076970166, agent episode reward: [-7.437449871457606, -8.464793456269254, -8.305685302922608, -11.788972139052197], time: 66.058
steps: 1874975, episodes: 75000, mean episode variance: 2.621791614547372, agent episode variance: [0.006828989252448082, 1.2082620425522328, 0.6587451331615448, 0.7479554495811462], time: 66.058
steps: 1874975, episodes: 75000, mean episode cvar: 0.5845293311476707, agent episode cvar: [0.3357117567062378, 0.46610601246356964, -0.10324896264076233, -0.11403947538137436], time: 66.059
Running avgs for agent 0: q_loss: 0.171905517578125, u_loss: 10.272069931030273, p_loss: -0.06746406853199005, mean_rew: -0.269129590290669, variance: 0.02731595700979233, cvar: 1.3428469896316528, v: 1.1432358026504517, mean_q: -0.11083880811929703, std_q: 3.006709575653076, lamda: 1.9319050312042236
Running avgs for agent 1: q_loss: 18.308870315551758, u_loss: 2.7703373432159424, p_loss: 1.5563640594482422, mean_rew: -0.33164371120080144, variance: 4.833048170208931, cvar: 1.8644241094589233, v: 0.05902869626879692, mean_q: -1.6627960205078125, std_q: 1.6425023078918457, lamda: 2.122879981994629
Running avgs for agent 2: q_loss: 0.011251785792410374, u_loss: 0.7102242708206177, p_loss: 1.0522902011871338, mean_rew: -0.36574032827387293, variance: 2.6349804401397705, cvar: -0.412995845079422, v: -0.41820135712623596, mean_q: -1.0933430194854736, std_q: 1.5830610990524292, lamda: 1.127505898475647
Running avgs for agent 3: q_loss: 0.025398127734661102, u_loss: 0.9017766118049622, p_loss: 0.9395951628684998, mean_rew: -0.3630157974531264, variance: 2.991821765899658, cvar: -0.4561578929424286, v: -0.46753886342048645, mean_q: -0.9556082487106323, std_q: 1.4713674783706665, lamda: 1.1530495882034302

steps: 1899975, episodes: 76000, mean episode reward: -33.7449292384361, agent episode reward: [-6.297392618799562, -7.498900257341224, -8.079452906017528, -11.869183456277794], time: 65.992
steps: 1899975, episodes: 76000, mean episode variance: 2.831450314134359, agent episode variance: [0.010321632415056228, 1.3438038290143013, 0.6790276839733124, 0.7982971687316894], time: 65.992
steps: 1899975, episodes: 76000, mean episode cvar: 0.5099135236144066, agent episode cvar: [0.34389482736587523, 0.38642841786146165, -0.1047932263314724, -0.1156164952814579], time: 65.993
Running avgs for agent 0: q_loss: 0.17175932228565216, u_loss: 10.398638725280762, p_loss: -0.09913318604230881, mean_rew: -0.27336047068477215, variance: 0.04128652966022491, cvar: 1.3755793571472168, v: 1.179049015045166, mean_q: -0.09811941534280777, std_q: 2.9339113235473633, lamda: 1.9381732940673828
Running avgs for agent 1: q_loss: 7.6947021484375, u_loss: 2.5832197666168213, p_loss: 1.3938204050064087, mean_rew: -0.3344440323737308, variance: 5.375215316057205, cvar: 1.5457136631011963, v: -0.05524202808737755, mean_q: -1.4946221113204956, std_q: 1.5782164335250854, lamda: 2.1390652656555176
Running avgs for agent 2: q_loss: 0.01232284028083086, u_loss: 0.689846396446228, p_loss: 1.0455141067504883, mean_rew: -0.36526621776167373, variance: 2.7161107063293457, cvar: -0.41917288303375244, v: -0.42712077498435974, mean_q: -1.08280611038208, std_q: 1.5824105739593506, lamda: 1.1295080184936523
Running avgs for agent 3: q_loss: 0.026794807985424995, u_loss: 0.9222655296325684, p_loss: 0.930852472782135, mean_rew: -0.365188099443514, variance: 3.193188428878784, cvar: -0.4624660015106201, v: -0.4762323498725891, mean_q: -0.9447063207626343, std_q: 1.4552602767944336, lamda: 1.151733636856079

steps: 1924975, episodes: 77000, mean episode reward: -33.01470244634041, agent episode reward: [-4.639617152564224, -8.022356158200234, -9.380983899388244, -10.971745236187717], time: 66.06
steps: 1924975, episodes: 77000, mean episode variance: 2.925607696682215, agent episode variance: [0.04671003331243992, 1.3593803403526545, 0.6956611506938934, 0.8238561723232269], time: 66.061
steps: 1924975, episodes: 77000, mean episode cvar: 0.5689981445670128, agent episode cvar: [0.3549881457090378, 0.43594410330057143, -0.10829994654655456, -0.11363415789604187], time: 66.061
Running avgs for agent 0: q_loss: 0.23420286178588867, u_loss: 10.366894721984863, p_loss: -0.09250137209892273, mean_rew: -0.27604858015871564, variance: 0.18684013324975968, cvar: 1.419952630996704, v: 1.1703083515167236, mean_q: -0.10384348779916763, std_q: 2.8093388080596924, lamda: 1.9487292766571045
Running avgs for agent 1: q_loss: 17.3681583404541, u_loss: 2.3500912189483643, p_loss: 1.2800530195236206, mean_rew: -0.33897169683568634, variance: 5.437521361410618, cvar: 1.7437764406204224, v: -0.020647602155804634, mean_q: -1.3844043016433716, std_q: 1.6346721649169922, lamda: 2.1566524505615234
Running avgs for agent 2: q_loss: 0.015192712657153606, u_loss: 0.6844645142555237, p_loss: 1.0315548181533813, mean_rew: -0.3671665123585516, variance: 2.782644510269165, cvar: -0.43319976329803467, v: -0.4437301456928253, mean_q: -1.0641030073165894, std_q: 1.593165397644043, lamda: 1.131233811378479
Running avgs for agent 3: q_loss: 0.02326282300055027, u_loss: 0.8438090682029724, p_loss: 0.9339818358421326, mean_rew: -0.3668544780161938, variance: 3.295424461364746, cvar: -0.4545366168022156, v: -0.46490809321403503, mean_q: -0.9521366953849792, std_q: 1.437930703163147, lamda: 1.1523349285125732

steps: 1949975, episodes: 78000, mean episode reward: -33.25729523288835, agent episode reward: [-4.570142159676465, -8.97642135984376, -10.129410747080847, -9.58132096628729], time: 66.304
steps: 1949975, episodes: 78000, mean episode variance: 2.9042679688036444, agent episode variance: [0.07966808991134167, 1.3211403897851706, 0.7164586715698242, 0.7870008175373078], time: 66.305
steps: 1949975, episodes: 78000, mean episode cvar: 0.5278317841291428, agent episode cvar: [0.34465388798713686, 0.4063989019393921, -0.11117257219552994, -0.11204843360185623], time: 66.305
Running avgs for agent 0: q_loss: 0.1451454609632492, u_loss: 9.400096893310547, p_loss: -0.05872577801346779, mean_rew: -0.2754246162434689, variance: 0.31867235964536667, cvar: 1.3786156177520752, v: 1.1127897500991821, mean_q: -0.11923372000455856, std_q: 2.6112122535705566, lamda: 1.9567139148712158
Running avgs for agent 1: q_loss: 9.813251495361328, u_loss: 2.3431003093719482, p_loss: 1.4469621181488037, mean_rew: -0.34312435008932723, variance: 5.284561559140682, cvar: 1.6255955696105957, v: 0.008212699554860592, mean_q: -1.5508217811584473, std_q: 1.6056886911392212, lamda: 2.174302577972412
Running avgs for agent 2: q_loss: 0.017840605229139328, u_loss: 0.6937264204025269, p_loss: 1.0031659603118896, mean_rew: -0.36844572088435507, variance: 2.8658347129821777, cvar: -0.4446903169155121, v: -0.45688560605049133, mean_q: -1.0292699337005615, std_q: 1.5671050548553467, lamda: 1.1328444480895996
Running avgs for agent 3: q_loss: 0.018866488710045815, u_loss: 0.8300672173500061, p_loss: 0.9560105204582214, mean_rew: -0.3684349582657726, variance: 3.148003101348877, cvar: -0.4481937289237976, v: -0.45384669303894043, mean_q: -0.9763786792755127, std_q: 1.4543062448501587, lamda: 1.1527146100997925

steps: 1974975, episodes: 79000, mean episode reward: -35.20673720216486, agent episode reward: [-4.375044349502579, -10.285652127946246, -11.05397068566498, -9.492070039051058], time: 66.278
steps: 1974975, episodes: 79000, mean episode variance: 3.3580928940698502, agent episode variance: [0.11184858956187964, 1.7661280990540982, 0.750767627954483, 0.7293485774993896], time: 66.279
steps: 1974975, episodes: 79000, mean episode cvar: 0.5072209032177926, agent episode cvar: [0.3403036891222, 0.39182819813489916, -0.11407037070393562, -0.11084061333537101], time: 66.279
Running avgs for agent 0: q_loss: 0.13699433207511902, u_loss: 10.578017234802246, p_loss: 0.02673579752445221, mean_rew: -0.2762262676585375, variance: 0.44739435824751855, cvar: 1.3612147569656372, v: 1.0229451656341553, mean_q: -0.1901230812072754, std_q: 2.632235288619995, lamda: 1.9628987312316895
Running avgs for agent 1: q_loss: 11.296388626098633, u_loss: 2.163270950317383, p_loss: 1.0943304300308228, mean_rew: -0.35022837037467475, variance: 7.064512396216393, cvar: 1.5673127174377441, v: 0.10645247995853424, mean_q: -1.1926308870315552, std_q: 1.4260002374649048, lamda: 2.1907899379730225
Running avgs for agent 2: q_loss: 0.02083536982536316, u_loss: 0.7104557156562805, p_loss: 0.9718910455703735, mean_rew: -0.36913897633109066, variance: 3.003070592880249, cvar: -0.45628148317337036, v: -0.4689187705516815, mean_q: -0.9931440353393555, std_q: 1.5212512016296387, lamda: 1.1307470798492432
Running avgs for agent 3: q_loss: 0.017391204833984375, u_loss: 0.790594220161438, p_loss: 0.968437135219574, mean_rew: -0.36661835358001793, variance: 2.9173946380615234, cvar: -0.44336244463920593, v: -0.4535885155200958, mean_q: -0.9906594157218933, std_q: 1.4731230735778809, lamda: 1.1547906398773193

steps: 1999975, episodes: 80000, mean episode reward: -36.032496841988, agent episode reward: [-4.406549093314154, -11.611362967390592, -10.97593255062847, -9.038652230654792], time: 66.046
steps: 1999975, episodes: 80000, mean episode variance: 3.1737056736424565, agent episode variance: [0.09158241081982851, 1.5918521131873131, 0.7924876494407653, 0.6977835001945496], time: 66.046
steps: 1999975, episodes: 80000, mean episode cvar: 0.6369699291586876, agent episode cvar: [0.3392226153612137, 0.525217164516449, -0.11616570341587067, -0.1113041473031044], time: 66.047
Running avgs for agent 0: q_loss: 0.14546237885951996, u_loss: 8.141070365905762, p_loss: 0.12790226936340332, mean_rew: -0.277900444879596, variance: 0.36632964327931405, cvar: 1.3568906784057617, v: 0.9575352072715759, mean_q: -0.2861722409725189, std_q: 2.663935661315918, lamda: 1.9689946174621582/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

Running avgs for agent 1: q_loss: 31.724855422973633, u_loss: 2.073241949081421, p_loss: 1.2294836044311523, mean_rew: -0.3554274018587981, variance: 6.3674084527492525, cvar: 2.1008687019348145, v: 0.20907281339168549, mean_q: -1.3287336826324463, std_q: 1.467535138130188, lamda: 2.208158254623413
Running avgs for agent 2: q_loss: 0.022398926317691803, u_loss: 0.8060205578804016, p_loss: 0.955683708190918, mean_rew: -0.37058283635410405, variance: 3.1699507236480713, cvar: -0.4646628201007843, v: -0.47798264026641846, mean_q: -0.9715297818183899, std_q: 1.480926513671875, lamda: 1.1319142580032349
Running avgs for agent 3: q_loss: 0.01852971874177456, u_loss: 0.7416937351226807, p_loss: 0.9741489887237549, mean_rew: -0.36661075042770697, variance: 2.7911341190338135, cvar: -0.4452166259288788, v: -0.4579482972621918, mean_q: -0.9938305616378784, std_q: 1.4878222942352295, lamda: 1.1569896936416626

...Finished total of 80001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -36.19869130555552, agent episode reward: [-4.209487978173165, -11.810487118858527, -10.974200755929179, -9.204515452594645], time: 42.099
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 42.099
steps: 24975, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0, 0.0], time: 42.1
Running avgs for agent 0: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -36.094221131467854, agent episode reward: [-4.082675489341707, -11.905990333560004, -10.982008750507932, -9.12354655805821], time: 55.774
steps: 49975, episodes: 2000, mean episode variance: 3.9958055009841917, agent episode variance: [0.0, 2.3667584266662596, 0.9749680693149567, 0.6540790050029754], time: 55.774
steps: 49975, episodes: 2000, mean episode cvar: 0.8074626369774341, agent episode cvar: [0.38194947707653043, 0.6480275150537491, -0.11541064363718033, -0.10710371151566506], time: 55.774
Running avgs for agent 0: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.16668960827429977, variance: 0.0, cvar: 1.5653668642044067, v: 1.071969985961914, mean_q: 0.008253290317952633, std_q: 1.4405807256698608, lamda: 1.973867416381836
Running avgs for agent 1: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.4767831428878098, variance: 9.6998291015625, cvar: 2.6558501720428467, v: 1.2743520736694336, mean_q: 0.5290324687957764, std_q: 2.0335259437561035, lamda: 2.2152726650238037
Running avgs for agent 2: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.4402527616394407, variance: 3.9957709312438965, cvar: -0.47299444675445557, v: -0.4830244779586792, mean_q: -0.9643654823303223, std_q: 1.490151286125183, lamda: 1.1321468353271484
Running avgs for agent 3: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.3656901471536188, variance: 2.680651903152466, cvar: -0.4389496445655823, v: -0.4462642967700958, mean_q: -0.9377957582473755, std_q: 1.4526301622390747, lamda: 1.1552478075027466

...Finished total of 2001 episodes with the fixed policy.
