# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 15.0 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies12/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies12/02-non-linear-exp_var/
Job <1092102> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc253>>
arglist.u_estimation True
2019-09-06 05:17:50.191369: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -522.3024375268656, agent episode reward: [-174.10081250895516, -174.10081250895516, -174.10081250895516], time: 151.833
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 151.834
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -659.5434282928992, agent episode reward: [-219.84780943096638, -219.84780943096638, -219.84780943096638], time: 178.48
steps: 49975, episodes: 2000, mean episode variance: 8.634360555648804, agent episode variance: [2.849862382888794, 2.9644170470237734, 2.8200811257362366], time: 178.481
Running avgs for agent 0: q_loss: 74.49181365966797, p_loss: -5.333824157714844, mean_rew: -7.483609349551492, variance: 11.679763864298335, lamda: 1.0091301202774048
Running avgs for agent 1: q_loss: 90.6213150024414, p_loss: -5.361191272735596, mean_rew: -7.484921328327725, variance: 12.149250030517578, lamda: 1.009124517440796
Running avgs for agent 2: q_loss: 70.12519073486328, p_loss: -5.374441623687744, mean_rew: -7.483009606097519, variance: 11.557709531705887, lamda: 1.0090398788452148

steps: 74975, episodes: 3000, mean episode reward: -817.3210783489724, agent episode reward: [-272.4403594496574, -272.4403594496574, -272.4403594496574], time: 173.035
steps: 74975, episodes: 3000, mean episode variance: 6.219763291597366, agent episode variance: [2.0553358232975008, 2.086197764873505, 2.078229703426361], time: 173.036
Running avgs for agent 0: q_loss: 38.5157585144043, p_loss: -4.639094352722168, mean_rew: -8.474445039295436, variance: 8.221343040466309, lamda: 1.0317878723144531
Running avgs for agent 1: q_loss: 37.43860626220703, p_loss: -4.71115255355835, mean_rew: -8.48797620392408, variance: 8.344792366027832, lamda: 1.0322744846343994
Running avgs for agent 2: q_loss: 38.896331787109375, p_loss: -4.762584209442139, mean_rew: -8.484240998410298, variance: 8.312918663024902, lamda: 1.0312856435775757

steps: 99975, episodes: 4000, mean episode reward: -776.6980549775993, agent episode reward: [-258.89935165919974, -258.89935165919974, -258.89935165919974], time: 179.682
steps: 99975, episodes: 4000, mean episode variance: 6.398460258483887, agent episode variance: [2.1326557569503786, 2.138481629371643, 2.1273228721618653], time: 179.682
Running avgs for agent 0: q_loss: 43.81199264526367, p_loss: -4.730673789978027, mean_rew: -9.111459448125315, variance: 8.530622482299805, lamda: 1.0567920207977295
Running avgs for agent 1: q_loss: 48.4055061340332, p_loss: -4.651384353637695, mean_rew: -9.121772452185581, variance: 8.553926467895508, lamda: 1.0572752952575684
Running avgs for agent 2: q_loss: 47.16917419433594, p_loss: -4.68635892868042, mean_rew: -9.104808549401394, variance: 8.50929069519043, lamda: 1.0562978982925415

steps: 124975, episodes: 5000, mean episode reward: -826.1223677533947, agent episode reward: [-275.37412258446494, -275.37412258446494, -275.37412258446494], time: 177.648
steps: 124975, episodes: 5000, mean episode variance: 6.557655737638473, agent episode variance: [2.1824398725032808, 2.1937181351184845, 2.181497730016708], time: 177.649
Running avgs for agent 0: q_loss: 60.15580749511719, p_loss: -4.874597549438477, mean_rew: -9.505680213558374, variance: 8.72976016998291, lamda: 1.0817960500717163
Running avgs for agent 1: q_loss: 48.15345764160156, p_loss: -4.812000274658203, mean_rew: -9.486797709538799, variance: 8.774872779846191, lamda: 1.0822234153747559
Running avgs for agent 2: q_loss: 47.66066360473633, p_loss: -4.784802436828613, mean_rew: -9.500848300860888, variance: 8.725990295410156, lamda: 1.081281304359436

steps: 149975, episodes: 6000, mean episode reward: -860.6807514164711, agent episode reward: [-286.8935838054904, -286.8935838054904, -286.8935838054904], time: 176.954
steps: 149975, episodes: 6000, mean episode variance: 6.631616991758347, agent episode variance: [2.1994619228839873, 2.2045300188064574, 2.2276250500679016], time: 176.955
Running avgs for agent 0: q_loss: 49.06315612792969, p_loss: -5.0127763748168945, mean_rew: -9.783620198256383, variance: 8.797847747802734, lamda: 1.1066149473190308
Running avgs for agent 1: q_loss: 35.191219329833984, p_loss: -4.930872917175293, mean_rew: -9.772440741864354, variance: 8.818120002746582, lamda: 1.1056501865386963
Running avgs for agent 2: q_loss: 33.744815826416016, p_loss: -4.94185733795166, mean_rew: -9.76941467740641, variance: 8.910499572753906, lamda: 1.1041417121887207

steps: 174975, episodes: 7000, mean episode reward: -853.138307647792, agent episode reward: [-284.37943588259736, -284.37943588259736, -284.37943588259736], time: 174.448
steps: 174975, episodes: 7000, mean episode variance: 6.738041326522827, agent episode variance: [2.2197686462402344, 2.2319071016311645, 2.286365578651428], time: 174.449
Running avgs for agent 0: q_loss: 52.545494079589844, p_loss: -5.1468119621276855, mean_rew: -10.041117043044455, variance: 8.879075050354004, lamda: 1.1314035654067993
Running avgs for agent 1: q_loss: 35.78983688354492, p_loss: -5.069523334503174, mean_rew: -10.025502918953922, variance: 8.927628517150879, lamda: 1.124136209487915
Running avgs for agent 2: q_loss: 48.62613296508789, p_loss: -5.076048851013184, mean_rew: -10.03202798472808, variance: 9.145462989807129, lamda: 1.1160495281219482

steps: 199975, episodes: 8000, mean episode reward: -864.9203030670236, agent episode reward: [-288.3067676890079, -288.3067676890079, -288.3067676890079], time: 175.215
steps: 199975, episodes: 8000, mean episode variance: 6.768291858196259, agent episode variance: [2.2205297837257385, 2.238176064014435, 2.3095860104560852], time: 175.215
Running avgs for agent 0: q_loss: 50.61320495605469, p_loss: -5.173592567443848, mean_rew: -10.219625483916952, variance: 8.882120132446289, lamda: 1.156292200088501
Running avgs for agent 1: q_loss: 40.340946197509766, p_loss: -5.17365026473999, mean_rew: -10.234761587249878, variance: 8.952704429626465, lamda: 1.1442443132400513
Running avgs for agent 2: q_loss: 43.56576156616211, p_loss: -5.182247161865234, mean_rew: -10.227375010217719, variance: 9.238344192504883, lamda: 1.122267723083496

steps: 224975, episodes: 9000, mean episode reward: -861.8940553165316, agent episode reward: [-287.2980184388439, -287.2980184388439, -287.2980184388439], time: 172.773
steps: 224975, episodes: 9000, mean episode variance: 6.759629007339478, agent episode variance: [2.1893307499885557, 2.258477568626404, 2.311820688724518], time: 172.774
Running avgs for agent 0: q_loss: 69.43278503417969, p_loss: -5.265275478363037, mean_rew: -10.383164382796123, variance: 8.757322311401367, lamda: 1.1803089380264282
Running avgs for agent 1: q_loss: 37.68205261230469, p_loss: -5.25969934463501, mean_rew: -10.387756301865906, variance: 9.033909797668457, lamda: 1.158720850944519
Running avgs for agent 2: q_loss: 30.363903045654297, p_loss: -5.250472068786621, mean_rew: -10.391682552676317, variance: 9.247282981872559, lamda: 1.1333529949188232

steps: 249975, episodes: 10000, mean episode reward: -848.6982802435415, agent episode reward: [-282.8994267478472, -282.8994267478472, -282.8994267478472], time: 174.588
steps: 249975, episodes: 10000, mean episode variance: 6.737161854743958, agent episode variance: [2.1892208280563357, 2.2558052091598513, 2.292135817527771], time: 174.589
Running avgs for agent 0: q_loss: 70.35823059082031, p_loss: -5.317994594573975, mean_rew: -10.488322662956637, variance: 8.756882667541504, lamda: 1.2004921436309814
Running avgs for agent 1: q_loss: 34.55707931518555, p_loss: -5.309784412384033, mean_rew: -10.48032151494809, variance: 9.023221015930176, lamda: 1.1680890321731567
Running avgs for agent 2: q_loss: 30.63001251220703, p_loss: -5.321319103240967, mean_rew: -10.512105476528996, variance: 9.168542861938477, lamda: 1.1451798677444458

steps: 274975, episodes: 11000, mean episode reward: -880.4916869251978, agent episode reward: [-293.4972289750659, -293.4972289750659, -293.4972289750659], time: 176.556
steps: 274975, episodes: 11000, mean episode variance: 6.731689602613449, agent episode variance: [2.1586676397323608, 2.277100426673889, 2.295921536207199], time: 176.556
Running avgs for agent 0: q_loss: 67.6720962524414, p_loss: -5.353847980499268, mean_rew: -10.58180372254518, variance: 8.63467025756836, lamda: 1.2177280187606812
Running avgs for agent 1: q_loss: 34.848880767822266, p_loss: -5.3553643226623535, mean_rew: -10.58313129036184, variance: 9.10840129852295, lamda: 1.1771154403686523
Running avgs for agent 2: q_loss: 50.142967224121094, p_loss: -5.336389064788818, mean_rew: -10.578960695071837, variance: 9.183686256408691, lamda: 1.154672622680664

steps: 299975, episodes: 12000, mean episode reward: -952.9094477807074, agent episode reward: [-317.63648259356916, -317.63648259356916, -317.63648259356916], time: 169.285
steps: 299975, episodes: 12000, mean episode variance: 6.855412935733795, agent episode variance: [2.1927379794120787, 2.280204459667206, 2.3824704966545105], time: 169.285
Running avgs for agent 0: q_loss: 41.84816360473633, p_loss: -5.393133640289307, mean_rew: -10.717345164812523, variance: 8.770951271057129, lamda: 1.238163709640503
Running avgs for agent 1: q_loss: 36.4184455871582, p_loss: -5.410440444946289, mean_rew: -10.718708614757462, variance: 9.120818138122559, lamda: 1.1902192831039429
Running avgs for agent 2: q_loss: 50.677772521972656, p_loss: -5.387001037597656, mean_rew: -10.718966009462914, variance: 9.529881477355957, lamda: 1.1561719179153442

steps: 324975, episodes: 13000, mean episode reward: -954.1871655526236, agent episode reward: [-318.06238851754125, -318.06238851754125, -318.06238851754125], time: 165.376
steps: 324975, episodes: 13000, mean episode variance: 6.808097182989121, agent episode variance: [2.176048891067505, 2.2679829103946685, 2.364065381526947], time: 165.377
Running avgs for agent 0: q_loss: 44.11735534667969, p_loss: -5.505081653594971, mean_rew: -10.903006094610497, variance: 8.704195976257324, lamda: 1.257205843925476
Running avgs for agent 1: q_loss: 36.802467346191406, p_loss: -5.509397506713867, mean_rew: -10.8959820697476, variance: 9.071931838989258, lamda: 1.2046700716018677
Running avgs for agent 2: q_loss: 51.617218017578125, p_loss: -5.499080657958984, mean_rew: -10.889324632404461, variance: 9.45626163482666, lamda: 1.1572524309158325

steps: 349975, episodes: 14000, mean episode reward: -942.4788002116163, agent episode reward: [-314.15960007053883, -314.15960007053883, -314.15960007053883], time: 164.305
steps: 349975, episodes: 14000, mean episode variance: 6.881280747652053, agent episode variance: [2.1902494411468507, 2.276709730386734, 2.414321576118469], time: 164.306
Running avgs for agent 0: q_loss: 50.80113983154297, p_loss: -5.546518325805664, mean_rew: -11.01777060053693, variance: 8.760997772216797, lamda: 1.2702746391296387
Running avgs for agent 1: q_loss: 50.90462875366211, p_loss: -5.54241418838501, mean_rew: -11.00974485626429, variance: 9.106839179992676, lamda: 1.2133064270019531
Running avgs for agent 2: q_loss: 53.76785659790039, p_loss: -5.54596471786499, mean_rew: -11.024768380493104, variance: 9.657285690307617, lamda: 1.1584124565124512

steps: 374975, episodes: 15000, mean episode reward: -912.7205938128454, agent episode reward: [-304.2401979376151, -304.2401979376151, -304.2401979376151], time: 165.801
steps: 374975, episodes: 15000, mean episode variance: 6.9247261528968815, agent episode variance: [2.1978466584682463, 2.3092530148029327, 2.417626479625702], time: 165.801
Running avgs for agent 0: q_loss: 69.21440887451172, p_loss: -5.5838799476623535, mean_rew: -11.110384030475108, variance: 8.791386604309082, lamda: 1.2796595096588135
Running avgs for agent 1: q_loss: 61.802757263183594, p_loss: -5.587632179260254, mean_rew: -11.123272928545221, variance: 9.23701286315918, lamda: 1.2183310985565186
Running avgs for agent 2: q_loss: 52.59495162963867, p_loss: -5.5811614990234375, mean_rew: -11.128956591547874, variance: 9.67050552368164, lamda: 1.1597354412078857

steps: 399975, episodes: 16000, mean episode reward: -888.9234076975085, agent episode reward: [-296.3078025658362, -296.3078025658362, -296.3078025658362], time: 163.302
steps: 399975, episodes: 16000, mean episode variance: 6.878703727722168, agent episode variance: [2.1949783787727357, 2.2902866151332857, 2.3934387338161467], time: 163.303
Running avgs for agent 0: q_loss: 60.150718688964844, p_loss: -5.601243495941162, mean_rew: -11.150020790971165, variance: 8.779913902282715, lamda: 1.2862725257873535
Running avgs for agent 1: q_loss: 56.03125762939453, p_loss: -5.6027703285217285, mean_rew: -11.169452841192149, variance: 9.16114616394043, lamda: 1.2217339277267456
Running avgs for agent 2: q_loss: 46.76284408569336, p_loss: -5.577956199645996, mean_rew: -11.145065659112799, variance: 9.573755264282227, lamda: 1.1685175895690918

steps: 424975, episodes: 17000, mean episode reward: -906.0458003861625, agent episode reward: [-302.01526679538756, -302.01526679538756, -302.01526679538756], time: 161.907
steps: 424975, episodes: 17000, mean episode variance: 6.852364792108536, agent episode variance: [2.1568860073089597, 2.301888859510422, 2.393589925289154], time: 161.907
Running avgs for agent 0: q_loss: 41.746421813964844, p_loss: -5.637530326843262, mean_rew: -11.234942585234755, variance: 8.627544403076172, lamda: 1.3023688793182373
Running avgs for agent 1: q_loss: 54.5921630859375, p_loss: -5.63059139251709, mean_rew: -11.243628532870726, variance: 9.207554817199707, lamda: 1.2280207872390747
Running avgs for agent 2: q_loss: 42.298492431640625, p_loss: -5.628575325012207, mean_rew: -11.22944018959999, variance: 9.574358940124512, lamda: 1.1760808229446411

steps: 449975, episodes: 18000, mean episode reward: -928.9271745714362, agent episode reward: [-309.6423915238121, -309.6423915238121, -309.6423915238121], time: 167.384
steps: 449975, episodes: 18000, mean episode variance: 6.772264702081681, agent episode variance: [2.12225563955307, 2.2830226140022276, 2.3669864485263825], time: 167.385
Running avgs for agent 0: q_loss: 41.15325927734375, p_loss: -5.656383514404297, mean_rew: -11.268434686591776, variance: 8.489022254943848, lamda: 1.3190908432006836
Running avgs for agent 1: q_loss: 51.251983642578125, p_loss: -5.662385940551758, mean_rew: -11.281498922089233, variance: 9.132089614868164, lamda: 1.2398993968963623
Running avgs for agent 2: q_loss: 40.38539505004883, p_loss: -5.659613132476807, mean_rew: -11.261644972922015, variance: 9.467945098876953, lamda: 1.1909914016723633

steps: 474975, episodes: 19000, mean episode reward: -1005.4457536031134, agent episode reward: [-335.14858453437114, -335.14858453437114, -335.14858453437114], time: 164.457
steps: 474975, episodes: 19000, mean episode variance: 6.786622224330902, agent episode variance: [2.123814297199249, 2.31399232006073, 2.348815607070923], time: 164.458
Running avgs for agent 0: q_loss: 38.11503601074219, p_loss: -5.719624042510986, mean_rew: -11.365158959267422, variance: 8.495256423950195, lamda: 1.335190773010254
Running avgs for agent 1: q_loss: 51.65776443481445, p_loss: -5.701319217681885, mean_rew: -11.369102175904413, variance: 9.255969047546387, lamda: 1.2467665672302246
Running avgs for agent 2: q_loss: 36.668418884277344, p_loss: -5.674261569976807, mean_rew: -11.3762088667399, variance: 9.395262718200684, lamda: 1.205446481704712

steps: 499975, episodes: 20000, mean episode reward: -968.3352830384939, agent episode reward: [-322.778427679498, -322.778427679498, -322.778427679498], time: 158.477
steps: 499975, episodes: 20000, mean episode variance: 6.767151369571685, agent episode variance: [2.126298237323761, 2.287294451236725, 2.3535586810112], time: 158.478
Running avgs for agent 0: q_loss: 41.55756759643555, p_loss: -5.760946273803711, mean_rew: -11.461830729046708, variance: 8.505193710327148, lamda: 1.3498259782791138
Running avgs for agent 1: q_loss: 36.131858825683594, p_loss: -5.7400994300842285, mean_rew: -11.469887375524813, variance: 9.149177551269531, lamda: 1.2603620290756226
Running avgs for agent 2: q_loss: 37.076969146728516, p_loss: -5.7460618019104, mean_rew: -11.474248205223313, variance: 9.414234161376953, lamda: 1.220278263092041

steps: 524975, episodes: 21000, mean episode reward: -923.1394661854582, agent episode reward: [-307.71315539515274, -307.71315539515274, -307.71315539515274], time: 162.959
steps: 524975, episodes: 21000, mean episode variance: 6.719167539596557, agent episode variance: [2.1072261142730713, 2.2507587366104125, 2.3611826887130736], time: 162.959
Running avgs for agent 0: q_loss: 40.197113037109375, p_loss: -5.795483589172363, mean_rew: -11.525754819276555, variance: 8.42890453338623, lamda: 1.3662282228469849
Running avgs for agent 1: q_loss: 37.05298614501953, p_loss: -5.781407833099365, mean_rew: -11.540828353836314, variance: 9.003035545349121, lamda: 1.279160499572754
Running avgs for agent 2: q_loss: 34.329742431640625, p_loss: -5.774129867553711, mean_rew: -11.525889224814636, variance: 9.444730758666992, lamda: 1.2330522537231445

steps: 549975, episodes: 22000, mean episode reward: -972.307534161501, agent episode reward: [-324.10251138716706, -324.10251138716706, -324.10251138716706], time: 171.364
steps: 549975, episodes: 22000, mean episode variance: 6.645007070541382, agent episode variance: [2.094245433330536, 2.2338750338554383, 2.3168866033554076], time: 171.364
Running avgs for agent 0: q_loss: 44.06233596801758, p_loss: -5.799365043640137, mean_rew: -11.555610981352471, variance: 8.376981735229492, lamda: 1.3851796388626099
Running avgs for agent 1: q_loss: 40.013771057128906, p_loss: -5.802736759185791, mean_rew: -11.583328190678522, variance: 8.935500144958496, lamda: 1.2986629009246826
Running avgs for agent 2: q_loss: 52.19865798950195, p_loss: -5.806212425231934, mean_rew: -11.582424535499896, variance: 9.267546653747559, lamda: 1.2442656755447388

steps: 574975, episodes: 23000, mean episode reward: -995.6809086260879, agent episode reward: [-331.893636208696, -331.893636208696, -331.893636208696], time: 169.996
steps: 574975, episodes: 23000, mean episode variance: 6.664533271312713, agent episode variance: [2.0848193106651305, 2.208762966156006, 2.370950994491577], time: 169.996
Running avgs for agent 0: q_loss: 43.42695236206055, p_loss: -5.858713150024414, mean_rew: -11.655132318831388, variance: 8.339277267456055, lamda: 1.402734637260437
Running avgs for agent 1: q_loss: 38.634681701660156, p_loss: -5.843315601348877, mean_rew: -11.647192649811082, variance: 8.835051536560059, lamda: 1.3187254667282104
Running avgs for agent 2: q_loss: 54.624820709228516, p_loss: -5.8290114402771, mean_rew: -11.627685612210081, variance: 9.483803749084473, lamda: 1.2477999925613403

steps: 599975, episodes: 24000, mean episode reward: -991.7512206684174, agent episode reward: [-330.58374022280583, -330.58374022280583, -330.58374022280583], time: 170.968
steps: 599975, episodes: 24000, mean episode variance: 6.602529262065888, agent episode variance: [2.066488928318024, 2.188197017669678, 2.347843316078186], time: 170.969
Running avgs for agent 0: q_loss: 41.9732551574707, p_loss: -5.887991428375244, mean_rew: -11.708504383977036, variance: 8.265954971313477, lamda: 1.4182145595550537
Running avgs for agent 1: q_loss: 38.042118072509766, p_loss: -5.876071929931641, mean_rew: -11.726534728067822, variance: 8.752789497375488, lamda: 1.3354355096817017
Running avgs for agent 2: q_loss: 54.38008499145508, p_loss: -5.869683265686035, mean_rew: -11.708376113686837, variance: 9.391372680664062, lamda: 1.2509748935699463

steps: 624975, episodes: 25000, mean episode reward: -973.396518888854, agent episode reward: [-324.4655062962846, -324.4655062962846, -324.4655062962846], time: 172.269
steps: 624975, episodes: 25000, mean episode variance: 6.584338399410248, agent episode variance: [2.037725175857544, 2.1857583026885985, 2.360854920864105], time: 172.269
Running avgs for agent 0: q_loss: 44.494590759277344, p_loss: -5.91568660736084, mean_rew: -11.756077859605345, variance: 8.150900840759277, lamda: 1.4334453344345093
Running avgs for agent 1: q_loss: 40.67577362060547, p_loss: -5.885991096496582, mean_rew: -11.76269744187384, variance: 8.743033409118652, lamda: 1.3520501852035522
Running avgs for agent 2: q_loss: 51.737945556640625, p_loss: -5.89436674118042, mean_rew: -11.77376550348574, variance: 9.443419456481934, lamda: 1.2543506622314453

steps: 649975, episodes: 26000, mean episode reward: -980.1753480455704, agent episode reward: [-326.72511601519017, -326.72511601519017, -326.72511601519017], time: 170.052
steps: 649975, episodes: 26000, mean episode variance: 6.583343905448913, agent episode variance: [2.0300551552772523, 2.2015565891265867, 2.3517321610450743], time: 170.052
Running avgs for agent 0: q_loss: 42.891998291015625, p_loss: -5.932086944580078, mean_rew: -11.802863338041508, variance: 8.120221138000488, lamda: 1.450463891029358
Running avgs for agent 1: q_loss: 57.167572021484375, p_loss: -5.907888412475586, mean_rew: -11.815635098870557, variance: 8.80622673034668, lamda: 1.3611152172088623
Running avgs for agent 2: q_loss: 34.32986831665039, p_loss: -5.927613258361816, mean_rew: -11.813555411935718, variance: 9.406929969787598, lamda: 1.264744520187378

steps: 674975, episodes: 27000, mean episode reward: -985.2557734038462, agent episode reward: [-328.4185911346154, -328.4185911346154, -328.4185911346154], time: 173.783
steps: 674975, episodes: 27000, mean episode variance: 6.558130037784577, agent episode variance: [2.0268415904045103, 2.1958104467391966, 2.3354780006408693], time: 173.784
Running avgs for agent 0: q_loss: 42.67670822143555, p_loss: -5.957830905914307, mean_rew: -11.854265200538944, variance: 8.107366561889648, lamda: 1.4655027389526367
Running avgs for agent 1: q_loss: 56.465877532958984, p_loss: -5.956477642059326, mean_rew: -11.88089965217062, variance: 8.783242225646973, lamda: 1.3636164665222168
Running avgs for agent 2: q_loss: 34.51720428466797, p_loss: -5.95869779586792, mean_rew: -11.87995817111242, variance: 9.341911315917969, lamda: 1.2790417671203613

steps: 699975, episodes: 28000, mean episode reward: -970.2298036757301, agent episode reward: [-323.40993455857665, -323.40993455857665, -323.40993455857665], time: 170.562
steps: 699975, episodes: 28000, mean episode variance: 6.528837106227875, agent episode variance: [2.009865575790405, 2.204025803565979, 2.3149457268714904], time: 170.563
Running avgs for agent 0: q_loss: 42.54819869995117, p_loss: -5.993226051330566, mean_rew: -11.911073209486561, variance: 8.039462089538574, lamda: 1.477108359336853
Running avgs for agent 1: q_loss: 54.88177490234375, p_loss: -5.960555553436279, mean_rew: -11.911654629936221, variance: 8.8161039352417, lamda: 1.3650593757629395
Running avgs for agent 2: q_loss: 34.21257781982422, p_loss: -5.976233959197998, mean_rew: -11.919977763555137, variance: 9.259782791137695, lamda: 1.2929950952529907

steps: 724975, episodes: 29000, mean episode reward: -943.4369848208868, agent episode reward: [-314.4789949402956, -314.4789949402956, -314.4789949402956], time: 170.855
steps: 724975, episodes: 29000, mean episode variance: 6.557300610542297, agent episode variance: [2.0236029152870176, 2.1984521956443785, 2.3352454996109007], time: 170.855
Running avgs for agent 0: q_loss: 43.486907958984375, p_loss: -6.004508972167969, mean_rew: -11.95746404159313, variance: 8.09441089630127, lamda: 1.4912996292114258
Running avgs for agent 1: q_loss: 43.571739196777344, p_loss: -5.973112106323242, mean_rew: -11.95008998018258, variance: 8.793808937072754, lamda: 1.372162103652954
Running avgs for agent 2: q_loss: 46.680503845214844, p_loss: -5.987540245056152, mean_rew: -11.952665457971998, variance: 9.340981483459473, lamda: 1.3025317192077637

steps: 749975, episodes: 30000, mean episode reward: -931.2790948441229, agent episode reward: [-310.42636494804094, -310.42636494804094, -310.42636494804094], time: 175.358
steps: 749975, episodes: 30000, mean episode variance: 6.4260602474212645, agent episode variance: [1.9772056460380554, 2.16418497133255, 2.2846696300506593], time: 175.359
Running avgs for agent 0: q_loss: 44.748844146728516, p_loss: -6.006421089172363, mean_rew: -11.956341074213306, variance: 7.908822536468506, lamda: 1.5077217817306519
Running avgs for agent 1: q_loss: 36.61571502685547, p_loss: -6.008762359619141, mean_rew: -11.968496508262955, variance: 8.656740188598633, lamda: 1.3874372243881226
Running avgs for agent 2: q_loss: 32.15275573730469, p_loss: -5.988514423370361, mean_rew: -11.945275630197619, variance: 9.138677597045898, lamda: 1.3104896545410156

steps: 774975, episodes: 31000, mean episode reward: -921.6629995683871, agent episode reward: [-307.2209998561291, -307.2209998561291, -307.2209998561291], time: 166.7
steps: 774975, episodes: 31000, mean episode variance: 6.357869366884231, agent episode variance: [1.9528540484905244, 2.1432159872055054, 2.261799331188202], time: 166.7
Running avgs for agent 0: q_loss: 45.65147018432617, p_loss: -6.0189619064331055, mean_rew: -11.978421364656082, variance: 7.811416149139404, lamda: 1.5223695039749146
Running avgs for agent 1: q_loss: 35.221927642822266, p_loss: -6.0130486488342285, mean_rew: -11.973144215118255, variance: 8.572863578796387, lamda: 1.3995534181594849
Running avgs for agent 2: q_loss: 32.22345733642578, p_loss: -5.997010707855225, mean_rew: -11.96483598090208, variance: 9.047198295593262, lamda: 1.323330283164978

steps: 799975, episodes: 32000, mean episode reward: -915.4894295710495, agent episode reward: [-305.16314319034984, -305.16314319034984, -305.16314319034984], time: 172.433
steps: 799975, episodes: 32000, mean episode variance: 6.333704058885575, agent episode variance: [1.946224766254425, 2.131723118066788, 2.2557561745643615], time: 172.433
Running avgs for agent 0: q_loss: 43.30350112915039, p_loss: -6.016803741455078, mean_rew: -11.985362143497584, variance: 7.7848992347717285, lamda: 1.5381464958190918
Running avgs for agent 1: q_loss: 42.798179626464844, p_loss: -5.997601509094238, mean_rew: -11.973427804924437, variance: 8.52689266204834, lamda: 1.4140684604644775
Running avgs for agent 2: q_loss: 30.092021942138672, p_loss: -6.00735330581665, mean_rew: -11.984274036681562, variance: 9.023025512695312, lamda: 1.3321056365966797

steps: 824975, episodes: 33000, mean episode reward: -919.4688156418396, agent episode reward: [-306.48960521394656, -306.48960521394656, -306.48960521394656], time: 174.633
steps: 824975, episodes: 33000, mean episode variance: 6.327853593349457, agent episode variance: [1.9295059204101563, 2.155514028072357, 2.2428336448669435], time: 174.633
Running avgs for agent 0: q_loss: 42.85420227050781, p_loss: -6.026315212249756, mean_rew: -11.993821796271126, variance: 7.718023300170898, lamda: 1.552951693534851
Running avgs for agent 1: q_loss: 57.37778854370117, p_loss: -5.996615409851074, mean_rew: -12.002955094113792, variance: 8.622056007385254, lamda: 1.422613501548767
Running avgs for agent 2: q_loss: 33.026817321777344, p_loss: -6.020850658416748, mean_rew: -11.988475749377942, variance: 8.971334457397461, lamda: 1.3416346311569214

steps: 849975, episodes: 34000, mean episode reward: -909.6669897051414, agent episode reward: [-303.2223299017138, -303.2223299017138, -303.2223299017138], time: 171.473
steps: 849975, episodes: 34000, mean episode variance: 6.267927753806114, agent episode variance: [1.9187288378477096, 2.137778126239777, 2.2114207897186278], time: 171.473
Running avgs for agent 0: q_loss: 45.193946838378906, p_loss: -6.023359298706055, mean_rew: -11.99575712406421, variance: 7.674915313720703, lamda: 1.5680302381515503
Running avgs for agent 1: q_loss: 53.7680549621582, p_loss: -6.009947776794434, mean_rew: -11.991001798192228, variance: 8.55111312866211, lamda: 1.424169898033142
Running avgs for agent 2: q_loss: 31.586645126342773, p_loss: -6.010733604431152, mean_rew: -11.996619236310748, variance: 8.845684051513672, lamda: 1.354532241821289

steps: 874975, episodes: 35000, mean episode reward: -938.5448652000921, agent episode reward: [-312.8482884000307, -312.8482884000307, -312.8482884000307], time: 175.832
steps: 874975, episodes: 35000, mean episode variance: 6.234556581497192, agent episode variance: [1.896850037574768, 2.1340493516922, 2.203657192230225], time: 175.833
Running avgs for agent 0: q_loss: 45.207237243652344, p_loss: -6.0366692543029785, mean_rew: -12.005385047294956, variance: 7.587400436401367, lamda: 1.5839465856552124
Running avgs for agent 1: q_loss: 52.18309020996094, p_loss: -6.020849704742432, mean_rew: -12.01828930807978, variance: 8.536197662353516, lamda: 1.4258414506912231
Running avgs for agent 2: q_loss: 34.057376861572266, p_loss: -6.018500804901123, mean_rew: -12.00591673138414, variance: 8.814628601074219, lamda: 1.3638441562652588

steps: 899975, episodes: 36000, mean episode reward: -954.5280519502132, agent episode reward: [-318.1760173167378, -318.1760173167378, -318.1760173167378], time: 172.713
steps: 899975, episodes: 36000, mean episode variance: 6.240385053634643, agent episode variance: [1.8957951860427857, 2.147583782672882, 2.197006084918976], time: 172.714
Running avgs for agent 0: q_loss: 46.737422943115234, p_loss: -6.032769680023193, mean_rew: -12.01881471278747, variance: 7.5831804275512695, lamda: 1.5990921258926392
Running avgs for agent 1: q_loss: 51.40687942504883, p_loss: -6.028825283050537, mean_rew: -12.024131542137233, variance: 8.590335845947266, lamda: 1.4269871711730957
Running avgs for agent 2: q_loss: 36.859352111816406, p_loss: -6.02438497543335, mean_rew: -12.020280742580052, variance: 8.788022994995117, lamda: 1.3720484972000122

steps: 924975, episodes: 37000, mean episode reward: -952.6680213748361, agent episode reward: [-317.5560071249454, -317.5560071249454, -317.5560071249454], time: 173.723
steps: 924975, episodes: 37000, mean episode variance: 6.143255244255066, agent episode variance: [1.8537927799224854, 2.098346722602844, 2.191115741729736], time: 173.724
Running avgs for agent 0: q_loss: 45.10662841796875, p_loss: -6.042024612426758, mean_rew: -12.03784358894097, variance: 7.415170669555664, lamda: 1.616247296333313
Running avgs for agent 1: q_loss: 36.3050537109375, p_loss: -6.042581081390381, mean_rew: -12.047029983474307, variance: 8.393386840820312, lamda: 1.4310280084609985
Running avgs for agent 2: q_loss: 50.20878982543945, p_loss: -6.019278526306152, mean_rew: -12.025863537637763, variance: 8.7644624710083, lamda: 1.3775761127471924

steps: 949975, episodes: 38000, mean episode reward: -955.3513135559277, agent episode reward: [-318.4504378519759, -318.4504378519759, -318.4504378519759], time: 177.428
steps: 949975, episodes: 38000, mean episode variance: 6.163961631774902, agent episode variance: [1.8470431823730469, 2.1159151649475096, 2.201003284454346], time: 177.428
Running avgs for agent 0: q_loss: 44.30039978027344, p_loss: -6.042364597320557, mean_rew: -12.036189802923186, variance: 7.388172626495361, lamda: 1.6300773620605469
Running avgs for agent 1: q_loss: 35.6687126159668, p_loss: -6.033019542694092, mean_rew: -12.044691983979638, variance: 8.46366024017334, lamda: 1.44221830368042
Running avgs for agent 2: q_loss: 49.915531158447266, p_loss: -6.041914939880371, mean_rew: -12.06943497167542, variance: 8.8040132522583, lamda: 1.3789258003234863

steps: 974975, episodes: 39000, mean episode reward: -951.0783531350801, agent episode reward: [-317.0261177116933, -317.0261177116933, -317.0261177116933], time: 169.932
steps: 974975, episodes: 39000, mean episode variance: 6.092661689758301, agent episode variance: [1.8282856736183166, 2.0871721391677855, 2.1772038769721984], time: 169.932
Running avgs for agent 0: q_loss: 58.98221206665039, p_loss: -6.072227954864502, mean_rew: -12.105966687625276, variance: 7.313143253326416, lamda: 1.6455960273742676
Running avgs for agent 1: q_loss: 30.81719207763672, p_loss: -6.059817314147949, mean_rew: -12.082903568574398, variance: 8.348689079284668, lamda: 1.454342246055603
Running avgs for agent 2: q_loss: 48.195655822753906, p_loss: -6.042360305786133, mean_rew: -12.071988140044745, variance: 8.708816528320312, lamda: 1.3806878328323364

steps: 999975, episodes: 40000, mean episode reward: -928.8167726888794, agent episode reward: [-309.60559089629317, -309.60559089629317, -309.60559089629317], time: 168.297
steps: 999975, episodes: 40000, mean episode variance: 6.126841090917587, agent episode variance: [1.849680071592331, 2.0766698808670045, 2.200491138458252], time: 168.297
Running avgs for agent 0: q_loss: 72.19088745117188, p_loss: -6.060406684875488, mean_rew: -12.092138743351512, variance: 7.398719787597656, lamda: 1.6540122032165527
Running avgs for agent 1: q_loss: 33.12326431274414, p_loss: -6.069314479827881, mean_rew: -12.106698876438996, variance: 8.306679725646973, lamda: 1.4630790948867798
Running avgs for agent 2: q_loss: 48.41209030151367, p_loss: -6.040783405303955, mean_rew: -12.081060075502952, variance: 8.80196475982666, lamda: 1.3832719326019287

steps: 1024975, episodes: 41000, mean episode reward: -931.0594212645228, agent episode reward: [-310.3531404215076, -310.3531404215076, -310.3531404215076], time: 169.664
steps: 1024975, episodes: 41000, mean episode variance: 6.101202213525772, agent episode variance: [1.8409794068336487, 2.055858651399612, 2.204364155292511], time: 169.664
Running avgs for agent 0: q_loss: 43.97343063354492, p_loss: -6.113823413848877, mean_rew: -12.172895464594818, variance: 7.363917827606201, lamda: 1.6646913290023804
Running avgs for agent 1: q_loss: 49.12989807128906, p_loss: -6.097719192504883, mean_rew: -12.1633507414699, variance: 8.223434448242188, lamda: 1.4702444076538086
Running avgs for agent 2: q_loss: 46.758934020996094, p_loss: -6.077561855316162, mean_rew: -12.15386114674115, variance: 8.81745719909668, lamda: 1.3886699676513672

steps: 1049975, episodes: 42000, mean episode reward: -943.0159423273446, agent episode reward: [-314.3386474424482, -314.3386474424482, -314.3386474424482], time: 167.405
steps: 1049975, episodes: 42000, mean episode variance: 6.150046814441681, agent episode variance: [1.8296095399856567, 2.1089555230140684, 2.2114817514419554], time: 167.406
Running avgs for agent 0: q_loss: 46.06482696533203, p_loss: -6.167845726013184, mean_rew: -12.286705868912494, variance: 7.318437576293945, lamda: 1.6820008754730225
Running avgs for agent 1: q_loss: 49.09572219848633, p_loss: -6.14189338684082, mean_rew: -12.273226953749173, variance: 8.435822486877441, lamda: 1.4714382886886597
Running avgs for agent 2: q_loss: 47.678524017333984, p_loss: -6.14655065536499, mean_rew: -12.275829737738388, variance: 8.845926284790039, lamda: 1.390782356262207

steps: 1074975, episodes: 43000, mean episode reward: -960.8658691709095, agent episode reward: [-320.2886230569698, -320.2886230569698, -320.2886230569698], time: 172.567
steps: 1074975, episodes: 43000, mean episode variance: 6.146051331996918, agent episode variance: [1.8167340683937072, 2.093231384754181, 2.2360858788490297], time: 172.568
Running avgs for agent 0: q_loss: 47.25779724121094, p_loss: -6.19780969619751, mean_rew: -12.352906283860994, variance: 7.2669358253479, lamda: 1.6989045143127441
Running avgs for agent 1: q_loss: 49.032474517822266, p_loss: -6.185840606689453, mean_rew: -12.347499950830922, variance: 8.372925758361816, lamda: 1.4721403121948242
Running avgs for agent 2: q_loss: 47.96875762939453, p_loss: -6.170888900756836, mean_rew: -12.343195100331803, variance: 8.944343566894531, lamda: 1.3919645547866821

steps: 1099975, episodes: 44000, mean episode reward: -942.3695746940635, agent episode reward: [-314.1231915646879, -314.1231915646879, -314.1231915646879], time: 169.613
steps: 1099975, episodes: 44000, mean episode variance: 6.183396447181702, agent episode variance: [1.827307221889496, 2.1057944283485415, 2.2502947969436646], time: 169.613
Running avgs for agent 0: q_loss: 43.740108489990234, p_loss: -6.214379787445068, mean_rew: -12.386617458751559, variance: 7.309228897094727, lamda: 1.7149834632873535
Running avgs for agent 1: q_loss: 48.42107009887695, p_loss: -6.223973751068115, mean_rew: -12.408412015855204, variance: 8.423177719116211, lamda: 1.472894310951233
Running avgs for agent 2: q_loss: 47.65074157714844, p_loss: -6.1988019943237305, mean_rew: -12.394637456461608, variance: 9.001179695129395, lamda: 1.3932673931121826

steps: 1124975, episodes: 45000, mean episode reward: -933.0579829781179, agent episode reward: [-311.01932765937266, -311.01932765937266, -311.01932765937266], time: 166.927
steps: 1124975, episodes: 45000, mean episode variance: 6.170112663269043, agent episode variance: [1.8099467458724976, 2.140069474220276, 2.2200964431762698], time: 166.928
Running avgs for agent 0: q_loss: 46.168975830078125, p_loss: -6.248687267303467, mean_rew: -12.450149953175119, variance: 7.2397871017456055, lamda: 1.7263820171356201
Running avgs for agent 1: q_loss: 48.35175323486328, p_loss: -6.227420806884766, mean_rew: -12.436153917579283, variance: 8.560276985168457, lamda: 1.4734477996826172
Running avgs for agent 2: q_loss: 47.37248229980469, p_loss: -6.226768493652344, mean_rew: -12.42697129505258, variance: 8.880385398864746, lamda: 1.3953617811203003

steps: 1149975, episodes: 46000, mean episode reward: -939.0250121125044, agent episode reward: [-313.00833737083485, -313.00833737083485, -313.00833737083485], time: 172.409
steps: 1149975, episodes: 46000, mean episode variance: 6.178481313705444, agent episode variance: [1.810392144203186, 2.1268703589439393, 2.241218810558319], time: 172.41
Running avgs for agent 0: q_loss: 41.81689453125, p_loss: -6.256923675537109, mean_rew: -12.477991894793206, variance: 7.241568565368652, lamda: 1.7355124950408936
Running avgs for agent 1: q_loss: 47.718624114990234, p_loss: -6.244286060333252, mean_rew: -12.472428652226114, variance: 8.507481575012207, lamda: 1.473893404006958
Running avgs for agent 2: q_loss: 46.998905181884766, p_loss: -6.246570587158203, mean_rew: -12.487124054899102, variance: 8.964875221252441, lamda: 1.3976002931594849

steps: 1174975, episodes: 47000, mean episode reward: -914.0288986949733, agent episode reward: [-304.6762995649911, -304.6762995649911, -304.6762995649911], time: 163.883
steps: 1174975, episodes: 47000, mean episode variance: 6.172082984924317, agent episode variance: [1.8057732405662537, 2.115547420501709, 2.2507623238563537], time: 163.883
Running avgs for agent 0: q_loss: 42.141849517822266, p_loss: -6.2712602615356445, mean_rew: -12.49683912903247, variance: 7.223093032836914, lamda: 1.741992473602295
Running avgs for agent 1: q_loss: 48.12431335449219, p_loss: -6.276482105255127, mean_rew: -12.51391473368035, variance: 8.462189674377441, lamda: 1.4747923612594604
Running avgs for agent 2: q_loss: 47.17985153198242, p_loss: -6.2536139488220215, mean_rew: -12.500044879877084, variance: 9.00304889678955, lamda: 1.3999409675598145

steps: 1199975, episodes: 48000, mean episode reward: -918.8485870058744, agent episode reward: [-306.28286233529144, -306.28286233529144, -306.28286233529144], time: 166.242
steps: 1199975, episodes: 48000, mean episode variance: 6.197542081832886, agent episode variance: [1.7979142937660217, 2.153816333770752, 2.245811454296112], time: 166.243
Running avgs for agent 0: q_loss: 42.658626556396484, p_loss: -6.277559757232666, mean_rew: -12.520446887226594, variance: 7.191656589508057, lamda: 1.7511694431304932
Running avgs for agent 1: q_loss: 46.82955551147461, p_loss: -6.268627166748047, mean_rew: -12.517810488874467, variance: 8.615265846252441, lamda: 1.4756715297698975
Running avgs for agent 2: q_loss: 45.67915725708008, p_loss: -6.259582042694092, mean_rew: -12.506612279733243, variance: 8.983245849609375, lamda: 1.4020872116088867

steps: 1224975, episodes: 49000, mean episode reward: -898.7057607183077, agent episode reward: [-299.5685869061026, -299.5685869061026, -299.5685869061026], time: 160.505
steps: 1224975, episodes: 49000, mean episode variance: 6.170924121379852, agent episode variance: [1.7890338282585143, 2.128086843967438, 2.2538034491539003], time: 160.505
Running avgs for agent 0: q_loss: 41.52055358886719, p_loss: -6.279564380645752, mean_rew: -12.519857128587326, variance: 7.156135082244873, lamda: 1.7597036361694336
Running avgs for agent 1: q_loss: 46.51325607299805, p_loss: -6.283807754516602, mean_rew: -12.533089618047432, variance: 8.512346267700195, lamda: 1.4761149883270264
Running avgs for agent 2: q_loss: 46.08757019042969, p_loss: -6.269196510314941, mean_rew: -12.534901234044181, variance: 9.015214920043945, lamda: 1.4043503999710083

steps: 1249975, episodes: 50000, mean episode reward: -899.5888544713176, agent episode reward: [-299.8629514904391, -299.8629514904391, -299.8629514904391], time: 163.581
steps: 1249975, episodes: 50000, mean episode variance: 6.189465254306794, agent episode variance: [1.792456934452057, 2.1463478326797487, 2.250660487174988], time: 163.581
Running avgs for agent 0: q_loss: 41.903099060058594, p_loss: -6.298771381378174, mean_rew: -12.561923676380397, variance: 7.169827461242676, lamda: 1.7667509317398071
Running avgs for agent 1: q_loss: 46.57913589477539, p_loss: -6.293863296508789, mean_rew: -12.555237111801645, variance: 8.585391998291016, lamda: 1.4764413833618164
Running avgs for agent 2: q_loss: 40.259342193603516, p_loss: -6.286895275115967, mean_rew: -12.547803830257669, variance: 9.002642631530762, lamda: 1.4070181846618652

steps: 1274975, episodes: 51000, mean episode reward: -903.5662470086356, agent episode reward: [-301.1887490028785, -301.1887490028785, -301.1887490028785], time: 168.4
steps: 1274975, episodes: 51000, mean episode variance: 6.155541702747345, agent episode variance: [1.770217514514923, 2.1634987812042237, 2.221825407028198], time: 168.401
Running avgs for agent 0: q_loss: 44.92986297607422, p_loss: -6.31423807144165, mean_rew: -12.581951160304122, variance: 7.080870151519775, lamda: 1.7762837409973145
Running avgs for agent 1: q_loss: 46.09963607788086, p_loss: -6.290658950805664, mean_rew: -12.564295174738346, variance: 8.6539945602417, lamda: 1.4766957759857178
Running avgs for agent 2: q_loss: 29.314910888671875, p_loss: -6.292591094970703, mean_rew: -12.570695254788978, variance: 8.887300491333008, lamda: 1.415930151939392

steps: 1299975, episodes: 52000, mean episode reward: -908.600905060037, agent episode reward: [-302.86696835334567, -302.86696835334567, -302.86696835334567], time: 177.748
steps: 1299975, episodes: 52000, mean episode variance: 6.144001040458679, agent episode variance: [1.7772007451057434, 2.1469428033828737, 2.219857491970062], time: 177.749
Running avgs for agent 0: q_loss: 39.617645263671875, p_loss: -6.2956132888793945, mean_rew: -12.56269988408843, variance: 7.108802795410156, lamda: 1.7821600437164307
Running avgs for agent 1: q_loss: 45.307342529296875, p_loss: -6.303160667419434, mean_rew: -12.584514661849878, variance: 8.58777141571045, lamda: 1.477281928062439
Running avgs for agent 2: q_loss: 27.686628341674805, p_loss: -6.275103569030762, mean_rew: -12.548346216628357, variance: 8.879429817199707, lamda: 1.422098159790039

steps: 1324975, episodes: 53000, mean episode reward: -908.0942318731078, agent episode reward: [-302.698077291036, -302.698077291036, -302.698077291036], time: 173.897
steps: 1324975, episodes: 53000, mean episode variance: 6.104099621772766, agent episode variance: [1.760022943496704, 2.1282436513900755, 2.2158330268859863], time: 173.897
Running avgs for agent 0: q_loss: 40.8592643737793, p_loss: -6.301189422607422, mean_rew: -12.558902980599601, variance: 7.0400919914245605, lamda: 1.7890121936798096
Running avgs for agent 1: q_loss: 44.4339599609375, p_loss: -6.28916597366333, mean_rew: -12.544491679995273, variance: 8.512974739074707, lamda: 1.4786244630813599
Running avgs for agent 2: q_loss: 26.881122589111328, p_loss: -6.269484996795654, mean_rew: -12.549875674463506, variance: 8.86333179473877, lamda: 1.429673671722412

steps: 1349975, episodes: 54000, mean episode reward: -845.3474364438914, agent episode reward: [-281.7824788146304, -281.7824788146304, -281.7824788146304], time: 171.412
steps: 1349975, episodes: 54000, mean episode variance: 6.064828196525574, agent episode variance: [1.7400489382743836, 2.146659041404724, 2.178120216846466], time: 171.412
Running avgs for agent 0: q_loss: 38.675506591796875, p_loss: -6.278041362762451, mean_rew: -12.508616388599382, variance: 6.960195541381836, lamda: 1.7941546440124512
Running avgs for agent 1: q_loss: 43.190155029296875, p_loss: -6.2629714012146, mean_rew: -12.512331721256013, variance: 8.586636543273926, lamda: 1.478931188583374
Running avgs for agent 2: q_loss: 26.597652435302734, p_loss: -6.259586334228516, mean_rew: -12.504850048259728, variance: 8.712480545043945, lamda: 1.4355438947677612

steps: 1374975, episodes: 55000, mean episode reward: -861.5516075210585, agent episode reward: [-287.18386917368616, -287.18386917368616, -287.18386917368616], time: 176.395
steps: 1374975, episodes: 55000, mean episode variance: 6.046222627639771, agent episode variance: [1.7531599283218384, 2.1220095491409303, 2.171053150177002], time: 176.396
Running avgs for agent 0: q_loss: 40.23288345336914, p_loss: -6.252537250518799, mean_rew: -12.483541768577012, variance: 7.01263952255249, lamda: 1.8013652563095093
Running avgs for agent 1: q_loss: 42.813011169433594, p_loss: -6.264469623565674, mean_rew: -12.489702550122459, variance: 8.488038063049316, lamda: 1.479274034500122
Running avgs for agent 2: q_loss: 25.589426040649414, p_loss: -6.24664306640625, mean_rew: -12.505957877716371, variance: 8.684212684631348, lamda: 1.4427502155303955

steps: 1399975, episodes: 56000, mean episode reward: -883.9788059474288, agent episode reward: [-294.65960198247626, -294.65960198247626, -294.65960198247626], time: 175.766
steps: 1399975, episodes: 56000, mean episode variance: 6.042514388561249, agent episode variance: [1.7346660194396972, 2.1222580366134642, 2.185590332508087], time: 175.766
Running avgs for agent 0: q_loss: 37.95447540283203, p_loss: -6.263047695159912, mean_rew: -12.476900444218673, variance: 6.938663959503174, lamda: 1.8120003938674927
Running avgs for agent 1: q_loss: 28.34521484375, p_loss: -6.258787155151367, mean_rew: -12.477999201513077, variance: 8.489031791687012, lamda: 1.4845730066299438
Running avgs for agent 2: q_loss: 35.15791702270508, p_loss: -6.2469868659973145, mean_rew: -12.490597782845425, variance: 8.742361068725586, lamda: 1.4469467401504517

steps: 1424975, episodes: 57000, mean episode reward: -898.2368431945331, agent episode reward: [-299.4122810648444, -299.4122810648444, -299.4122810648444], time: 174.581
steps: 1424975, episodes: 57000, mean episode variance: 6.02828853225708, agent episode variance: [1.7387255930900574, 2.1037061333656313, 2.1858568058013916], time: 174.581
Running avgs for agent 0: q_loss: 50.07316970825195, p_loss: -6.265336036682129, mean_rew: -12.489596774972489, variance: 6.954902172088623, lamda: 1.819638967514038
Running avgs for agent 1: q_loss: 25.64846420288086, p_loss: -6.256423473358154, mean_rew: -12.47402682312441, variance: 8.414824485778809, lamda: 1.490790843963623
Running avgs for agent 2: q_loss: 42.4871826171875, p_loss: -6.242973804473877, mean_rew: -12.486921121589356, variance: 8.743427276611328, lamda: 1.4484490156173706

steps: 1449975, episodes: 58000, mean episode reward: -921.0749692739297, agent episode reward: [-307.0249897579765, -307.0249897579765, -307.0249897579765], time: 171.39
steps: 1449975, episodes: 58000, mean episode variance: 5.99029523229599, agent episode variance: [1.7206653733253479, 2.1127442812919615, 2.1568855776786804], time: 171.39
Running avgs for agent 0: q_loss: 42.07695388793945, p_loss: -6.262143611907959, mean_rew: -12.472583147185384, variance: 6.882661819458008, lamda: 1.828546404838562
Running avgs for agent 1: q_loss: 24.59442901611328, p_loss: -6.2671308517456055, mean_rew: -12.496749730804073, variance: 8.450976371765137, lamda: 1.4936765432357788
Running avgs for agent 2: q_loss: 40.10725784301758, p_loss: -6.241600513458252, mean_rew: -12.475036336796895, variance: 8.627542495727539, lamda: 1.4501346349716187

steps: 1474975, episodes: 59000, mean episode reward: -910.0112052236077, agent episode reward: [-303.3370684078692, -303.3370684078692, -303.3370684078692], time: 169.749
steps: 1474975, episodes: 59000, mean episode variance: 5.962422355413437, agent episode variance: [1.7167690925598145, 2.089168114185333, 2.156485148668289], time: 169.749
Running avgs for agent 0: q_loss: 38.528656005859375, p_loss: -6.2639923095703125, mean_rew: -12.467191513124636, variance: 6.867076396942139, lamda: 1.834599256515503
Running avgs for agent 1: q_loss: 23.73072624206543, p_loss: -6.259716033935547, mean_rew: -12.47017817425113, variance: 8.356672286987305, lamda: 1.49648118019104
Running avgs for agent 2: q_loss: 28.947872161865234, p_loss: -6.246502876281738, mean_rew: -12.46355287619846, variance: 8.625940322875977, lamda: 1.4556431770324707

steps: 1499975, episodes: 60000, mean episode reward: -898.6139677791194, agent episode reward: [-299.5379892597065, -299.5379892597065, -299.5379892597065], time: 172.288
steps: 1499975, episodes: 60000, mean episode variance: 5.93188433265686, agent episode variance: [1.6985737948417663, 2.0908988237380983, 2.142411714076996], time: 172.288
Running avgs for agent 0: q_loss: 55.09601593017578, p_loss: -6.2502241134643555, mean_rew: -12.425322367869576, variance: 6.794294834136963, lamda: 1.8396025896072388
Running avgs for agent 1: q_loss: 38.48352813720703, p_loss: -6.248806476593018, mean_rew: -12.449071185056575, variance: 8.363595962524414, lamda: 1.4986118078231812
Running avgs for agent 2: q_loss: 25.983535766601562, p_loss: -6.21488618850708, mean_rew: -12.424231790601963, variance: 8.569646835327148, lamda: 1.4628114700317383

steps: 1524975, episodes: 61000, mean episode reward: -903.8212565825908, agent episode reward: [-301.2737521941969, -301.2737521941969, -301.2737521941969], time: 168.721
steps: 1524975, episodes: 61000, mean episode variance: 5.9333635413646695, agent episode variance: [1.7012073619365693, 2.099866617679596, 2.1322895617485047], time: 168.722
Running avgs for agent 0: q_loss: 44.158023834228516, p_loss: -6.2609028816223145, mean_rew: -12.43243936440469, variance: 6.8048295974731445, lamda: 1.8452231884002686
Running avgs for agent 1: q_loss: 40.31071090698242, p_loss: -6.242178916931152, mean_rew: -12.436468866543924, variance: 8.399466514587402, lamda: 1.4991878271102905
Running avgs for agent 2: q_loss: 25.584598541259766, p_loss: -6.2183332443237305, mean_rew: -12.422779910397576, variance: 8.529158592224121, lamda: 1.469964623451233

steps: 1549975, episodes: 62000, mean episode reward: -925.925714887392, agent episode reward: [-308.641904962464, -308.641904962464, -308.641904962464], time: 171.611
steps: 1549975, episodes: 62000, mean episode variance: 5.880970630168915, agent episode variance: [1.6849904656410217, 2.0857555050849914, 2.1102246594429017], time: 171.612
Running avgs for agent 0: q_loss: 40.893314361572266, p_loss: -6.253090858459473, mean_rew: -12.411633096594871, variance: 6.739961624145508, lamda: 1.85337495803833
Running avgs for agent 1: q_loss: 39.755592346191406, p_loss: -6.229520320892334, mean_rew: -12.409952319707775, variance: 8.343021392822266, lamda: 1.5000622272491455
Running avgs for agent 2: q_loss: 23.414480209350586, p_loss: -6.206464767456055, mean_rew: -12.406475143834836, variance: 8.440898895263672, lamda: 1.4742735624313354

steps: 1574975, episodes: 63000, mean episode reward: -915.6610777627972, agent episode reward: [-305.22035925426565, -305.22035925426565, -305.22035925426565], time: 173.408
steps: 1574975, episodes: 63000, mean episode variance: 5.905805181026459, agent episode variance: [1.6752530908584595, 2.0986031222343446, 2.131948967933655], time: 173.409
Running avgs for agent 0: q_loss: 41.9084358215332, p_loss: -6.224126815795898, mean_rew: -12.378607341205432, variance: 6.70101261138916, lamda: 1.8643501996994019
Running avgs for agent 1: q_loss: 39.468997955322266, p_loss: -6.212436676025391, mean_rew: -12.390046103139632, variance: 8.394412994384766, lamda: 1.500287652015686
Running avgs for agent 2: q_loss: 37.28553771972656, p_loss: -6.190594673156738, mean_rew: -12.379745782799692, variance: 8.527796745300293, lamda: 1.4772588014602661

steps: 1599975, episodes: 64000, mean episode reward: -916.7758772046457, agent episode reward: [-305.5919590682152, -305.5919590682152, -305.5919590682152], time: 171.101
steps: 1599975, episodes: 64000, mean episode variance: 5.8378512303829195, agent episode variance: [1.6494760525226593, 2.0970896201133726, 2.0912855577468874], time: 171.102
Running avgs for agent 0: q_loss: 39.69737243652344, p_loss: -6.2109832763671875, mean_rew: -12.352229188862577, variance: 6.597904205322266, lamda: 1.8701808452606201
Running avgs for agent 1: q_loss: 39.6204833984375, p_loss: -6.205602645874023, mean_rew: -12.370766421438878, variance: 8.388358116149902, lamda: 1.5006718635559082
Running avgs for agent 2: q_loss: 40.773799896240234, p_loss: -6.182653427124023, mean_rew: -12.360202056815746, variance: 8.365141868591309, lamda: 1.4791667461395264

steps: 1624975, episodes: 65000, mean episode reward: -906.4912310168023, agent episode reward: [-302.1637436722674, -302.1637436722674, -302.1637436722674], time: 164.522
steps: 1624975, episodes: 65000, mean episode variance: 5.8521817519664765, agent episode variance: [1.6651163561344147, 2.0792993545532226, 2.1077660412788393], time: 164.522
Running avgs for agent 0: q_loss: 42.63188934326172, p_loss: -6.201547145843506, mean_rew: -12.330784406954214, variance: 6.660465240478516, lamda: 1.8785241842269897
Running avgs for agent 1: q_loss: 38.548709869384766, p_loss: -6.17963981628418, mean_rew: -12.328826721595826, variance: 8.3171968460083, lamda: 1.500850796699524
Running avgs for agent 2: q_loss: 24.36577796936035, p_loss: -6.174511432647705, mean_rew: -12.338629175886036, variance: 8.43106460571289, lamda: 1.48148775100708

steps: 1649975, episodes: 66000, mean episode reward: -918.656489578405, agent episode reward: [-306.2188298594683, -306.2188298594683, -306.2188298594683], time: 163.872
steps: 1649975, episodes: 66000, mean episode variance: 5.787828612208366, agent episode variance: [1.6332399686574937, 2.074552062034607, 2.080036581516266], time: 163.873
Running avgs for agent 0: q_loss: 68.37700653076172, p_loss: -6.201458930969238, mean_rew: -12.326921849390983, variance: 6.532959938049316, lamda: 1.8847960233688354
Running avgs for agent 1: q_loss: 38.9395866394043, p_loss: -6.186491012573242, mean_rew: -12.335335534177824, variance: 8.298208236694336, lamda: 1.5008544921875
Running avgs for agent 2: q_loss: 25.605684280395508, p_loss: -6.166808605194092, mean_rew: -12.30731687150571, variance: 8.320146560668945, lamda: 1.4851877689361572

steps: 1674975, episodes: 67000, mean episode reward: -906.7138675149201, agent episode reward: [-302.23795583830673, -302.23795583830673, -302.23795583830673], time: 162.22
steps: 1674975, episodes: 67000, mean episode variance: 5.789398555994034, agent episode variance: [1.628103568792343, 2.0714411830902097, 2.0898538041114807], time: 162.22
Running avgs for agent 0: q_loss: 49.41101837158203, p_loss: -6.180819511413574, mean_rew: -12.28610901982331, variance: 6.51241397857666, lamda: 1.8880903720855713
Running avgs for agent 1: q_loss: 38.21467590332031, p_loss: -6.158998489379883, mean_rew: -12.29155707573921, variance: 8.285764694213867, lamda: 1.5009369850158691
Running avgs for agent 2: q_loss: 25.59379768371582, p_loss: -6.163917541503906, mean_rew: -12.304429030059438, variance: 8.359416007995605, lamda: 1.4908831119537354

steps: 1699975, episodes: 68000, mean episode reward: -916.9883346012354, agent episode reward: [-305.6627782004118, -305.6627782004118, -305.6627782004118], time: 161.829
steps: 1699975, episodes: 68000, mean episode variance: 5.7371295294761655, agent episode variance: [1.6251022124290466, 2.058841514587402, 2.053185802459717], time: 161.829
Running avgs for agent 0: q_loss: 43.074764251708984, p_loss: -6.174308776855469, mean_rew: -12.271256138932626, variance: 6.50040864944458, lamda: 1.90114426612854
Running avgs for agent 1: q_loss: 37.94953918457031, p_loss: -6.170872211456299, mean_rew: -12.293883561123737, variance: 8.23536491394043, lamda: 1.5011227130889893
Running avgs for agent 2: q_loss: 25.038354873657227, p_loss: -6.161393642425537, mean_rew: -12.279342799866354, variance: 8.212742805480957, lamda: 1.5013290643692017

steps: 1724975, episodes: 69000, mean episode reward: -916.1223837153602, agent episode reward: [-305.37412790512, -305.37412790512, -305.37412790512], time: 162.936
steps: 1724975, episodes: 69000, mean episode variance: 5.709610679149628, agent episode variance: [1.6121250541210175, 2.062952270269394, 2.0345333547592164], time: 162.936
Running avgs for agent 0: q_loss: 36.94384765625, p_loss: -6.174986839294434, mean_rew: -12.254969397614937, variance: 6.448500633239746, lamda: 1.9103577136993408
Running avgs for agent 1: q_loss: 38.410282135009766, p_loss: -6.152884483337402, mean_rew: -12.273887082576449, variance: 8.251808166503906, lamda: 1.5015490055084229
Running avgs for agent 2: q_loss: 24.692907333374023, p_loss: -6.133922100067139, mean_rew: -12.241905972574568, variance: 8.13813304901123, lamda: 1.5119085311889648

steps: 1749975, episodes: 70000, mean episode reward: -921.0347766932983, agent episode reward: [-307.0115922310994, -307.0115922310994, -307.0115922310994], time: 166.652
steps: 1749975, episodes: 70000, mean episode variance: 5.704046592473984, agent episode variance: [1.611303382396698, 2.063497833251953, 2.0292453768253327], time: 166.652
Running avgs for agent 0: q_loss: 36.10142517089844, p_loss: -6.174139022827148, mean_rew: -12.259302153469996, variance: 6.445213794708252, lamda: 1.9149887561798096
Running avgs for agent 1: q_loss: 37.44001007080078, p_loss: -6.131462097167969, mean_rew: -12.227515361721371, variance: 8.25399112701416, lamda: 1.5020384788513184
Running avgs for agent 2: q_loss: 25.76035499572754, p_loss: -6.145288944244385, mean_rew: -12.252102688712945, variance: 8.116981506347656, lamda: 1.5181130170822144

steps: 1774975, episodes: 71000, mean episode reward: -934.6860609398555, agent episode reward: [-311.56202031328513, -311.56202031328513, -311.56202031328513], time: 171.994
steps: 1774975, episodes: 71000, mean episode variance: 5.682329769134522, agent episode variance: [1.6108893303871155, 2.0510849385261536, 2.0203555002212523], time: 171.995
Running avgs for agent 0: q_loss: 40.867469787597656, p_loss: -6.1529765129089355, mean_rew: -12.244063128648433, variance: 6.4435577392578125, lamda: 1.9205729961395264
Running avgs for agent 1: q_loss: 38.385257720947266, p_loss: -6.1418538093566895, mean_rew: -12.24928763918269, variance: 8.204339981079102, lamda: 1.5025339126586914
Running avgs for agent 2: q_loss: 24.360910415649414, p_loss: -6.139772415161133, mean_rew: -12.256069121765183, variance: 8.081421852111816, lamda: 1.5244860649108887

steps: 1799975, episodes: 72000, mean episode reward: -925.2580460050797, agent episode reward: [-308.41934866835993, -308.41934866835993, -308.41934866835993], time: 168.438
steps: 1799975, episodes: 72000, mean episode variance: 5.708401549577713, agent episode variance: [1.6110464060306549, 2.072533381938934, 2.024821761608124], time: 168.439
Running avgs for agent 0: q_loss: 67.3592758178711, p_loss: -6.158946514129639, mean_rew: -12.2617960975285, variance: 6.444185733795166, lamda: 1.9307149648666382
Running avgs for agent 1: q_loss: 37.24870681762695, p_loss: -6.13124418258667, mean_rew: -12.246126963225487, variance: 8.290133476257324, lamda: 1.5029441118240356
Running avgs for agent 2: q_loss: 22.970458984375, p_loss: -6.129821300506592, mean_rew: -12.242435744309747, variance: 8.099287033081055, lamda: 1.5288194417953491

steps: 1824975, episodes: 73000, mean episode reward: -922.5665237318187, agent episode reward: [-307.52217457727295, -307.52217457727295, -307.52217457727295], time: 181.72
steps: 1824975, episodes: 73000, mean episode variance: 5.691723186016083, agent episode variance: [1.5978150792121888, 2.0809613766670227, 2.0129467301368713], time: 181.72
Running avgs for agent 0: q_loss: 43.895233154296875, p_loss: -6.170857906341553, mean_rew: -12.254813637805091, variance: 6.391260623931885, lamda: 1.9364255666732788
Running avgs for agent 1: q_loss: 38.16842269897461, p_loss: -6.162430763244629, mean_rew: -12.290641554379894, variance: 8.323845863342285, lamda: 1.5032873153686523
Running avgs for agent 2: q_loss: 25.250253677368164, p_loss: -6.146637916564941, mean_rew: -12.263905214381714, variance: 8.051786422729492, lamda: 1.535460114479065

steps: 1849975, episodes: 74000, mean episode reward: -920.8778812108797, agent episode reward: [-306.9592937369599, -306.9592937369599, -306.9592937369599], time: 183.785
steps: 1849975, episodes: 74000, mean episode variance: 5.618273663282395, agent episode variance: [1.582485021352768, 2.042969774246216, 1.9928188676834107], time: 183.786
Running avgs for agent 0: q_loss: 49.164424896240234, p_loss: -6.166071891784668, mean_rew: -12.266466800744348, variance: 6.329940319061279, lamda: 1.9478671550750732
Running avgs for agent 1: q_loss: 38.28804016113281, p_loss: -6.142467498779297, mean_rew: -12.249111881606726, variance: 8.171878814697266, lamda: 1.5034286975860596
Running avgs for agent 2: q_loss: 23.73252296447754, p_loss: -6.1495585441589355, mean_rew: -12.247400942895197, variance: 7.971275329589844, lamda: 1.5419528484344482

steps: 1874975, episodes: 75000, mean episode reward: -917.0303355703468, agent episode reward: [-305.67677852344895, -305.67677852344895, -305.67677852344895], time: 180.239
steps: 1874975, episodes: 75000, mean episode variance: 5.63650595998764, agent episode variance: [1.574859628200531, 2.060786438465118, 2.0008598933219908], time: 180.239
Running avgs for agent 0: q_loss: 37.242942810058594, p_loss: -6.161338806152344, mean_rew: -12.247495101786914, variance: 6.2994384765625, lamda: 1.9565236568450928
Running avgs for agent 1: q_loss: 38.270355224609375, p_loss: -6.150055885314941, mean_rew: -12.276345665060532, variance: 8.243146896362305, lamda: 1.503836989402771
Running avgs for agent 2: q_loss: 23.337160110473633, p_loss: -6.131080150604248, mean_rew: -12.235604814391419, variance: 8.003439903259277, lamda: 1.5432151556015015

steps: 1899975, episodes: 76000, mean episode reward: -924.8938377723737, agent episode reward: [-308.2979459241246, -308.2979459241246, -308.2979459241246], time: 166.42
steps: 1899975, episodes: 76000, mean episode variance: 5.633780050754547, agent episode variance: [1.5731686816215515, 2.0597886872291564, 2.000822681903839], time: 166.421
Running avgs for agent 0: q_loss: 39.75941848754883, p_loss: -6.157982349395752, mean_rew: -12.256772720483797, variance: 6.292675018310547, lamda: 1.9667472839355469
Running avgs for agent 1: q_loss: 37.974998474121094, p_loss: -6.139261722564697, mean_rew: -12.254985464712544, variance: 8.239153861999512, lamda: 1.5042964220046997
Running avgs for agent 2: q_loss: 26.85713005065918, p_loss: -6.14058256149292, mean_rew: -12.24991135075314, variance: 8.003290176391602, lamda: 1.5507185459136963

steps: 1924975, episodes: 77000, mean episode reward: -924.3026248078414, agent episode reward: [-308.1008749359471, -308.1008749359471, -308.1008749359471], time: 175.807
steps: 1924975, episodes: 77000, mean episode variance: 5.5956962115764615, agent episode variance: [1.5593183634281158, 2.060124670982361, 1.976253177165985], time: 175.807
Running avgs for agent 0: q_loss: 37.81285095214844, p_loss: -6.1584296226501465, mean_rew: -12.246917900197529, variance: 6.237273216247559, lamda: 1.9739481210708618
Running avgs for agent 1: q_loss: 38.294151306152344, p_loss: -6.134937286376953, mean_rew: -12.238475969089333, variance: 8.240499496459961, lamda: 1.5045238733291626
Running avgs for agent 2: q_loss: 25.71067237854004, p_loss: -6.144504070281982, mean_rew: -12.254156475634996, variance: 7.905012607574463, lamda: 1.558988332748413

steps: 1949975, episodes: 78000, mean episode reward: -929.3808054753296, agent episode reward: [-309.7936018251098, -309.7936018251098, -309.7936018251098], time: 171.38
steps: 1949975, episodes: 78000, mean episode variance: 5.583226633787155, agent episode variance: [1.569239253282547, 2.0390182576179505, 1.9749691228866577], time: 171.38
Running avgs for agent 0: q_loss: 63.02252960205078, p_loss: -6.139901161193848, mean_rew: -12.21872149580004, variance: 6.276957035064697, lamda: 1.9812827110290527
Running avgs for agent 1: q_loss: 38.266448974609375, p_loss: -6.1298112869262695, mean_rew: -12.231535568965985, variance: 8.156072616577148, lamda: 1.5055030584335327
Running avgs for agent 2: q_loss: 23.270545959472656, p_loss: -6.12687873840332, mean_rew: -12.237412452507286, variance: 7.899876117706299, lamda: 1.5623050928115845

steps: 1974975, episodes: 79000, mean episode reward: -922.6080829710829, agent episode reward: [-307.53602765702766, -307.53602765702766, -307.53602765702766], time: 180.212
steps: 1974975, episodes: 79000, mean episode variance: 5.578126838445663, agent episode variance: [1.5549678709506989, 2.067628313064575, 1.9555306544303894], time: 180.212
Running avgs for agent 0: q_loss: 54.63959503173828, p_loss: -6.135446071624756, mean_rew: -12.200687878650504, variance: 6.219871520996094, lamda: 1.9850072860717773
Running avgs for agent 1: q_loss: 37.87209701538086, p_loss: -6.114794731140137, mean_rew: -12.21010340891247, variance: 8.270513534545898, lamda: 1.506293773651123
Running avgs for agent 2: q_loss: 24.67784309387207, p_loss: -6.120665073394775, mean_rew: -12.210928934658822, variance: 7.822122097015381, lamda: 1.5659757852554321

steps: 1999975, episodes: 80000, mean episode reward: -934.12592796379, agent episode reward: [-311.37530932126333, -311.37530932126333, -311.37530932126333], time: 176.84
steps: 1999975, episodes: 80000, mean episode variance: 5.552988579034805, agent episode variance: [1.5388060796260834, 2.0583387293815614, 1.9558437700271607], time: 176.841
Running avgs for agent 0: q_loss: 40.52699279785156, p_loss: -6.1397480964660645, mean_rew: -12.201570460241953, variance: 6.155224800109863, lamda: 1.9966477155685425
Running avgs for agent 1: q_loss: 38.576629638671875, p_loss: -6.129290580749512, mean_rew: -12.241670718783876, variance: 8.233355522155762, lamda: 1.5073726177215576
Running avgs for agent 2: q_loss: 23.636945724487305, p_loss: -6.116878986358643, mean_rew: -12.209584283355515, variance: 7.823375225067139, lamda: 1.5701472759246826

steps: 2024975, episodes: 81000, mean episode reward: -885.1671323258022, agent episode reward: [-295.05571077526747, -295.05571077526747, -295.05571077526747], time: 167.899
steps: 2024975, episodes: 81000, mean episode variance: 5.534801225423813, agent episode variance: [1.5338715622425079, 2.0403443789482116, 1.9605852842330933], time: 167.899
Running avgs for agent 0: q_loss: 37.092376708984375, p_loss: -6.141512393951416, mean_rew: -12.213151775844239, variance: 6.135486125946045, lamda: 2.0061089992523193
Running avgs for agent 1: q_loss: 38.9421501159668, p_loss: -6.12227725982666, mean_rew: -12.203843700960956, variance: 8.161377906799316, lamda: 1.5094894170761108
Running avgs for agent 2: q_loss: 25.076183319091797, p_loss: -6.116147518157959, mean_rew: -12.204782194321194, variance: 7.84234094619751, lamda: 1.574009656906128

steps: 2049975, episodes: 82000, mean episode reward: -902.6392961375058, agent episode reward: [-300.8797653791686, -300.8797653791686, -300.8797653791686], time: 158.914
steps: 2049975, episodes: 82000, mean episode variance: 5.527637401342392, agent episode variance: [1.5322095143795014, 2.0461475687026978, 1.9492803182601928], time: 158.915
Running avgs for agent 0: q_loss: 37.78342056274414, p_loss: -6.130855083465576, mean_rew: -12.195487645625356, variance: 6.128838062286377, lamda: 2.013376235961914
Running avgs for agent 1: q_loss: 37.94998550415039, p_loss: -6.104944229125977, mean_rew: -12.20614905607663, variance: 8.184590339660645, lamda: 1.5097956657409668
Running avgs for agent 2: q_loss: 24.98345375061035, p_loss: -6.1098785400390625, mean_rew: -12.184441075289422, variance: 7.797121524810791, lamda: 1.5781842470169067

steps: 2074975, episodes: 83000, mean episode reward: -901.0667453414493, agent episode reward: [-300.3555817804831, -300.3555817804831, -300.3555817804831], time: 170.113
steps: 2074975, episodes: 83000, mean episode variance: 5.53848567199707, agent episode variance: [1.5204920310974122, 2.051726218700409, 1.9662674221992493], time: 170.114
Running avgs for agent 0: q_loss: 42.48357391357422, p_loss: -6.133187294006348, mean_rew: -12.18933920721759, variance: 6.081968307495117, lamda: 2.019974708557129
Running avgs for agent 1: q_loss: 37.3660774230957, p_loss: -6.098391056060791, mean_rew: -12.190116701143957, variance: 8.206905364990234, lamda: 1.5103198289871216
Running avgs for agent 2: q_loss: 32.49028778076172, p_loss: -6.1086859703063965, mean_rew: -12.190781866964425, variance: 7.86506986618042, lamda: 1.5852288007736206

steps: 2099975, episodes: 84000, mean episode reward: -925.9491270861176, agent episode reward: [-308.64970902870584, -308.64970902870584, -308.64970902870584], time: 180.118
steps: 2099975, episodes: 84000, mean episode variance: 5.486817103624344, agent episode variance: [1.4987206456661224, 2.0452869791984556, 1.9428094787597656], time: 180.119
Running avgs for agent 0: q_loss: 40.243595123291016, p_loss: -6.102198123931885, mean_rew: -12.141615992130395, variance: 5.994882583618164, lamda: 2.0298569202423096
Running avgs for agent 1: q_loss: 38.370582580566406, p_loss: -6.089885234832764, mean_rew: -12.156355760424333, variance: 8.181147575378418, lamda: 1.511102557182312
Running avgs for agent 2: q_loss: 38.959388732910156, p_loss: -6.098945140838623, mean_rew: -12.178393715788479, variance: 7.771238327026367, lamda: 1.591310739517212

steps: 2124975, episodes: 85000, mean episode reward: -919.186560669035, agent episode reward: [-306.3955202230116, -306.3955202230116, -306.3955202230116], time: 181.285
steps: 2124975, episodes: 85000, mean episode variance: 5.486154474079609, agent episode variance: [1.5046688397526742, 2.0436951503753664, 1.9377904839515685], time: 181.285
Running avgs for agent 0: q_loss: 47.31032180786133, p_loss: -6.120582103729248, mean_rew: -12.179808711972425, variance: 6.018675327301025, lamda: 2.0448179244995117
Running avgs for agent 1: q_loss: 37.77182388305664, p_loss: -6.079254627227783, mean_rew: -12.145328591128084, variance: 8.174779891967773, lamda: 1.5121203660964966
Running avgs for agent 2: q_loss: 24.687183380126953, p_loss: -6.09812068939209, mean_rew: -12.163219527038535, variance: 7.751161575317383, lamda: 1.5960454940795898

steps: 2149975, episodes: 86000, mean episode reward: -913.9252299234029, agent episode reward: [-304.64174330780094, -304.64174330780094, -304.64174330780094], time: 170.466
steps: 2149975, episodes: 86000, mean episode variance: 5.4629425196647645, agent episode variance: [1.4849026741981506, 2.047262069225311, 1.9307777762413025], time: 170.467
Running avgs for agent 0: q_loss: 39.4287109375, p_loss: -6.1125922203063965, mean_rew: -12.15232746814971, variance: 5.939610958099365, lamda: 2.05941104888916
Running avgs for agent 1: q_loss: 37.61302947998047, p_loss: -6.091892719268799, mean_rew: -12.166578515783188, variance: 8.189048767089844, lamda: 1.5123252868652344
Running avgs for agent 2: q_loss: 24.885164260864258, p_loss: -6.094247817993164, mean_rew: -12.152476618278332, variance: 7.723111152648926, lamda: 1.6000245809555054

steps: 2174975, episodes: 87000, mean episode reward: -900.4698150076356, agent episode reward: [-300.15660500254523, -300.15660500254523, -300.15660500254523], time: 168.118
steps: 2174975, episodes: 87000, mean episode variance: 5.4100929691791535, agent episode variance: [1.4817434935569762, 2.0201329834461212, 1.9082164921760558], time: 168.118
Running avgs for agent 0: q_loss: 51.64499282836914, p_loss: -6.102376937866211, mean_rew: -12.129915922550436, variance: 5.926973819732666, lamda: 2.0697126388549805
Running avgs for agent 1: q_loss: 38.656646728515625, p_loss: -6.084471702575684, mean_rew: -12.141101266863487, variance: 8.080531120300293, lamda: 1.5129626989364624
Running avgs for agent 2: q_loss: 34.460819244384766, p_loss: -6.101008892059326, mean_rew: -12.15410240339063, variance: 7.632865905761719, lamda: 1.6040390729904175

steps: 2199975, episodes: 88000, mean episode reward: -912.0698278746912, agent episode reward: [-304.0232759582304, -304.0232759582304, -304.0232759582304], time: 167.665
steps: 2199975, episodes: 88000, mean episode variance: 5.396915061712265, agent episode variance: [1.4772470619678497, 2.0183953046798706, 1.9012726950645447], time: 167.666
Running avgs for agent 0: q_loss: 59.693050384521484, p_loss: -6.115407943725586, mean_rew: -12.158827800052588, variance: 5.9089884757995605, lamda: 2.078627586364746
Running avgs for agent 1: q_loss: 37.554603576660156, p_loss: -6.085307598114014, mean_rew: -12.14679910512585, variance: 8.07358169555664, lamda: 1.5139501094818115
Running avgs for agent 2: q_loss: 28.0379581451416, p_loss: -6.082499027252197, mean_rew: -12.13764639263443, variance: 7.605090618133545, lamda: 1.6106655597686768

steps: 2224975, episodes: 89000, mean episode reward: -921.9486975867674, agent episode reward: [-307.3162325289225, -307.3162325289225, -307.3162325289225], time: 171.13
steps: 2224975, episodes: 89000, mean episode variance: 5.406849756240844, agent episode variance: [1.4696193952560426, 2.0286821098327636, 1.9085482511520386], time: 171.131
Running avgs for agent 0: q_loss: 48.363922119140625, p_loss: -6.116127967834473, mean_rew: -12.163721223845847, variance: 5.878477573394775, lamda: 2.089198350906372
Running avgs for agent 1: q_loss: 37.581321716308594, p_loss: -6.081093788146973, mean_rew: -12.158118126407386, variance: 8.114728927612305, lamda: 1.5142297744750977
Running avgs for agent 2: q_loss: 26.890254974365234, p_loss: -6.079683780670166, mean_rew: -12.133985912089445, variance: 7.634193420410156, lamda: 1.6185561418533325

steps: 2249975, episodes: 90000, mean episode reward: -895.8528255851995, agent episode reward: [-298.61760852839984, -298.61760852839984, -298.61760852839984], time: 175.318
steps: 2249975, episodes: 90000, mean episode variance: 5.37099157834053, agent episode variance: [1.4586157531738282, 2.0208832297325134, 1.8914925954341888], time: 175.318
Running avgs for agent 0: q_loss: 38.01450729370117, p_loss: -6.108570098876953, mean_rew: -12.144736856090235, variance: 5.834463596343994, lamda: 2.099153757095337
Running avgs for agent 1: q_loss: 38.27531814575195, p_loss: -6.094118118286133, mean_rew: -12.166898424588322, variance: 8.083532333374023, lamda: 1.5146528482437134
Running avgs for agent 2: q_loss: 43.62309646606445, p_loss: -6.090627670288086, mean_rew: -12.15207350529699, variance: 7.565970420837402, lamda: 1.6225862503051758

steps: 2274975, episodes: 91000, mean episode reward: -923.1007491475142, agent episode reward: [-307.7002497158381, -307.7002497158381, -307.7002497158381], time: 174.471
steps: 2274975, episodes: 91000, mean episode variance: 5.3683577048778535, agent episode variance: [1.458133111000061, 2.0163766572475432, 1.893847936630249], time: 174.472
Running avgs for agent 0: q_loss: 40.97022247314453, p_loss: -6.105435848236084, mean_rew: -12.143038381887237, variance: 5.8325324058532715, lamda: 2.1105799674987793
Running avgs for agent 1: q_loss: 37.219295501708984, p_loss: -6.08414363861084, mean_rew: -12.156166423546791, variance: 8.065506935119629, lamda: 1.5157634019851685
Running avgs for agent 2: q_loss: 38.275169372558594, p_loss: -6.087933540344238, mean_rew: -12.153351899492353, variance: 7.57539176940918, lamda: 1.6270416975021362

steps: 2299975, episodes: 92000, mean episode reward: -886.6510490469908, agent episode reward: [-295.55034968233025, -295.55034968233025, -295.55034968233025], time: 181.311
steps: 2299975, episodes: 92000, mean episode variance: 5.364443259954452, agent episode variance: [1.4361796057224274, 2.0408375697135925, 1.8874260845184325], time: 181.311
Running avgs for agent 0: q_loss: 39.812469482421875, p_loss: -6.1137871742248535, mean_rew: -12.155235971360902, variance: 5.744718074798584, lamda: 2.122422456741333
Running avgs for agent 1: q_loss: 37.20392608642578, p_loss: -6.086991310119629, mean_rew: -12.159724772958219, variance: 8.163351058959961, lamda: 1.5159064531326294
Running avgs for agent 2: q_loss: 26.444812774658203, p_loss: -6.091066837310791, mean_rew: -12.170956807831082, variance: 7.549704074859619, lamda: 1.633516788482666

steps: 2324975, episodes: 93000, mean episode reward: -890.620241402796, agent episode reward: [-296.873413800932, -296.873413800932, -296.873413800932], time: 156.515
steps: 2324975, episodes: 93000, mean episode variance: 5.316603949546814, agent episode variance: [1.427359152317047, 2.0148978843688963, 1.8743469128608703], time: 156.515
Running avgs for agent 0: q_loss: 38.94341278076172, p_loss: -6.1087141036987305, mean_rew: -12.139463707623053, variance: 5.709436416625977, lamda: 2.1331100463867188
Running avgs for agent 1: q_loss: 37.703895568847656, p_loss: -6.076266765594482, mean_rew: -12.144908084953729, variance: 8.059591293334961, lamda: 1.5160812139511108
Running avgs for agent 2: q_loss: 24.10211753845215, p_loss: -6.090800762176514, mean_rew: -12.152345154023283, variance: 7.497387886047363, lamda: 1.6372963190078735

steps: 2349975, episodes: 94000, mean episode reward: -885.7549082128168, agent episode reward: [-295.2516360709389, -295.2516360709389, -295.2516360709389], time: 114.998
steps: 2349975, episodes: 94000, mean episode variance: 5.323577035188675, agent episode variance: [1.4322938964366914, 2.021302817821503, 1.869980320930481], time: 114.998
Running avgs for agent 0: q_loss: 40.421043395996094, p_loss: -6.102023601531982, mean_rew: -12.133500379408225, variance: 5.729175567626953, lamda: 2.1421449184417725
Running avgs for agent 1: q_loss: 37.38022232055664, p_loss: -6.087911128997803, mean_rew: -12.1554760855566, variance: 8.085211753845215, lamda: 1.516326665878296
Running avgs for agent 2: q_loss: 25.097328186035156, p_loss: -6.090848445892334, mean_rew: -12.144884214256406, variance: 7.479921340942383, lamda: 1.6415224075317383

steps: 2374975, episodes: 95000, mean episode reward: -900.8435358259372, agent episode reward: [-300.2811786086458, -300.2811786086458, -300.2811786086458], time: 112.759
steps: 2374975, episodes: 95000, mean episode variance: 5.309734235048294, agent episode variance: [1.4152210576534272, 2.0213337593078613, 1.8731794180870056], time: 112.76
Running avgs for agent 0: q_loss: 49.74931335449219, p_loss: -6.127206325531006, mean_rew: -12.172550025413072, variance: 5.660883903503418, lamda: 2.156856060028076
Running avgs for agent 1: q_loss: 36.80118179321289, p_loss: -6.098171710968018, mean_rew: -12.182548710196842, variance: 8.085334777832031, lamda: 1.5167484283447266
Running avgs for agent 2: q_loss: 24.15093421936035, p_loss: -6.108156204223633, mean_rew: -12.179301928838246, variance: 7.492717742919922, lamda: 1.645816445350647

steps: 2399975, episodes: 96000, mean episode reward: -902.6608818392236, agent episode reward: [-300.8869606130746, -300.8869606130746, -300.8869606130746], time: 117.329
steps: 2399975, episodes: 96000, mean episode variance: 5.298395256519318, agent episode variance: [1.4155396175384523, 2.024491781234741, 1.8583638577461243], time: 117.33
Running avgs for agent 0: q_loss: 43.61834716796875, p_loss: -6.108786106109619, mean_rew: -12.156361689564651, variance: 5.662158489227295, lamda: 2.1681954860687256
Running avgs for agent 1: q_loss: 37.42770004272461, p_loss: -6.090502738952637, mean_rew: -12.173242158103356, variance: 8.097967147827148, lamda: 1.5171623229980469
Running avgs for agent 2: q_loss: 25.84510612487793, p_loss: -6.09526252746582, mean_rew: -12.145781308371294, variance: 7.433454990386963, lamda: 1.6515268087387085

steps: 2424975, episodes: 97000, mean episode reward: -893.8234105838171, agent episode reward: [-297.9411368612724, -297.9411368612724, -297.9411368612724], time: 108.278
steps: 2424975, episodes: 97000, mean episode variance: 5.298419064760208, agent episode variance: [1.4058153202533723, 2.0361016426086427, 1.8565021018981933], time: 108.278
Running avgs for agent 0: q_loss: 46.197296142578125, p_loss: -6.119131088256836, mean_rew: -12.173859670769929, variance: 5.623261451721191, lamda: 2.1828019618988037
Running avgs for agent 1: q_loss: 37.71220397949219, p_loss: -6.098356246948242, mean_rew: -12.187338261600624, variance: 8.14440631866455, lamda: 1.5177737474441528
Running avgs for agent 2: q_loss: 29.44064712524414, p_loss: -6.113473892211914, mean_rew: -12.18574436345573, variance: 7.426008701324463, lamda: 1.6590148210525513

steps: 2449975, episodes: 98000, mean episode reward: -911.3842506412572, agent episode reward: [-303.7947502137524, -303.7947502137524, -303.7947502137524], time: 101.59
steps: 2449975, episodes: 98000, mean episode variance: 5.2643647954463955, agent episode variance: [1.3925176899433136, 2.01542852973938, 1.8564185757637024], time: 101.591
Running avgs for agent 0: q_loss: 53.28468704223633, p_loss: -6.1203837394714355, mean_rew: -12.182197369168987, variance: 5.570070743560791, lamda: 2.1961543560028076
Running avgs for agent 1: q_loss: 37.365692138671875, p_loss: -6.093099117279053, mean_rew: -12.17524415563434, variance: 8.061714172363281, lamda: 1.518277883529663
Running avgs for agent 2: q_loss: 24.126136779785156, p_loss: -6.106951713562012, mean_rew: -12.17518409931345, variance: 7.4256744384765625, lamda: 1.6640464067459106

steps: 2474975, episodes: 99000, mean episode reward: -924.8162706365003, agent episode reward: [-308.2720902121668, -308.2720902121668, -308.2720902121668], time: 101.906
steps: 2474975, episodes: 99000, mean episode variance: 5.237603214025498, agent episode variance: [1.3891024975776671, 2.007220904827118, 1.8412798116207123], time: 101.907
Running avgs for agent 0: q_loss: 60.03009796142578, p_loss: -6.106928825378418, mean_rew: -12.155198631913972, variance: 5.556410312652588, lamda: 2.2087440490722656
Running avgs for agent 1: q_loss: 34.241905212402344, p_loss: -6.094308376312256, mean_rew: -12.178709020581476, variance: 8.028883934020996, lamda: 1.5185803174972534
Running avgs for agent 2: q_loss: 42.31713104248047, p_loss: -6.113238334655762, mean_rew: -12.191739387685109, variance: 7.365118980407715, lamda: 1.667630910873413

steps: 2499975, episodes: 100000, mean episode reward: -925.2015257305079, agent episode reward: [-308.400508576836, -308.400508576836, -308.400508576836], time: 106.7
steps: 2499975, episodes: 100000, mean episode variance: 5.22035619199276, agent episode variance: [1.377711202979088, 2.0025361771583556, 1.8401088118553162], time: 106.7
Running avgs for agent 0: q_loss: 56.540348052978516, p_loss: -6.1229448318481445, mean_rew: -12.18300315029849, variance: 5.510844707489014, lamda: 2.2167136669158936
Running avgs for agent 1: q_loss: 24.259151458740234, p_loss: -6.0925750732421875, mean_rew: -12.173391952615434, variance: 8.01014518737793, lamda: 1.521878957748413
Running avgs for agent 2: q_loss: 23.53410530090332, p_loss: -6.10358190536499, mean_rew: -12.193078714629445, variance: 7.3604350090026855, lamda: 1.6708978414535522

steps: 2524975, episodes: 101000, mean episode reward: -926.782070774264, agent episode reward: [-308.9273569247547, -308.9273569247547, -308.9273569247547], time: 96.838
steps: 2524975, episodes: 101000, mean episode variance: 5.221047066450119, agent episode variance: [1.3811746361255646, 1.998555067062378, 1.8413173632621764], time: 96.838
Running avgs for agent 0: q_loss: 54.4891471862793, p_loss: -6.117218971252441, mean_rew: -12.173563127752692, variance: 5.524698734283447, lamda: 2.2262182235717773
Running avgs for agent 1: q_loss: 20.985946655273438, p_loss: -6.096392631530762, mean_rew: -12.176804752538452, variance: 7.99422025680542, lamda: 1.5257776975631714
Running avgs for agent 2: q_loss: 24.217273712158203, p_loss: -6.095914840698242, mean_rew: -12.17471654353935, variance: 7.365269660949707, lamda: 1.6744309663772583

steps: 2549975, episodes: 102000, mean episode reward: -901.6724026940608, agent episode reward: [-300.5574675646869, -300.5574675646869, -300.5574675646869], time: 91.037
steps: 2549975, episodes: 102000, mean episode variance: 5.200461285114288, agent episode variance: [1.3810939819812775, 1.9985308074951171, 1.8208364956378937], time: 91.038
Running avgs for agent 0: q_loss: 43.574092864990234, p_loss: -6.129897594451904, mean_rew: -12.206993686226967, variance: 5.524375915527344, lamda: 2.2417356967926025
Running avgs for agent 1: q_loss: 21.406091690063477, p_loss: -6.108147621154785, mean_rew: -12.196603664768949, variance: 7.9941229820251465, lamda: 1.5280766487121582
Running avgs for agent 2: q_loss: 38.7296142578125, p_loss: -6.114896297454834, mean_rew: -12.193252502204832, variance: 7.283345699310303, lamda: 1.678754448890686

steps: 2574975, episodes: 103000, mean episode reward: -883.4428180231026, agent episode reward: [-294.4809393410342, -294.4809393410342, -294.4809393410342], time: 89.044
steps: 2574975, episodes: 103000, mean episode variance: 5.18410359287262, agent episode variance: [1.3576457591056823, 1.9964818668365478, 1.8299759669303894], time: 89.044
Running avgs for agent 0: q_loss: 47.10943603515625, p_loss: -6.117712497711182, mean_rew: -12.182409825380539, variance: 5.4305830001831055, lamda: 2.254709005355835
Running avgs for agent 1: q_loss: 20.208621978759766, p_loss: -6.09372615814209, mean_rew: -12.176660546079066, variance: 7.985927104949951, lamda: 1.5327978134155273
Running avgs for agent 2: q_loss: 25.492347717285156, p_loss: -6.107571601867676, mean_rew: -12.19272914746163, variance: 7.31990385055542, lamda: 1.6832307577133179

steps: 2599975, episodes: 104000, mean episode reward: -883.8571793021033, agent episode reward: [-294.6190597673678, -294.6190597673678, -294.6190597673678], time: 88.321
steps: 2599975, episodes: 104000, mean episode variance: 5.1569480645656585, agent episode variance: [1.3498690392971038, 1.9957516770362853, 1.8113273482322694], time: 88.322
Running avgs for agent 0: q_loss: 68.85111236572266, p_loss: -6.1233134269714355, mean_rew: -12.159391933576304, variance: 5.399476528167725, lamda: 2.2632482051849365
Running avgs for agent 1: q_loss: 21.717138290405273, p_loss: -6.096930503845215, mean_rew: -12.170382197323638, variance: 7.983006477355957, lamda: 1.5352836847305298
Running avgs for agent 2: q_loss: 25.824020385742188, p_loss: -6.100466728210449, mean_rew: -12.17069963931575, variance: 7.245309352874756, lamda: 1.6890524625778198

steps: 2624975, episodes: 105000, mean episode reward: -894.1661985181559, agent episode reward: [-298.05539950605197, -298.05539950605197, -298.05539950605197], time: 84.871
steps: 2624975, episodes: 105000, mean episode variance: 5.132752604484558, agent episode variance: [1.342626585960388, 1.9933156175613402, 1.7968104009628296], time: 84.872
Running avgs for agent 0: q_loss: 69.15058898925781, p_loss: -6.14041805267334, mean_rew: -12.182739663581392, variance: 5.370506286621094, lamda: 2.2674639225006104
Running avgs for agent 1: q_loss: 21.611454010009766, p_loss: -6.088801860809326, mean_rew: -12.153899504220501, variance: 7.973262310028076, lamda: 1.5379137992858887
Running avgs for agent 2: q_loss: 35.52257537841797, p_loss: -6.0906853675842285, mean_rew: -12.152309181244805, variance: 7.187241077423096, lamda: 1.696972370147705

steps: 2649975, episodes: 106000, mean episode reward: -897.202797435787, agent episode reward: [-299.06759914526236, -299.06759914526236, -299.06759914526236], time: 86.786
steps: 2649975, episodes: 106000, mean episode variance: 5.141605339407921, agent episode variance: [1.353360204577446, 1.9950726585388183, 1.7931724762916565], time: 86.787
Running avgs for agent 0: q_loss: 47.36998748779297, p_loss: -6.122771739959717, mean_rew: -12.167344280655328, variance: 5.413441181182861, lamda: 2.2752230167388916
Running avgs for agent 1: q_loss: 31.477901458740234, p_loss: -6.0921406745910645, mean_rew: -12.163296719425434, variance: 7.98029088973999, lamda: 1.54271399974823
Running avgs for agent 2: q_loss: 24.203065872192383, p_loss: -6.088106632232666, mean_rew: -12.138994244557068, variance: 7.172689437866211, lamda: 1.704811692237854

steps: 2674975, episodes: 107000, mean episode reward: -892.0950279547882, agent episode reward: [-297.3650093182627, -297.3650093182627, -297.3650093182627], time: 85.559
steps: 2674975, episodes: 107000, mean episode variance: 5.132592308521271, agent episode variance: [1.336137836933136, 2.003402829170227, 1.7930516424179077], time: 85.56
Running avgs for agent 0: q_loss: 57.514827728271484, p_loss: -6.125119209289551, mean_rew: -12.157887748162226, variance: 5.3445515632629395, lamda: 2.2869184017181396
Running avgs for agent 1: q_loss: 38.074493408203125, p_loss: -6.0850138664245605, mean_rew: -12.17024581738885, variance: 8.013611793518066, lamda: 1.544836163520813
Running avgs for agent 2: q_loss: 28.9143009185791, p_loss: -6.099271297454834, mean_rew: -12.158513449395302, variance: 7.172206401824951, lamda: 1.7129184007644653

steps: 2699975, episodes: 108000, mean episode reward: -881.8231664618103, agent episode reward: [-293.9410554872701, -293.9410554872701, -293.9410554872701], time: 91.088
steps: 2699975, episodes: 108000, mean episode variance: 5.145423425674439, agent episode variance: [1.3540319566726684, 1.998200975894928, 1.793190493106842], time: 91.089
Running avgs for agent 0: q_loss: 67.7087631225586, p_loss: -6.112270355224609, mean_rew: -12.138718418791552, variance: 5.416128158569336, lamda: 2.293022394180298
Running avgs for agent 1: q_loss: 37.15296173095703, p_loss: -6.082095623016357, mean_rew: -12.147947631395077, variance: 7.992803573608398, lamda: 1.545320987701416
Running avgs for agent 2: q_loss: 24.394359588623047, p_loss: -6.093980312347412, mean_rew: -12.148233446452677, variance: 7.172761917114258, lamda: 1.7195327281951904

steps: 2724975, episodes: 109000, mean episode reward: -887.15286613501, agent episode reward: [-295.7176220450032, -295.7176220450032, -295.7176220450032], time: 81.998
steps: 2724975, episodes: 109000, mean episode variance: 5.079694676399231, agent episode variance: [1.328102044582367, 1.978290361404419, 1.773302270412445], time: 81.999
Running avgs for agent 0: q_loss: 68.60379791259766, p_loss: -6.113470077514648, mean_rew: -12.139940654085704, variance: 5.312407970428467, lamda: 2.297252655029297
Running avgs for agent 1: q_loss: 37.24525833129883, p_loss: -6.076506614685059, mean_rew: -12.132793169808778, variance: 7.9131622314453125, lamda: 1.5453218221664429
Running avgs for agent 2: q_loss: 28.077220916748047, p_loss: -6.095633506774902, mean_rew: -12.142026568505807, variance: 7.0932087898254395, lamda: 1.7260397672653198

steps: 2749975, episodes: 110000, mean episode reward: -902.6956889420863, agent episode reward: [-300.8985629806954, -300.8985629806954, -300.8985629806954], time: 72.958
steps: 2749975, episodes: 110000, mean episode variance: 5.073138602495193, agent episode variance: [1.3283482472896575, 1.9881842503547669, 1.756606104850769], time: 72.958
Running avgs for agent 0: q_loss: 41.506046295166016, p_loss: -6.096292972564697, mean_rew: -12.11759201636959, variance: 5.3133931159973145, lamda: 2.306941509246826
Running avgs for agent 1: q_loss: 37.202781677246094, p_loss: -6.06818151473999, mean_rew: -12.12708831793738, variance: 7.952737331390381, lamda: 1.545767068862915
Running avgs for agent 2: q_loss: 37.86317825317383, p_loss: -6.08859395980835, mean_rew: -12.138222821632421, variance: 7.026424407958984, lamda: 1.7347761392593384

steps: 2774975, episodes: 111000, mean episode reward: -888.6115286160759, agent episode reward: [-296.20384287202535, -296.20384287202535, -296.20384287202535], time: 73.453
steps: 2774975, episodes: 111000, mean episode variance: 5.058731322050095, agent episode variance: [1.327158584356308, 1.980051568031311, 1.7515211696624755], time: 73.453
Running avgs for agent 0: q_loss: 48.313480377197266, p_loss: -6.083087921142578, mean_rew: -12.101604495263253, variance: 5.3086347579956055, lamda: 2.3201186656951904
Running avgs for agent 1: q_loss: 37.08863830566406, p_loss: -6.054520606994629, mean_rew: -12.093251521727009, variance: 7.920206069946289, lamda: 1.5459856986999512
Running avgs for agent 2: q_loss: 26.732284545898438, p_loss: -6.067288398742676, mean_rew: -12.119992411211237, variance: 7.00608491897583, lamda: 1.7442383766174316

steps: 2799975, episodes: 112000, mean episode reward: -886.5832814143612, agent episode reward: [-295.5277604714537, -295.5277604714537, -295.5277604714537], time: 73.592
steps: 2799975, episodes: 112000, mean episode variance: 5.021161296129227, agent episode variance: [1.2987057840824128, 1.9804203486442566, 1.7420351634025573], time: 73.592
Running avgs for agent 0: q_loss: 42.957401275634766, p_loss: -6.083062648773193, mean_rew: -12.097944975230236, variance: 5.194823265075684, lamda: 2.332223653793335
Running avgs for agent 1: q_loss: 37.31177520751953, p_loss: -6.066842555999756, mean_rew: -12.1157365932826, variance: 7.9216814041137695, lamda: 1.5463720560073853
Running avgs for agent 2: q_loss: 25.693851470947266, p_loss: -6.074136734008789, mean_rew: -12.115232742654042, variance: 6.968140602111816, lamda: 1.7547472715377808

steps: 2824975, episodes: 113000, mean episode reward: -874.2204030763658, agent episode reward: [-291.40680102545525, -291.40680102545525, -291.40680102545525], time: 74.051
steps: 2824975, episodes: 113000, mean episode variance: 4.999029982924461, agent episode variance: [1.2929346259832382, 1.9706489214897156, 1.7354464354515076], time: 74.052
Running avgs for agent 0: q_loss: 61.96901321411133, p_loss: -6.069271087646484, mean_rew: -12.085422343422257, variance: 5.171738624572754, lamda: 2.3480441570281982
Running avgs for agent 1: q_loss: 36.971195220947266, p_loss: -6.05173921585083, mean_rew: -12.091255156277624, variance: 7.882595539093018, lamda: 1.5465607643127441
Running avgs for agent 2: q_loss: 26.073711395263672, p_loss: -6.0620551109313965, mean_rew: -12.094926273285353, variance: 6.94178581237793, lamda: 1.763488531112671

steps: 2849975, episodes: 114000, mean episode reward: -912.3254860738956, agent episode reward: [-304.10849535796524, -304.10849535796524, -304.10849535796524], time: 76.64
steps: 2849975, episodes: 114000, mean episode variance: 4.971795300364494, agent episode variance: [1.286393648505211, 1.9680320315361024, 1.7173696203231812], time: 76.64
Running avgs for agent 0: q_loss: 69.88845825195312, p_loss: -6.074089050292969, mean_rew: -12.090378346674333, variance: 5.145574569702148, lamda: 2.3538825511932373
Running avgs for agent 1: q_loss: 36.50138854980469, p_loss: -6.045403003692627, mean_rew: -12.069539955190164, variance: 7.872128009796143, lamda: 1.5467296838760376
Running avgs for agent 2: q_loss: 24.038715362548828, p_loss: -6.047062397003174, mean_rew: -12.071343117104796, variance: 6.869478702545166, lamda: 1.7736639976501465

steps: 2874975, episodes: 115000, mean episode reward: -884.6893691670194, agent episode reward: [-294.89645638900646, -294.89645638900646, -294.89645638900646], time: 71.327
steps: 2874975, episodes: 115000, mean episode variance: 4.96103119468689, agent episode variance: [1.290855806350708, 1.963373056411743, 1.7068023319244385], time: 71.328
Running avgs for agent 0: q_loss: 55.75807189941406, p_loss: -6.0640869140625, mean_rew: -12.074717953242253, variance: 5.16342306137085, lamda: 2.3598244190216064
Running avgs for agent 1: q_loss: 37.08984375, p_loss: -6.045682430267334, mean_rew: -12.064389652656832, variance: 7.853492259979248, lamda: 1.5470341444015503
Running avgs for agent 2: q_loss: 24.7357234954834, p_loss: -6.057837963104248, mean_rew: -12.07423344889853, variance: 6.82720947265625, lamda: 1.7798044681549072

steps: 2899975, episodes: 116000, mean episode reward: -894.4876400143095, agent episode reward: [-298.1625466714365, -298.1625466714365, -298.1625466714365], time: 69.498
steps: 2899975, episodes: 116000, mean episode variance: 4.947147388696671, agent episode variance: [1.2835604212284089, 1.9532565937042237, 1.710330373764038], time: 69.499
Running avgs for agent 0: q_loss: 44.64797592163086, p_loss: -6.056978702545166, mean_rew: -12.0622548947883, variance: 5.134241580963135, lamda: 2.3755605220794678
Running avgs for agent 1: q_loss: 33.06856918334961, p_loss: -6.028865814208984, mean_rew: -12.051916731365203, variance: 7.813026428222656, lamda: 1.5479302406311035
Running avgs for agent 2: q_loss: 28.446868896484375, p_loss: -6.038845062255859, mean_rew: -12.075669577696901, variance: 6.8413214683532715, lamda: 1.7857558727264404

steps: 2924975, episodes: 117000, mean episode reward: -871.2333482569233, agent episode reward: [-290.4111160856411, -290.4111160856411, -290.4111160856411], time: 66.322
steps: 2924975, episodes: 117000, mean episode variance: 4.924748711705208, agent episode variance: [1.276437972187996, 1.9551492900848388, 1.693161449432373], time: 66.323
Running avgs for agent 0: q_loss: 56.12565612792969, p_loss: -6.040412902832031, mean_rew: -12.055354449249295, variance: 5.105751991271973, lamda: 2.387406587600708
Running avgs for agent 1: q_loss: 36.32355499267578, p_loss: -6.039652347564697, mean_rew: -12.063258950873394, variance: 7.820596694946289, lamda: 1.5534900426864624
Running avgs for agent 2: q_loss: 25.09667205810547, p_loss: -6.049173831939697, mean_rew: -12.053622830747738, variance: 6.772645473480225, lamda: 1.793502926826477

steps: 2949975, episodes: 118000, mean episode reward: -872.3251049247297, agent episode reward: [-290.7750349749099, -290.7750349749099, -290.7750349749099], time: 68.008
steps: 2949975, episodes: 118000, mean episode variance: 4.918539296746254, agent episode variance: [1.2696356855630875, 1.949444251537323, 1.6994593596458436], time: 68.009
Running avgs for agent 0: q_loss: 71.50237274169922, p_loss: -6.046546459197998, mean_rew: -12.02975873826571, variance: 5.078543186187744, lamda: 2.3935325145721436
Running avgs for agent 1: q_loss: 36.20068359375, p_loss: -6.028749942779541, mean_rew: -12.030979409890007, variance: 7.79777717590332, lamda: 1.5546658039093018
Running avgs for agent 2: q_loss: 23.771711349487305, p_loss: -6.01810359954834, mean_rew: -12.011049444984284, variance: 6.797837734222412, lamda: 1.7956764698028564

steps: 2974975, episodes: 119000, mean episode reward: -868.0804657478966, agent episode reward: [-289.3601552492989, -289.3601552492989, -289.3601552492989], time: 64.487
steps: 2974975, episodes: 119000, mean episode variance: 4.906269142985344, agent episode variance: [1.2640441006422043, 1.9650138754844666, 1.677211166858673], time: 64.488
Running avgs for agent 0: q_loss: 69.42900085449219, p_loss: -6.03496789932251, mean_rew: -12.006378775399277, variance: 5.056176662445068, lamda: 2.397378921508789
Running avgs for agent 1: q_loss: 35.80265808105469, p_loss: -5.998451232910156, mean_rew: -11.99254548552861, variance: 7.860055446624756, lamda: 1.5549414157867432
Running avgs for agent 2: q_loss: 23.184003829956055, p_loss: -6.0199384689331055, mean_rew: -11.997142492477304, variance: 6.7088446617126465, lamda: 1.7979015111923218

steps: 2999975, episodes: 120000, mean episode reward: -878.1731393531321, agent episode reward: [-292.7243797843774, -292.7243797843774, -292.7243797843774], time: 63.163
steps: 2999975, episodes: 120000, mean episode variance: 4.919280262947082, agent episode variance: [1.268151198863983, 1.959984244823456, 1.6911448192596437], time: 63.163
Running avgs for agent 0: q_loss: 59.0786018371582, p_loss: -6.023998260498047, mean_rew: -11.998911594661097, variance: 5.072605133056641, lamda: 2.4025614261627197
Running avgs for agent 1: q_loss: 35.848365783691406, p_loss: -6.007198333740234, mean_rew: -11.989699493176214, variance: 7.83993673324585, lamda: 1.5549412965774536
Running avgs for agent 2: q_loss: 24.634870529174805, p_loss: -6.008601665496826, mean_rew: -11.987246766804462, variance: 6.764579772949219, lamda: 1.800420880317688

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -877.7309764090304, agent episode reward: [-292.57699213634345, -292.57699213634345, -292.57699213634345], time: 43.487
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 43.487
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -878.9344336868651, agent episode reward: [-292.9781445622883, -292.9781445622883, -292.9781445622883], time: 54.136
steps: 49975, episodes: 2000, mean episode variance: 3.443704530954361, agent episode variance: [1.0402526609897613, 1.0739101352691651, 1.3295417346954346], time: 54.136
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -11.712343285053342, variance: 4.263330459594727, lamda: 2.4081711769104004
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -11.716597643839306, variance: 4.401271343231201, lamda: 1.554948091506958
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -11.698021115266997, variance: 5.448941230773926, lamda: 1.8049237728118896

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567761459.1092102: line 9: --exp_var_alpha: command not found
