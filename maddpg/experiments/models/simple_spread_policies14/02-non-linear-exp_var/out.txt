# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 15.0 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies14/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies14/02-non-linear-exp_var/
Job <1092150> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc045>>
arglist.u_estimation True
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -514.5296869100338, agent episode reward: [-171.50989563667792, -171.50989563667792, -171.50989563667792], time: 160.348
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 160.349
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -767.2554600261387, agent episode reward: [-255.75182000871288, -255.75182000871288, -255.75182000871288], time: 190.315
steps: 49975, episodes: 2000, mean episode variance: 8.36537422311306, agent episode variance: [2.4415698883533477, 3.002341563463211, 2.9214627712965013], time: 190.316
Running avgs for agent 0: q_loss: 537.0291137695312, p_loss: 4.691697597503662, mean_rew: -7.901728218354053, variance: 10.006433968661261, lamda: 1.010858178138733
Running avgs for agent 1: q_loss: 148.1761474609375, p_loss: -5.67648983001709, mean_rew: -7.908557554318619, variance: 12.304678538783651, lamda: 1.0103439092636108
Running avgs for agent 2: q_loss: 121.36087036132812, p_loss: -5.8443121910095215, mean_rew: -7.907261778868503, variance: 11.97320807908402, lamda: 1.0111366510391235

steps: 74975, episodes: 3000, mean episode reward: -662.6994635251405, agent episode reward: [-220.89982117504678, -220.89982117504678, -220.89982117504678], time: 188.792
steps: 74975, episodes: 3000, mean episode variance: 6.41714640134573, agent episode variance: [2.1848960847258567, 2.131787700653076, 2.100462615966797], time: 188.793
Running avgs for agent 0: q_loss: 2526.5673828125, p_loss: 18.61278533935547, mean_rew: -8.653401767738885, variance: 8.739584338903427, lamda: 1.0355271100997925
Running avgs for agent 1: q_loss: 26.756128311157227, p_loss: -4.75796365737915, mean_rew: -8.669477174987895, variance: 8.527151107788086, lamda: 1.030795931816101
Running avgs for agent 2: q_loss: 26.517921447753906, p_loss: -4.688071250915527, mean_rew: -8.642915791048319, variance: 8.401849746704102, lamda: 1.0336482524871826

steps: 99975, episodes: 4000, mean episode reward: -712.9029054792377, agent episode reward: [-237.63430182641255, -237.63430182641255, -237.63430182641255], time: 189.804
steps: 99975, episodes: 4000, mean episode variance: 11.089869886994363, agent episode variance: [6.90368189060688, 2.1008930344581604, 2.0852949619293213], time: 189.805
Running avgs for agent 0: q_loss: 12671.4736328125, p_loss: 33.5513801574707, mean_rew: -8.724729251768165, variance: 27.61472756242752, lamda: 1.0605312585830688
Running avgs for agent 1: q_loss: 33.41876220703125, p_loss: -4.6300530433654785, mean_rew: -8.736115179448111, variance: 8.403572082519531, lamda: 1.0548781156539917
Running avgs for agent 2: q_loss: 31.130844116210938, p_loss: -4.550525188446045, mean_rew: -8.735517115688701, variance: 8.341179847717285, lamda: 1.0582327842712402

steps: 124975, episodes: 5000, mean episode reward: -578.3843412935169, agent episode reward: [-192.79478043117226, -192.79478043117226, -192.79478043117226], time: 184.634
steps: 124975, episodes: 5000, mean episode variance: 15.486145639419556, agent episode variance: [11.361306007385254, 2.069249683380127, 2.0555899486541747], time: 184.635
Running avgs for agent 0: q_loss: 30671.994140625, p_loss: 49.188961029052734, mean_rew: -8.769565894920706, variance: 45.445224029541016, lamda: 1.0855354070663452
Running avgs for agent 1: q_loss: 32.24214553833008, p_loss: -4.5004706382751465, mean_rew: -8.763778089276885, variance: 8.276999473571777, lamda: 1.0798823833465576
Running avgs for agent 2: q_loss: 29.297039031982422, p_loss: -4.547767639160156, mean_rew: -8.77330214194691, variance: 8.222359657287598, lamda: 1.0828397274017334

steps: 149975, episodes: 6000, mean episode reward: -551.5591698172555, agent episode reward: [-183.8530566057518, -183.8530566057518, -183.8530566057518], time: 174.913
steps: 149975, episodes: 6000, mean episode variance: 17.0636017267704, agent episode variance: [13.189887075424194, 1.9504443509578704, 1.9232703003883362], time: 174.914
Running avgs for agent 0: q_loss: 50382.4375, p_loss: 63.1097412109375, mean_rew: -8.535755022348491, variance: 52.759548301696775, lamda: 1.1105395555496216
Running avgs for agent 1: q_loss: 35.162532806396484, p_loss: -4.40268087387085, mean_rew: -8.532164295470098, variance: 7.801777362823486, lamda: 1.1023615598678589
Running avgs for agent 2: q_loss: 24.431983947753906, p_loss: -4.43001651763916, mean_rew: -8.525740861465609, variance: 7.693081378936768, lamda: 1.1065707206726074

steps: 174975, episodes: 7000, mean episode reward: -544.9098608660405, agent episode reward: [-181.63662028868012, -181.63662028868012, -181.63662028868012], time: 175.006
steps: 174975, episodes: 7000, mean episode variance: 21.5427517080307, agent episode variance: [17.81038355255127, 1.8651305031776428, 1.8672376523017884], time: 175.007
Running avgs for agent 0: q_loss: 86031.375, p_loss: 75.24620056152344, mean_rew: -8.337700604891657, variance: 71.24153421020507, lamda: 1.1355438232421875
Running avgs for agent 1: q_loss: 21.970890045166016, p_loss: -4.28547477722168, mean_rew: -8.332088003419782, variance: 7.460521697998047, lamda: 1.121564507484436
Running avgs for agent 2: q_loss: 22.582874298095703, p_loss: -4.277304649353027, mean_rew: -8.326833271177282, variance: 7.4689507484436035, lamda: 1.1276724338531494

steps: 199975, episodes: 8000, mean episode reward: -550.3593724370352, agent episode reward: [-183.4531241456784, -183.4531241456784, -183.4531241456784], time: 173.942
steps: 199975, episodes: 8000, mean episode variance: 26.257432640790938, agent episode variance: [22.654661628723144, 1.813475245475769, 1.7892957665920257], time: 173.943
Running avgs for agent 0: q_loss: 172526.53125, p_loss: 86.60087585449219, mean_rew: -8.209343627527767, variance: 90.61864651489257, lamda: 1.1605478525161743
Running avgs for agent 1: q_loss: 21.29344940185547, p_loss: -4.226757049560547, mean_rew: -8.21243813680511, variance: 7.25390100479126, lamda: 1.1447489261627197
Running avgs for agent 2: q_loss: 24.571319580078125, p_loss: -4.236659526824951, mean_rew: -8.205954195563605, variance: 7.1571831703186035, lamda: 1.149674892425537

steps: 224975, episodes: 9000, mean episode reward: -529.4367834547812, agent episode reward: [-176.47892781826036, -176.47892781826036, -176.47892781826036], time: 175.637
steps: 224975, episodes: 9000, mean episode variance: 46.71545741868019, agent episode variance: [43.19079417419434, 1.7637655997276307, 1.7608976447582245], time: 175.638
Running avgs for agent 0: q_loss: 374478.875, p_loss: 95.02584075927734, mean_rew: -8.086572081570013, variance: 172.76317669677735, lamda: 1.1855520009994507
Running avgs for agent 1: q_loss: 18.685848236083984, p_loss: -4.174970626831055, mean_rew: -8.08973757264634, variance: 7.0550618171691895, lamda: 1.1665304899215698
Running avgs for agent 2: q_loss: 22.051891326904297, p_loss: -4.169548511505127, mean_rew: -8.099543543878122, variance: 7.043591022491455, lamda: 1.1653958559036255

steps: 249975, episodes: 10000, mean episode reward: -514.3473343585887, agent episode reward: [-171.44911145286292, -171.44911145286292, -171.44911145286292], time: 179.571
steps: 249975, episodes: 10000, mean episode variance: 50.29225252342224, agent episode variance: [46.869769729614255, 1.714501540184021, 1.7079812536239625], time: 179.572
Running avgs for agent 0: q_loss: 467154.65625, p_loss: 101.99922180175781, mean_rew: -7.957138645388886, variance: 187.47907891845702, lamda: 1.210556149482727
Running avgs for agent 1: q_loss: 19.106258392333984, p_loss: -4.095824718475342, mean_rew: -7.97185795729081, variance: 6.858006000518799, lamda: 1.1880629062652588
Running avgs for agent 2: q_loss: 17.958707809448242, p_loss: -4.102328300476074, mean_rew: -7.9558644856862095, variance: 6.831924915313721, lamda: 1.1886085271835327

steps: 274975, episodes: 11000, mean episode reward: -514.7147247264853, agent episode reward: [-171.57157490882844, -171.57157490882844, -171.57157490882844], time: 176.982
steps: 274975, episodes: 11000, mean episode variance: 51.29522277116776, agent episode variance: [47.990369270324706, 1.6563826298713684, 1.6484708709716798], time: 176.982
Running avgs for agent 0: q_loss: 598009.25, p_loss: 108.32276916503906, mean_rew: -7.861857246334033, variance: 191.96147708129882, lamda: 1.2355602979660034
Running avgs for agent 1: q_loss: 23.242673873901367, p_loss: -4.026488304138184, mean_rew: -7.857441515564962, variance: 6.62553071975708, lamda: 1.2073044776916504
Running avgs for agent 2: q_loss: 18.143739700317383, p_loss: -4.026208877563477, mean_rew: -7.855310049039328, variance: 6.593883514404297, lamda: 1.21271550655365

steps: 299975, episodes: 12000, mean episode reward: -510.70668960867937, agent episode reward: [-170.23556320289313, -170.23556320289313, -170.23556320289313], time: 176.877
steps: 299975, episodes: 12000, mean episode variance: 53.0668514585495, agent episode variance: [49.85983377838135, 1.6049345192909241, 1.6020831608772277], time: 176.878
Running avgs for agent 0: q_loss: 689713.9375, p_loss: 113.52140045166016, mean_rew: -7.774988145505773, variance: 199.4393351135254, lamda: 1.2605644464492798
Running avgs for agent 1: q_loss: 21.05422592163086, p_loss: -3.988511323928833, mean_rew: -7.7760312302443975, variance: 6.419738292694092, lamda: 1.2160029411315918
Running avgs for agent 2: q_loss: 17.09665298461914, p_loss: -3.98016095161438, mean_rew: -7.774056784317762, variance: 6.408332824707031, lamda: 1.236687421798706

steps: 324975, episodes: 13000, mean episode reward: -500.8402782012174, agent episode reward: [-166.9467594004058, -166.9467594004058, -166.9467594004058], time: 176.785
steps: 324975, episodes: 13000, mean episode variance: 40.98289771461487, agent episode variance: [37.825048629760744, 1.592329803943634, 1.5655192809104919], time: 176.786
Running avgs for agent 0: q_loss: 689747.3125, p_loss: 117.28916931152344, mean_rew: -7.693832531818552, variance: 151.30019451904298, lamda: 1.2855685949325562
Running avgs for agent 1: q_loss: 16.933473587036133, p_loss: -3.93211030960083, mean_rew: -7.693757534791307, variance: 6.369319438934326, lamda: 1.2323482036590576
Running avgs for agent 2: q_loss: 16.597183227539062, p_loss: -3.9453978538513184, mean_rew: -7.6956259162160485, variance: 6.262077331542969, lamda: 1.2582491636276245

steps: 349975, episodes: 14000, mean episode reward: -486.9029817590279, agent episode reward: [-162.300993919676, -162.300993919676, -162.300993919676], time: 183.645
steps: 349975, episodes: 14000, mean episode variance: 31.87216881585121, agent episode variance: [28.823201015472414, 1.5434638063907624, 1.505503993988037], time: 183.646
Running avgs for agent 0: q_loss: 463282.125, p_loss: 119.49712371826172, mean_rew: -7.605287881347189, variance: 115.29280406188965, lamda: 1.3105727434158325
Running avgs for agent 1: q_loss: 15.921247482299805, p_loss: -3.8932690620422363, mean_rew: -7.604232303211749, variance: 6.173855304718018, lamda: 1.2479931116104126
Running avgs for agent 2: q_loss: 17.318206787109375, p_loss: -3.917344808578491, mean_rew: -7.606872218676458, variance: 6.0220160484313965, lamda: 1.2788476943969727

steps: 374975, episodes: 15000, mean episode reward: -481.0494017400651, agent episode reward: [-160.34980058002168, -160.34980058002168, -160.34980058002168], time: 182.85
steps: 374975, episodes: 15000, mean episode variance: 29.668097101688385, agent episode variance: [26.687959028720854, 1.50866899061203, 1.4714690823554992], time: 182.85
Running avgs for agent 0: q_loss: 498795.46875, p_loss: 120.4582748413086, mean_rew: -7.533578819388894, variance: 106.75183611488342, lamda: 1.3355768918991089
Running avgs for agent 1: q_loss: 14.373899459838867, p_loss: -3.8775527477264404, mean_rew: -7.529921357811173, variance: 6.034675598144531, lamda: 1.2647686004638672
Running avgs for agent 2: q_loss: 13.77454662322998, p_loss: -3.8598361015319824, mean_rew: -7.5384243661266295, variance: 5.885876178741455, lamda: 1.2947254180908203

steps: 399975, episodes: 16000, mean episode reward: -480.3040773681583, agent episode reward: [-160.1013591227194, -160.1013591227194, -160.1013591227194], time: 179.718
steps: 399975, episodes: 16000, mean episode variance: 52.29154459953308, agent episode variance: [49.36479853820801, 1.479548990726471, 1.4471970705986024], time: 179.719
Running avgs for agent 0: q_loss: 948189.375, p_loss: 121.03736877441406, mean_rew: -7.456865015546414, variance: 197.45919415283203, lamda: 1.3605810403823853
Running avgs for agent 1: q_loss: 14.844281196594238, p_loss: -3.8318822383880615, mean_rew: -7.4648097807077125, variance: 5.918196201324463, lamda: 1.2829653024673462
Running avgs for agent 2: q_loss: 13.784236907958984, p_loss: -3.8169171810150146, mean_rew: -7.463465534963695, variance: 5.788788795471191, lamda: 1.3165550231933594

steps: 424975, episodes: 17000, mean episode reward: -483.5950328613266, agent episode reward: [-161.19834428710885, -161.19834428710885, -161.19834428710885], time: 182.276
steps: 424975, episodes: 17000, mean episode variance: 43.024678275108336, agent episode variance: [40.17341065597534, 1.4479917542934417, 1.4032758648395538], time: 182.277
Running avgs for agent 0: q_loss: 711547.125, p_loss: 121.4151382446289, mean_rew: -7.389753389418777, variance: 160.69364262390135, lamda: 1.3855851888656616
Running avgs for agent 1: q_loss: 14.695149421691895, p_loss: -3.7595529556274414, mean_rew: -7.394690052267926, variance: 5.791967391967773, lamda: 1.3048650026321411
Running avgs for agent 2: q_loss: 13.537726402282715, p_loss: -3.7819926738739014, mean_rew: -7.3968366807969295, variance: 5.61310338973999, lamda: 1.3365217447280884

steps: 449975, episodes: 18000, mean episode reward: -483.7205063109794, agent episode reward: [-161.24016877032645, -161.24016877032645, -161.24016877032645], time: 179.968
steps: 449975, episodes: 18000, mean episode variance: 48.819385432481766, agent episode variance: [46.02789030838013, 1.4210518715381621, 1.3704432525634767], time: 179.969
Running avgs for agent 0: q_loss: 814561.9375, p_loss: 121.93921661376953, mean_rew: -7.348039314861586, variance: 184.11156123352052, lamda: 1.410589337348938
Running avgs for agent 1: q_loss: 13.61596393585205, p_loss: -3.744070291519165, mean_rew: -7.337809355780635, variance: 5.684207439422607, lamda: 1.3241227865219116
Running avgs for agent 2: q_loss: 11.549871444702148, p_loss: -3.7651705741882324, mean_rew: -7.340272282418716, variance: 5.481773376464844, lamda: 1.3556746244430542

steps: 474975, episodes: 19000, mean episode reward: -488.61351411658455, agent episode reward: [-162.87117137219485, -162.87117137219485, -162.87117137219485], time: 177.283
steps: 474975, episodes: 19000, mean episode variance: 46.76338245487213, agent episode variance: [44.033234202384946, 1.379498054265976, 1.3506501982212067], time: 177.283
Running avgs for agent 0: q_loss: 799311.625, p_loss: 122.27531433105469, mean_rew: -7.294710152400413, variance: 176.13293680953979, lamda: 1.4355934858322144
Running avgs for agent 1: q_loss: 14.553306579589844, p_loss: -3.7209830284118652, mean_rew: -7.298675240184571, variance: 5.51799201965332, lamda: 1.346165657043457
Running avgs for agent 2: q_loss: 11.819127082824707, p_loss: -3.746462821960449, mean_rew: -7.2908477111502075, variance: 5.40260124206543, lamda: 1.3754414319992065

steps: 499975, episodes: 20000, mean episode reward: -486.32892643020347, agent episode reward: [-162.10964214340115, -162.10964214340115, -162.10964214340115], time: 180.932
steps: 499975, episodes: 20000, mean episode variance: 46.80455188345909, agent episode variance: [44.118680809020994, 1.3541855731010437, 1.3316855013370514], time: 180.932
Running avgs for agent 0: q_loss: 792370.5625, p_loss: 122.0978775024414, mean_rew: -7.249921586861734, variance: 176.47472323608397, lamda: 1.4605976343154907
Running avgs for agent 1: q_loss: 18.815336227416992, p_loss: -3.7052035331726074, mean_rew: -7.253702669506295, variance: 5.416742324829102, lamda: 1.3619626760482788
Running avgs for agent 2: q_loss: 14.009255409240723, p_loss: -3.7248129844665527, mean_rew: -7.256699548133667, variance: 5.326742172241211, lamda: 1.3948183059692383

steps: 524975, episodes: 21000, mean episode reward: -489.1551138114768, agent episode reward: [-163.0517046038256, -163.0517046038256, -163.0517046038256], time: 178.424
steps: 524975, episodes: 21000, mean episode variance: 49.19472256815433, agent episode variance: [46.54464698600769, 1.3577550393342972, 1.2923205428123474], time: 178.424
Running avgs for agent 0: q_loss: 932356.8125, p_loss: 121.75447082519531, mean_rew: -7.20548041106554, variance: 186.17858794403077, lamda: 1.485601782798767
Running avgs for agent 1: q_loss: 17.517473220825195, p_loss: -3.685058116912842, mean_rew: -7.223710246232642, variance: 5.431020736694336, lamda: 1.3695952892303467
Running avgs for agent 2: q_loss: 11.497030258178711, p_loss: -3.7121169567108154, mean_rew: -7.21251364437997, variance: 5.16928243637085, lamda: 1.4140058755874634

steps: 549975, episodes: 22000, mean episode reward: -489.0038032753145, agent episode reward: [-163.0012677584382, -163.0012677584382, -163.0012677584382], time: 176.866
steps: 549975, episodes: 22000, mean episode variance: 37.57417819452286, agent episode variance: [34.968865088939665, 1.3307602722644807, 1.2745528333187104], time: 176.867
Running avgs for agent 0: q_loss: 816066.4375, p_loss: 121.67768859863281, mean_rew: -7.193917490691212, variance: 139.87546035575866, lamda: 1.5106059312820435
Running avgs for agent 1: q_loss: 17.324552536010742, p_loss: -3.689223527908325, mean_rew: -7.189760616451729, variance: 5.323040962219238, lamda: 1.3758584260940552
Running avgs for agent 2: q_loss: 14.992049217224121, p_loss: -3.69549822807312, mean_rew: -7.1913380488574, variance: 5.09821081161499, lamda: 1.4264225959777832

steps: 574975, episodes: 23000, mean episode reward: -489.8515239869516, agent episode reward: [-163.28384132898387, -163.28384132898387, -163.28384132898387], time: 178.234
steps: 574975, episodes: 23000, mean episode variance: 44.87959894514084, agent episode variance: [42.29642864608765, 1.3194007024765015, 1.2637695965766906], time: 178.235
Running avgs for agent 0: q_loss: 876269.3125, p_loss: 121.54625701904297, mean_rew: -7.164048748076457, variance: 169.1857145843506, lamda: 1.5356100797653198
Running avgs for agent 1: q_loss: 13.241006851196289, p_loss: -3.65421462059021, mean_rew: -7.158260110243503, variance: 5.277602672576904, lamda: 1.3921716213226318
Running avgs for agent 2: q_loss: 11.745298385620117, p_loss: -3.6700222492218018, mean_rew: -7.157456802176821, variance: 5.055078029632568, lamda: 1.4421902894973755

steps: 599975, episodes: 24000, mean episode reward: -494.913808367947, agent episode reward: [-164.97126945598237, -164.97126945598237, -164.97126945598237], time: 178.399
steps: 599975, episodes: 24000, mean episode variance: 24.5406492395401, agent episode variance: [22.006492695331573, 1.2958071398735047, 1.238349404335022], time: 178.399
Running avgs for agent 0: q_loss: 622530.4375, p_loss: 121.6709976196289, mean_rew: -7.129427088266224, variance: 88.0259707813263, lamda: 1.5606142282485962
Running avgs for agent 1: q_loss: 15.292381286621094, p_loss: -3.6447198390960693, mean_rew: -7.13466167259517, variance: 5.183228492736816, lamda: 1.4104084968566895
Running avgs for agent 2: q_loss: 11.283447265625, p_loss: -3.661104440689087, mean_rew: -7.128828188482115, variance: 4.953397274017334, lamda: 1.464219331741333

steps: 624975, episodes: 25000, mean episode reward: -490.56444347088376, agent episode reward: [-163.52148115696124, -163.52148115696124, -163.52148115696124], time: 180.245
steps: 624975, episodes: 25000, mean episode variance: 35.763164842844006, agent episode variance: [33.24535141706467, 1.2938931729793548, 1.2239202527999877], time: 180.245
Running avgs for agent 0: q_loss: 778196.75, p_loss: 122.22576141357422, mean_rew: -7.113242707978169, variance: 132.98140566825867, lamda: 1.5856183767318726
Running avgs for agent 1: q_loss: 15.916933059692383, p_loss: -3.636787176132202, mean_rew: -7.110339508784614, variance: 5.175572872161865, lamda: 1.417531132698059
Running avgs for agent 2: q_loss: 11.516119956970215, p_loss: -3.633528470993042, mean_rew: -7.101815070985319, variance: 4.895681858062744, lamda: 1.486545443534851

steps: 649975, episodes: 26000, mean episode reward: -502.8960411852995, agent episode reward: [-167.63201372843318, -167.63201372843318, -167.63201372843318], time: 178.008
steps: 649975, episodes: 26000, mean episode variance: 24.35245844769478, agent episode variance: [21.873209124565125, 1.2804640367031097, 1.1987852864265442], time: 178.008
Running avgs for agent 0: q_loss: 569012.625, p_loss: 122.75330352783203, mean_rew: -7.101880303772544, variance: 87.4928364982605, lamda: 1.610622525215149
Running avgs for agent 1: q_loss: 15.179779052734375, p_loss: -3.6402764320373535, mean_rew: -7.094063426618923, variance: 5.121856212615967, lamda: 1.422351598739624
Running avgs for agent 2: q_loss: 11.204148292541504, p_loss: -3.6290957927703857, mean_rew: -7.083881499902479, variance: 4.795141220092773, lamda: 1.5068411827087402

steps: 674975, episodes: 27000, mean episode reward: -502.9306945291632, agent episode reward: [-167.6435648430544, -167.6435648430544, -167.6435648430544], time: 179.539
steps: 674975, episodes: 27000, mean episode variance: 26.03969469547272, agent episode variance: [23.59860050201416, 1.2661317052841186, 1.1749624881744385], time: 179.539
Running avgs for agent 0: q_loss: 581714.25, p_loss: 122.8788070678711, mean_rew: -7.08349230196136, variance: 94.39440200805664, lamda: 1.6356266736984253
Running avgs for agent 1: q_loss: 14.828460693359375, p_loss: -3.638664960861206, mean_rew: -7.074896194814255, variance: 5.0645270347595215, lamda: 1.4275000095367432
Running avgs for agent 2: q_loss: 12.685832023620605, p_loss: -3.6233022212982178, mean_rew: -7.073027777540304, variance: 4.699850082397461, lamda: 1.5226635932922363

steps: 699975, episodes: 28000, mean episode reward: -505.7429894290555, agent episode reward: [-168.58099647635186, -168.58099647635186, -168.58099647635186], time: 183.867
steps: 699975, episodes: 28000, mean episode variance: 48.502043826818465, agent episode variance: [46.08360599470139, 1.2551729538440703, 1.1632648782730102], time: 183.868
Running avgs for agent 0: q_loss: 960836.25, p_loss: 123.1185302734375, mean_rew: -7.062787676402916, variance: 184.33442397880555, lamda: 1.6606308221817017
Running avgs for agent 1: q_loss: 14.174529075622559, p_loss: -3.620913028717041, mean_rew: -7.0623560456915095, variance: 5.020691871643066, lamda: 1.434181809425354
Running avgs for agent 2: q_loss: 11.328179359436035, p_loss: -3.6188864707946777, mean_rew: -7.0627449107838025, variance: 4.653059482574463, lamda: 1.5401206016540527

steps: 724975, episodes: 29000, mean episode reward: -520.055094249643, agent episode reward: [-173.35169808321436, -173.35169808321436, -173.35169808321436], time: 179.246
steps: 724975, episodes: 29000, mean episode variance: 36.149052688360214, agent episode variance: [33.73607023715973, 1.2572298605442047, 1.1557525906562804], time: 179.246
Running avgs for agent 0: q_loss: 789300.0625, p_loss: 123.3381576538086, mean_rew: -7.063004752811893, variance: 134.94428094863892, lamda: 1.685634970664978
Running avgs for agent 1: q_loss: 13.870241165161133, p_loss: -3.6223838329315186, mean_rew: -7.0507144099418575, variance: 5.028919219970703, lamda: 1.4407902956008911
Running avgs for agent 2: q_loss: 11.257143020629883, p_loss: -3.6224141120910645, mean_rew: -7.060196805867883, variance: 4.623010635375977, lamda: 1.5547960996627808

steps: 749975, episodes: 30000, mean episode reward: -528.2280264621013, agent episode reward: [-176.07600882070042, -176.07600882070042, -176.07600882070042], time: 180.46
steps: 749975, episodes: 30000, mean episode variance: 36.23865574073791, agent episode variance: [33.86020711660385, 1.2415484280586242, 1.1369001960754395], time: 180.461
Running avgs for agent 0: q_loss: 858051.9375, p_loss: 123.70037841796875, mean_rew: -7.051221792423072, variance: 135.4408284664154, lamda: 1.7106391191482544
Running avgs for agent 1: q_loss: 13.252140045166016, p_loss: -3.621351957321167, mean_rew: -7.046189153960386, variance: 4.966193675994873, lamda: 1.4467582702636719
Running avgs for agent 2: q_loss: 10.56149959564209, p_loss: -3.614238739013672, mean_rew: -7.0557409680310625, variance: 4.547600746154785, lamda: 1.5753997564315796

steps: 774975, episodes: 31000, mean episode reward: -533.4385799604198, agent episode reward: [-177.81285998680664, -177.81285998680664, -177.81285998680664], time: 178.142
steps: 774975, episodes: 31000, mean episode variance: 30.367897926926613, agent episode variance: [28.00922223186493, 1.234348235964775, 1.1243274590969086], time: 178.142
Running avgs for agent 0: q_loss: 779306.375, p_loss: 124.38896179199219, mean_rew: -7.057027620380049, variance: 112.03688892745971, lamda: 1.7356432676315308
Running avgs for agent 1: q_loss: 15.306650161743164, p_loss: -3.6185195446014404, mean_rew: -7.053884705034077, variance: 4.937392711639404, lamda: 1.453018307685852
Running avgs for agent 2: q_loss: 9.875777244567871, p_loss: -3.611863136291504, mean_rew: -7.048550828859636, variance: 4.497309684753418, lamda: 1.5944942235946655

steps: 799975, episodes: 32000, mean episode reward: -537.2805686418199, agent episode reward: [-179.09352288060663, -179.09352288060663, -179.09352288060663], time: 178.555
steps: 799975, episodes: 32000, mean episode variance: 39.82701028561592, agent episode variance: [37.479108131408694, 1.2310956571102143, 1.1168064970970153], time: 178.555
Running avgs for agent 0: q_loss: 878300.9375, p_loss: 125.11571502685547, mean_rew: -7.05475534402516, variance: 149.91643252563478, lamda: 1.7606474161148071
Running avgs for agent 1: q_loss: 11.887495994567871, p_loss: -3.6106250286102295, mean_rew: -7.054114501279079, variance: 4.924382209777832, lamda: 1.4692860841751099
Running avgs for agent 2: q_loss: 10.864941596984863, p_loss: -3.6260132789611816, mean_rew: -7.056804553209544, variance: 4.467226028442383, lamda: 1.6127591133117676

steps: 824975, episodes: 33000, mean episode reward: -538.0477674244721, agent episode reward: [-179.34925580815735, -179.34925580815735, -179.34925580815735], time: 179.617
steps: 824975, episodes: 33000, mean episode variance: 45.05536151814461, agent episode variance: [42.741890377044676, 1.2104069819450378, 1.103064159154892], time: 179.618
Running avgs for agent 0: q_loss: 998014.75, p_loss: 125.86720275878906, mean_rew: -7.053294126550443, variance: 170.9675615081787, lamda: 1.7856515645980835
Running avgs for agent 1: q_loss: 12.907846450805664, p_loss: -3.6401162147521973, mean_rew: -7.0675667826625155, variance: 4.841628074645996, lamda: 1.479061245918274
Running avgs for agent 2: q_loss: 10.419638633728027, p_loss: -3.613820791244507, mean_rew: -7.061453248666183, variance: 4.412256717681885, lamda: 1.6308406591415405

steps: 849975, episodes: 34000, mean episode reward: -538.5703372677067, agent episode reward: [-179.52344575590223, -179.52344575590223, -179.52344575590223], time: 175.495
steps: 849975, episodes: 34000, mean episode variance: 46.14566511011124, agent episode variance: [43.83680462646485, 1.2162719376087188, 1.0925885460376739], time: 175.496
Running avgs for agent 0: q_loss: 975851.4375, p_loss: 126.4561538696289, mean_rew: -7.071935265191837, variance: 175.3472185058594, lamda: 1.8106557130813599
Running avgs for agent 1: q_loss: 12.436529159545898, p_loss: -3.6157705783843994, mean_rew: -7.066543686136238, variance: 4.865087985992432, lamda: 1.4848949909210205
Running avgs for agent 2: q_loss: 9.391087532043457, p_loss: -3.6207242012023926, mean_rew: -7.068654130065316, variance: 4.370354175567627, lamda: 1.6513186693191528

steps: 874975, episodes: 35000, mean episode reward: -537.1086886512812, agent episode reward: [-179.03622955042707, -179.03622955042707, -179.03622955042707], time: 175.261
steps: 874975, episodes: 35000, mean episode variance: 45.1826121058464, agent episode variance: [42.89662823486328, 1.2159418272972107, 1.070042043685913], time: 175.262
Running avgs for agent 0: q_loss: 987998.5, p_loss: 127.20823669433594, mean_rew: -7.0634935991681695, variance: 171.58651293945312, lamda: 1.8356598615646362
Running avgs for agent 1: q_loss: 12.974566459655762, p_loss: -3.635176181793213, mean_rew: -7.070531685996201, variance: 4.863767623901367, lamda: 1.4914826154708862
Running avgs for agent 2: q_loss: 11.310843467712402, p_loss: -3.62567400932312, mean_rew: -7.075617850976009, variance: 4.280168056488037, lamda: 1.6707239151000977

steps: 899975, episodes: 36000, mean episode reward: -537.8868792235959, agent episode reward: [-179.29562640786526, -179.29562640786526, -179.29562640786526], time: 175.818
steps: 899975, episodes: 36000, mean episode variance: 43.33607720756531, agent episode variance: [41.06220545959473, 1.2011263477802276, 1.0727454001903534], time: 175.818
Running avgs for agent 0: q_loss: 972880.625, p_loss: 127.81537628173828, mean_rew: -7.0704825771440545, variance: 164.24882183837892, lamda: 1.8606640100479126
Running avgs for agent 1: q_loss: 10.815261840820312, p_loss: -3.634164571762085, mean_rew: -7.074083286591795, variance: 4.804505348205566, lamda: 1.503772497177124
Running avgs for agent 2: q_loss: 10.458853721618652, p_loss: -3.626816511154175, mean_rew: -7.067458527433197, variance: 4.290981292724609, lamda: 1.6868906021118164

steps: 924975, episodes: 37000, mean episode reward: -539.3734400010084, agent episode reward: [-179.79114666700275, -179.79114666700275, -179.79114666700275], time: 178.276
steps: 924975, episodes: 37000, mean episode variance: 45.4716370921135, agent episode variance: [43.22784484100342, 1.1865120208263398, 1.0572802302837372], time: 178.276
Running avgs for agent 0: q_loss: 978006.0, p_loss: 128.59475708007812, mean_rew: -7.075050107142222, variance: 172.91137936401367, lamda: 1.885668158531189
Running avgs for agent 1: q_loss: 11.108119010925293, p_loss: -3.6354777812957764, mean_rew: -7.07810558100576, variance: 4.7460479736328125, lamda: 1.522699236869812
Running avgs for agent 2: q_loss: 10.12635612487793, p_loss: -3.6207685470581055, mean_rew: -7.071418556481616, variance: 4.229121208190918, lamda: 1.705967903137207

steps: 949975, episodes: 38000, mean episode reward: -537.7750847474847, agent episode reward: [-179.2583615824949, -179.2583615824949, -179.2583615824949], time: 174.861
steps: 949975, episodes: 38000, mean episode variance: 33.61664036011696, agent episode variance: [31.389352076530457, 1.182295200586319, 1.044993083000183], time: 174.862
Running avgs for agent 0: q_loss: 764419.1875, p_loss: 129.21600341796875, mean_rew: -7.073487159164045, variance: 125.55740830612183, lamda: 1.9106723070144653
Running avgs for agent 1: q_loss: 11.82838249206543, p_loss: -3.6292543411254883, mean_rew: -7.077051401456165, variance: 4.729180812835693, lamda: 1.5297842025756836
Running avgs for agent 2: q_loss: 9.914172172546387, p_loss: -3.624788999557495, mean_rew: -7.074532859896035, variance: 4.1799726486206055, lamda: 1.7236104011535645

steps: 974975, episodes: 39000, mean episode reward: -546.5983547417119, agent episode reward: [-182.19945158057064, -182.19945158057064, -182.19945158057064], time: 177.791
steps: 974975, episodes: 39000, mean episode variance: 34.82468949508667, agent episode variance: [32.631069149017335, 1.157677242040634, 1.0359431040287017], time: 177.791
Running avgs for agent 0: q_loss: 779517.6875, p_loss: 129.7173309326172, mean_rew: -7.078537784088874, variance: 130.52427659606934, lamda: 1.9356764554977417
Running avgs for agent 1: q_loss: 11.44688606262207, p_loss: -3.6419007778167725, mean_rew: -7.084454687884168, variance: 4.630709171295166, lamda: 1.5420995950698853
Running avgs for agent 2: q_loss: 10.561592102050781, p_loss: -3.6328177452087402, mean_rew: -7.082763304101152, variance: 4.143772602081299, lamda: 1.744538426399231

steps: 999975, episodes: 40000, mean episode reward: -547.8860645165823, agent episode reward: [-182.62868817219413, -182.62868817219413, -182.62868817219413], time: 179.236
steps: 999975, episodes: 40000, mean episode variance: 44.29819377660751, agent episode variance: [42.10273036193848, 1.1674639673233032, 1.0279994473457337], time: 179.237
Running avgs for agent 0: q_loss: 984123.375, p_loss: 130.15982055664062, mean_rew: -7.084045380697073, variance: 168.4109214477539, lamda: 1.9606807231903076
Running avgs for agent 1: q_loss: 12.009546279907227, p_loss: -3.635568141937256, mean_rew: -7.082876553337376, variance: 4.66985559463501, lamda: 1.5490590333938599
Running avgs for agent 2: q_loss: 9.688035011291504, p_loss: -3.6350884437561035, mean_rew: -7.090099214805824, variance: 4.111998081207275, lamda: 1.76487398147583

steps: 1024975, episodes: 41000, mean episode reward: -542.9834045389917, agent episode reward: [-180.9944681796639, -180.9944681796639, -180.9944681796639], time: 177.645
steps: 1024975, episodes: 41000, mean episode variance: 45.18783499121666, agent episode variance: [42.997748046875, 1.1738148543834686, 1.016272089958191], time: 177.646
Running avgs for agent 0: q_loss: 995350.25, p_loss: 130.23594665527344, mean_rew: -7.100847310096537, variance: 171.9909921875, lamda: 1.9856846332550049
Running avgs for agent 1: q_loss: 11.699305534362793, p_loss: -3.645747184753418, mean_rew: -7.094298252962503, variance: 4.6952595710754395, lamda: 1.5546562671661377
Running avgs for agent 2: q_loss: 9.4020357131958, p_loss: -3.64249587059021, mean_rew: -7.0881742955304565, variance: 4.065088272094727, lamda: 1.787044882774353

steps: 1049975, episodes: 42000, mean episode reward: -556.8717797005173, agent episode reward: [-185.6239265668391, -185.6239265668391, -185.6239265668391], time: 187.828
steps: 1049975, episodes: 42000, mean episode variance: 42.28163502144814, agent episode variance: [40.12645068359375, 1.1578773694038391, 0.9973069684505462], time: 187.828
Running avgs for agent 0: q_loss: 918016.3125, p_loss: 129.50186157226562, mean_rew: -7.071553522115529, variance: 160.505802734375, lamda: 2.010676145553589
Running avgs for agent 1: q_loss: 10.215825080871582, p_loss: -3.6354281902313232, mean_rew: -7.066395237073131, variance: 4.631509304046631, lamda: 1.5634301900863647
Running avgs for agent 2: q_loss: 9.987818717956543, p_loss: -3.6364166736602783, mean_rew: -7.06560090482417, variance: 3.9892280101776123, lamda: 1.8040627241134644

steps: 1074975, episodes: 43000, mean episode reward: -562.114646868141, agent episode reward: [-187.37154895604704, -187.37154895604704, -187.37154895604704], time: 191.027
steps: 1074975, episodes: 43000, mean episode variance: 42.18326026916504, agent episode variance: [40.06438684082031, 1.1347966437339783, 0.9840767846107483], time: 191.028
Running avgs for agent 0: q_loss: 818639.0, p_loss: 128.22329711914062, mean_rew: -7.015464866656635, variance: 160.25754736328125, lamda: 2.0356504917144775
Running avgs for agent 1: q_loss: 9.363726615905762, p_loss: -3.603724479675293, mean_rew: -7.008327011686354, variance: 4.539186477661133, lamda: 1.580294132232666
Running avgs for agent 2: q_loss: 7.569925308227539, p_loss: -3.6007027626037598, mean_rew: -7.003449203092335, variance: 3.936307191848755, lamda: 1.8183742761611938

steps: 1099975, episodes: 44000, mean episode reward: -561.2719453079495, agent episode reward: [-187.0906484359832, -187.0906484359832, -187.0906484359832], time: 186.578
steps: 1099975, episodes: 44000, mean episode variance: 40.72032608795166, agent episode variance: [38.62929791259766, 1.117695485830307, 0.9733326895236969], time: 186.578
Running avgs for agent 0: q_loss: 754145.6875, p_loss: 127.5869369506836, mean_rew: -6.9758213381951775, variance: 154.51719165039063, lamda: 2.0606250762939453
Running avgs for agent 1: q_loss: 9.038631439208984, p_loss: -3.599839448928833, mean_rew: -6.980622850468786, variance: 4.470782279968262, lamda: 1.5840352773666382
Running avgs for agent 2: q_loss: 6.739990234375, p_loss: -3.5958242416381836, mean_rew: -6.97287153094828, variance: 3.8933308124542236, lamda: 1.831220269203186

steps: 1124975, episodes: 45000, mean episode reward: -563.2299065694965, agent episode reward: [-187.7433021898322, -187.7433021898322, -187.7433021898322], time: 176.41
steps: 1124975, episodes: 45000, mean episode variance: 39.82080826854706, agent episode variance: [37.74806294250488, 1.1094799253940582, 0.963265400648117], time: 176.411
Running avgs for agent 0: q_loss: 718994.6875, p_loss: 127.12665557861328, mean_rew: -6.941594230315104, variance: 150.99225177001952, lamda: 2.085599184036255
Running avgs for agent 1: q_loss: 8.262857437133789, p_loss: -3.5804271697998047, mean_rew: -6.941188220656629, variance: 4.437920093536377, lamda: 1.584989309310913
Running avgs for agent 2: q_loss: 5.388261795043945, p_loss: -3.566585063934326, mean_rew: -6.934213570113536, variance: 3.8530614376068115, lamda: 1.8395342826843262

steps: 1149975, episodes: 46000, mean episode reward: -571.4404493591705, agent episode reward: [-190.48014978639014, -190.48014978639014, -190.48014978639014], time: 177.566
steps: 1149975, episodes: 46000, mean episode variance: 39.29034974598885, agent episode variance: [37.219475662231446, 1.110125262260437, 0.9607488214969635], time: 177.567
Running avgs for agent 0: q_loss: 705593.9375, p_loss: 127.21504974365234, mean_rew: -6.9478674662022, variance: 148.87790264892578, lamda: 2.1105735301971436
Running avgs for agent 1: q_loss: 7.988400459289551, p_loss: -3.579821825027466, mean_rew: -6.943481568079832, variance: 4.4405012130737305, lamda: 1.5851950645446777
Running avgs for agent 2: q_loss: 4.908758163452148, p_loss: -3.575854539871216, mean_rew: -6.9418781163433945, variance: 3.8429954051971436, lamda: 1.846032738685608

steps: 1174975, episodes: 47000, mean episode reward: -570.8951833954806, agent episode reward: [-190.29839446516021, -190.29839446516021, -190.29839446516021], time: 177.908
steps: 1174975, episodes: 47000, mean episode variance: 39.50698502707481, agent episode variance: [37.44067353820801, 1.1073327796459198, 0.9589787092208862], time: 177.909
Running avgs for agent 0: q_loss: 696015.5, p_loss: 127.32331848144531, mean_rew: -6.951578884906871, variance: 149.76269415283204, lamda: 2.1355478763580322
Running avgs for agent 1: q_loss: 7.911890983581543, p_loss: -3.579958438873291, mean_rew: -6.9507973258855404, variance: 4.429330825805664, lamda: 1.5856460332870483
Running avgs for agent 2: q_loss: 5.119436740875244, p_loss: -3.5799224376678467, mean_rew: -6.948222358956093, variance: 3.8359150886535645, lamda: 1.8528786897659302

steps: 1199975, episodes: 48000, mean episode reward: -569.8396652049573, agent episode reward: [-189.94655506831913, -189.94655506831913, -189.94655506831913], time: 172.466
steps: 1199975, episodes: 48000, mean episode variance: 38.20812324512005, agent episode variance: [36.14099345397949, 1.113496855020523, 0.9536329361200333], time: 172.466
Running avgs for agent 0: q_loss: 710206.1875, p_loss: 127.88198852539062, mean_rew: -6.964354977488035, variance: 144.56397381591796, lamda: 2.1605224609375
Running avgs for agent 1: q_loss: 7.770966529846191, p_loss: -3.583690643310547, mean_rew: -6.959319090238276, variance: 4.453987121582031, lamda: 1.5865190029144287
Running avgs for agent 2: q_loss: 6.004141807556152, p_loss: -3.5851542949676514, mean_rew: -6.959499402938003, variance: 3.8145318031311035, lamda: 1.8592278957366943

steps: 1224975, episodes: 49000, mean episode reward: -575.7111184865009, agent episode reward: [-191.90370616216697, -191.90370616216697, -191.90370616216697], time: 183.07
steps: 1224975, episodes: 49000, mean episode variance: 38.23460718631745, agent episode variance: [36.167376022338864, 1.1155996572971345, 0.9516315066814423], time: 183.071
Running avgs for agent 0: q_loss: 698894.25, p_loss: 128.29469299316406, mean_rew: -6.965449098474922, variance: 144.66950408935546, lamda: 2.1854965686798096
Running avgs for agent 1: q_loss: 7.0684404373168945, p_loss: -3.584242105484009, mean_rew: -6.96715893569649, variance: 4.462398529052734, lamda: 1.587761402130127
Running avgs for agent 2: q_loss: 6.700778484344482, p_loss: -3.594987392425537, mean_rew: -6.973895503526259, variance: 3.806525945663452, lamda: 1.8611257076263428

steps: 1249975, episodes: 50000, mean episode reward: -571.5799731393212, agent episode reward: [-190.52665771310706, -190.52665771310706, -190.52665771310706], time: 190.547
steps: 1249975, episodes: 50000, mean episode variance: 38.220668120145795, agent episode variance: [36.14802030944824, 1.1184524614810945, 0.9541953492164612], time: 190.548
Running avgs for agent 0: q_loss: 700854.5, p_loss: 128.78109741210938, mean_rew: -6.990765691240794, variance: 144.59208123779297, lamda: 2.2104711532592773
Running avgs for agent 1: q_loss: 5.141517162322998, p_loss: -3.59938907623291, mean_rew: -6.993419218443679, variance: 4.473809719085693, lamda: 1.5926252603530884
Running avgs for agent 2: q_loss: 5.340340614318848, p_loss: -3.6031196117401123, mean_rew: -6.989856577679469, variance: 3.8167812824249268, lamda: 1.863488793373108

steps: 1274975, episodes: 51000, mean episode reward: -576.3968459915768, agent episode reward: [-192.13228199719225, -192.13228199719225, -192.13228199719225], time: 186.918
steps: 1274975, episodes: 51000, mean episode variance: 37.306976150512696, agent episode variance: [35.233519653320315, 1.117819975376129, 0.9556365218162537], time: 186.918
Running avgs for agent 0: q_loss: 727200.375, p_loss: 129.54983520507812, mean_rew: -7.001029591666692, variance: 140.93407861328126, lamda: 2.235445261001587
Running avgs for agent 1: q_loss: 4.9752655029296875, p_loss: -3.601555824279785, mean_rew: -7.000727381612022, variance: 4.471279621124268, lamda: 1.5944194793701172
Running avgs for agent 2: q_loss: 5.223479270935059, p_loss: -3.6055848598480225, mean_rew: -7.010723849327731, variance: 3.8225460052490234, lamda: 1.8689578771591187

steps: 1299975, episodes: 52000, mean episode reward: -573.5756027646936, agent episode reward: [-191.1918675882312, -191.1918675882312, -191.1918675882312], time: 186.592
steps: 1299975, episodes: 52000, mean episode variance: 37.5387610142231, agent episode variance: [35.46241799926758, 1.1209168107509613, 0.9554262042045594], time: 186.593
Running avgs for agent 0: q_loss: 729742.9375, p_loss: 130.37014770507812, mean_rew: -7.031494605264095, variance: 141.84967199707032, lamda: 2.2604193687438965
Running avgs for agent 1: q_loss: 5.0282511711120605, p_loss: -3.612274169921875, mean_rew: -7.020413963305288, variance: 4.483666896820068, lamda: 1.5956454277038574
Running avgs for agent 2: q_loss: 7.046177864074707, p_loss: -3.6228432655334473, mean_rew: -7.030375524845444, variance: 3.821704864501953, lamda: 1.8719384670257568

steps: 1324975, episodes: 53000, mean episode reward: -579.3925236309678, agent episode reward: [-193.1308412103226, -193.1308412103226, -193.1308412103226], time: 185.61
steps: 1324975, episodes: 53000, mean episode variance: 37.858837997436524, agent episode variance: [35.77818264770508, 1.1239521033763886, 0.9567032463550568], time: 185.61
Running avgs for agent 0: q_loss: 733334.0, p_loss: 130.8546600341797, mean_rew: -7.053681855548974, variance: 143.11273059082032, lamda: 2.2853939533233643
Running avgs for agent 1: q_loss: 5.099300384521484, p_loss: -3.632888078689575, mean_rew: -7.055426079591511, variance: 4.4958086013793945, lamda: 1.5986950397491455
Running avgs for agent 2: q_loss: 5.79518461227417, p_loss: -3.6350390911102295, mean_rew: -7.054064112280531, variance: 3.826812982559204, lamda: 1.8730016946792603

steps: 1349975, episodes: 54000, mean episode reward: -576.0369889921476, agent episode reward: [-192.01232966404916, -192.01232966404916, -192.01232966404916], time: 185.836
steps: 1349975, episodes: 54000, mean episode variance: 37.27377547693253, agent episode variance: [35.20143499755859, 1.1134234817028046, 0.9589169976711274], time: 185.837
Running avgs for agent 0: q_loss: 746260.9375, p_loss: 131.6542510986328, mean_rew: -7.083406039262353, variance: 140.80573999023437, lamda: 2.310368061065674
Running avgs for agent 1: q_loss: 7.659689426422119, p_loss: -3.6433305740356445, mean_rew: -7.081311033497304, variance: 4.453693866729736, lamda: 1.6049935817718506
Running avgs for agent 2: q_loss: 4.65091609954834, p_loss: -3.6480326652526855, mean_rew: -7.077174320605654, variance: 3.8356680870056152, lamda: 1.878183126449585

steps: 1374975, episodes: 55000, mean episode reward: -575.3056681835695, agent episode reward: [-191.76855606118986, -191.76855606118986, -191.76855606118986], time: 188.936
steps: 1374975, episodes: 55000, mean episode variance: 38.358738011837005, agent episode variance: [36.2724518661499, 1.12203218793869, 0.964253957748413], time: 188.936
Running avgs for agent 0: q_loss: 765939.5, p_loss: 132.62171936035156, mean_rew: -7.112666902107878, variance: 145.0898074645996, lamda: 2.3353428840637207
Running avgs for agent 1: q_loss: 7.465847492218018, p_loss: -3.6560962200164795, mean_rew: -7.10808528527885, variance: 4.488129138946533, lamda: 1.6052255630493164
Running avgs for agent 2: q_loss: 4.567445278167725, p_loss: -3.666728973388672, mean_rew: -7.119782741026209, variance: 3.85701584815979, lamda: 1.881445288658142

steps: 1399975, episodes: 56000, mean episode reward: -588.3845208125978, agent episode reward: [-196.12817360419928, -196.12817360419928, -196.12817360419928], time: 190.795
steps: 1399975, episodes: 56000, mean episode variance: 39.40355303621292, agent episode variance: [37.30606862640381, 1.129230976343155, 0.9682534334659576], time: 190.795
Running avgs for agent 0: q_loss: 804672.875, p_loss: 133.72265625, mean_rew: -7.147825632884478, variance: 149.22427450561523, lamda: 2.3603172302246094
Running avgs for agent 1: q_loss: 7.14351749420166, p_loss: -3.679936408996582, mean_rew: -7.147812352287757, variance: 4.516923904418945, lamda: 1.6059186458587646
Running avgs for agent 2: q_loss: 4.599538803100586, p_loss: -3.6694087982177734, mean_rew: -7.142219622201641, variance: 3.873013734817505, lamda: 1.8864426612854004

steps: 1424975, episodes: 57000, mean episode reward: -580.6041925424728, agent episode reward: [-193.53473084749092, -193.53473084749092, -193.53473084749092], time: 189.035
steps: 1424975, episodes: 57000, mean episode variance: 39.40946026468277, agent episode variance: [37.304510803222655, 1.1382779145240784, 0.9666715469360352], time: 189.036
Running avgs for agent 0: q_loss: 841459.8125, p_loss: 135.0069580078125, mean_rew: -7.17473492629978, variance: 149.21804321289062, lamda: 2.385291576385498
Running avgs for agent 1: q_loss: 5.165876865386963, p_loss: -3.6884429454803467, mean_rew: -7.180370238260361, variance: 4.553111553192139, lamda: 1.6090764999389648
Running avgs for agent 2: q_loss: 4.672844886779785, p_loss: -3.6948180198669434, mean_rew: -7.178355938219611, variance: 3.8666861057281494, lamda: 1.8902122974395752

steps: 1449975, episodes: 58000, mean episode reward: -578.28300125155, agent episode reward: [-192.76100041718334, -192.76100041718334, -192.76100041718334], time: 188.329
steps: 1449975, episodes: 58000, mean episode variance: 40.30819259881973, agent episode variance: [38.19476815795898, 1.1434845011234283, 0.96993993973732], time: 188.33
Running avgs for agent 0: q_loss: 861628.8125, p_loss: 136.0892791748047, mean_rew: -7.210007514213379, variance: 152.77907263183593, lamda: 2.4102656841278076
Running avgs for agent 1: q_loss: 5.24515438079834, p_loss: -3.707751512527466, mean_rew: -7.211330009128205, variance: 4.57393741607666, lamda: 1.6108375787734985
Running avgs for agent 2: q_loss: 4.8417463302612305, p_loss: -3.7134644985198975, mean_rew: -7.212986500487176, variance: 3.8797597885131836, lamda: 1.8953957557678223

steps: 1474975, episodes: 59000, mean episode reward: -585.3546613285009, agent episode reward: [-195.11822044283363, -195.11822044283363, -195.11822044283363], time: 188.548
steps: 1474975, episodes: 59000, mean episode variance: 40.6503299279213, agent episode variance: [38.53909001159668, 1.1434433627128602, 0.9677965536117553], time: 188.549
Running avgs for agent 0: q_loss: 903963.5, p_loss: 137.24179077148438, mean_rew: -7.241904738397064, variance: 154.1563600463867, lamda: 2.4352400302886963
Running avgs for agent 1: q_loss: 5.599495887756348, p_loss: -3.7260830402374268, mean_rew: -7.2422042286944395, variance: 4.573773384094238, lamda: 1.6159477233886719
Running avgs for agent 2: q_loss: 7.521160125732422, p_loss: -3.7324163913726807, mean_rew: -7.2416901379393055, variance: 3.8711860179901123, lamda: 1.8990497589111328

steps: 1499975, episodes: 60000, mean episode reward: -583.1431596925288, agent episode reward: [-194.38105323084292, -194.38105323084292, -194.38105323084292], time: 189.902
steps: 1499975, episodes: 60000, mean episode variance: 40.1967987165451, agent episode variance: [38.082900299072264, 1.143848117828369, 0.9700502996444702], time: 189.902
Running avgs for agent 0: q_loss: 927971.1875, p_loss: 138.37353515625, mean_rew: -7.277673507099371, variance: 152.33160119628906, lamda: 2.460214376449585
Running avgs for agent 1: q_loss: 5.415916919708252, p_loss: -3.7430124282836914, mean_rew: -7.275121298222379, variance: 4.575392723083496, lamda: 1.6190910339355469
Running avgs for agent 2: q_loss: 7.659114360809326, p_loss: -3.756263494491577, mean_rew: -7.277191930617175, variance: 3.8802011013031006, lamda: 1.8991345167160034

steps: 1524975, episodes: 61000, mean episode reward: -587.1194278552101, agent episode reward: [-195.7064759517367, -195.7064759517367, -195.7064759517367], time: 190.592
steps: 1524975, episodes: 61000, mean episode variance: 41.00912015914917, agent episode variance: [38.883993865966794, 1.1472090170383453, 0.9779172761440277], time: 190.593
Running avgs for agent 0: q_loss: 949182.875, p_loss: 139.3118133544922, mean_rew: -7.304395389045848, variance: 155.53597546386717, lamda: 2.4851889610290527
Running avgs for agent 1: q_loss: 5.746294975280762, p_loss: -3.7537076473236084, mean_rew: -7.304066472029489, variance: 4.588835716247559, lamda: 1.6256206035614014
Running avgs for agent 2: q_loss: 7.720438480377197, p_loss: -3.77339506149292, mean_rew: -7.3110717294729985, variance: 3.9116690158843994, lamda: 1.8991738557815552

steps: 1549975, episodes: 62000, mean episode reward: -584.0736981420156, agent episode reward: [-194.6912327140052, -194.6912327140052, -194.6912327140052], time: 191.661
steps: 1549975, episodes: 62000, mean episode variance: 41.81119374871254, agent episode variance: [39.67811243438721, 1.150136367082596, 0.9829449472427368], time: 191.662
Running avgs for agent 0: q_loss: 991457.8125, p_loss: 140.31375122070312, mean_rew: -7.339943647842876, variance: 158.71244973754884, lamda: 2.5101630687713623
Running avgs for agent 1: q_loss: 5.542872428894043, p_loss: -3.781320810317993, mean_rew: -7.335561999991786, variance: 4.600545406341553, lamda: 1.627867341041565
Running avgs for agent 2: q_loss: 7.749317169189453, p_loss: -3.7812788486480713, mean_rew: -7.334884692031231, variance: 3.9317798614501953, lamda: 1.8991975784301758

steps: 1574975, episodes: 63000, mean episode reward: -584.9439037419979, agent episode reward: [-194.98130124733265, -194.98130124733265, -194.98130124733265], time: 190.038
steps: 1574975, episodes: 63000, mean episode variance: 41.684541902542115, agent episode variance: [39.55294508361816, 1.1502180066108703, 0.9813788123130799], time: 190.039
Running avgs for agent 0: q_loss: 996255.625, p_loss: 141.19772338867188, mean_rew: -7.374577693243174, variance: 158.21178033447265, lamda: 2.535137414932251
Running avgs for agent 1: q_loss: 5.593181610107422, p_loss: -3.794564962387085, mean_rew: -7.366210266125476, variance: 4.600872039794922, lamda: 1.6300362348556519
Running avgs for agent 2: q_loss: 7.902683734893799, p_loss: -3.8111484050750732, mean_rew: -7.371346807480701, variance: 3.9255151748657227, lamda: 1.8991976976394653

steps: 1599975, episodes: 64000, mean episode reward: -596.6439478295149, agent episode reward: [-198.88131594317161, -198.88131594317161, -198.88131594317161], time: 193.437
steps: 1599975, episodes: 64000, mean episode variance: 42.33670845866203, agent episode variance: [40.18219171142578, 1.1586520788669585, 0.9958646683692932], time: 193.437
Running avgs for agent 0: q_loss: 1025598.125, p_loss: 142.08753967285156, mean_rew: -7.406543397152529, variance: 160.72876684570312, lamda: 2.5601117610931396
Running avgs for agent 1: q_loss: 5.473410129547119, p_loss: -3.81101655960083, mean_rew: -7.40504754891711, variance: 4.634607791900635, lamda: 1.6308740377426147
Running avgs for agent 2: q_loss: 8.02319049835205, p_loss: -3.8191699981689453, mean_rew: -7.403557526602599, variance: 3.9834587574005127, lamda: 1.8991976976394653

steps: 1624975, episodes: 65000, mean episode reward: -592.7802449516468, agent episode reward: [-197.59341498388227, -197.59341498388227, -197.59341498388227], time: 189.238
steps: 1624975, episodes: 65000, mean episode variance: 42.666463206768036, agent episode variance: [40.50726954650879, 1.1662430775165558, 0.992950582742691], time: 189.239
Running avgs for agent 0: q_loss: 1060281.625, p_loss: 142.92481994628906, mean_rew: -7.4390494580540585, variance: 162.02907818603515, lamda: 2.5850861072540283
Running avgs for agent 1: q_loss: 5.407930850982666, p_loss: -3.8234200477600098, mean_rew: -7.4355890973003484, variance: 4.664972305297852, lamda: 1.6314277648925781
Running avgs for agent 2: q_loss: 8.013294219970703, p_loss: -3.8410916328430176, mean_rew: -7.438968308838421, variance: 3.971802234649658, lamda: 1.8992217779159546

steps: 1649975, episodes: 66000, mean episode reward: -591.7017093945974, agent episode reward: [-197.23390313153251, -197.23390313153251, -197.23390313153251], time: 179.795
steps: 1649975, episodes: 66000, mean episode variance: 41.717672325372696, agent episode variance: [39.55283728027344, 1.1652161431312562, 0.9996189019680023], time: 179.795
Running avgs for agent 0: q_loss: 1067143.875, p_loss: 143.51522827148438, mean_rew: -7.464475465328047, variance: 158.21134912109375, lamda: 2.610060453414917
Running avgs for agent 1: q_loss: 5.650753974914551, p_loss: -3.845874071121216, mean_rew: -7.470455049736767, variance: 4.66086483001709, lamda: 1.6322468519210815
Running avgs for agent 2: q_loss: 8.225382804870605, p_loss: -3.8534319400787354, mean_rew: -7.4663912404177735, variance: 3.9984755516052246, lamda: 1.899435043334961

steps: 1674975, episodes: 67000, mean episode reward: -594.4102538113873, agent episode reward: [-198.1367512704624, -198.1367512704624, -198.1367512704624], time: 181.113
steps: 1674975, episodes: 67000, mean episode variance: 42.82862518072128, agent episode variance: [40.65583627319336, 1.1670074424743653, 1.0057814650535584], time: 181.114
Running avgs for agent 0: q_loss: 1091216.0, p_loss: 143.96328735351562, mean_rew: -7.503219293300752, variance: 162.62334509277343, lamda: 2.6350347995758057
Running avgs for agent 1: q_loss: 5.377941131591797, p_loss: -3.861621856689453, mean_rew: -7.497807545572446, variance: 4.66802978515625, lamda: 1.6333329677581787
Running avgs for agent 2: q_loss: 8.096296310424805, p_loss: -3.863860845565796, mean_rew: -7.49606309846351, variance: 4.023126125335693, lamda: 1.8996093273162842

steps: 1699975, episodes: 68000, mean episode reward: -581.6622352043894, agent episode reward: [-193.8874117347965, -193.8874117347965, -193.8874117347965], time: 187.396
steps: 1699975, episodes: 68000, mean episode variance: 42.42970319747925, agent episode variance: [40.2457526550293, 1.1747280530929565, 1.0092224893569945], time: 187.397
Running avgs for agent 0: q_loss: 1095854.75, p_loss: 144.05494689941406, mean_rew: -7.530004166502269, variance: 160.9830106201172, lamda: 2.6600093841552734
Running avgs for agent 1: q_loss: 5.4732184410095215, p_loss: -3.883718729019165, mean_rew: -7.530231387931537, variance: 4.698912143707275, lamda: 1.633725643157959
Running avgs for agent 2: q_loss: 8.217154502868652, p_loss: -3.8814709186553955, mean_rew: -7.52337713352012, variance: 4.036890029907227, lamda: 1.899667501449585

steps: 1724975, episodes: 69000, mean episode reward: -579.7795718174332, agent episode reward: [-193.25985727247777, -193.25985727247777, -193.25985727247777], time: 182.961
steps: 1724975, episodes: 69000, mean episode variance: 41.64624236822128, agent episode variance: [39.454548400878906, 1.1813966615200042, 1.0102973058223725], time: 182.962
Running avgs for agent 0: q_loss: 1097894.125, p_loss: 143.99630737304688, mean_rew: -7.557586330326832, variance: 157.81819360351562, lamda: 2.684983491897583
Running avgs for agent 1: q_loss: 5.434695243835449, p_loss: -3.8925986289978027, mean_rew: -7.558099483430881, variance: 4.725586414337158, lamda: 1.6342228651046753
Running avgs for agent 2: q_loss: 8.356109619140625, p_loss: -3.8994317054748535, mean_rew: -7.557347017545445, variance: 4.041189193725586, lamda: 1.8996987342834473

steps: 1749975, episodes: 70000, mean episode reward: -583.9173576926461, agent episode reward: [-194.63911923088205, -194.63911923088205, -194.63911923088205], time: 182.17
steps: 1749975, episodes: 70000, mean episode variance: 42.42464478731156, agent episode variance: [40.225776748657225, 1.181506549835205, 1.0173614888191223], time: 182.171
Running avgs for agent 0: q_loss: 1097271.75, p_loss: 143.83880615234375, mean_rew: -7.56708614985656, variance: 160.9031069946289, lamda: 2.709958076477051
Running avgs for agent 1: q_loss: 5.400694370269775, p_loss: -3.891022205352783, mean_rew: -7.565608454980294, variance: 4.72602653503418, lamda: 1.6344053745269775
Running avgs for agent 2: q_loss: 8.175064086914062, p_loss: -3.9007201194763184, mean_rew: -7.572011163177088, variance: 4.069446086883545, lamda: 1.8996986150741577

steps: 1774975, episodes: 71000, mean episode reward: -586.9413585015448, agent episode reward: [-195.64711950051495, -195.64711950051495, -195.64711950051495], time: 181.609
steps: 1774975, episodes: 71000, mean episode variance: 42.50414313149452, agent episode variance: [40.30934625244141, 1.1803546109199523, 1.0144422681331635], time: 181.61
Running avgs for agent 0: q_loss: 1088980.625, p_loss: 143.7672882080078, mean_rew: -7.58627766509806, variance: 161.23738500976563, lamda: 2.7349321842193604
Running avgs for agent 1: q_loss: 5.449265480041504, p_loss: -3.910621166229248, mean_rew: -7.5900333615448, variance: 4.721418857574463, lamda: 1.6346973180770874
Running avgs for agent 2: q_loss: 8.272618293762207, p_loss: -3.9085819721221924, mean_rew: -7.582513188806787, variance: 4.057768821716309, lamda: 1.8997946977615356

steps: 1799975, episodes: 72000, mean episode reward: -580.8657717803884, agent episode reward: [-193.62192392679611, -193.62192392679611, -193.62192392679611], time: 192.321
steps: 1799975, episodes: 72000, mean episode variance: 41.91652195715904, agent episode variance: [39.71196258544922, 1.188674967288971, 1.0158844044208526], time: 192.321
Running avgs for agent 0: q_loss: 1096861.5, p_loss: 143.72987365722656, mean_rew: -7.600382674912624, variance: 158.84785034179689, lamda: 2.759906768798828
Running avgs for agent 1: q_loss: 5.469603061676025, p_loss: -3.9150161743164062, mean_rew: -7.605943998897664, variance: 4.754700183868408, lamda: 1.6347092390060425
Running avgs for agent 2: q_loss: 8.469169616699219, p_loss: -3.917928695678711, mean_rew: -7.606133368976432, variance: 4.06353759765625, lamda: 1.9001781940460205

steps: 1824975, episodes: 73000, mean episode reward: -579.2717234307418, agent episode reward: [-193.09057447691396, -193.09057447691396, -193.09057447691396], time: 193.089
steps: 1824975, episodes: 73000, mean episode variance: 42.047184396266935, agent episode variance: [39.83047883605957, 1.192600928068161, 1.024104632139206], time: 193.09
Running avgs for agent 0: q_loss: 1109158.5, p_loss: 143.73419189453125, mean_rew: -7.616437125950114, variance: 159.32191534423828, lamda: 2.7848808765411377
Running avgs for agent 1: q_loss: 5.610672473907471, p_loss: -3.9275453090667725, mean_rew: -7.625736357969091, variance: 4.7704033851623535, lamda: 1.6351617574691772
Running avgs for agent 2: q_loss: 8.281225204467773, p_loss: -3.928684949874878, mean_rew: -7.627516725154691, variance: 4.096418380737305, lamda: 1.9003249406814575

steps: 1849975, episodes: 74000, mean episode reward: -585.2669682706185, agent episode reward: [-195.0889894235395, -195.0889894235395, -195.0889894235395], time: 190.121
steps: 1849975, episodes: 74000, mean episode variance: 41.98625141096115, agent episode variance: [39.77879405212402, 1.1890374388694762, 1.0184199199676514], time: 190.122
Running avgs for agent 0: q_loss: 1110951.375, p_loss: 143.6016845703125, mean_rew: -7.631441209911111, variance: 159.11517620849608, lamda: 2.8098552227020264
Running avgs for agent 1: q_loss: 5.577208995819092, p_loss: -3.9283649921417236, mean_rew: -7.635964811303993, variance: 4.7561492919921875, lamda: 1.6361316442489624
Running avgs for agent 2: q_loss: 8.353604316711426, p_loss: -3.935739755630493, mean_rew: -7.638235972212696, variance: 4.0736799240112305, lamda: 1.900324821472168

steps: 1874975, episodes: 75000, mean episode reward: -582.4840993327771, agent episode reward: [-194.16136644425902, -194.16136644425902, -194.16136644425902], time: 181.25
steps: 1874975, episodes: 75000, mean episode variance: 42.49289571857452, agent episode variance: [40.28333052062988, 1.1798649344444274, 1.0297002635002137], time: 181.25
Running avgs for agent 0: q_loss: 1104734.5, p_loss: 143.56285095214844, mean_rew: -7.651054478181897, variance: 161.13332208251953, lamda: 2.834829568862915
Running avgs for agent 1: q_loss: 9.080854415893555, p_loss: -3.9453814029693604, mean_rew: -7.651877402535769, variance: 4.7194600105285645, lamda: 1.6375226974487305
Running avgs for agent 2: q_loss: 8.484338760375977, p_loss: -3.9405174255371094, mean_rew: -7.649540033860617, variance: 4.118800640106201, lamda: 1.9004724025726318

steps: 1899975, episodes: 76000, mean episode reward: -581.2788945165192, agent episode reward: [-193.7596315055064, -193.7596315055064, -193.7596315055064], time: 179.4
steps: 1899975, episodes: 76000, mean episode variance: 40.86355771970749, agent episode variance: [38.64630290222168, 1.189485221862793, 1.0277695956230164], time: 179.4
Running avgs for agent 0: q_loss: 1106220.5, p_loss: 143.59246826171875, mean_rew: -7.658325627005554, variance: 154.5852116088867, lamda: 2.8598039150238037
Running avgs for agent 1: q_loss: 9.366472244262695, p_loss: -3.9522793292999268, mean_rew: -7.669378969469557, variance: 4.757940292358398, lamda: 1.6376585960388184
Running avgs for agent 2: q_loss: 8.455137252807617, p_loss: -3.9460248947143555, mean_rew: -7.6613335422227, variance: 4.111078262329102, lamda: 1.9006240367889404

steps: 1924975, episodes: 77000, mean episode reward: -586.820540627631, agent episode reward: [-195.606846875877, -195.606846875877, -195.606846875877], time: 180.661
steps: 1924975, episodes: 77000, mean episode variance: 41.34103536844253, agent episode variance: [39.11790844726563, 1.194081860780716, 1.0290450603961945], time: 180.662
Running avgs for agent 0: q_loss: 1119676.625, p_loss: 143.5227508544922, mean_rew: -7.678368585909683, variance: 156.4716337890625, lamda: 2.8847782611846924
Running avgs for agent 1: q_loss: 9.411890983581543, p_loss: -3.9526829719543457, mean_rew: -7.675652793360899, variance: 4.776328086853027, lamda: 1.6376584768295288
Running avgs for agent 2: q_loss: 8.43298625946045, p_loss: -3.9601728916168213, mean_rew: -7.683389864098296, variance: 4.116179943084717, lamda: 1.9006242752075195

steps: 1949975, episodes: 78000, mean episode reward: -582.6734097307439, agent episode reward: [-194.22446991024796, -194.22446991024796, -194.22446991024796], time: 182.394
steps: 1949975, episodes: 78000, mean episode variance: 41.069610810756686, agent episode variance: [38.83755610656738, 1.1999514756202698, 1.0321032285690308], time: 182.395
Running avgs for agent 0: q_loss: 1122535.375, p_loss: 143.54617309570312, mean_rew: -7.697161009234145, variance: 155.35022442626953, lamda: 2.909752607345581
Running avgs for agent 1: q_loss: 9.402283668518066, p_loss: -3.958523750305176, mean_rew: -7.686507870457707, variance: 4.799805641174316, lamda: 1.6376585960388184
Running avgs for agent 2: q_loss: 8.47433090209961, p_loss: -3.9572806358337402, mean_rew: -7.692456998496498, variance: 4.128413200378418, lamda: 1.9006245136260986

steps: 1974975, episodes: 79000, mean episode reward: -578.1561526910875, agent episode reward: [-192.71871756369586, -192.71871756369586, -192.71871756369586], time: 182.607
steps: 1974975, episodes: 79000, mean episode variance: 41.16624133825302, agent episode variance: [38.93871350097656, 1.1957364451885224, 1.0317913920879365], time: 182.607
Running avgs for agent 0: q_loss: 1143025.125, p_loss: 143.47193908691406, mean_rew: -7.708952025944367, variance: 155.75485400390625, lamda: 2.9347269535064697
Running avgs for agent 1: q_loss: 9.347139358520508, p_loss: -3.9709906578063965, mean_rew: -7.704792101164447, variance: 4.7829461097717285, lamda: 1.637668490409851
Running avgs for agent 2: q_loss: 8.432863235473633, p_loss: -3.975224494934082, mean_rew: -7.703654905348084, variance: 4.127165794372559, lamda: 1.900624394416809

steps: 1999975, episodes: 80000, mean episode reward: -585.0454556047081, agent episode reward: [-195.01515186823605, -195.01515186823605, -195.01515186823605], time: 184.473
steps: 1999975, episodes: 80000, mean episode variance: 41.96288048291206, agent episode variance: [39.726730926513675, 1.202762706041336, 1.0333868503570556], time: 184.474
Running avgs for agent 0: q_loss: 1144910.75, p_loss: 143.35812377929688, mean_rew: -7.713833388510367, variance: 158.9069237060547, lamda: 2.9597010612487793
Running avgs for agent 1: q_loss: 9.365002632141113, p_loss: -3.9822914600372314, mean_rew: -7.720721474595072, variance: 4.811050891876221, lamda: 1.6376971006393433
Running avgs for agent 2: q_loss: 8.408685684204102, p_loss: -3.9717037677764893, mean_rew: -7.71248664403509, variance: 4.133547306060791, lamda: 1.9006242752075195

steps: 2024975, episodes: 81000, mean episode reward: -582.1966581435806, agent episode reward: [-194.0655527145269, -194.0655527145269, -194.0655527145269], time: 183.035
steps: 2024975, episodes: 81000, mean episode variance: 40.899524723529815, agent episode variance: [38.66556170654297, 1.1985190246105195, 1.0354439923763274], time: 183.036
Running avgs for agent 0: q_loss: 1158010.0, p_loss: 143.3897705078125, mean_rew: -7.729375830388943, variance: 154.66224682617187, lamda: 2.984675884246826
Running avgs for agent 1: q_loss: 9.311847686767578, p_loss: -3.981773853302002, mean_rew: -7.730328492264576, variance: 4.794075965881348, lamda: 1.6376969814300537
Running avgs for agent 2: q_loss: 8.424227714538574, p_loss: -3.978215217590332, mean_rew: -7.7297461892213075, variance: 4.1417765617370605, lamda: 1.9006245136260986

steps: 2049975, episodes: 82000, mean episode reward: -580.6056361260561, agent episode reward: [-193.53521204201866, -193.53521204201866, -193.53521204201866], time: 183.817
steps: 2049975, episodes: 82000, mean episode variance: 40.52385944509506, agent episode variance: [38.276730422973635, 1.2020334153175354, 1.0450956068038941], time: 183.818
Running avgs for agent 0: q_loss: 1162094.25, p_loss: 143.2945098876953, mean_rew: -7.731576021889154, variance: 153.10692169189454, lamda: 3.009650230407715
Running avgs for agent 1: q_loss: 9.317912101745605, p_loss: -3.985318183898926, mean_rew: -7.740191605482843, variance: 4.808133602142334, lamda: 1.6376972198486328
Running avgs for agent 2: q_loss: 8.446706771850586, p_loss: -3.98435115814209, mean_rew: -7.739460226769894, variance: 4.180382251739502, lamda: 1.900624394416809

steps: 2074975, episodes: 83000, mean episode reward: -585.0175652797938, agent episode reward: [-195.0058550932646, -195.0058550932646, -195.0058550932646], time: 181.149
steps: 2074975, episodes: 83000, mean episode variance: 40.67601996183395, agent episode variance: [38.42053352355957, 1.2108135209083557, 1.0446729173660279], time: 181.15
Running avgs for agent 0: q_loss: 1173095.375, p_loss: 143.3107147216797, mean_rew: -7.756053832260215, variance: 153.68213409423828, lamda: 3.0346245765686035
Running avgs for agent 1: q_loss: 9.252202987670898, p_loss: -3.989917755126953, mean_rew: -7.752685633599006, variance: 4.843254089355469, lamda: 1.6376971006393433
Running avgs for agent 2: q_loss: 8.438525199890137, p_loss: -3.9888155460357666, mean_rew: -7.749183604017802, variance: 4.17869234085083, lamda: 1.9006242752075195

steps: 2099975, episodes: 84000, mean episode reward: -583.6016227588975, agent episode reward: [-194.53387425296586, -194.53387425296586, -194.53387425296586], time: 176.78
steps: 2099975, episodes: 84000, mean episode variance: 41.146476022720336, agent episode variance: [38.895525634765626, 1.2132414457798004, 1.0377089421749115], time: 176.78
Running avgs for agent 0: q_loss: 1186924.75, p_loss: 143.20870971679688, mean_rew: -7.753070025189142, variance: 155.5821025390625, lamda: 3.059598684310913
Running avgs for agent 1: q_loss: 9.27901554107666, p_loss: -4.000487804412842, mean_rew: -7.766399355824775, variance: 4.852965831756592, lamda: 1.6376969814300537
Running avgs for agent 2: q_loss: 8.413189888000488, p_loss: -3.993882417678833, mean_rew: -7.755436075052609, variance: 4.150835990905762, lamda: 1.900642991065979

steps: 2124975, episodes: 85000, mean episode reward: -585.1996965875704, agent episode reward: [-195.06656552919011, -195.06656552919011, -195.06656552919011], time: 177.154
steps: 2124975, episodes: 85000, mean episode variance: 40.888724218130115, agent episode variance: [38.632692108154295, 1.2037454419136047, 1.05228666806221], time: 177.155
Running avgs for agent 0: q_loss: 1195793.0, p_loss: 143.27847290039062, mean_rew: -7.769580293547617, variance: 154.53076843261718, lamda: 3.0845730304718018
Running avgs for agent 1: q_loss: 9.213274002075195, p_loss: -3.9957938194274902, mean_rew: -7.758028638853073, variance: 4.814981937408447, lamda: 1.6376972198486328
Running avgs for agent 2: q_loss: 8.412603378295898, p_loss: -3.9905948638916016, mean_rew: -7.7629721460983365, variance: 4.209146499633789, lamda: 1.9007099866867065

steps: 2149975, episodes: 86000, mean episode reward: -583.1420875015582, agent episode reward: [-194.38069583385274, -194.38069583385274, -194.38069583385274], time: 181.655
steps: 2149975, episodes: 86000, mean episode variance: 40.910310507059094, agent episode variance: [38.65589778137207, 1.211669322013855, 1.042743403673172], time: 181.655
Running avgs for agent 0: q_loss: 1229094.25, p_loss: 143.262451171875, mean_rew: -7.765818801938477, variance: 154.62359112548828, lamda: 3.1095473766326904
Running avgs for agent 1: q_loss: 9.190675735473633, p_loss: -4.005866050720215, mean_rew: -7.776190950664711, variance: 4.846677303314209, lamda: 1.6376971006393433
Running avgs for agent 2: q_loss: 8.25326919555664, p_loss: -4.000362396240234, mean_rew: -7.773123856103519, variance: 4.170973777770996, lamda: 1.9007099866867065

steps: 2174975, episodes: 87000, mean episode reward: -585.8464221191897, agent episode reward: [-195.2821407063966, -195.2821407063966, -195.2821407063966], time: 170.11
steps: 2174975, episodes: 87000, mean episode variance: 41.37826964211464, agent episode variance: [39.127203796386716, 1.2126214489936828, 1.0384443967342376], time: 170.111
Running avgs for agent 0: q_loss: 1233028.25, p_loss: 143.322998046875, mean_rew: -7.774838265903463, variance: 156.50881518554687, lamda: 3.134521961212158
Running avgs for agent 1: q_loss: 9.447003364562988, p_loss: -4.002786159515381, mean_rew: -7.774907895371371, variance: 4.850485801696777, lamda: 1.6376969814300537
Running avgs for agent 2: q_loss: 8.42444133758545, p_loss: -4.001532077789307, mean_rew: -7.77206204718974, variance: 4.153777122497559, lamda: 1.9007326364517212

steps: 2199975, episodes: 88000, mean episode reward: -591.6782654299492, agent episode reward: [-197.22608847664975, -197.22608847664975, -197.22608847664975], time: 126.208
steps: 2199975, episodes: 88000, mean episode variance: 40.92925360870361, agent episode variance: [38.67603472900391, 1.2103452286720275, 1.0428736510276795], time: 126.208
Running avgs for agent 0: q_loss: 1244276.75, p_loss: 143.3072052001953, mean_rew: -7.773652547219022, variance: 154.70413891601564, lamda: 3.1594960689544678
Running avgs for agent 1: q_loss: 9.309406280517578, p_loss: -4.005572319030762, mean_rew: -7.780507680157507, variance: 4.841381072998047, lamda: 1.6376972198486328
Running avgs for agent 2: q_loss: 8.297499656677246, p_loss: -4.000941276550293, mean_rew: -7.775408807598233, variance: 4.171494483947754, lamda: 1.9009106159210205

steps: 2224975, episodes: 89000, mean episode reward: -592.8110707689528, agent episode reward: [-197.6036902563176, -197.6036902563176, -197.6036902563176], time: 123.531
steps: 2224975, episodes: 89000, mean episode variance: 39.98779867529869, agent episode variance: [37.71719024658203, 1.2192741594314576, 1.0513342692852021], time: 123.532
Running avgs for agent 0: q_loss: 1267789.875, p_loss: 143.44407653808594, mean_rew: -7.78628128905006, variance: 150.8687609863281, lamda: 3.1844704151153564
Running avgs for agent 1: q_loss: 9.209738731384277, p_loss: -4.007652282714844, mean_rew: -7.787492581203065, variance: 4.877096652984619, lamda: 1.6376972198486328
Running avgs for agent 2: q_loss: 8.233050346374512, p_loss: -3.997852325439453, mean_rew: -7.786223998729613, variance: 4.205337047576904, lamda: 1.9009106159210205

steps: 2249975, episodes: 90000, mean episode reward: -591.8832289626471, agent episode reward: [-197.2944096542157, -197.2944096542157, -197.2944096542157], time: 125.227
steps: 2249975, episodes: 90000, mean episode variance: 40.666482627391815, agent episode variance: [38.40547491455078, 1.2182347564697265, 1.0427729563713073], time: 125.227
Running avgs for agent 0: q_loss: 1269512.75, p_loss: 143.511962890625, mean_rew: -7.791253777143037, variance: 153.62189965820312, lamda: 3.209444761276245
Running avgs for agent 1: q_loss: 9.25246524810791, p_loss: -4.010003089904785, mean_rew: -7.7917135483530995, variance: 4.872938632965088, lamda: 1.6376969814300537
Running avgs for agent 2: q_loss: 8.423309326171875, p_loss: -4.0135393142700195, mean_rew: -7.795149058215792, variance: 4.1710920333862305, lamda: 1.901054859161377

steps: 2274975, episodes: 91000, mean episode reward: -586.7176267326887, agent episode reward: [-195.5725422442296, -195.5725422442296, -195.5725422442296], time: 118.135
steps: 2274975, episodes: 91000, mean episode variance: 40.666204133749005, agent episode variance: [38.39825828552246, 1.216833008289337, 1.05111283993721], time: 118.136
Running avgs for agent 0: q_loss: 1285607.75, p_loss: 143.6022186279297, mean_rew: -7.79979803572022, variance: 153.59303314208984, lamda: 3.2344188690185547
Running avgs for agent 1: q_loss: 9.278119087219238, p_loss: -4.0179829597473145, mean_rew: -7.8039503170188675, variance: 4.8673319816589355, lamda: 1.6376972198486328
Running avgs for agent 2: q_loss: 8.358004570007324, p_loss: -4.007521152496338, mean_rew: -7.7967251269994176, variance: 4.204451084136963, lamda: 1.9013550281524658

steps: 2299975, episodes: 92000, mean episode reward: -586.8171134317215, agent episode reward: [-195.60570447724052, -195.60570447724052, -195.60570447724052], time: 109.425
steps: 2299975, episodes: 92000, mean episode variance: 41.734952016592025, agent episode variance: [39.47259506225586, 1.214061557531357, 1.0482953968048097], time: 109.425
Running avgs for agent 0: q_loss: 1306950.625, p_loss: 143.56040954589844, mean_rew: -7.800629621464826, variance: 157.89038024902345, lamda: 3.2593934535980225
Running avgs for agent 1: q_loss: 9.26313304901123, p_loss: -4.011242389678955, mean_rew: -7.806324179999025, variance: 4.856245994567871, lamda: 1.6376982927322388
Running avgs for agent 2: q_loss: 8.417329788208008, p_loss: -4.011289596557617, mean_rew: -7.801628260930404, variance: 4.19318151473999, lamda: 1.9015541076660156

steps: 2324975, episodes: 93000, mean episode reward: -587.3207887012989, agent episode reward: [-195.77359623376628, -195.77359623376628, -195.77359623376628], time: 113.804
steps: 2324975, episodes: 93000, mean episode variance: 40.57860605621338, agent episode variance: [38.31514265441894, 1.2173159418106079, 1.0461474599838256], time: 113.805
Running avgs for agent 0: q_loss: 1319568.0, p_loss: 143.7801513671875, mean_rew: -7.815402055543785, variance: 153.26057061767577, lamda: 3.2843680381774902
Running avgs for agent 1: q_loss: 9.256199836730957, p_loss: -4.014328479766846, mean_rew: -7.803439837706672, variance: 4.869263648986816, lamda: 1.637893557548523
Running avgs for agent 2: q_loss: 8.470161437988281, p_loss: -4.021077632904053, mean_rew: -7.8097278140438355, variance: 4.184589862823486, lamda: 1.9015777111053467

steps: 2349975, episodes: 94000, mean episode reward: -591.6454721090782, agent episode reward: [-197.21515736969272, -197.21515736969272, -197.21515736969272], time: 111.985
steps: 2349975, episodes: 94000, mean episode variance: 41.207606923103334, agent episode variance: [38.93363636779785, 1.2162754566669465, 1.0576950986385345], time: 111.985
Running avgs for agent 0: q_loss: 1339853.25, p_loss: 143.82928466796875, mean_rew: -7.811064540818046, variance: 155.7345454711914, lamda: 3.309342384338379
Running avgs for agent 1: q_loss: 9.130928039550781, p_loss: -4.013164520263672, mean_rew: -7.808907459363347, variance: 4.865101337432861, lamda: 1.6378982067108154
Running avgs for agent 2: q_loss: 8.326254844665527, p_loss: -4.016724586486816, mean_rew: -7.814018044610915, variance: 4.230780124664307, lamda: 1.9015775918960571

steps: 2374975, episodes: 95000, mean episode reward: -585.3506271770325, agent episode reward: [-195.1168757256775, -195.1168757256775, -195.1168757256775], time: 103.477
steps: 2374975, episodes: 95000, mean episode variance: 41.42474079394341, agent episode variance: [39.14382992553711, 1.2275391681194305, 1.0533717002868652], time: 103.477
Running avgs for agent 0: q_loss: 1357187.125, p_loss: 143.9686279296875, mean_rew: -7.807462147416101, variance: 156.57531970214845, lamda: 3.3343167304992676
Running avgs for agent 1: q_loss: 9.174854278564453, p_loss: -4.020703315734863, mean_rew: -7.817330828544879, variance: 4.910156726837158, lamda: 1.6379280090332031
Running avgs for agent 2: q_loss: 8.254940032958984, p_loss: -4.013957977294922, mean_rew: -7.8122880795789476, variance: 4.213486671447754, lamda: 1.9015775918960571

steps: 2399975, episodes: 96000, mean episode reward: -596.1344226195058, agent episode reward: [-198.71147420650192, -198.71147420650192, -198.71147420650192], time: 100.925
steps: 2399975, episodes: 96000, mean episode variance: 40.69887286710739, agent episode variance: [38.436941207885745, 1.2130625884532928, 1.0488690707683563], time: 100.925
Running avgs for agent 0: q_loss: 1371259.625, p_loss: 144.1312255859375, mean_rew: -7.8160516690678286, variance: 153.74776483154298, lamda: 3.359290838241577
Running avgs for agent 1: q_loss: 9.022497177124023, p_loss: -4.0180583000183105, mean_rew: -7.815551733586732, variance: 4.852250099182129, lamda: 1.6380949020385742
Running avgs for agent 2: q_loss: 8.322031021118164, p_loss: -4.0202956199646, mean_rew: -7.8160292014398305, variance: 4.195476055145264, lamda: 1.9015777111053467

steps: 2424975, episodes: 97000, mean episode reward: -591.4442401480458, agent episode reward: [-197.1480800493486, -197.1480800493486, -197.1480800493486], time: 102.826
steps: 2424975, episodes: 97000, mean episode variance: 39.196297189235686, agent episode variance: [36.927887374877926, 1.2147788243293762, 1.0536309900283813], time: 102.826
Running avgs for agent 0: q_loss: 1381675.0, p_loss: 144.2963104248047, mean_rew: -7.814870117305114, variance: 147.7115494995117, lamda: 3.384265184402466
Running avgs for agent 1: q_loss: 9.171248435974121, p_loss: -4.0164618492126465, mean_rew: -7.815031011584711, variance: 4.859115123748779, lamda: 1.6380951404571533
Running avgs for agent 2: q_loss: 8.388283729553223, p_loss: -4.018007278442383, mean_rew: -7.822109106079305, variance: 4.214523792266846, lamda: 1.901593804359436

steps: 2449975, episodes: 98000, mean episode reward: -593.3727332780192, agent episode reward: [-197.790911092673, -197.790911092673, -197.790911092673], time: 102.211
steps: 2449975, episodes: 98000, mean episode variance: 41.37497337198258, agent episode variance: [39.09891677856445, 1.2245981650352478, 1.0514584283828736], time: 102.212
Running avgs for agent 0: q_loss: 1416565.625, p_loss: 144.46414184570312, mean_rew: -7.819947911362564, variance: 156.3956671142578, lamda: 3.4092397689819336
Running avgs for agent 1: q_loss: 9.278592109680176, p_loss: -4.0176286697387695, mean_rew: -7.824336006609959, variance: 4.898392677307129, lamda: 1.6380951404571533
Running avgs for agent 2: q_loss: 8.274908065795898, p_loss: -4.017064094543457, mean_rew: -7.825217097072349, variance: 4.205833911895752, lamda: 1.9016801118850708

steps: 2474975, episodes: 99000, mean episode reward: -589.2014065470579, agent episode reward: [-196.40046884901932, -196.40046884901932, -196.40046884901932], time: 102.173
steps: 2474975, episodes: 99000, mean episode variance: 40.269558304786685, agent episode variance: [38.00775854492188, 1.2129500052928925, 1.0488497545719146], time: 102.173
Running avgs for agent 0: q_loss: 1434083.375, p_loss: 144.71559143066406, mean_rew: -7.832748715289801, variance: 152.0310341796875, lamda: 3.434213876724243
Running avgs for agent 1: q_loss: 9.262507438659668, p_loss: -4.025838375091553, mean_rew: -7.825033522257394, variance: 4.851799964904785, lamda: 1.6380950212478638
Running avgs for agent 2: q_loss: 8.268710136413574, p_loss: -4.01600456237793, mean_rew: -7.817906324250868, variance: 4.195398807525635, lamda: 1.901706337928772

steps: 2499975, episodes: 100000, mean episode reward: -592.0940158918597, agent episode reward: [-197.3646719639533, -197.3646719639533, -197.3646719639533], time: 102.475
steps: 2499975, episodes: 100000, mean episode variance: 39.2736812813282, agent episode variance: [36.998553161621096, 1.2200656943321229, 1.0550624253749847], time: 102.476
Running avgs for agent 0: q_loss: 1420018.0, p_loss: 144.8589630126953, mean_rew: -7.832493571906838, variance: 147.99421264648439, lamda: 3.4591879844665527
Running avgs for agent 1: q_loss: 9.08544921875, p_loss: -4.016979694366455, mean_rew: -7.82972105637025, variance: 4.880262851715088, lamda: 1.6380951404571533
Running avgs for agent 2: q_loss: 8.249208450317383, p_loss: -4.023348808288574, mean_rew: -7.8311445648143145, variance: 4.220250129699707, lamda: 1.9017318487167358

steps: 2524975, episodes: 101000, mean episode reward: -595.8345736483608, agent episode reward: [-198.61152454945358, -198.61152454945358, -198.61152454945358], time: 108.015
steps: 2524975, episodes: 101000, mean episode variance: 41.534180646657944, agent episode variance: [39.26236660766602, 1.220547066450119, 1.0512669725418091], time: 108.015
Running avgs for agent 0: q_loss: 1434538.5, p_loss: 144.9427490234375, mean_rew: -7.833288598430396, variance: 157.04946643066407, lamda: 3.4841625690460205
Running avgs for agent 1: q_loss: 9.203367233276367, p_loss: -4.0245137214660645, mean_rew: -7.833584851816433, variance: 4.882188320159912, lamda: 1.6380951404571533
Running avgs for agent 2: q_loss: 8.153006553649902, p_loss: -4.019010066986084, mean_rew: -7.8365532105795515, variance: 4.2050676345825195, lamda: 1.9017317295074463

steps: 2549975, episodes: 102000, mean episode reward: -591.5374289599876, agent episode reward: [-197.17914298666258, -197.17914298666258, -197.17914298666258], time: 98.947
steps: 2549975, episodes: 102000, mean episode variance: 40.42966169333458, agent episode variance: [38.15521353149414, 1.218245774269104, 1.0562023875713349], time: 98.948
Running avgs for agent 0: q_loss: 1446484.25, p_loss: 145.0397491455078, mean_rew: -7.832128422238109, variance: 152.62085412597656, lamda: 3.509136915206909
Running avgs for agent 1: q_loss: 8.990421295166016, p_loss: -4.018946647644043, mean_rew: -7.83228785819278, variance: 4.872982978820801, lamda: 1.6380959749221802
Running avgs for agent 2: q_loss: 8.286191940307617, p_loss: -4.0181684494018555, mean_rew: -7.831497519186391, variance: 4.224809646606445, lamda: 1.9017319679260254

steps: 2574975, episodes: 103000, mean episode reward: -593.8436864000715, agent episode reward: [-197.94789546669048, -197.94789546669048, -197.94789546669048], time: 99.982
steps: 2574975, episodes: 103000, mean episode variance: 40.323279810905454, agent episode variance: [38.05233432006836, 1.2197294957637788, 1.0512159950733184], time: 99.982
Running avgs for agent 0: q_loss: 1477644.875, p_loss: 145.0548858642578, mean_rew: -7.835587877321077, variance: 152.20933728027345, lamda: 3.534111261367798
Running avgs for agent 1: q_loss: 9.039058685302734, p_loss: -4.026256084442139, mean_rew: -7.8345545024206, variance: 4.878918170928955, lamda: 1.6381282806396484
Running avgs for agent 2: q_loss: 8.354762077331543, p_loss: -4.022146224975586, mean_rew: -7.83331499723497, variance: 4.204863548278809, lamda: 1.901827335357666

steps: 2599975, episodes: 104000, mean episode reward: -597.753109442568, agent episode reward: [-199.25103648085602, -199.25103648085602, -199.25103648085602], time: 100.086
steps: 2599975, episodes: 104000, mean episode variance: 40.77232942199707, agent episode variance: [38.485311477661135, 1.2288575115203857, 1.0581604328155518], time: 100.087
Running avgs for agent 0: q_loss: 1476595.25, p_loss: 145.32443237304688, mean_rew: -7.838788060510773, variance: 153.94124591064454, lamda: 3.5590856075286865
Running avgs for agent 1: q_loss: 9.062759399414062, p_loss: -4.023921966552734, mean_rew: -7.8412862366574005, variance: 4.915429592132568, lamda: 1.638128399848938
Running avgs for agent 2: q_loss: 8.186214447021484, p_loss: -4.022590637207031, mean_rew: -7.841529205367159, variance: 4.232641696929932, lamda: 1.9019255638122559

steps: 2624975, episodes: 105000, mean episode reward: -587.1918447483429, agent episode reward: [-195.73061491611432, -195.73061491611432, -195.73061491611432], time: 102.57
steps: 2624975, episodes: 105000, mean episode variance: 39.54891882586479, agent episode variance: [37.27535264587402, 1.2234099841117858, 1.0501561958789825], time: 102.57
Running avgs for agent 0: q_loss: 1488771.75, p_loss: 145.44752502441406, mean_rew: -7.841636151596929, variance: 149.1014105834961, lamda: 3.584059953689575
Running avgs for agent 1: q_loss: 8.942066192626953, p_loss: -4.021387100219727, mean_rew: -7.834103225884106, variance: 4.89363956451416, lamda: 1.6381285190582275
Running avgs for agent 2: q_loss: 8.178574562072754, p_loss: -4.025230407714844, mean_rew: -7.838354691724918, variance: 4.200624465942383, lamda: 1.901925802230835

steps: 2649975, episodes: 106000, mean episode reward: -591.3982040008642, agent episode reward: [-197.13273466695472, -197.13273466695472, -197.13273466695472], time: 96.019
steps: 2649975, episodes: 106000, mean episode variance: 40.33934875392914, agent episode variance: [38.06170574951172, 1.224444092273712, 1.0531989121437073], time: 96.02
Running avgs for agent 0: q_loss: 1515589.625, p_loss: 145.50030517578125, mean_rew: -7.839092904540702, variance: 152.24682299804687, lamda: 3.609034538269043
Running avgs for agent 1: q_loss: 8.99773120880127, p_loss: -4.028052806854248, mean_rew: -7.846122822553921, variance: 4.8977766036987305, lamda: 1.638128638267517
Running avgs for agent 2: q_loss: 8.237414360046387, p_loss: -4.023457050323486, mean_rew: -7.842329575819496, variance: 4.212795257568359, lamda: 1.9019256830215454

steps: 2674975, episodes: 107000, mean episode reward: -596.7762648216391, agent episode reward: [-198.92542160721302, -198.92542160721302, -198.92542160721302], time: 92.314
steps: 2674975, episodes: 107000, mean episode variance: 41.256861790895464, agent episode variance: [38.97907165527344, 1.226281510591507, 1.0515086250305177], time: 92.315
Running avgs for agent 0: q_loss: 1505290.875, p_loss: 145.4952392578125, mean_rew: -7.838971200272407, variance: 155.91628662109375, lamda: 3.6340088844299316
Running avgs for agent 1: q_loss: 8.916449546813965, p_loss: -4.020885467529297, mean_rew: -7.841623846947139, variance: 4.905126094818115, lamda: 1.638128638267517
Running avgs for agent 2: q_loss: 8.197108268737793, p_loss: -4.020495891571045, mean_rew: -7.837299581240368, variance: 4.206034183502197, lamda: 1.9019285440444946

steps: 2699975, episodes: 108000, mean episode reward: -595.9165638777632, agent episode reward: [-198.63885462592108, -198.63885462592108, -198.63885462592108], time: 79.883
steps: 2699975, episodes: 108000, mean episode variance: 38.872886335372925, agent episode variance: [36.598514022827146, 1.2231285848617555, 1.051243727684021], time: 79.883
Running avgs for agent 0: q_loss: 1546086.375, p_loss: 145.62144470214844, mean_rew: -7.839806274294339, variance: 146.39405609130858, lamda: 3.658982992172241
Running avgs for agent 1: q_loss: 8.790582656860352, p_loss: -4.022862911224365, mean_rew: -7.8435147618917345, variance: 4.892514228820801, lamda: 1.6381285190582275
Running avgs for agent 2: q_loss: 8.270354270935059, p_loss: -4.019388675689697, mean_rew: -7.83760566605838, variance: 4.20497465133667, lamda: 1.9020212888717651

steps: 2724975, episodes: 109000, mean episode reward: -591.3002640194053, agent episode reward: [-197.1000880064684, -197.1000880064684, -197.1000880064684], time: 78.843
steps: 2724975, episodes: 109000, mean episode variance: 39.802399601459506, agent episode variance: [37.52930792236328, 1.2196413705348967, 1.0534503085613252], time: 78.844
Running avgs for agent 0: q_loss: 1539761.125, p_loss: 145.8643798828125, mean_rew: -7.844196308912582, variance: 150.11723168945312, lamda: 3.683957576751709
Running avgs for agent 1: q_loss: 8.834358215332031, p_loss: -4.026138782501221, mean_rew: -7.846387516977734, variance: 4.878565311431885, lamda: 1.638128638267517
Running avgs for agent 2: q_loss: 8.276723861694336, p_loss: -4.024519443511963, mean_rew: -7.8458077026866775, variance: 4.213801383972168, lamda: 1.9020349979400635

steps: 2749975, episodes: 110000, mean episode reward: -595.9242404714879, agent episode reward: [-198.64141349049595, -198.64141349049595, -198.64141349049595], time: 75.55
steps: 2749975, episodes: 110000, mean episode variance: 39.66011757516861, agent episode variance: [37.387072525024415, 1.2252573375701905, 1.0477877125740052], time: 75.551
Running avgs for agent 0: q_loss: 1587413.25, p_loss: 146.10162353515625, mean_rew: -7.851060828855723, variance: 149.54829010009766, lamda: 3.7089316844940186
Running avgs for agent 1: q_loss: 9.002657890319824, p_loss: -4.022995948791504, mean_rew: -7.848214257338236, variance: 4.901029109954834, lamda: 1.638128638267517
Running avgs for agent 2: q_loss: 8.240901947021484, p_loss: -4.028456211090088, mean_rew: -7.850563634657166, variance: 4.191150665283203, lamda: 1.9021592140197754

steps: 2774975, episodes: 111000, mean episode reward: -591.4428030224157, agent episode reward: [-197.14760100747188, -197.14760100747188, -197.14760100747188], time: 76.057
steps: 2774975, episodes: 111000, mean episode variance: 39.76273205852509, agent episode variance: [37.48870661926269, 1.2223505373001098, 1.0516749019622802], time: 76.057
Running avgs for agent 0: q_loss: 1579104.75, p_loss: 146.1392059326172, mean_rew: -7.845115033161503, variance: 149.95482647705077, lamda: 3.7339062690734863
Running avgs for agent 1: q_loss: 8.804306983947754, p_loss: -4.026718616485596, mean_rew: -7.851167560166992, variance: 4.889402389526367, lamda: 1.6381285190582275
Running avgs for agent 2: q_loss: 8.236083030700684, p_loss: -4.022913455963135, mean_rew: -7.853835376609649, variance: 4.206699848175049, lamda: 1.9023547172546387

steps: 2799975, episodes: 112000, mean episode reward: -591.202654636302, agent episode reward: [-197.067551545434, -197.067551545434, -197.067551545434], time: 73.367
steps: 2799975, episodes: 112000, mean episode variance: 39.22358315873146, agent episode variance: [36.948949478149416, 1.221755250453949, 1.0528784301280976], time: 73.368
Running avgs for agent 0: q_loss: 1577678.125, p_loss: 146.25839233398438, mean_rew: -7.851059177066374, variance: 147.79579791259766, lamda: 3.758880376815796
Running avgs for agent 1: q_loss: 8.94619369506836, p_loss: -4.031540393829346, mean_rew: -7.854592290073109, variance: 4.887021064758301, lamda: 1.63822340965271
Running avgs for agent 2: q_loss: 8.210603713989258, p_loss: -4.032600402832031, mean_rew: -7.85315154594537, variance: 4.211513519287109, lamda: 1.902357578277588

steps: 2824975, episodes: 113000, mean episode reward: -602.4785189321382, agent episode reward: [-200.82617297737943, -200.82617297737943, -200.82617297737943], time: 74.053
steps: 2824975, episodes: 113000, mean episode variance: 39.01068581819534, agent episode variance: [36.72853257751465, 1.226544020175934, 1.0556092205047607], time: 74.054
Running avgs for agent 0: q_loss: 1670098.75, p_loss: 146.51759338378906, mean_rew: -7.859699015843281, variance: 146.9141303100586, lamda: 3.7838547229766846
Running avgs for agent 1: q_loss: 8.897624015808105, p_loss: -4.025524139404297, mean_rew: -7.857168371405061, variance: 4.90617561340332, lamda: 1.6383293867111206
Running avgs for agent 2: q_loss: 8.103555679321289, p_loss: -4.022182464599609, mean_rew: -7.863554092500228, variance: 4.222436428070068, lamda: 1.9033042192459106

steps: 2849975, episodes: 114000, mean episode reward: -599.5068087986102, agent episode reward: [-199.83560293287005, -199.83560293287005, -199.83560293287005], time: 73.913
steps: 2849975, episodes: 114000, mean episode variance: 39.70464577579498, agent episode variance: [37.42779313659668, 1.2217077062129975, 1.0551449329853058], time: 73.913
Running avgs for agent 0: q_loss: 1627851.125, p_loss: 146.73094177246094, mean_rew: -7.8667907963021015, variance: 149.7111725463867, lamda: 3.8088293075561523
Running avgs for agent 1: q_loss: 9.020523071289062, p_loss: -4.0344133377075195, mean_rew: -7.866353599974655, variance: 4.8868303298950195, lamda: 1.638329267501831
Running avgs for agent 2: q_loss: 8.387144088745117, p_loss: -4.030508041381836, mean_rew: -7.867857292478607, variance: 4.220579624176025, lamda: 1.904998779296875

steps: 2874975, episodes: 115000, mean episode reward: -592.4027503747665, agent episode reward: [-197.4675834582555, -197.4675834582555, -197.4675834582555], time: 73.356
steps: 2874975, episodes: 115000, mean episode variance: 39.63229412317276, agent episode variance: [37.35946325683594, 1.2169290387630463, 1.0559018275737762], time: 73.357
Running avgs for agent 0: q_loss: 1656113.875, p_loss: 146.71481323242188, mean_rew: -7.863889977992223, variance: 149.43785302734375, lamda: 3.833803415298462
Running avgs for agent 1: q_loss: 8.959443092346191, p_loss: -4.02951192855835, mean_rew: -7.865469386533852, variance: 4.867716312408447, lamda: 1.6383293867111206
Running avgs for agent 2: q_loss: 8.336492538452148, p_loss: -4.02816915512085, mean_rew: -7.867640441576654, variance: 4.223607540130615, lamda: 1.9049988985061646

steps: 2899975, episodes: 116000, mean episode reward: -597.4829078066043, agent episode reward: [-199.1609692688681, -199.1609692688681, -199.1609692688681], time: 72.261
steps: 2899975, episodes: 116000, mean episode variance: 40.09741069030762, agent episode variance: [37.81038610839844, 1.2299610030651094, 1.0570635788440705], time: 72.261
Running avgs for agent 0: q_loss: 1709240.75, p_loss: 146.82875061035156, mean_rew: -7.874305492682143, variance: 151.24154443359376, lamda: 3.8587777614593506
Running avgs for agent 1: q_loss: 9.013151168823242, p_loss: -4.030786514282227, mean_rew: -7.8727976030331055, variance: 4.919843673706055, lamda: 1.6383293867111206
Running avgs for agent 2: q_loss: 8.189764976501465, p_loss: -4.027001857757568, mean_rew: -7.870500605424855, variance: 4.2282538414001465, lamda: 1.904998779296875

steps: 2924975, episodes: 117000, mean episode reward: -596.3986969030424, agent episode reward: [-198.79956563434743, -198.79956563434743, -198.79956563434743], time: 69.321
steps: 2924975, episodes: 117000, mean episode variance: 38.81742279267311, agent episode variance: [36.53008837890625, 1.2298773403167724, 1.0574570734500885], time: 69.322
Running avgs for agent 0: q_loss: 1764501.625, p_loss: 147.13352966308594, mean_rew: -7.880928157013702, variance: 146.120353515625, lamda: 3.88375186920166
Running avgs for agent 1: q_loss: 8.956671714782715, p_loss: -4.0336503982543945, mean_rew: -7.875480238868867, variance: 4.919508934020996, lamda: 1.638329267501831
Running avgs for agent 2: q_loss: 8.483173370361328, p_loss: -4.0310282707214355, mean_rew: -7.873527407891767, variance: 4.229828357696533, lamda: 1.9050298929214478

steps: 2949975, episodes: 118000, mean episode reward: -594.5697541111006, agent episode reward: [-198.18991803703352, -198.18991803703352, -198.18991803703352], time: 69.564
steps: 2949975, episodes: 118000, mean episode variance: 39.46991039085388, agent episode variance: [37.205766738891604, 1.2150229516029358, 1.0491207003593446], time: 69.565
Running avgs for agent 0: q_loss: 1754747.25, p_loss: 147.27439880371094, mean_rew: -7.883851061976006, variance: 148.82306695556642, lamda: 3.908726453781128
Running avgs for agent 1: q_loss: 9.085029602050781, p_loss: -4.041911602020264, mean_rew: -7.884791152582614, variance: 4.860091686248779, lamda: 1.6383293867111206
Running avgs for agent 2: q_loss: 8.613687515258789, p_loss: -4.040844440460205, mean_rew: -7.879237433210905, variance: 4.196483135223389, lamda: 1.9055782556533813

steps: 2974975, episodes: 119000, mean episode reward: -594.0366226054877, agent episode reward: [-198.01220753516256, -198.01220753516256, -198.01220753516256], time: 68.898
steps: 2974975, episodes: 119000, mean episode variance: 40.576136785030364, agent episode variance: [38.290999282836914, 1.2245154571533203, 1.0606220450401307], time: 68.899
Running avgs for agent 0: q_loss: 1780547.625, p_loss: 147.32493591308594, mean_rew: -7.87938586612451, variance: 153.16399713134766, lamda: 3.9337007999420166
Running avgs for agent 1: q_loss: 8.986016273498535, p_loss: -4.035109043121338, mean_rew: -7.882441913226868, variance: 4.898061752319336, lamda: 1.6383293867111206
Running avgs for agent 2: q_loss: 8.368490219116211, p_loss: -4.034908771514893, mean_rew: -7.883516914963853, variance: 4.242488384246826, lamda: 1.905652642250061

steps: 2999975, episodes: 120000, mean episode reward: -595.7864282974974, agent episode reward: [-198.5954760991658, -198.5954760991658, -198.5954760991658], time: 68.845
steps: 2999975, episodes: 120000, mean episode variance: 39.29862965607643, agent episode variance: [37.01480595397949, 1.2270931022167206, 1.0567305998802186], time: 68.846
Running avgs for agent 0: q_loss: 1799128.875, p_loss: 147.3773956298828, mean_rew: -7.890559984599653, variance: 148.05922381591796, lamda: 3.9586753845214844
Running avgs for agent 1: q_loss: 8.955263137817383, p_loss: -4.040759086608887, mean_rew: -7.892346823674499, variance: 4.908372402191162, lamda: 1.638329267501831
Running avgs for agent 2: q_loss: 8.438937187194824, p_loss: -4.042304992675781, mean_rew: -7.892758516322967, variance: 4.226922988891602, lamda: 1.9056689739227295

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -596.4899361029686, agent episode reward: [-198.82997870098956, -198.82997870098956, -198.82997870098956], time: 41.883
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 41.883
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -592.827086287873, agent episode reward: [-197.60902876262438, -197.60902876262438, -197.60902876262438], time: 53.868
steps: 49975, episodes: 2000, mean episode variance: 99.26167827606201, agent episode variance: [96.71462774658202, 1.3748023223876953, 1.172248207092285], time: 53.868
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.949983972255023, variance: 396.37139892578125, lamda: 3.9712133407592773
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.949036421214343, variance: 5.634436130523682, lamda: 1.6383382081985474
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.942236099512634, variance: 4.804296493530273, lamda: 1.9056897163391113

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567761475.1092150: line 9: --exp_var_alpha: command not found
