WARNING: Logging before flag parsing goes to stderr.
W0903 15:58:07.069396 4686259648 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0903 15:58:07.069652 4686259648 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-09-03 15:58:07.070046: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0903 15:58:07.072463 4686259648 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0903 15:58:07.074278 4686259648 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0903 15:58:07.074416 4686259648 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0903 15:58:07.074501 4686259648 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0903 15:58:07.488790 4686259648 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0903 15:58:07.646310 4686259648 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0903 15:58:07.652491 4686259648 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0903 15:58:08.149990 4686259648 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation True
adversary agent:  -0.01674283668398857
good agent:  29.040218353271484
good agent:  29.040218353271484
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 29970, episodes: 1000, mean episode reward: -31.364854827853947, agent episode reward: [-45.107654560289596, 6.8713998662178275, 6.8713998662178275], time: 26.737
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 26.737
steps: 29970, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0], time: 26.738
Running avgs for agent 0: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 59970, episodes: 2000, mean episode reward: -76.49671054930047, agent episode reward: [-62.613200756846, -6.9417548962272395, -6.9417548962272395], time: 50.115
steps: 59970, episodes: 2000, mean episode variance: 0.8737839053198695, agent episode variance: [0.873770360827446, 1.3544492423534394e-05, 0.0], time: 50.116
steps: 59970, episodes: 2000, mean episode cvar: 15.306785435676575, agent episode cvar: [0.23541844993829728, 7.486521748006344, 7.584845237731933], time: 50.116
Running avgs for agent 0: q_loss: 0.6781334280967712, u_loss: 7477.94873046875, p_loss: 4.02580451965332, mean_rew: -1.8625869405348408, variance: 2.9923642494090616, cvar: 0.8062275648117065, v: 0.8062275648117065, mean_q: -4.13593864440918, std_q: 3.702709436416626, lamda: 0.99937504529953
Running avgs for agent 1: q_loss: 49.52925109863281, u_loss: 2.81595778465271, p_loss: -3.679630994796753, mean_rew: 0.14993403368816907, variance: 4.638524802580272e-05, cvar: 25.63877296447754, v: 1.167686104774475, mean_q: 3.5838873386383057, std_q: 1.0128268003463745, lamda: 1.0038747787475586
Running avgs for agent 2: q_loss: 35.392669677734375, u_loss: 2.811939001083374, p_loss: -3.7218921184539795, mean_rew: 0.14564401410734554, variance: 0.0, cvar: 25.97549819946289, v: 1.1917061805725098, mean_q: 3.659548044204712, std_q: 1.0714274644851685, lamda: 1.0043054819107056

steps: 89970, episodes: 3000, mean episode reward: -62.542287447401556, agent episode reward: [-19.31788018150346, -21.612203632949043, -21.612203632949043], time: 48.928
steps: 89970, episodes: 3000, mean episode variance: 1.1567828965187072, agent episode variance: [1.1567828965187072, 0.0, 0.0], time: 48.929
steps: 89970, episodes: 3000, mean episode cvar: 17.20877043277025, agent episode cvar: [0.12455489283800125, 8.550404529571534, 8.533811010360719], time: 48.929
Running avgs for agent 0: q_loss: 0.17396359145641327, u_loss: 34861.37890625, p_loss: 7.520757675170898, mean_rew: -1.5989129384568777, variance: 3.855942988395691, cvar: 0.4151829779148102, v: 0.4151829779148102, mean_q: -7.818273067474365, std_q: 8.665975570678711, lamda: 0.9960957765579224
Running avgs for agent 1: q_loss: 1.6198090314865112, u_loss: 7.773711681365967, p_loss: -4.247152328491211, mean_rew: -0.12512829827025687, variance: 0.0, cvar: 28.5013484954834, v: 1.5590656995773315, mean_q: 4.189592361450195, std_q: 1.8656774759292603, lamda: 1.013139247894287
Running avgs for agent 2: q_loss: 1.5686885118484497, u_loss: 7.665828227996826, p_loss: -4.319330215454102, mean_rew: -0.12586078634210301, variance: 0.0, cvar: 28.4460391998291, v: 1.5839756727218628, mean_q: 4.232690334320068, std_q: 1.7933933734893799, lamda: 1.0152274370193481

steps: 119970, episodes: 4000, mean episode reward: -70.64208790817791, agent episode reward: [-15.718439797446546, -27.461824055365682, -27.461824055365682], time: 48.211
steps: 119970, episodes: 4000, mean episode variance: 3.4038843104839325, agent episode variance: [3.4038843104839325, 0.0, 0.0], time: 48.211
steps: 119970, episodes: 4000, mean episode cvar: 17.15741034446703, agent episode cvar: [0.012292033538687974, 8.567276002883911, 8.577842308044433], time: 48.212
Running avgs for agent 0: q_loss: 0.2128087878227234, u_loss: 69043.1484375, p_loss: 8.698297500610352, mean_rew: -1.2886647792894628, variance: 11.346281034946442, cvar: 0.04097343981266022, v: 0.04097343981266022, mean_q: -9.142387390136719, std_q: 11.914711952209473, lamda: 0.9946975708007812
Running avgs for agent 1: q_loss: 3.028926134109497, u_loss: 15.737951278686523, p_loss: -4.168663024902344, mean_rew: -0.326089027792897, variance: 0.0, cvar: 28.557588577270508, v: 1.9337064027786255, mean_q: 4.11803674697876, std_q: 3.1711056232452393, lamda: 1.0354820489883423
Running avgs for agent 2: q_loss: 2.947171688079834, u_loss: 15.115968704223633, p_loss: -4.375916481018066, mean_rew: -0.3279501406928766, variance: 0.0, cvar: 28.592805862426758, v: 1.9585859775543213, mean_q: 4.2842326164245605, std_q: 2.9200594425201416, lamda: 1.0362955331802368

steps: 149970, episodes: 5000, mean episode reward: -75.29220406146332, agent episode reward: [-14.734156840605609, -30.279023610428855, -30.279023610428855], time: 48.981
steps: 149970, episodes: 5000, mean episode variance: 8.33714650440216, agent episode variance: [8.33714650440216, 0.0, 0.0], time: 48.982
steps: 149970, episodes: 5000, mean episode cvar: 17.158908472701906, agent episode cvar: [-0.09353469021618366, 8.617167964935303, 8.635275197982788], time: 48.982
Running avgs for agent 0: q_loss: 0.2503073513507843, u_loss: 89660.3203125, p_loss: 8.534576416015625, mean_rew: -1.1160940979778078, variance: 27.7904883480072, cvar: -0.3117823302745819, v: -0.3154936134815216, mean_q: -9.0210542678833, std_q: 13.625725746154785, lamda: 0.9947433471679688
Running avgs for agent 1: q_loss: 2.7984423637390137, u_loss: 28.632448196411133, p_loss: -4.042518615722656, mean_rew: -0.4750091512234648, variance: 0.0, cvar: 28.723894119262695, v: 2.290395498275757, mean_q: 3.893784523010254, std_q: 4.270928859710693, lamda: 1.0611681938171387
Running avgs for agent 2: q_loss: 3.8616626262664795, u_loss: 24.51207733154297, p_loss: -4.277308940887451, mean_rew: -0.4718399705940265, variance: 0.0, cvar: 28.784252166748047, v: 2.3152594566345215, mean_q: 4.1993088722229, std_q: 3.8763070106506348, lamda: 1.0620918273925781

steps: 179970, episodes: 6000, mean episode reward: -62.324103393868135, agent episode reward: [-15.271107430344584, -23.526497981761775, -23.526497981761775], time: 48.833
steps: 179970, episodes: 6000, mean episode variance: 11.20390776014328, agent episode variance: [11.20390776014328, 0.0, 0.0], time: 48.834
steps: 179970, episodes: 6000, mean episode cvar: 16.963968318134548, agent episode cvar: [-0.1320159271657467, 8.57171858215332, 8.524265663146974], time: 48.834
Running avgs for agent 0: q_loss: 0.2775281071662903, u_loss: 124187.53125, p_loss: 7.617397308349609, mean_rew: -1.0039637096318814, variance: 37.3463592004776, cvar: -0.4400531053543091, v: -0.6544685959815979, mean_q: -8.076641082763672, std_q: 14.6887845993042, lamda: 0.9949277639389038
Running avgs for agent 1: q_loss: 2.81056809425354, u_loss: 43.2637939453125, p_loss: -4.058106422424316, mean_rew: -0.5568073635955254, variance: 0.0, cvar: 28.57239532470703, v: 2.6332573890686035, mean_q: 3.8770687580108643, std_q: 4.988776683807373, lamda: 1.0848095417022705
Running avgs for agent 2: q_loss: 3.5138301849365234, u_loss: 34.52091979980469, p_loss: -4.109175205230713, mean_rew: -0.5548295717573869, variance: 0.0, cvar: 28.41421890258789, v: 2.658111810684204, mean_q: 4.017468452453613, std_q: 4.813119411468506, lamda: 1.088739275932312

steps: 209970, episodes: 7000, mean episode reward: -72.21688272432947, agent episode reward: [-14.924235226003786, -28.64632374916284, -28.64632374916284], time: 49.015
steps: 209970, episodes: 7000, mean episode variance: 12.525288862228393, agent episode variance: [12.525288862228393, 0.0, 0.0], time: 49.015
steps: 209970, episodes: 7000, mean episode cvar: 17.131500401586294, agent episode cvar: [-0.07141168680787087, 8.618234867095948, 8.584677221298218], time: 49.015
Running avgs for agent 0: q_loss: 0.37411612272262573, u_loss: 116876.25, p_loss: 7.168471813201904, mean_rew: -0.9265461216398118, variance: 41.75096287409465, cvar: -0.23803897202014923, v: -0.8636360764503479, mean_q: -7.601804256439209, std_q: 15.187481880187988, lamda: 1.0054410696029663
Running avgs for agent 1: q_loss: 3.547637462615967, u_loss: 62.70148468017578, p_loss: -3.912815809249878, mean_rew: -0.597982894447367, variance: 0.0, cvar: 28.72745132446289, v: 2.965526819229126, mean_q: 3.712735891342163, std_q: 5.650838851928711, lamda: 1.1097711324691772
Running avgs for agent 2: q_loss: 3.4033005237579346, u_loss: 53.08726501464844, p_loss: -3.7822213172912598, mean_rew: -0.5974084342228954, variance: 0.0, cvar: 28.61559295654297, v: 2.990375280380249, mean_q: 3.5932347774505615, std_q: 5.779436111450195, lamda: 1.113002061843872

steps: 239970, episodes: 8000, mean episode reward: -65.6205087903313, agent episode reward: [-14.82440973160405, -25.398049529363625, -25.398049529363625], time: 48.331
steps: 239970, episodes: 8000, mean episode variance: 14.82461985874176, agent episode variance: [14.82461985874176, 0.0, 0.0], time: 48.331
steps: 239970, episodes: 8000, mean episode cvar: 17.150597825825216, agent episode cvar: [-0.04803908175230026, 8.603992938995361, 8.594643968582153], time: 48.331
Running avgs for agent 0: q_loss: 0.37030792236328125, u_loss: 113969.15625, p_loss: 7.176539897918701, mean_rew: -0.8674859701957215, variance: 49.4153995291392, cvar: -0.16013026237487793, v: -0.931749701499939, mean_q: -7.574603080749512, std_q: 15.284337997436523, lamda: 1.0207300186157227
Running avgs for agent 1: q_loss: 4.257083415985107, u_loss: 74.01014709472656, p_loss: -4.111832141876221, mean_rew: -0.636992512824418, variance: 0.0, cvar: 28.679973602294922, v: 3.2897322177886963, mean_q: 3.868309736251831, std_q: 5.988976001739502, lamda: 1.1365984678268433
Running avgs for agent 2: q_loss: 4.101080894470215, u_loss: 63.849822998046875, p_loss: -3.9529662132263184, mean_rew: -0.6464082073708985, variance: 0.0, cvar: 28.648815155029297, v: 3.314577579498291, mean_q: 3.7022058963775635, std_q: 6.194448947906494, lamda: 1.1383379697799683

steps: 269970, episodes: 9000, mean episode reward: -54.40728095644155, agent episode reward: [-15.090010967451024, -19.658634994495262, -19.658634994495262], time: 49.668
steps: 269970, episodes: 9000, mean episode variance: 16.669079035758973, agent episode variance: [16.669079035758973, 0.0, 0.0], time: 49.668
steps: 269970, episodes: 9000, mean episode cvar: 17.09549995008111, agent episode cvar: [-0.04293008455634117, 8.549129224777221, 8.58930080986023], time: 49.669
Running avgs for agent 0: q_loss: 0.4038403034210205, u_loss: 124633.8828125, p_loss: 7.244328498840332, mean_rew: -0.8243153596162841, variance: 55.56359678586324, cvar: -0.14310027658939362, v: -0.9707936644554138, mean_q: -7.617171287536621, std_q: 15.461116790771484, lamda: 1.0357104539871216
Running avgs for agent 1: q_loss: 5.074127674102783, u_loss: 80.73696899414062, p_loss: -4.3642048835754395, mean_rew: -0.6628839377200626, variance: 0.0, cvar: 28.497098922729492, v: 3.607837438583374, mean_q: 4.09196138381958, std_q: 6.18513298034668, lamda: 1.1624257564544678
Running avgs for agent 2: q_loss: 4.549927711486816, u_loss: 73.49483489990234, p_loss: -4.276413917541504, mean_rew: -0.6592384435818321, variance: 0.0, cvar: 28.63100242614746, v: 3.632678985595703, mean_q: 4.022982597351074, std_q: 6.37898588180542, lamda: 1.1625169515609741

steps: 299970, episodes: 10000, mean episode reward: -31.054921087655938, agent episode reward: [-14.697151810716615, -8.178884638469661, -8.178884638469661], time: 49.773
steps: 299970, episodes: 10000, mean episode variance: 16.985957817077637, agent episode variance: [16.985957817077637, 0.0, 0.0], time: 49.773
steps: 299970, episodes: 10000, mean episode cvar: 17.062814557909967, agent episode cvar: [-0.04639567196369171, 8.55090874671936, 8.558301483154297], time: 49.774
Running avgs for agent 0: q_loss: 0.3285316824913025, u_loss: 122921.5078125, p_loss: 7.185629844665527, mean_rew: -0.7906731550295356, variance: 56.61985939025879, cvar: -0.15465223789215088, v: -1.0009276866912842, mean_q: -7.523446559906006, std_q: 15.340279579162598, lamda: 1.0510287284851074
Running avgs for agent 1: q_loss: 4.7060227394104, u_loss: 85.90471649169922, p_loss: -4.310584545135498, mean_rew: -0.636526343213633, variance: 0.0, cvar: 28.503026962280273, v: 3.9213485717773438, mean_q: 4.035587310791016, std_q: 6.48604154586792, lamda: 1.1867445707321167
Running avgs for agent 2: q_loss: 4.583064556121826, u_loss: 79.08586883544922, p_loss: -4.307493686676025, mean_rew: -0.6347582412662103, variance: 0.0, cvar: 28.52766990661621, v: 3.946188449859619, mean_q: 4.041710376739502, std_q: 6.644344806671143, lamda: 1.1875008344650269

steps: 329970, episodes: 11000, mean episode reward: -22.52702721115289, agent episode reward: [-13.867843325844847, -4.329591942654022, -4.329591942654022], time: 48.747
steps: 329970, episodes: 11000, mean episode variance: 19.53632026386261, agent episode variance: [19.53632026386261, 0.0, 0.0], time: 48.747
steps: 329970, episodes: 11000, mean episode cvar: 17.121431714236735, agent episode cvar: [-0.04613864976167679, 8.549789920806885, 8.617780443191528], time: 48.747
Running avgs for agent 0: q_loss: 0.3291986584663391, u_loss: 99664.3046875, p_loss: 7.075416088104248, mean_rew: -0.7643819648420379, variance: 65.12106754620869, cvar: -0.1537954956293106, v: -1.0007330179214478, mean_q: -7.392897129058838, std_q: 15.254329681396484, lamda: 1.0639662742614746
Running avgs for agent 1: q_loss: 5.436707973480225, u_loss: 94.62761688232422, p_loss: -4.217203140258789, mean_rew: -0.5918325748282954, variance: 0.0, cvar: 28.499296188354492, v: 4.2314133644104, mean_q: 3.9701476097106934, std_q: 6.866751194000244, lamda: 1.2114536762237549
Running avgs for agent 2: q_loss: 5.696922779083252, u_loss: 88.96585845947266, p_loss: -4.3837480545043945, mean_rew: -0.5946282500117606, variance: 0.0, cvar: 28.725934982299805, v: 4.256251811981201, mean_q: 4.080716609954834, std_q: 7.00087308883667, lamda: 1.2140178680419922

steps: 359970, episodes: 12000, mean episode reward: -20.636394408997702, agent episode reward: [-14.622952671154964, -3.0067208689213683, -3.0067208689213683], time: 49.18
steps: 359970, episodes: 12000, mean episode variance: 19.49378789615631, agent episode variance: [19.49378789615631, 0.0, 0.0], time: 49.18
steps: 359970, episodes: 12000, mean episode cvar: 17.155236954033374, agent episode cvar: [-0.032811723411083224, 8.600200160980224, 8.587848516464234], time: 49.181
Running avgs for agent 0: q_loss: 0.31173598766326904, u_loss: 135522.84375, p_loss: 6.896936893463135, mean_rew: -0.7362799230578229, variance: 64.9792929871877, cvar: -0.10937242209911346, v: -0.9573584794998169, mean_q: -7.192351818084717, std_q: 15.146251678466797, lamda: 1.0775734186172485
Running avgs for agent 1: q_loss: 5.147401332855225, u_loss: 100.62181091308594, p_loss: -4.3406476974487305, mean_rew: -0.5511056676174431, variance: 0.0, cvar: 28.667333602905273, v: 4.538898944854736, mean_q: 4.082036018371582, std_q: 7.16488790512085, lamda: 1.2370238304138184
Running avgs for agent 2: q_loss: 4.477973461151123, u_loss: 93.16679382324219, p_loss: -4.513949871063232, mean_rew: -0.5519164810925584, variance: 0.0, cvar: 28.626161575317383, v: 4.563736915588379, mean_q: 4.199409008026123, std_q: 7.260392189025879, lamda: 1.2384214401245117

steps: 389970, episodes: 13000, mean episode reward: -19.99758060389723, agent episode reward: [-14.223753933014333, -2.886913335441447, -2.886913335441447], time: 49.175
steps: 389970, episodes: 13000, mean episode variance: 17.865577597618103, agent episode variance: [17.865577597618103, 0.0, 0.0], time: 49.176
steps: 389970, episodes: 13000, mean episode cvar: 17.138890852093695, agent episode cvar: [-0.02320114505290985, 8.601976140975951, 8.560115856170654], time: 49.176
Running avgs for agent 0: q_loss: 0.29188811779022217, u_loss: 108529.0234375, p_loss: 6.808426856994629, mean_rew: -0.7169475023887787, variance: 59.551925325393675, cvar: -0.07733715325593948, v: -0.8942694067955017, mean_q: -7.086277484893799, std_q: 15.143817901611328, lamda: 1.090470790863037
Running avgs for agent 1: q_loss: 4.768915176391602, u_loss: 105.88710021972656, p_loss: -4.455060958862305, mean_rew: -0.5184113168362541, variance: 0.0, cvar: 28.673255920410156, v: 4.844460964202881, mean_q: 4.220331192016602, std_q: 7.442578315734863, lamda: 1.2632079124450684
Running avgs for agent 2: q_loss: 6.216770648956299, u_loss: 100.72989654541016, p_loss: -4.677845001220703, mean_rew: -0.5172297192820622, variance: 0.0, cvar: 28.53371810913086, v: 4.869298458099365, mean_q: 4.343985080718994, std_q: 7.548235893249512, lamda: 1.2646058797836304

steps: 419970, episodes: 14000, mean episode reward: -18.85735772726668, agent episode reward: [-14.730874157546378, -2.0632417848601516, -2.0632417848601516], time: 49.973
steps: 419970, episodes: 14000, mean episode variance: 17.524933344841003, agent episode variance: [17.524933344841003, 0.0, 0.0], time: 49.973
steps: 419970, episodes: 14000, mean episode cvar: 17.157789281904698, agent episode cvar: [-0.017835718095302582, 8.56846381187439, 8.60716118812561], time: 49.973
Running avgs for agent 0: q_loss: 0.2779850363731384, u_loss: 100705.5625, p_loss: 6.70060396194458, mean_rew: -0.6973460696794, variance: 58.41644448280334, cvar: -0.05945238843560219, v: -0.8528846502304077, mean_q: -6.957488536834717, std_q: 14.331517219543457, lamda: 1.101445198059082
Running avgs for agent 1: q_loss: 4.386232852935791, u_loss: 111.87487030029297, p_loss: -4.570608615875244, mean_rew: -0.4837817869233702, variance: 0.0, cvar: 28.561546325683594, v: 5.1485915184021, mean_q: 4.366562843322754, std_q: 7.723809242248535, lamda: 1.2881762981414795
Running avgs for agent 2: q_loss: 4.572658061981201, u_loss: 106.9399185180664, p_loss: -4.885913372039795, mean_rew: -0.486532935027823, variance: 0.0, cvar: 28.690540313720703, v: 5.173427581787109, mean_q: 4.545053482055664, std_q: 7.794703960418701, lamda: 1.2897870540618896

steps: 449970, episodes: 15000, mean episode reward: -21.85658181913774, agent episode reward: [-13.768218937888173, -4.044181440624783, -4.044181440624783], time: 48.637
steps: 449970, episodes: 15000, mean episode variance: 16.871072010040283, agent episode variance: [16.871072010040283, 0.0, 0.0], time: 48.638
steps: 449970, episodes: 15000, mean episode cvar: 17.159250493526457, agent episode cvar: [-0.017411295413970948, 8.626086038589477, 8.550575750350951], time: 48.638
Running avgs for agent 0: q_loss: 0.2992762327194214, u_loss: 90841.1328125, p_loss: 6.601594924926758, mean_rew: -0.679059072213219, variance: 56.23690670013428, cvar: -0.0580376498401165, v: -0.8116217255592346, mean_q: -6.844277381896973, std_q: 13.753974914550781, lamda: 1.113063097000122
Running avgs for agent 1: q_loss: 5.469489574432373, u_loss: 122.26953125, p_loss: -4.857278823852539, mean_rew: -0.4572024527496217, variance: 0.0, cvar: 28.753620147705078, v: 5.451655864715576, mean_q: 4.647436618804932, std_q: 7.883299350738525, lamda: 1.3132762908935547
Running avgs for agent 2: q_loss: 5.36409854888916, u_loss: 113.5999755859375, p_loss: -5.089663505554199, mean_rew: -0.46035228875111256, variance: 0.0, cvar: 28.501920700073242, v: 5.4764909744262695, mean_q: 4.740144729614258, std_q: 7.957341194152832, lamda: 1.3137441873550415

steps: 479970, episodes: 16000, mean episode reward: -20.517298819274707, agent episode reward: [-13.312406113936348, -3.602446352669179, -3.602446352669179], time: 49.243
steps: 479970, episodes: 16000, mean episode variance: 17.805934771537782, agent episode variance: [17.805934771537782, 0.0, 0.0], time: 49.244
steps: 479970, episodes: 16000, mean episode cvar: 17.17813114976883, agent episode cvar: [-0.015343343257904053, 8.588219779968261, 8.605254713058471], time: 49.244
Running avgs for agent 0: q_loss: 0.2814187705516815, u_loss: 111876.9296875, p_loss: 6.600294589996338, mean_rew: -0.6701503803543641, variance: 59.35311590512594, cvar: -0.05114448070526123, v: -0.7809330224990845, mean_q: -6.836764335632324, std_q: 14.144586563110352, lamda: 1.1274255514144897
Running avgs for agent 1: q_loss: 4.320500373840332, u_loss: 126.92520904541016, p_loss: -5.13567590713501, mean_rew: -0.4349904443156254, variance: 0.0, cvar: 28.627403259277344, v: 5.753929138183594, mean_q: 4.930570602416992, std_q: 7.982159614562988, lamda: 1.3378382921218872
Running avgs for agent 2: q_loss: 7.165681838989258, u_loss: 121.93228912353516, p_loss: -5.291175842285156, mean_rew: -0.4369638327357809, variance: 0.0, cvar: 28.684179306030273, v: 5.778763294219971, mean_q: 4.935421943664551, std_q: 8.189910888671875, lamda: 1.339381456375122

steps: 509970, episodes: 17000, mean episode reward: -28.27821244586231, agent episode reward: [-14.104832296309334, -7.086690074776486, -7.086690074776486], time: 49.31
steps: 509970, episodes: 17000, mean episode variance: 21.103533757686616, agent episode variance: [21.103533757686616, 0.0, 0.0], time: 49.31
steps: 509970, episodes: 17000, mean episode cvar: 17.166167744547128, agent episode cvar: [-0.018191088765859603, 8.596467590332031, 8.587891242980957], time: 49.311
Running avgs for agent 0: q_loss: 0.26833581924438477, u_loss: 72616.2109375, p_loss: 6.500557899475098, mean_rew: -0.6534104748031203, variance: 70.34511252562206, cvar: -0.060636963695287704, v: -0.7471604347229004, mean_q: -6.730424880981445, std_q: 13.378047943115234, lamda: 1.1397866010665894
Running avgs for agent 1: q_loss: 4.809269905090332, u_loss: 133.8817596435547, p_loss: -5.463561058044434, mean_rew: -0.4190661589357337, variance: 0.0, cvar: 28.654890060424805, v: 6.0556135177612305, mean_q: 5.255739688873291, std_q: 8.038763046264648, lamda: 1.3630164861679077
Running avgs for agent 2: q_loss: 4.770116329193115, u_loss: 125.80708312988281, p_loss: -5.510119915008545, mean_rew: -0.4202002390507491, variance: 0.0, cvar: 28.62630271911621, v: 6.080448627471924, mean_q: 5.154750347137451, std_q: 8.384081840515137, lamda: 1.3653596639633179

steps: 539970, episodes: 18000, mean episode reward: -29.11482448588056, agent episode reward: [-13.861460079188321, -7.626682203346122, -7.626682203346122], time: 49.996
steps: 539970, episodes: 18000, mean episode variance: 21.203551042556764, agent episode variance: [21.203551042556764, 0.0, 0.0], time: 49.996
steps: 539970, episodes: 18000, mean episode cvar: 17.14109482496977, agent episode cvar: [-0.021280106365680694, 8.592206256866454, 8.570168674468993], time: 49.996
Running avgs for agent 0: q_loss: 0.27607640624046326, u_loss: 99742.9609375, p_loss: 6.511586666107178, mean_rew: -0.6464659731110957, variance: 70.6785034751892, cvar: -0.07093368470668793, v: -0.7344333529472351, mean_q: -6.743910312652588, std_q: 13.884207725524902, lamda: 1.1524220705032349
Running avgs for agent 1: q_loss: 4.619920253753662, u_loss: 134.1769256591797, p_loss: -5.800520896911621, mean_rew: -0.4108256580285833, variance: 0.0, cvar: 28.640689849853516, v: 6.356863021850586, mean_q: 5.597590446472168, std_q: 8.060281753540039, lamda: 1.3885059356689453
Running avgs for agent 2: q_loss: 6.969643592834473, u_loss: 132.1613311767578, p_loss: -5.814591407775879, mean_rew: -0.41033174830326263, variance: 0.0, cvar: 28.56722640991211, v: 6.381697654724121, mean_q: 5.458916187286377, std_q: 8.399572372436523, lamda: 1.3902512788772583

steps: 569970, episodes: 19000, mean episode reward: -28.05765082992546, agent episode reward: [-13.912903485975244, -7.072373671975108, -7.072373671975108], time: 49.407
steps: 569970, episodes: 19000, mean episode variance: 23.13238366127014, agent episode variance: [23.13238366127014, 0.0, 0.0], time: 49.407
steps: 569970, episodes: 19000, mean episode cvar: 17.166529228299858, agent episode cvar: [-0.02217694655060768, 8.607674541473388, 8.581031633377075], time: 49.408
Running avgs for agent 0: q_loss: 0.266843318939209, u_loss: 64423.28125, p_loss: 6.427781105041504, mean_rew: -0.6347773753323032, variance: 77.10794553756713, cvar: -0.07392316311597824, v: -0.7298409938812256, mean_q: -6.654016971588135, std_q: 13.367690086364746, lamda: 1.166008472442627
Running avgs for agent 1: q_loss: 4.52083158493042, u_loss: 134.64984130859375, p_loss: -6.1961541175842285, mean_rew: -0.40131741508921076, variance: 0.0, cvar: 28.692245483398438, v: 6.657788276672363, mean_q: 6.000514507293701, std_q: 7.983767032623291, lamda: 1.4133856296539307
Running avgs for agent 2: q_loss: 5.268073081970215, u_loss: 136.77139282226562, p_loss: -6.153207778930664, mean_rew: -0.4025075531830291, variance: 0.0, cvar: 28.603437423706055, v: 6.682623386383057, mean_q: 5.799986839294434, std_q: 8.413164138793945, lamda: 1.416040062904358

steps: 599970, episodes: 20000, mean episode reward: -20.90198973308454, agent episode reward: [-12.847668669604358, -4.0271605317400905, -4.0271605317400905], time: 49.401
steps: 599970, episodes: 20000, mean episode variance: 24.106889451980592, agent episode variance: [24.106889451980592, 0.0, 0.0], time: 49.401
steps: 599970, episodes: 20000, mean episode cvar: 17.160253123939036, agent episode cvar: [-0.020295742332935334, 8.585819150924683, 8.59472971534729], time: 49.402
Running avgs for agent 0: q_loss: 0.2565934360027313, u_loss: 60204.75390625, p_loss: 6.320487976074219, mean_rew: -0.6233863744802971, variance: 80.35629817326864, cvar: -0.06765247136354446, v: -0.7232468724250793, mean_q: -6.5393595695495605, std_q: 12.59533977508545, lamda: 1.1800612211227417
Running avgs for agent 1: q_loss: 4.351161003112793, u_loss: 136.22251892089844, p_loss: -6.522858142852783, mean_rew: -0.3924013596792104, variance: 0.0, cvar: 28.619394302368164, v: 6.958474159240723, mean_q: 6.337640285491943, std_q: 7.892937183380127, lamda: 1.4383693933486938
Running avgs for agent 2: q_loss: 5.767055034637451, u_loss: 135.6182098388672, p_loss: -6.523982048034668, mean_rew: -0.39313709822524157, variance: 0.0, cvar: 28.649097442626953, v: 6.983308792114258, mean_q: 6.175792694091797, std_q: 8.308321952819824, lamda: 1.4398521184921265

steps: 629970, episodes: 21000, mean episode reward: -16.158939176008783, agent episode reward: [-12.734298744265754, -1.7123202158715143, -1.7123202158715143], time: 50.02
steps: 629970, episodes: 21000, mean episode variance: 24.89373452949524, agent episode variance: [24.89373452949524, 0.0, 0.0], time: 50.021
steps: 629970, episodes: 21000, mean episode cvar: 17.129192403525114, agent episode cvar: [-0.01622176197171211, 8.579350625991822, 8.566063539505006], time: 50.021
Running avgs for agent 0: q_loss: 0.24578702449798584, u_loss: 61420.65234375, p_loss: 6.230594158172607, mean_rew: -0.6119474075094098, variance: 82.97911509831746, cvar: -0.0540725402534008, v: -0.7029430270195007, mean_q: -6.43815279006958, std_q: 12.384209632873535, lamda: 1.192609190940857
Running avgs for agent 1: q_loss: 4.512721538543701, u_loss: 135.6453399658203, p_loss: -6.869805335998535, mean_rew: -0.37699402094228474, variance: 0.0, cvar: 28.59783172607422, v: 7.2589826583862305, mean_q: 6.684810161590576, std_q: 7.753749847412109, lamda: 1.462391972541809
Running avgs for agent 2: q_loss: 4.717176914215088, u_loss: 129.73106384277344, p_loss: -6.926709175109863, mean_rew: -0.3748637203866524, variance: 0.0, cvar: 28.553544998168945, v: 7.283816814422607, mean_q: 6.589195251464844, std_q: 8.052815437316895, lamda: 1.4640177488327026

steps: 659970, episodes: 22000, mean episode reward: -15.258054551971615, agent episode reward: [-12.995218769027803, -1.1314178914719046, -1.1314178914719046], time: 49.363
steps: 659970, episodes: 22000, mean episode variance: 25.615210289001464, agent episode variance: [25.615210289001464, 0.0, 0.0], time: 49.364
steps: 659970, episodes: 22000, mean episode cvar: 17.158311700314282, agent episode cvar: [-0.011235371142625808, 8.585950204849244, 8.583596866607666], time: 49.364
Running avgs for agent 0: q_loss: 0.2510456442832947, u_loss: 56790.2265625, p_loss: 6.204039573669434, mean_rew: -0.6077114965348849, variance: 85.38403429667154, cvar: -0.037451233714818954, v: -0.6710578203201294, mean_q: -6.413827419281006, std_q: 12.654494285583496, lamda: 1.2064462900161743
Running avgs for agent 1: q_loss: 4.067576885223389, u_loss: 130.63619995117188, p_loss: -7.2289910316467285, mean_rew: -0.3623183464654375, variance: 0.0, cvar: 28.619836807250977, v: 7.559360980987549, mean_q: 7.0518927574157715, std_q: 7.614400863647461, lamda: 1.4865598678588867
Running avgs for agent 2: q_loss: 4.599364280700684, u_loss: 128.73716735839844, p_loss: -7.324257850646973, mean_rew: -0.3602377699154226, variance: 0.0, cvar: 28.61199188232422, v: 7.584196090698242, mean_q: 6.989589214324951, std_q: 7.875654220581055, lamda: 1.4875435829162598

steps: 689970, episodes: 23000, mean episode reward: -14.29173757231257, agent episode reward: [-13.231509174306675, -0.5301141990029473, -0.5301141990029473], time: 48.969
steps: 689970, episodes: 23000, mean episode variance: 24.92155213165283, agent episode variance: [24.92155213165283, 0.0, 0.0], time: 48.97
steps: 689970, episodes: 23000, mean episode cvar: 17.126570803165436, agent episode cvar: [-0.009529965877532959, 8.598215106964112, 8.537885662078857], time: 48.97
Running avgs for agent 0: q_loss: 0.25800153613090515, u_loss: 79116.84375, p_loss: 6.119309902191162, mean_rew: -0.5984366772183182, variance: 83.07184043884277, cvar: -0.031766556203365326, v: -0.6367942094802856, mean_q: -6.321558475494385, std_q: 12.588881492614746, lamda: 1.2194647789001465
Running avgs for agent 1: q_loss: 4.950499057769775, u_loss: 128.2241973876953, p_loss: -7.6483659744262695, mean_rew: -0.3436978033566064, variance: 0.0, cvar: 28.660717010498047, v: 7.859635353088379, mean_q: 7.478147029876709, std_q: 7.33608865737915, lamda: 1.510923147201538
Running avgs for agent 2: q_loss: 4.907224655151367, u_loss: 126.7320327758789, p_loss: -7.734550952911377, mean_rew: -0.3454821346648315, variance: 0.0, cvar: 28.459619522094727, v: 7.884469509124756, mean_q: 7.40576171875, std_q: 7.562950134277344, lamda: 1.5110435485839844

steps: 719970, episodes: 24000, mean episode reward: -13.777247777389963, agent episode reward: [-12.951387604781905, -0.4129300863040297, -0.4129300863040297], time: 49.942
steps: 719970, episodes: 24000, mean episode variance: 25.100669973373414, agent episode variance: [25.100669973373414, 0.0, 0.0], time: 49.943
steps: 719970, episodes: 24000, mean episode cvar: 17.10601578065753, agent episode cvar: [-0.009630369931459427, 8.5496533203125, 8.565992830276489], time: 49.943
Running avgs for agent 0: q_loss: 0.24630427360534668, u_loss: 54704.171875, p_loss: 6.008383750915527, mean_rew: -0.5896406327069005, variance: 83.66889991124471, cvar: -0.03210122883319855, v: -0.5955003499984741, mean_q: -6.199862003326416, std_q: 12.122980117797852, lamda: 1.2360246181488037
Running avgs for agent 1: q_loss: 5.442902088165283, u_loss: 124.5781021118164, p_loss: -8.00500202178955, mean_rew: -0.3334918955420659, variance: 0.0, cvar: 28.49884033203125, v: 8.159823417663574, mean_q: 7.831461429595947, std_q: 7.078009605407715, lamda: 1.5357738733291626
Running avgs for agent 2: q_loss: 6.209825038909912, u_loss: 123.17744445800781, p_loss: -8.09801959991455, mean_rew: -0.33449060869627517, variance: 0.0, cvar: 28.55331039428711, v: 8.184646606445312, mean_q: 7.7801361083984375, std_q: 7.366415023803711, lamda: 1.5361379384994507

steps: 749970, episodes: 25000, mean episode reward: -15.10828289578974, agent episode reward: [-13.198889690668315, -0.9546966025607132, -0.9546966025607132], time: 49.422
steps: 749970, episodes: 25000, mean episode variance: 25.515187485694884, agent episode variance: [25.515187485694884, 0.0, 0.0], time: 49.422
steps: 749970, episodes: 25000, mean episode cvar: 17.176357334583997, agent episode cvar: [-0.015308419734239579, 8.608771213531494, 8.582894540786743], time: 49.423
Running avgs for agent 0: q_loss: 0.23762238025665283, u_loss: 57286.1484375, p_loss: 5.951536178588867, mean_rew: -0.5855686017948158, variance: 85.05062495231628, cvar: -0.051028069108724594, v: -0.5744260549545288, mean_q: -6.141552925109863, std_q: 12.125529289245605, lamda: 1.2490750551223755
Running avgs for agent 1: q_loss: 4.898891448974609, u_loss: 119.41625213623047, p_loss: -8.426365852355957, mean_rew: -0.3213180764178919, variance: 0.0, cvar: 28.695905685424805, v: 8.459944725036621, mean_q: 8.254314422607422, std_q: 6.81520938873291, lamda: 1.5599132776260376
Running avgs for agent 2: q_loss: 4.6312174797058105, u_loss: 119.58731842041016, p_loss: -8.494298934936523, mean_rew: -0.3196937923662319, variance: 0.0, cvar: 28.609647750854492, v: 8.48476791381836, mean_q: 8.186590194702148, std_q: 7.098534107208252, lamda: 1.5604201555252075

steps: 779970, episodes: 26000, mean episode reward: -15.269592533081616, agent episode reward: [-12.741536533495397, -1.2640279997931083, -1.2640279997931083], time: 49.296
steps: 779970, episodes: 26000, mean episode variance: 24.989189091682434, agent episode variance: [24.989189091682434, 0.0, 0.0], time: 49.296
steps: 779970, episodes: 26000, mean episode cvar: 17.11520399674773, agent episode cvar: [-0.014395481020212174, 8.586700119018555, 8.54289935874939], time: 49.297
Running avgs for agent 0: q_loss: 0.25017842650413513, u_loss: 67997.203125, p_loss: 5.83749532699585, mean_rew: -0.5781478373062612, variance: 83.29729697227478, cvar: -0.04798493534326553, v: -0.5505422949790955, mean_q: -6.021402835845947, std_q: 12.10547161102295, lamda: 1.2620786428451538
Running avgs for agent 1: q_loss: 3.861374616622925, u_loss: 118.20877838134766, p_loss: -8.77235221862793, mean_rew: -0.3109375028184322, variance: 0.0, cvar: 28.622331619262695, v: 8.760066032409668, mean_q: 8.609697341918945, std_q: 6.6037139892578125, lamda: 1.5828043222427368
Running avgs for agent 2: q_loss: 5.999812602996826, u_loss: 117.56474304199219, p_loss: -8.834552764892578, mean_rew: -0.3084825163376646, variance: 0.0, cvar: 28.476327896118164, v: 8.784889221191406, mean_q: 8.536489486694336, std_q: 6.900062084197998, lamda: 1.584925889968872

steps: 809970, episodes: 27000, mean episode reward: -14.275097204576287, agent episode reward: [-13.021011961520744, -0.6270426215277721, -0.6270426215277721], time: 50.121
steps: 809970, episodes: 27000, mean episode variance: 25.469763576507567, agent episode variance: [25.469763576507567, 0.0, 0.0], time: 50.122
steps: 809970, episodes: 27000, mean episode cvar: 17.083181611299516, agent episode cvar: [-0.014410845518112182, 8.543353336334228, 8.554239120483398], time: 50.122
Running avgs for agent 0: q_loss: 0.22935429215431213, u_loss: 48927.10546875, p_loss: 5.7850661277771, mean_rew: -0.5778961637891005, variance: 84.8992119216919, cvar: -0.04803615063428879, v: -0.5266422629356384, mean_q: -5.970665454864502, std_q: 12.038293838500977, lamda: 1.2763423919677734
Running avgs for agent 1: q_loss: 5.586698532104492, u_loss: 117.59964752197266, p_loss: -9.09803581237793, mean_rew: -0.29920178809401315, variance: 0.0, cvar: 28.47784423828125, v: 9.060187339782715, mean_q: 8.937410354614258, std_q: 6.422840118408203, lamda: 1.606926679611206
Running avgs for agent 2: q_loss: 5.3103156089782715, u_loss: 115.14885711669922, p_loss: -9.210489273071289, mean_rew: -0.298504344845112, variance: 0.0, cvar: 28.514131546020508, v: 9.085009574890137, mean_q: 8.916672706604004, std_q: 6.6549787521362305, lamda: 1.6094211339950562

steps: 839970, episodes: 28000, mean episode reward: -14.586462870525843, agent episode reward: [-13.270098807413662, -0.6581820315560893, -0.6581820315560893], time: 50.088
steps: 839970, episodes: 28000, mean episode variance: 23.752392431259157, agent episode variance: [23.752392431259157, 0.0, 0.0], time: 50.089
steps: 839970, episodes: 28000, mean episode cvar: 17.175925238817932, agent episode cvar: [-0.01389455869793892, 8.586472709655762, 8.603347087860108], time: 50.089
Running avgs for agent 0: q_loss: 0.2172093391418457, u_loss: 42614.0625, p_loss: 5.689774036407471, mean_rew: -0.5695977613145489, variance: 79.17464143753051, cvar: -0.04631519317626953, v: -0.49800923466682434, mean_q: -5.863671779632568, std_q: 12.098384857177734, lamda: 1.2870874404907227
Running avgs for agent 1: q_loss: 4.095428943634033, u_loss: 114.63078308105469, p_loss: -9.437432289123535, mean_rew: -0.2878446680170032, variance: 0.0, cvar: 28.6215763092041, v: 9.360309600830078, mean_q: 9.279452323913574, std_q: 6.298330307006836, lamda: 1.6322680711746216
Running avgs for agent 2: q_loss: 5.232548713684082, u_loss: 113.39173126220703, p_loss: -9.573195457458496, mean_rew: -0.28779871382895683, variance: 0.0, cvar: 28.677825927734375, v: 9.3851318359375, mean_q: 9.287189483642578, std_q: 6.497678279876709, lamda: 1.6343780755996704

steps: 869970, episodes: 29000, mean episode reward: -13.899848311006565, agent episode reward: [-13.418842146320326, -0.24050308234311973, -0.24050308234311973], time: 49.884
steps: 869970, episodes: 29000, mean episode variance: 23.79254881477356, agent episode variance: [23.79254881477356, 0.0, 0.0], time: 49.885
steps: 869970, episodes: 29000, mean episode cvar: 17.151637593239546, agent episode cvar: [-0.01254328253865242, 8.607974746704102, 8.556206129074097], time: 49.885
Running avgs for agent 0: q_loss: 0.22822138667106628, u_loss: 43718.83203125, p_loss: 5.627924919128418, mean_rew: -0.5676813233534551, variance: 79.30849604924519, cvar: -0.04181094095110893, v: -0.4827982187271118, mean_q: -5.799447536468506, std_q: 12.239039421081543, lamda: 1.3008795976638794
Running avgs for agent 1: q_loss: 7.318344593048096, u_loss: 114.63273620605469, p_loss: -9.72874927520752, mean_rew: -0.27887852130600777, variance: 0.0, cvar: 28.693248748779297, v: 9.660429954528809, mean_q: 9.574886322021484, std_q: 6.238464832305908, lamda: 1.6580890417099
Running avgs for agent 2: q_loss: 4.494550704956055, u_loss: 112.22866821289062, p_loss: -9.854801177978516, mean_rew: -0.2789420547826428, variance: 0.0, cvar: 28.520687103271484, v: 9.685253143310547, mean_q: 9.582499504089355, std_q: 6.328590393066406, lamda: 1.6604223251342773

steps: 899970, episodes: 30000, mean episode reward: -15.71876701895691, agent episode reward: [-12.756080515252732, -1.4813432518520901, -1.4813432518520901], time: 48.991
steps: 899970, episodes: 30000, mean episode variance: 23.30392883682251, agent episode variance: [23.30392883682251, 0.0, 0.0], time: 48.991
steps: 899970, episodes: 30000, mean episode cvar: 17.118750045999885, agent episode cvar: [-0.014588290944695473, 8.541668754577637, 8.591669582366944], time: 48.991
Running avgs for agent 0: q_loss: 0.20511753857135773, u_loss: 45794.46484375, p_loss: 5.4832844734191895, mean_rew: -0.5573226915253086, variance: 77.67976278940837, cvar: -0.04862763732671738, v: -0.468136191368103, mean_q: -5.644885063171387, std_q: 11.065065383911133, lamda: 1.3131604194641113
Running avgs for agent 1: q_loss: 5.698159694671631, u_loss: 112.05091094970703, p_loss: -9.987712860107422, mean_rew: -0.2699925616741868, variance: 0.0, cvar: 28.472227096557617, v: 9.960551261901855, mean_q: 9.832164764404297, std_q: 6.185329437255859, lamda: 1.6842446327209473
Running avgs for agent 2: q_loss: 6.313650608062744, u_loss: 111.37382507324219, p_loss: -10.165271759033203, mean_rew: -0.27023231193374764, variance: 0.0, cvar: 28.638898849487305, v: 9.985374450683594, mean_q: 9.90025520324707, std_q: 6.2411298751831055, lamda: 1.6851203441619873

steps: 929970, episodes: 31000, mean episode reward: -15.161655322903488, agent episode reward: [-13.129816061385071, -1.0159196307592098, -1.0159196307592098], time: 50.285
steps: 929970, episodes: 31000, mean episode variance: 22.1677557888031, agent episode variance: [22.1677557888031, 0.0, 0.0], time: 50.285
steps: 929970, episodes: 31000, mean episode cvar: 17.11102738071978, agent episode cvar: [-0.01603165362775326, 8.573574054718017, 8.553484979629516], time: 50.285
Running avgs for agent 0: q_loss: 0.2293446809053421, u_loss: 67058.4296875, p_loss: 5.475943565368652, mean_rew: -0.5573950159073445, variance: 73.89251929601033, cvar: -0.05343884229660034, v: -0.4638814330101013, mean_q: -5.6376190185546875, std_q: 11.645536422729492, lamda: 1.3266631364822388
Running avgs for agent 1: q_loss: 5.893945693969727, u_loss: 114.12333679199219, p_loss: -10.28127384185791, mean_rew: -0.2637579812749308, variance: 0.0, cvar: 28.578580856323242, v: 10.260672569274902, mean_q: 10.133888244628906, std_q: 6.14447021484375, lamda: 1.709058403968811
Running avgs for agent 2: q_loss: 7.664152145385742, u_loss: 113.53187561035156, p_loss: -10.43908405303955, mean_rew: -0.2601078809123221, variance: 0.0, cvar: 28.51161766052246, v: 10.28549575805664, mean_q: 10.179579734802246, std_q: 6.1723527908325195, lamda: 1.711005449295044

steps: 959970, episodes: 32000, mean episode reward: -14.67875496451482, agent episode reward: [-13.138637354192651, -0.7700588051610845, -0.7700588051610845], time: 49.514
steps: 959970, episodes: 32000, mean episode variance: 21.78982960033417, agent episode variance: [21.78982960033417, 0.0, 0.0], time: 49.515
steps: 959970, episodes: 32000, mean episode cvar: 17.120278165966273, agent episode cvar: [-0.012486333698034287, 8.56192303276062, 8.570841466903687], time: 49.515
Running avgs for agent 0: q_loss: 0.2055482566356659, u_loss: 50248.9375, p_loss: 5.378459453582764, mean_rew: -0.5502026147404567, variance: 72.63276533444723, cvar: -0.04162111133337021, v: -0.4511447250843048, mean_q: -5.5319695472717285, std_q: 11.08311653137207, lamda: 1.340441107749939
Running avgs for agent 1: q_loss: 4.4696431159973145, u_loss: 111.8023452758789, p_loss: -10.558838844299316, mean_rew: -0.2540627039828918, variance: 0.0, cvar: 28.539745330810547, v: 10.56079387664795, mean_q: 10.420585632324219, std_q: 6.074526786804199, lamda: 1.733729600906372
Running avgs for agent 2: q_loss: 6.0291619300842285, u_loss: 112.49140930175781, p_loss: -10.694748878479004, mean_rew: -0.25511036679022625, variance: 0.0, cvar: 28.569473266601562, v: 10.585617065429688, mean_q: 10.439337730407715, std_q: 6.163320541381836, lamda: 1.7366050481796265

steps: 989970, episodes: 33000, mean episode reward: -15.80267213742528, agent episode reward: [-12.596364176010521, -1.6031539807073791, -1.6031539807073791], time: 49.1
steps: 989970, episodes: 33000, mean episode variance: 21.796232373714446, agent episode variance: [21.796232373714446, 0.0, 0.0], time: 49.1
steps: 989970, episodes: 33000, mean episode cvar: 17.1538001268059, agent episode cvar: [-0.01350272010266781, 8.575094984054566, 8.592207862854004], time: 49.101
Running avgs for agent 0: q_loss: 0.21255187690258026, u_loss: 30899.57421875, p_loss: 5.342112064361572, mean_rew: -0.5498483619576454, variance: 72.65410791238149, cvar: -0.0450090654194355, v: -0.4425044655799866, mean_q: -5.496181964874268, std_q: 11.183255195617676, lamda: 1.3503899574279785
Running avgs for agent 1: q_loss: 6.127488136291504, u_loss: 112.4819564819336, p_loss: -10.831528663635254, mean_rew: -0.24783821617711524, variance: 0.0, cvar: 28.583648681640625, v: 10.860915184020996, mean_q: 10.689167022705078, std_q: 6.043313980102539, lamda: 1.7575000524520874
Running avgs for agent 2: q_loss: 6.746294975280762, u_loss: 113.39140319824219, p_loss: -10.978056907653809, mean_rew: -0.246943542268408, variance: 0.0, cvar: 28.64069366455078, v: 10.885738372802734, mean_q: 10.73031997680664, std_q: 6.113730430603027, lamda: 1.7613525390625

steps: 1019970, episodes: 34000, mean episode reward: -15.677930875958186, agent episode reward: [-12.632811768946965, -1.5225595535056091, -1.5225595535056091], time: 49.532
steps: 1019970, episodes: 34000, mean episode variance: 21.503367913246155, agent episode variance: [21.503367913246155, 0.0, 0.0], time: 49.532
steps: 1019970, episodes: 34000, mean episode cvar: 17.121634478971362, agent episode cvar: [-0.013540966585278511, 8.546351703643799, 8.588823741912842], time: 49.532
Running avgs for agent 0: q_loss: 0.2162242829799652, u_loss: 44832.51953125, p_loss: 5.250164985656738, mean_rew: -0.5373835790555757, variance: 71.67789304415385, cvar: -0.04513655602931976, v: -0.4411682188510895, mean_q: -5.39913272857666, std_q: 11.194497108459473, lamda: 1.367998480796814
Running avgs for agent 1: q_loss: 6.6743035316467285, u_loss: 112.11444091796875, p_loss: -11.0825834274292, mean_rew: -0.24657427802482648, variance: 0.0, cvar: 28.487838745117188, v: 11.161035537719727, mean_q: 10.947956085205078, std_q: 5.987878799438477, lamda: 1.7836024761199951
Running avgs for agent 2: q_loss: 6.49786901473999, u_loss: 114.04405975341797, p_loss: -11.291112899780273, mean_rew: -0.24479724669382705, variance: 0.0, cvar: 28.629413604736328, v: 11.185859680175781, mean_q: 11.05070686340332, std_q: 5.948365688323975, lamda: 1.7869412899017334

steps: 1049970, episodes: 35000, mean episode reward: -14.671570424428431, agent episode reward: [-12.73076703805731, -0.9704016931855605, -0.9704016931855605], time: 49.019
steps: 1049970, episodes: 35000, mean episode variance: 18.037110666275023, agent episode variance: [18.037110666275023, 0.0, 0.0], time: 49.019
steps: 1049970, episodes: 35000, mean episode cvar: 17.14973380793631, agent episode cvar: [-0.010323380097746849, 8.571729766845703, 8.588327421188355], time: 49.019
Running avgs for agent 0: q_loss: 0.19611120223999023, u_loss: 25291.494140625, p_loss: 4.9102020263671875, mean_rew: -0.49726595722582545, variance: 60.12370222091675, cvar: -0.034411266446113586, v: -0.4204276502132416, mean_q: -5.010020732879639, std_q: 8.3845796585083, lamda: 1.3825472593307495
Running avgs for agent 1: q_loss: 5.019570350646973, u_loss: 110.63642883300781, p_loss: -11.336756706237793, mean_rew: -0.2519543142573755, variance: 0.0, cvar: 28.57243537902832, v: 11.461156845092773, mean_q: 11.21627426147461, std_q: 5.991499900817871, lamda: 1.8068009614944458
Running avgs for agent 2: q_loss: 4.359072208404541, u_loss: 113.03408813476562, p_loss: -11.558791160583496, mean_rew: -0.2533727620280826, variance: 0.0, cvar: 28.627758026123047, v: 11.485980987548828, mean_q: 11.325471878051758, std_q: 5.926634311676025, lamda: 1.8103703260421753

steps: 1079970, episodes: 36000, mean episode reward: -15.873554593314816, agent episode reward: [-12.63547541597243, -1.6190395886711935, -1.6190395886711935], time: 50.338
steps: 1079970, episodes: 36000, mean episode variance: 14.301415512084962, agent episode variance: [14.301415512084962, 0.0, 0.0], time: 50.338
steps: 1079970, episodes: 36000, mean episode cvar: 17.163797036647797, agent episode cvar: [-0.011965046405792237, 8.60496588897705, 8.570796194076538], time: 50.338
Running avgs for agent 0: q_loss: 0.18633364140987396, u_loss: 473.3273010253906, p_loss: 4.5119309425354, mean_rew: -0.46154992894552094, variance: 47.67138671875, cvar: -0.03988349065184593, v: -0.41171371936798096, mean_q: -4.565650939941406, std_q: 4.741934299468994, lamda: 1.3992482423782349
Running avgs for agent 1: q_loss: 5.443563461303711, u_loss: 108.44634246826172, p_loss: -11.627994537353516, mean_rew: -0.24512046721644154, variance: 0.0, cvar: 28.6832218170166, v: 11.761279106140137, mean_q: 11.514350891113281, std_q: 5.981518268585205, lamda: 1.8290505409240723
Running avgs for agent 2: q_loss: 4.957828998565674, u_loss: 110.7890853881836, p_loss: -11.881200790405273, mean_rew: -0.24279091134965938, variance: 0.0, cvar: 28.56932258605957, v: 11.786102294921875, mean_q: 11.65541934967041, std_q: 5.751616954803467, lamda: 1.8324306011199951

steps: 1109970, episodes: 37000, mean episode reward: -15.49054216704727, agent episode reward: [-12.666214587948584, -1.4121637895493435, -1.4121637895493435], time: 49.699
steps: 1109970, episodes: 37000, mean episode variance: 13.668738531112671, agent episode variance: [13.668738531112671, 0.0, 0.0], time: 49.699
steps: 1109970, episodes: 37000, mean episode cvar: 17.156917517066002, agent episode cvar: [-0.014005431771278382, 8.587442455291749, 8.583480493545533], time: 49.7
Running avgs for agent 0: q_loss: 0.17700855433940887, u_loss: 448.3273010253906, p_loss: 4.394056797027588, mean_rew: -0.45536629292593045, variance: 45.562461853027344, cvar: -0.04668477177619934, v: -0.422188401222229, mean_q: -4.445183277130127, std_q: 4.643976211547852, lamda: 1.4175535440444946
Running avgs for agent 1: q_loss: 5.04890775680542, u_loss: 107.23804473876953, p_loss: -11.973557472229004, mean_rew: -0.2199275909952714, variance: 0.0, cvar: 28.624807357788086, v: 12.061399459838867, mean_q: 11.8728666305542, std_q: 5.772504806518555, lamda: 1.8537837266921997
Running avgs for agent 2: q_loss: 4.92802095413208, u_loss: 107.96109771728516, p_loss: -12.168293952941895, mean_rew: -0.22187088478157468, variance: 0.0, cvar: 28.611600875854492, v: 12.086223602294922, mean_q: 11.954992294311523, std_q: 5.7362751960754395, lamda: 1.855678677558899

steps: 1139970, episodes: 38000, mean episode reward: -16.9762291631371, agent episode reward: [-12.487810665645844, -2.2442092487456264, -2.2442092487456264], time: 49.755
steps: 1139970, episodes: 38000, mean episode variance: 12.650308334350585, agent episode variance: [12.650308334350585, 0.0, 0.0], time: 49.755
steps: 1139970, episodes: 38000, mean episode cvar: 17.17671267157793, agent episode cvar: [-0.014092191398143768, 8.584774810791016, 8.606030052185059], time: 49.756
Running avgs for agent 0: q_loss: 0.16638365387916565, u_loss: 435.24609375, p_loss: 4.325826644897461, mean_rew: -0.4537110449823621, variance: 42.167694091796875, cvar: -0.04697396978735924, v: -0.4251413345336914, mean_q: -4.375465393066406, std_q: 4.61837911605835, lamda: 1.4358875751495361
Running avgs for agent 1: q_loss: 6.817869663238525, u_loss: 105.00224304199219, p_loss: -12.327699661254883, mean_rew: -0.19779431966617625, variance: 0.0, cvar: 28.615917205810547, v: 12.361520767211914, mean_q: 12.235259056091309, std_q: 5.495543003082275, lamda: 1.8786998987197876
Running avgs for agent 2: q_loss: 7.593944549560547, u_loss: 103.39443969726562, p_loss: -12.480895042419434, mean_rew: -0.19565964469808936, variance: 0.0, cvar: 28.686763763427734, v: 12.386343955993652, mean_q: 12.29211139678955, std_q: 5.566799163818359, lamda: 1.8803613185882568

steps: 1169970, episodes: 39000, mean episode reward: -17.688044751622, agent episode reward: [-12.20950035932365, -2.7392721961491757, -2.7392721961491757], time: 49.21
steps: 1169970, episodes: 39000, mean episode variance: 11.536030813217163, agent episode variance: [11.536030813217163, 0.0, 0.0], time: 49.21
steps: 1169970, episodes: 39000, mean episode cvar: 17.13267095722258, agent episode cvar: [-0.012668579444289207, 8.577033390045166, 8.568306146621705], time: 49.211
Running avgs for agent 0: q_loss: 0.16852037608623505, u_loss: 407.2799987792969, p_loss: 4.268420219421387, mean_rew: -0.44886777122890675, variance: 38.45343780517578, cvar: -0.04222859814763069, v: -0.4230744540691376, mean_q: -4.318808555603027, std_q: 4.575528144836426, lamda: 1.450922966003418
Running avgs for agent 1: q_loss: 5.162042617797852, u_loss: 102.56873321533203, p_loss: -12.691195487976074, mean_rew: -0.16751481513764288, variance: 0.0, cvar: 28.590110778808594, v: 12.661642074584961, mean_q: 12.61856746673584, std_q: 5.219522476196289, lamda: 1.903409481048584
Running avgs for agent 2: q_loss: 7.025418281555176, u_loss: 99.47566986083984, p_loss: -12.779251098632812, mean_rew: -0.16658717118839894, variance: 0.0, cvar: 28.56102180480957, v: 12.686466217041016, mean_q: 12.62597370147705, std_q: 5.2205328941345215, lamda: 1.904694676399231

steps: 1199970, episodes: 40000, mean episode reward: -16.61283263908833, agent episode reward: [-11.889122956863037, -2.361854841112646, -2.361854841112646], time: 50.454
steps: 1199970, episodes: 40000, mean episode variance: 10.350516632080078, agent episode variance: [10.350516632080078, 0.0, 0.0], time: 50.455
steps: 1199970, episodes: 40000, mean episode cvar: 17.172646169245244, agent episode cvar: [-0.01026560252904892, 8.603772621154786, 8.579139150619508], time: 50.455
Running avgs for agent 0: q_loss: 0.16871221363544464, u_loss: 421.65130615234375, p_loss: 4.219867706298828, mean_rew: -0.4477605442892514, variance: 34.5017204284668, cvar: -0.0342186763882637, v: -0.4137040674686432, mean_q: -4.270371437072754, std_q: 4.6113600730896, lamda: 1.4666603803634644
Running avgs for agent 1: q_loss: 4.947297096252441, u_loss: 101.35494232177734, p_loss: -13.023159980773926, mean_rew: -0.14867604587581448, variance: 0.0, cvar: 28.679241180419922, v: 12.961764335632324, mean_q: 12.960911750793457, std_q: 5.007568359375, lamda: 1.9268358945846558
Running avgs for agent 2: q_loss: 4.631105899810791, u_loss: 98.3017349243164, p_loss: -13.066088676452637, mean_rew: -0.14842161079360713, variance: 0.0, cvar: 28.597129821777344, v: 12.986586570739746, mean_q: 12.937663078308105, std_q: 5.074204444885254, lamda: 1.9301731586456299

steps: 1229970, episodes: 41000, mean episode reward: -16.44773613852734, agent episode reward: [-12.34392792951091, -2.051904104508213, -2.051904104508213], time: 49.843
steps: 1229970, episodes: 41000, mean episode variance: 9.357902080535888, agent episode variance: [9.357902080535888, 0.0, 0.0], time: 49.843
steps: 1229970, episodes: 41000, mean episode cvar: 17.145726074784996, agent episode cvar: [-0.011978789716959, 8.589865407943726, 8.567839456558227], time: 49.844
Running avgs for agent 0: q_loss: 0.17773842811584473, u_loss: 394.0160827636719, p_loss: 4.126607894897461, mean_rew: -0.4437634954455312, variance: 31.193008422851562, cvar: -0.03992929309606552, v: -0.4200354516506195, mean_q: -4.176443099975586, std_q: 4.531834125518799, lamda: 1.4861297607421875
Running avgs for agent 1: q_loss: 5.540019989013672, u_loss: 97.20462799072266, p_loss: -13.451946258544922, mean_rew: -0.12081497271768611, variance: 0.0, cvar: 28.63288688659668, v: 13.261884689331055, mean_q: 13.395089149475098, std_q: 4.344674587249756, lamda: 1.9525091648101807
Running avgs for agent 2: q_loss: 5.552222728729248, u_loss: 96.71244812011719, p_loss: -13.452337265014648, mean_rew: -0.12115066294865684, variance: 0.0, cvar: 28.559463500976562, v: 13.28670883178711, mean_q: 13.345284461975098, std_q: 4.535121440887451, lamda: 1.953522801399231

steps: 1259970, episodes: 42000, mean episode reward: -16.168949353366738, agent episode reward: [-11.42067544747326, -2.374136952946739, -2.374136952946739], time: 49.371
steps: 1259970, episodes: 42000, mean episode variance: 8.470155271530151, agent episode variance: [8.470155271530151, 0.0, 0.0], time: 49.371
steps: 1259970, episodes: 42000, mean episode cvar: 17.154533750995995, agent episode cvar: [-0.011017510905861854, 8.59879054260254, 8.566760719299316], time: 49.372
Running avgs for agent 0: q_loss: 0.1835411638021469, u_loss: 404.36505126953125, p_loss: 4.043552875518799, mean_rew: -0.44134023501040803, variance: 28.233850479125977, cvar: -0.036725036799907684, v: -0.42107251286506653, mean_q: -4.090637683868408, std_q: 4.526223182678223, lamda: 1.5040944814682007
Running avgs for agent 1: q_loss: 6.419559001922607, u_loss: 92.82158660888672, p_loss: -13.904031753540039, mean_rew: -0.09864909362965167, variance: 0.0, cvar: 28.662633895874023, v: 13.562006950378418, mean_q: 13.8495512008667, std_q: 3.670487880706787, lamda: 1.976173758506775
Running avgs for agent 2: q_loss: 5.1731858253479, u_loss: 90.2865982055664, p_loss: -13.860878944396973, mean_rew: -0.09748840236492501, variance: 0.0, cvar: 28.555866241455078, v: 13.58682918548584, mean_q: 13.77270793914795, std_q: 3.8772144317626953, lamda: 1.9760632514953613

steps: 1289970, episodes: 43000, mean episode reward: -17.495022728340068, agent episode reward: [-11.864579040649621, -2.8152218438452246, -2.8152218438452246], time: 49.777
steps: 1289970, episodes: 43000, mean episode variance: 7.928918363571167, agent episode variance: [7.928918363571167, 0.0, 0.0], time: 49.778
steps: 1289970, episodes: 43000, mean episode cvar: 17.177971280053256, agent episode cvar: [-0.012631716772913932, 8.59353564453125, 8.597067352294921], time: 49.778
Running avgs for agent 0: q_loss: 0.1878245323896408, u_loss: 403.73907470703125, p_loss: 3.9625375270843506, mean_rew: -0.437951094210963, variance: 26.429729461669922, cvar: -0.042105723172426224, v: -0.43204814195632935, mean_q: -4.009468078613281, std_q: 4.464722633361816, lamda: 1.5250250101089478
Running avgs for agent 1: q_loss: 5.908690452575684, u_loss: 89.34231567382812, p_loss: -14.189934730529785, mean_rew: -0.08432120448520351, variance: 0.0, cvar: 28.64512062072754, v: 13.862129211425781, mean_q: 14.145753860473633, std_q: 3.363844633102417, lamda: 2.000704526901245
Running avgs for agent 2: q_loss: 5.567476272583008, u_loss: 84.87178802490234, p_loss: -14.163549423217773, mean_rew: -0.08411322858243028, variance: 0.0, cvar: 28.65689468383789, v: 13.886951446533203, mean_q: 14.085285186767578, std_q: 3.5559065341949463, lamda: 1.9995877742767334

steps: 1319970, episodes: 44000, mean episode reward: -18.523627655471042, agent episode reward: [-11.655030823900207, -3.434298415785418, -3.434298415785418], time: 49.936
steps: 1319970, episodes: 44000, mean episode variance: 7.090508151054382, agent episode variance: [7.090508151054382, 0.0, 0.0], time: 49.936
steps: 1319970, episodes: 44000, mean episode cvar: 17.106225828528405, agent episode cvar: [-0.014650781273841857, 8.552367240905761, 8.568509368896484], time: 49.937
Running avgs for agent 0: q_loss: 0.1809503138065338, u_loss: 394.9013671875, p_loss: 3.9154350757598877, mean_rew: -0.4360957598713851, variance: 23.635026931762695, cvar: -0.048835936933755875, v: -0.4456983506679535, mean_q: -3.9621949195861816, std_q: 4.445270538330078, lamda: 1.541053056716919
Running avgs for agent 1: q_loss: 7.1038689613342285, u_loss: 86.66857147216797, p_loss: -14.369366645812988, mean_rew: -0.07870596546878059, variance: 0.0, cvar: 28.507890701293945, v: 14.162249565124512, mean_q: 14.323333740234375, std_q: 3.3798365592956543, lamda: 2.02463960647583
Running avgs for agent 2: q_loss: 6.256648540496826, u_loss: 82.31827545166016, p_loss: -14.3278226852417, mean_rew: -0.0795382744610113, variance: 0.0, cvar: 28.56169891357422, v: 14.187073707580566, mean_q: 14.249495506286621, std_q: 3.6024105548858643, lamda: 2.0208311080932617

steps: 1349970, episodes: 45000, mean episode reward: -19.064256828776482, agent episode reward: [-12.001148490356904, -3.531554169209788, -3.531554169209788], time: 49.909
steps: 1349970, episodes: 45000, mean episode variance: 6.4861605052948, agent episode variance: [6.4861605052948, 0.0, 0.0], time: 49.909
steps: 1349970, episodes: 45000, mean episode cvar: 17.1610594022125, agent episode cvar: [-0.012526600137352944, 8.598977451324464, 8.57460855102539], time: 49.91
Running avgs for agent 0: q_loss: 0.20808497071266174, u_loss: 383.77764892578125, p_loss: 3.852814197540283, mean_rew: -0.434014959227613, variance: 21.62053680419922, cvar: -0.04175533354282379, v: -0.47145888209342957, mean_q: -3.9010212421417236, std_q: 4.437465190887451, lamda: 1.5658650398254395
Running avgs for agent 1: q_loss: 6.71223783493042, u_loss: 85.89974212646484, p_loss: -14.578710556030273, mean_rew: -0.07728232043217455, variance: 0.0, cvar: 28.66325569152832, v: 14.462369918823242, mean_q: 14.53227710723877, std_q: 3.4484806060791016, lamda: 2.0494163036346436
Running avgs for agent 2: q_loss: 5.599380970001221, u_loss: 81.23741149902344, p_loss: -14.488759994506836, mean_rew: -0.07917606875093056, variance: 0.0, cvar: 28.58203125, v: 14.487194061279297, mean_q: 14.412389755249023, std_q: 3.686749219894409, lamda: 2.043877363204956

steps: 1379970, episodes: 46000, mean episode reward: -19.410438145630508, agent episode reward: [-11.460793650336537, -3.9748222476469857, -3.9748222476469857], time: 50.824
steps: 1379970, episodes: 46000, mean episode variance: 6.106412056922912, agent episode variance: [6.106412056922912, 0.0, 0.0], time: 50.825
steps: 1379970, episodes: 46000, mean episode cvar: 17.17412153556943, agent episode cvar: [-0.010991646498441697, 8.604003503799438, 8.581109678268433], time: 50.825
Running avgs for agent 0: q_loss: 0.186696395277977, u_loss: 381.1005859375, p_loss: 3.828155040740967, mean_rew: -0.4317645683430412, variance: 20.354707717895508, cvar: -0.03663882240653038, v: -0.4827720522880554, mean_q: -3.8779401779174805, std_q: 4.445167064666748, lamda: 1.5807547569274902
Running avgs for agent 1: q_loss: 4.303252220153809, u_loss: 85.84962463378906, p_loss: -14.764501571655273, mean_rew: -0.07993684861323158, variance: 0.0, cvar: 28.68001365661621, v: 14.762492179870605, mean_q: 14.719928741455078, std_q: 3.503507375717163, lamda: 2.0731582641601562
Running avgs for agent 2: q_loss: 6.980350971221924, u_loss: 81.59964752197266, p_loss: -14.676405906677246, mean_rew: -0.07942424714609841, variance: 0.0, cvar: 28.60369873046875, v: 14.787314414978027, mean_q: 14.602924346923828, std_q: 3.7160263061523438, lamda: 2.0679659843444824

steps: 1409970, episodes: 47000, mean episode reward: -19.861985322863866, agent episode reward: [-11.406726823393845, -4.227629249735011, -4.227629249735011], time: 50.968
steps: 1409970, episodes: 47000, mean episode variance: 5.705905836105346, agent episode variance: [5.705905836105346, 0.0, 0.0], time: 50.969
steps: 1409970, episodes: 47000, mean episode cvar: 17.175245349496603, agent episode cvar: [-0.006963444143533707, 8.599460037231445, 8.58274875640869], time: 50.969
Running avgs for agent 0: q_loss: 0.19833262264728546, u_loss: 370.0658264160156, p_loss: 3.788778066635132, mean_rew: -0.4270434736569349, variance: 19.019685745239258, cvar: -0.023211481049656868, v: -0.5068162083625793, mean_q: -3.8386824131011963, std_q: 4.405022621154785, lamda: 1.5973877906799316
Running avgs for agent 1: q_loss: 8.14659309387207, u_loss: 88.6136474609375, p_loss: -14.927499771118164, mean_rew: -0.08005311252667555, variance: 0.0, cvar: 28.66486358642578, v: 15.062613487243652, mean_q: 14.88412094116211, std_q: 3.5676863193511963, lamda: 2.0960800647735596
Running avgs for agent 2: q_loss: 7.77273416519165, u_loss: 84.06953430175781, p_loss: -14.88807487487793, mean_rew: -0.08029361480821717, variance: 0.0, cvar: 28.609163284301758, v: 15.087434768676758, mean_q: 14.81978702545166, std_q: 3.723125696182251, lamda: 2.093395948410034

steps: 1439970, episodes: 48000, mean episode reward: -18.344085236593845, agent episode reward: [-11.257876920801992, -3.5431041578959257, -3.5431041578959257], time: 50.052
steps: 1439970, episodes: 48000, mean episode variance: 5.431607629776001, agent episode variance: [5.431607629776001, 0.0, 0.0], time: 50.052
steps: 1439970, episodes: 48000, mean episode cvar: 17.1313991933465, agent episode cvar: [-0.009160478055477142, 8.571095123291016, 8.569464548110961], time: 50.052
Running avgs for agent 0: q_loss: 0.22277460992336273, u_loss: 371.0663146972656, p_loss: 3.748333215713501, mean_rew: -0.4258634972996402, variance: 18.10536003112793, cvar: -0.030534924939274788, v: -0.5331823229789734, mean_q: -3.7987124919891357, std_q: 4.381604194641113, lamda: 1.6168341636657715
Running avgs for agent 1: q_loss: 5.235862731933594, u_loss: 89.30339813232422, p_loss: -15.075675010681152, mean_rew: -0.0834814273364055, variance: 0.0, cvar: 28.5703182220459, v: 15.3627347946167, mean_q: 15.034957885742188, std_q: 3.6253294944763184, lamda: 2.1187992095947266
Running avgs for agent 2: q_loss: 5.163839340209961, u_loss: 86.6991958618164, p_loss: -15.084274291992188, mean_rew: -0.08159756785285775, variance: 0.0, cvar: 28.564882278442383, v: 15.387557029724121, mean_q: 15.017419815063477, std_q: 3.74127197265625, lamda: 2.117774486541748

steps: 1469970, episodes: 49000, mean episode reward: -17.29709733915496, agent episode reward: [-11.14177867649715, -3.077659331328904, -3.077659331328904], time: 50.756
steps: 1469970, episodes: 49000, mean episode variance: 5.132979156017304, agent episode variance: [5.132979156017304, 0.0, 0.0], time: 50.757
steps: 1469970, episodes: 49000, mean episode cvar: 17.186138731986283, agent episode cvar: [-0.0034715966880321503, 8.589786748886109, 8.599823579788207], time: 50.757
Running avgs for agent 0: q_loss: 0.228077694773674, u_loss: 361.4422302246094, p_loss: 3.6601054668426514, mean_rew: -0.42043398631823947, variance: 17.10993194580078, cvar: -0.011571989394724369, v: -0.5484437346458435, mean_q: -3.711886405944824, std_q: 4.320254802703857, lamda: 1.637842059135437
Running avgs for agent 1: q_loss: 6.759301662445068, u_loss: 92.63006591796875, p_loss: -15.328437805175781, mean_rew: -0.08167885093094363, variance: 0.0, cvar: 28.632619857788086, v: 15.662856101989746, mean_q: 15.284557342529297, std_q: 3.6066818237304688, lamda: 2.1410672664642334
Running avgs for agent 2: q_loss: 7.058175563812256, u_loss: 88.36380004882812, p_loss: -15.266083717346191, mean_rew: -0.08163670755594125, variance: 0.0, cvar: 28.666080474853516, v: 15.687679290771484, mean_q: 15.199310302734375, std_q: 3.8051350116729736, lamda: 2.1407241821289062

steps: 1499970, episodes: 50000, mean episode reward: -18.893914444127827, agent episode reward: [-11.516921769130127, -3.688496337498849, -3.688496337498849], time: 50.319
steps: 1499970, episodes: 50000, mean episode variance: 4.504409390449524, agent episode variance: [4.504409390449524, 0.0, 0.0], time: 50.32
steps: 1499970, episodes: 50000, mean episode cvar: 17.192862421125174, agent episode cvar: [-0.00442658606171608, 8.578569763183594, 8.618719244003296], time: 50.32
Running avgs for agent 0: q_loss: 0.23400425910949707, u_loss: 353.8672790527344, p_loss: 3.6436939239501953, mean_rew: -0.41814597270067116, variance: 15.014697074890137, cvar: -0.01475528720766306, v: -0.5644519329071045, mean_q: -3.698375701904297, std_q: 4.305729389190674, lamda: 1.6579153537750244
Running avgs for agent 1: q_loss: 4.705576419830322, u_loss: 94.7702865600586, p_loss: -15.542705535888672, mean_rew: -0.08044926735612808, variance: 0.0, cvar: 28.595233917236328, v: 15.962957382202148, mean_q: 15.500299453735352, std_q: 3.6088151931762695, lamda: 2.1636290550231934
Running avgs for agent 2: q_loss: 8.717874526977539, u_loss: 90.04214477539062, p_loss: -15.46524429321289, mean_rew: -0.0807976720464164, variance: 0.0, cvar: 28.729063034057617, v: 15.987770080566406, mean_q: 15.40218448638916, std_q: 3.8226916790008545, lamda: 2.1662662029266357

steps: 1529970, episodes: 51000, mean episode reward: -19.528845657357632, agent episode reward: [-10.872784210775615, -4.32803072329101, -4.32803072329101], time: 50.005
steps: 1529970, episodes: 51000, mean episode variance: 3.9123017234802244, agent episode variance: [3.9123017234802244, 0.0, 0.0], time: 50.005
steps: 1529970, episodes: 51000, mean episode cvar: 17.184000557005405, agent episode cvar: [-0.005366201341152191, 8.602897081375122, 8.586469676971436], time: 50.005
Running avgs for agent 0: q_loss: 0.23480598628520966, u_loss: 352.8768005371094, p_loss: 3.6163721084594727, mean_rew: -0.417860625110158, variance: 13.041006088256836, cvar: -0.017887337133288383, v: -0.5825082659721375, mean_q: -3.67215895652771, std_q: 4.278037071228027, lamda: 1.678507685661316
Running avgs for agent 1: q_loss: 5.036336898803711, u_loss: 95.07341003417969, p_loss: -15.782039642333984, mean_rew: -0.07731657058701812, variance: 0.0, cvar: 28.676321029663086, v: 16.262847900390625, mean_q: 15.742539405822754, std_q: 3.6047523021698, lamda: 2.186046600341797
Running avgs for agent 2: q_loss: 5.106406211853027, u_loss: 91.37123107910156, p_loss: -15.689759254455566, mean_rew: -0.07746161655788192, variance: 0.0, cvar: 28.621564865112305, v: 16.287647247314453, mean_q: 15.632305145263672, std_q: 3.7671515941619873, lamda: 2.189438581466675

steps: 1559970, episodes: 52000, mean episode reward: -19.467515947454924, agent episode reward: [-10.448885476326671, -4.509315235564127, -4.509315235564127], time: 50.181
steps: 1559970, episodes: 52000, mean episode variance: 3.15309370970726, agent episode variance: [3.15309370970726, 0.0, 0.0], time: 50.181
steps: 1559970, episodes: 52000, mean episode cvar: 17.17270277354121, agent episode cvar: [-0.005060129672288895, 8.586178245544433, 8.591584657669067], time: 50.182
Running avgs for agent 0: q_loss: 0.24300409853458405, u_loss: 334.7823486328125, p_loss: 3.5875895023345947, mean_rew: -0.41160514592532754, variance: 10.5103120803833, cvar: -0.016867101192474365, v: -0.6029884219169617, mean_q: -3.642505645751953, std_q: 4.227607250213623, lamda: 1.6979038715362549
Running avgs for agent 1: q_loss: 6.186352729797363, u_loss: 97.02334594726562, p_loss: -16.00104522705078, mean_rew: -0.07486797776201495, variance: 0.0, cvar: 28.62059211730957, v: 16.56268310546875, mean_q: 15.964265823364258, std_q: 3.5862996578216553, lamda: 2.2090933322906494
Running avgs for agent 2: q_loss: 6.404054164886475, u_loss: 93.001953125, p_loss: -15.917510032653809, mean_rew: -0.07346204949904572, variance: 0.0, cvar: 28.638612747192383, v: 16.58748435974121, mean_q: 15.866344451904297, std_q: 3.724271297454834, lamda: 2.2124905586242676

steps: 1589970, episodes: 53000, mean episode reward: -21.60771418366528, agent episode reward: [-10.917106508522046, -5.345303837571619, -5.345303837571619], time: 50.802
steps: 1589970, episodes: 53000, mean episode variance: 2.384991930127144, agent episode variance: [2.384991930127144, 0.0, 0.0], time: 50.803
steps: 1589970, episodes: 53000, mean episode cvar: 17.150763614356517, agent episode cvar: [-0.0037514174580574036, 8.583393802642822, 8.571121229171753], time: 50.803
Running avgs for agent 0: q_loss: 0.24106557667255402, u_loss: 333.9089050292969, p_loss: 3.576338768005371, mean_rew: -0.4102674147991949, variance: 7.949973100423813, cvar: -0.012504723854362965, v: -0.6240390539169312, mean_q: -3.6329703330993652, std_q: 4.255576133728027, lamda: 1.7194199562072754
Running avgs for agent 1: q_loss: 5.565086841583252, u_loss: 97.47865295410156, p_loss: -16.206832885742188, mean_rew: -0.0728832525095688, variance: 0.0, cvar: 28.61131477355957, v: 16.862520217895508, mean_q: 16.165973663330078, std_q: 3.5954761505126953, lamda: 2.2326865196228027
Running avgs for agent 2: q_loss: 7.405007362365723, u_loss: 92.61304473876953, p_loss: -16.165067672729492, mean_rew: -0.07268196542137106, variance: 0.0, cvar: 28.570404052734375, v: 16.887317657470703, mean_q: 16.114097595214844, std_q: 3.6529858112335205, lamda: 2.235340118408203

steps: 1619970, episodes: 54000, mean episode reward: -22.658206626014856, agent episode reward: [-10.637907631007465, -6.010149497503697, -6.010149497503697], time: 49.542
steps: 1619970, episodes: 54000, mean episode variance: 1.9063302313685417, agent episode variance: [1.9063302313685417, 0.0, 0.0], time: 49.542
steps: 1619970, episodes: 54000, mean episode cvar: 17.155508008897304, agent episode cvar: [-0.003226223051548004, 8.576429389953613, 8.582304841995239], time: 49.543
Running avgs for agent 0: q_loss: 0.24226278066635132, u_loss: 330.93829345703125, p_loss: 3.560135841369629, mean_rew: -0.40735739986712693, variance: 6.354434104561806, cvar: -0.010754076763987541, v: -0.6387137770652771, mean_q: -3.6156575679779053, std_q: 4.210264682769775, lamda: 1.7378102540969849
Running avgs for agent 1: q_loss: 5.815685272216797, u_loss: 102.55714416503906, p_loss: -16.38681411743164, mean_rew: -0.07452714380197238, variance: 0.0, cvar: 28.588098526000977, v: 17.162353515625, mean_q: 16.350618362426758, std_q: 3.6152050495147705, lamda: 2.256070375442505
Running avgs for agent 2: q_loss: 4.662113666534424, u_loss: 95.82965087890625, p_loss: -16.396678924560547, mean_rew: -0.0749536516106814, variance: 0.0, cvar: 28.607683181762695, v: 17.18715476989746, mean_q: 16.337434768676758, std_q: 3.6538119316101074, lamda: 2.256226062774658

steps: 1649970, episodes: 55000, mean episode reward: -20.76160035275577, agent episode reward: [-10.996916870792726, -4.882341740981523, -4.882341740981523], time: 50.852
steps: 1649970, episodes: 55000, mean episode variance: 1.425227961242199, agent episode variance: [1.425227961242199, 0.0, 0.0], time: 50.852
steps: 1649970, episodes: 55000, mean episode cvar: 17.1706332205534, agent episode cvar: [-0.006510139584541321, 8.595649690628052, 8.581493669509888], time: 50.853
Running avgs for agent 0: q_loss: 0.2449411004781723, u_loss: 322.8439025878906, p_loss: 3.532395839691162, mean_rew: -0.4057815642491739, variance: 4.75075987080733, cvar: -0.021700464189052582, v: -0.6649523973464966, mean_q: -3.5880823135375977, std_q: 4.189525127410889, lamda: 1.7565339803695679
Running avgs for agent 1: q_loss: 5.980431079864502, u_loss: 103.89401245117188, p_loss: -16.57796287536621, mean_rew: -0.08018489389063178, variance: 0.0, cvar: 28.65216064453125, v: 17.462186813354492, mean_q: 16.541200637817383, std_q: 3.654984474182129, lamda: 2.278532028198242
Running avgs for agent 2: q_loss: 6.557758808135986, u_loss: 97.13687133789062, p_loss: -16.548114776611328, mean_rew: -0.0782417284922084, variance: 0.0, cvar: 28.60498046875, v: 17.486988067626953, mean_q: 16.489349365234375, std_q: 3.7074756622314453, lamda: 2.279244899749756

steps: 1679970, episodes: 56000, mean episode reward: -21.04029717486949, agent episode reward: [-10.710795563679312, -5.164750805595089, -5.164750805595089], time: 50.818
steps: 1679970, episodes: 56000, mean episode variance: 1.2892406742572784, agent episode variance: [1.2892406742572784, 0.0, 0.0], time: 50.819
steps: 1679970, episodes: 56000, mean episode cvar: 17.154276846826075, agent episode cvar: [-0.006520337164402008, 8.576250469207764, 8.584546714782714], time: 50.819
Running avgs for agent 0: q_loss: 0.22008445858955383, u_loss: 319.9170227050781, p_loss: 3.4612057209014893, mean_rew: -0.4047618273788632, variance: 4.297468914190929, cvar: -0.021734459325671196, v: -0.66769939661026, mean_q: -3.516087293624878, std_q: 4.151867866516113, lamda: 1.7748156785964966
Running avgs for agent 1: q_loss: 4.855145454406738, u_loss: 105.78501892089844, p_loss: -16.769285202026367, mean_rew: -0.08348929066343429, variance: 0.0, cvar: 28.58750343322754, v: 17.76202392578125, mean_q: 16.73506736755371, std_q: 3.6565022468566895, lamda: 2.3002705574035645/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

Running avgs for agent 2: q_loss: 5.311893939971924, u_loss: 97.76177215576172, p_loss: -16.764511108398438, mean_rew: -0.08381888635779634, variance: 0.0, cvar: 28.61515235900879, v: 17.786823272705078, mean_q: 16.70264434814453, std_q: 3.71012806892395, lamda: 2.3018431663513184

steps: 1709970, episodes: 57000, mean episode reward: -19.934013636846327, agent episode reward: [-10.7586891667372, -4.587662235054565, -4.587662235054565], time: 50.705
steps: 1709970, episodes: 57000, mean episode variance: 1.1365974201261997, agent episode variance: [1.1365974201261997, 0.0, 0.0], time: 50.706
steps: 1709970, episodes: 57000, mean episode cvar: 17.150361752539872, agent episode cvar: [-0.007858347862958907, 8.58262844467163, 8.575591655731202], time: 50.706
Running avgs for agent 0: q_loss: 0.27898475527763367, u_loss: 319.1472473144531, p_loss: 3.3464221954345703, mean_rew: -0.40210625662303995, variance: 3.7886580670873324, cvar: -0.026194492354989052, v: -0.6492074728012085, mean_q: -3.398867130279541, std_q: 4.087263584136963, lamda: 1.7940441370010376
Running avgs for agent 1: q_loss: 5.677265644073486, u_loss: 109.47859191894531, p_loss: -16.96979331970215, mean_rew: -0.0861065516042785, variance: 0.0, cvar: 28.608762741088867, v: 18.061859130859375, mean_q: 16.932437896728516, std_q: 3.6787283420562744, lamda: 2.3221375942230225
Running avgs for agent 2: q_loss: 5.403614044189453, u_loss: 100.37397003173828, p_loss: -16.94268798828125, mean_rew: -0.08658302114073739, variance: 0.0, cvar: 28.58530616760254, v: 18.086660385131836, mean_q: 16.884489059448242, std_q: 3.7237417697906494, lamda: 2.3242058753967285

steps: 1739970, episodes: 58000, mean episode reward: -18.83417949592499, agent episode reward: [-10.453482136611917, -4.190348679656535, -4.190348679656535], time: 49.755
steps: 1739970, episodes: 58000, mean episode variance: 1.0506100290864706, agent episode variance: [1.0506100290864706, 0.0, 0.0], time: 49.755
steps: 1739970, episodes: 58000, mean episode cvar: 17.157778482466934, agent episode cvar: [-0.004153394669294357, 8.567548175811767, 8.594383701324462], time: 49.755
Running avgs for agent 0: q_loss: 0.2217087298631668, u_loss: 302.48077392578125, p_loss: 3.224930763244629, mean_rew: -0.3990210246130906, variance: 3.502033430288235, cvar: -0.01384464930742979, v: -0.6230077147483826, mean_q: -3.2773725986480713, std_q: 4.018078804016113, lamda: 1.81208336353302
Running avgs for agent 1: q_loss: 6.912968158721924, u_loss: 113.8226318359375, p_loss: -17.137014389038086, mean_rew: -0.0899921480734777, variance: 0.0, cvar: 28.558496475219727, v: 18.3616943359375, mean_q: 17.091569900512695, std_q: 3.715841770172119, lamda: 2.3445792198181152
Running avgs for agent 2: q_loss: 5.9867424964904785, u_loss: 102.57304382324219, p_loss: -17.15312385559082, mean_rew: -0.09237801870876733, variance: 0.0, cvar: 28.647945404052734, v: 18.386493682861328, mean_q: 17.09505844116211, std_q: 3.734260320663452, lamda: 2.3464293479919434

steps: 1769970, episodes: 59000, mean episode reward: -19.32616236823056, agent episode reward: [-10.375194832091115, -4.475483768069721, -4.475483768069721], time: 50.198
steps: 1769970, episodes: 59000, mean episode variance: 1.0057105719149113, agent episode variance: [1.0057105719149113, 0.0, 0.0], time: 50.199
steps: 1769970, episodes: 59000, mean episode cvar: 17.13241207385063, agent episode cvar: [-0.003779649019241333, 8.566554656982422, 8.56963706588745], time: 50.199
Running avgs for agent 0: q_loss: 0.2719149887561798, u_loss: 310.8426513671875, p_loss: 3.141805648803711, mean_rew: -0.39594956007580323, variance: 3.3523685730497044, cvar: -0.012598829343914986, v: -0.5929017066955566, mean_q: -3.1944127082824707, std_q: 3.963505744934082, lamda: 1.8274052143096924
Running avgs for agent 1: q_loss: 4.654454231262207, u_loss: 113.05984497070312, p_loss: -17.35174560546875, mean_rew: -0.09537463326881766, variance: 0.0, cvar: 28.5551815032959, v: 18.661529541015625, mean_q: 17.3009090423584, std_q: 3.713015079498291, lamda: 2.3659932613372803
Running avgs for agent 2: q_loss: 5.598642826080322, u_loss: 104.40020751953125, p_loss: -17.31097412109375, mean_rew: -0.09440591310330798, variance: 0.0, cvar: 28.56545639038086, v: 18.686328887939453, mean_q: 17.25868797302246, std_q: 3.7410035133361816, lamda: 2.3686134815216064

steps: 1799970, episodes: 60000, mean episode reward: -18.897813147958516, agent episode reward: [-10.140315712564162, -4.378748717697177, -4.378748717697177], time: 49.729
steps: 1799970, episodes: 60000, mean episode variance: 0.901875369772315, agent episode variance: [0.901875369772315, 0.0, 0.0], time: 49.729
steps: 1799970, episodes: 60000, mean episode cvar: 17.19859695008397, agent episode cvar: [-0.0055728497207164765, 8.580034883499145, 8.624134916305541], time: 49.73
Running avgs for agent 0: q_loss: 0.2590460479259491, u_loss: 293.1596984863281, p_loss: 3.0770809650421143, mean_rew: -0.393278897856521, variance: 3.0062512325743835, cvar: -0.018576165661215782, v: -0.5842005610466003, mean_q: -3.1274006366729736, std_q: 3.8944718837738037, lamda: 1.847102403640747
Running avgs for agent 1: q_loss: 6.0868988037109375, u_loss: 112.96131134033203, p_loss: -17.537803649902344, mean_rew: -0.09669576918213485, variance: 0.0, cvar: 28.600114822387695, v: 18.96136474609375, mean_q: 17.49289894104004, std_q: 3.737902879714966, lamda: 2.387157440185547
Running avgs for agent 2: q_loss: 7.054041385650635, u_loss: 103.43838500976562, p_loss: -17.519580841064453, mean_rew: -0.09871882763783349, variance: 0.0, cvar: 28.747116088867188, v: 18.98616600036621, mean_q: 17.466064453125, std_q: 3.7792651653289795, lamda: 2.391488552093506

...Finished total of 60001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 29970, episodes: 1000, mean episode reward: -20.357844228257914, agent episode reward: [-9.968278205219837, -5.194783011519038, -5.194783011519038], time: 33.719
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 33.72
steps: 29970, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0], time: 33.72
Running avgs for agent 0: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 59970, episodes: 2000, mean episode reward: -19.25855312358561, agent episode reward: [-9.802250015080837, -4.728151554252388, -4.728151554252388], time: 42.303
steps: 59970, episodes: 2000, mean episode variance: 0.9083992139697075, agent episode variance: [0.9083992139697075, 0.0, 0.0], time: 42.303
steps: 59970, episodes: 2000, mean episode cvar: 16.54382991889119, agent episode cvar: [0.11363944151997567, 8.145767208099365, 8.284423269271851], time: 42.304
Running avgs for agent 0: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.3291728494206234, variance: 3.110956212225026, cvar: 0.3891761600971222, v: -0.4099976420402527, mean_q: -2.7284700870513916, std_q: 3.6874051094055176, lamda: 1.8570257425308228
Running avgs for agent 1: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.16929673092878542, variance: 0.0, cvar: 27.89646339416504, v: 19.258201599121094, mean_q: 17.553714752197266, std_q: 3.622563600540161, lamda: 2.3977530002593994
Running avgs for agent 2: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.17139254777761223, variance: 0.0, cvar: 28.371315002441406, v: 19.28299903869629, mean_q: 17.65161895751953, std_q: 3.6782827377319336, lamda: 2.4021875858306885

...Finished total of 2001 episodes with the fixed policy.
