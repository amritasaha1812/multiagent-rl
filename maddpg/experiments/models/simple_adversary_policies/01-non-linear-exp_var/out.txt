WARNING: Logging before flag parsing goes to stderr.
W0903 15:10:58.377468 4635153856 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0903 15:10:58.377790 4635153856 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-09-03 15:10:58.378274: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0903 15:10:58.381147 4635153856 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0903 15:10:58.383410 4635153856 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0903 15:10:58.383594 4635153856 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0903 15:10:58.383699 4635153856 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0903 15:10:58.702938 4635153856 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0903 15:10:58.786509 4635153856 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0903 15:10:58.792621 4635153856 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0903 15:10:59.136137 4635153856 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation False
adversary agent:  -0.25
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 29970, episodes: 1000, mean episode reward: -23.13233093446382, agent episode reward: [-44.67245798803402, 10.770063526785096, 10.770063526785096], time: 24.808
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 24.808
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 59970, episodes: 2000, mean episode reward: -120.91143950053774, agent episode reward: [-177.0107729224285, 28.049666710945363, 28.049666710945363], time: 46.649
steps: 59970, episodes: 2000, mean episode variance: 0.6277911334969103, agent episode variance: [0.4742279247790575, 0.07295295348390937, 0.08061025523394347], time: 46.65
Running avgs for agent 0: q_loss: 649.70849609375, p_loss: -0.35120856761932373, mean_rew: -2.6533189952896548, variance: 1.6240682355447176, lamda: 1.0146523714065552
Running avgs for agent 1: q_loss: 1.2416660785675049, p_loss: -0.6468779444694519, mean_rew: 0.5142004015404206, variance: 0.24983888179421015, lamda: 1.0147086381912231
Running avgs for agent 2: q_loss: 1.6237897872924805, p_loss: -0.9741058349609375, mean_rew: 0.5156090599469153, variance: 0.2760625179244639, lamda: 1.0146523714065552

steps: 89970, episodes: 3000, mean episode reward: -114.18861857931776, agent episode reward: [-165.8889993099422, 25.85019036531221, 25.85019036531221], time: 44.919
steps: 89970, episodes: 3000, mean episode variance: 1.4694228354971857, agent episode variance: [1.2974189884066583, 0.08426531066559255, 0.08773853642493487], time: 44.92
Running avgs for agent 0: q_loss: 1651.0721435546875, p_loss: 1.3849912881851196, mean_rew: -4.469007380576871, variance: 4.324729961355527, lamda: 1.0442572832107544
Running avgs for agent 1: q_loss: 10.472577095031738, p_loss: -3.00022029876709, mean_rew: 0.7698167085248546, variance: 0.2808843688853085, lamda: 1.0447653532028198
Running avgs for agent 2: q_loss: 12.79594898223877, p_loss: -3.5141072273254395, mean_rew: 0.7693187658914762, variance: 0.29246178808311624, lamda: 1.0442572832107544

steps: 119970, episodes: 4000, mean episode reward: -21.240261694989574, agent episode reward: [-40.78272401864569, 9.77123116182806, 9.77123116182806], time: 44.587
steps: 119970, episodes: 4000, mean episode variance: 1.8396795019395649, agent episode variance: [1.6313942334651947, 0.10871787859499454, 0.09956738987937569], time: 44.587
Running avgs for agent 0: q_loss: 5147.203125, p_loss: 2.0044209957122803, mean_rew: -3.9185823743763475, variance: 5.437980778217316, lamda: 1.074262261390686
Running avgs for agent 1: q_loss: 26.129732131958008, p_loss: -4.234681129455566, mean_rew: 0.6659890440637976, variance: 0.3623929286499818, lamda: 1.0749679803848267
Running avgs for agent 2: q_loss: 27.925670623779297, p_loss: -4.814970016479492, mean_rew: 0.6620682221224382, variance: 0.33189129959791897, lamda: 1.074262261390686

steps: 149970, episodes: 5000, mean episode reward: -13.257864502542262, agent episode reward: [-40.53704358142443, 13.63958953944108, 13.63958953944108], time: 44.795
steps: 149970, episodes: 5000, mean episode variance: 1.7156113497000187, agent episode variance: [1.434241552233696, 0.14952328659780323, 0.13184651086851953], time: 44.795
Running avgs for agent 0: q_loss: 13137.6474609375, p_loss: 3.8764209747314453, mean_rew: -3.3473705451115547, variance: 4.78080517411232, lamda: 1.1042672395706177
Running avgs for agent 1: q_loss: 55.297447204589844, p_loss: -5.221805572509766, mean_rew: 0.6049425745613907, variance: 0.49841095532601076, lamda: 1.1050775051116943
Running avgs for agent 2: q_loss: 53.5325813293457, p_loss: -5.6752238273620605, mean_rew: 0.6030578966198405, variance: 0.4394883695617318, lamda: 1.1042672395706177

steps: 179970, episodes: 6000, mean episode reward: -4.335389006273343, agent episode reward: [-36.07178726020107, 15.868199126963862, 15.868199126963862], time: 46.163
steps: 179970, episodes: 6000, mean episode variance: 0.8731346010230482, agent episode variance: [0.5428405954837799, 0.16788832071423532, 0.16240568482503295], time: 46.163
Running avgs for agent 0: q_loss: 26038.916015625, p_loss: 7.384335041046143, mean_rew: -2.9427530729233924, variance: 1.8094686516125997, lamda: 1.1342722177505493
Running avgs for agent 1: q_loss: 106.2345962524414, p_loss: -6.1689300537109375, mean_rew: 0.5804691394285476, variance: 0.5596277357141177, lamda: 1.1351462602615356
Running avgs for agent 2: q_loss: 101.1647720336914, p_loss: -6.5401458740234375, mean_rew: 0.5803306851720603, variance: 0.5413522827501098, lamda: 1.1342722177505493

steps: 209970, episodes: 7000, mean episode reward: 3.3531318600977364, agent episode reward: [-26.627084833887174, 14.990108346992452, 14.990108346992452], time: 44.816
steps: 209970, episodes: 7000, mean episode variance: 0.7629218578264118, agent episode variance: [0.4158741243481636, 0.16957200599461794, 0.1774757274836302], time: 44.816
Running avgs for agent 0: q_loss: 43974.6015625, p_loss: 9.043465614318848, mean_rew: -2.6819676244114508, variance: 1.3862470811605454, lamda: 1.164277195930481
Running avgs for agent 1: q_loss: 154.46994018554688, p_loss: -6.955263614654541, mean_rew: 0.5726761247747003, variance: 0.5652400199820597, lamda: 1.1651885509490967
Running avgs for agent 2: q_loss: 144.8733673095703, p_loss: -7.2319817543029785, mean_rew: 0.5724235686152307, variance: 0.5915857582787672, lamda: 1.164277195930481

steps: 239970, episodes: 8000, mean episode reward: 2.598563299931032, agent episode reward: [-21.649850709769765, 12.1242070048504, 12.1242070048504], time: 45.486
steps: 239970, episodes: 8000, mean episode variance: 0.701144003227353, agent episode variance: [0.3113261520862579, 0.1853590561002493, 0.20445879504084588], time: 45.486
Running avgs for agent 0: q_loss: 65227.53515625, p_loss: 8.415918350219727, mean_rew: -2.4080346221187128, variance: 1.0377538402875264, lamda: 1.1942821741104126
Running avgs for agent 1: q_loss: 200.5810089111328, p_loss: -7.4870686531066895, mean_rew: 0.5588601945654537, variance: 0.6178635203341643, lamda: 1.1952286958694458
Running avgs for agent 2: q_loss: 189.09695434570312, p_loss: -7.7072601318359375, mean_rew: 0.558478193512873, variance: 0.6815293168028196, lamda: 1.1942821741104126

steps: 269970, episodes: 9000, mean episode reward: -0.18169352985750448, agent episode reward: [-16.70669917847617, 8.262502824309331, 8.262502824309331], time: 45.402
steps: 269970, episodes: 9000, mean episode variance: 0.9951804238371551, agent episode variance: [0.5906393809318542, 0.2058835816271603, 0.19865746127814055], time: 45.403
Running avgs for agent 0: q_loss: 118782.7890625, p_loss: 7.178366661071777, mean_rew: -2.2097540882764846, variance: 1.968797936439514, lamda: 1.2242871522903442
Running avgs for agent 1: q_loss: 258.9804382324219, p_loss: -7.6465744972229, mean_rew: 0.5313310286339099, variance: 0.6862786054238676, lamda: 1.2252455949783325
Running avgs for agent 2: q_loss: 243.25015258789062, p_loss: -7.605410099029541, mean_rew: 0.5316316723158739, variance: 0.6621915375938018, lamda: 1.2242871522903442

steps: 299970, episodes: 10000, mean episode reward: -9.64501502697787, agent episode reward: [-16.750955543835094, 3.552970258428611, 3.552970258428611], time: 45.062
steps: 299970, episodes: 10000, mean episode variance: 0.8912569592446089, agent episode variance: [0.48254795718193055, 0.23323493590205907, 0.17547406616061925], time: 45.063
Running avgs for agent 0: q_loss: 154052.984375, p_loss: 5.6344194412231445, mean_rew: -2.0289841102534867, variance: 1.608493190606435, lamda: 1.2542922496795654
Running avgs for agent 1: q_loss: 355.4439697265625, p_loss: -7.314306735992432, mean_rew: 0.5002039061383109, variance: 0.7774497863401969, lamda: 1.2552505731582642
Running avgs for agent 2: q_loss: 305.4815673828125, p_loss: -6.885100841522217, mean_rew: 0.49664321884686247, variance: 0.5849135538687309, lamda: 1.2542922496795654

steps: 329970, episodes: 11000, mean episode reward: -12.781901756243588, agent episode reward: [-16.7282329645922, 1.9731656041743044, 1.9731656041743044], time: 46.785
steps: 329970, episodes: 11000, mean episode variance: 0.9440310002043844, agent episode variance: [0.5451330723762512, 0.22071825809031725, 0.17817966973781585], time: 46.785
Running avgs for agent 0: q_loss: 205167.078125, p_loss: 4.057845592498779, mean_rew: -1.8956950443317164, variance: 1.8171102412541706, lamda: 1.2842971086502075
Running avgs for agent 1: q_loss: 428.2247619628906, p_loss: -6.627514839172363, mean_rew: 0.458965201948699, variance: 0.7357275269677241, lamda: 1.2852556705474854
Running avgs for agent 2: q_loss: 375.68963623046875, p_loss: -6.048895835876465, mean_rew: 0.4593589135508206, variance: 0.5939322324593862, lamda: 1.2842971086502075

steps: 359970, episodes: 12000, mean episode reward: -15.370127255427185, agent episode reward: [-17.8820554815523, 1.2559641130625574, 1.2559641130625574], time: 45.942
steps: 359970, episodes: 12000, mean episode variance: 1.385559160463512, agent episode variance: [0.9596519035100937, 0.2978664496541023, 0.12804080729931594], time: 45.942
Running avgs for agent 0: q_loss: 251126.34375, p_loss: 2.3632638454437256, mean_rew: -1.7680880482124197, variance: 3.198839678366979, lamda: 1.3143020868301392
Running avgs for agent 1: q_loss: 552.991943359375, p_loss: -6.039463520050049, mean_rew: 0.4213722291508255, variance: 0.9928881655136744, lamda: 1.315260648727417
Running avgs for agent 2: q_loss: 433.38916015625, p_loss: -5.379718780517578, mean_rew: 0.42376853541702125, variance: 0.4268026909977198, lamda: 1.3143020868301392

steps: 389970, episodes: 13000, mean episode reward: -6.365662409908994, agent episode reward: [-18.92686192390418, 6.280599756997592, 6.280599756997592], time: 45.195
steps: 389970, episodes: 13000, mean episode variance: 0.8139336872063577, agent episode variance: [0.46463102078437807, 0.23155237274989485, 0.11775029367208481], time: 45.196
Running avgs for agent 0: q_loss: 256577.28125, p_loss: 1.4791947603225708, mean_rew: -1.6835683060170137, variance: 1.5487700692812603, lamda: 1.34430730342865
Running avgs for agent 1: q_loss: 502.4113464355469, p_loss: -5.537211894989014, mean_rew: 0.3960846325087212, variance: 0.7718412424996495, lamda: 1.345265507698059
Running avgs for agent 2: q_loss: 428.7946472167969, p_loss: -4.801204681396484, mean_rew: 0.397195635537154, variance: 0.39250097890694935, lamda: 1.34430730342865

steps: 419970, episodes: 14000, mean episode reward: -1.2432917198610385, agent episode reward: [-20.23760554242498, 9.497156911281971, 9.497156911281971], time: 45.622
steps: 419970, episodes: 14000, mean episode variance: 1.2923629505559802, agent episode variance: [0.898152820110321, 0.23673258539289235, 0.1574775450527668], time: 45.622
Running avgs for agent 0: q_loss: 336566.03125, p_loss: 0.5967076420783997, mean_rew: -1.603389399503385, variance: 2.99384273370107, lamda: 1.374312162399292
Running avgs for agent 1: q_loss: 483.5871276855469, p_loss: -5.2617106437683105, mean_rew: 0.38850395295352386, variance: 0.7891086179763078, lamda: 1.3752706050872803
Running avgs for agent 2: q_loss: 423.41522216796875, p_loss: -4.523577690124512, mean_rew: 0.38984899375837634, variance: 0.5249251501758894, lamda: 1.374312162399292

steps: 449970, episodes: 15000, mean episode reward: 1.258519751023266, agent episode reward: [-20.652719638839493, 10.955619694931379, 10.955619694931379], time: 45.892
steps: 449970, episodes: 15000, mean episode variance: 1.7311149149239062, agent episode variance: [1.2978180792331695, 0.24942919615656137, 0.1838676395341754], time: 45.893
Running avgs for agent 0: q_loss: 421803.90625, p_loss: -0.055307161062955856, mean_rew: -1.5536731546674252, variance: 4.326060264110565, lamda: 1.404317021369934
Running avgs for agent 1: q_loss: 494.0705261230469, p_loss: -5.203402996063232, mean_rew: 0.3853292198540048, variance: 0.8314306538552045, lamda: 1.405275583267212
Running avgs for agent 2: q_loss: 458.1114501953125, p_loss: -4.48822546005249, mean_rew: 0.3842270181708529, variance: 0.6128921317805847, lamda: 1.404317021369934

steps: 479970, episodes: 16000, mean episode reward: 1.227430862757517, agent episode reward: [-19.157659189480096, 10.192545026118806, 10.192545026118806], time: 46.511
steps: 479970, episodes: 16000, mean episode variance: 2.3873738526441155, agent episode variance: [1.9653779027462006, 0.24546330972388386, 0.17653264017403125], time: 46.511
Running avgs for agent 0: q_loss: 395035.3125, p_loss: -0.1713922917842865, mean_rew: -1.4872050061227378, variance: 6.551259675820669, lamda: 1.4343221187591553
Running avgs for agent 1: q_loss: 488.533447265625, p_loss: -5.270648002624512, mean_rew: 0.38591289825649844, variance: 0.8182110324129462, lamda: 1.4352805614471436
Running avgs for agent 2: q_loss: 446.8372497558594, p_loss: -4.707374572753906, mean_rew: 0.3842737306703961, variance: 0.5884421339134375, lamda: 1.4343221187591553

steps: 509970, episodes: 17000, mean episode reward: 2.432263312456076, agent episode reward: [-18.382350761723412, 10.407307037089748, 10.407307037089748], time: 45.213
steps: 509970, episodes: 17000, mean episode variance: 2.7120221041962504, agent episode variance: [2.2629002010822297, 0.26784098506718873, 0.18128091804683208], time: 45.214
Running avgs for agent 0: q_loss: 483451.9375, p_loss: -1.0356107950210571, mean_rew: -1.4273756125484889, variance: 7.543000670274099, lamda: 1.464327096939087
Running avgs for agent 1: q_loss: 501.1535949707031, p_loss: -5.337285995483398, mean_rew: 0.38203717716720953, variance: 0.8928032835572958, lamda: 1.4652854204177856
Running avgs for agent 2: q_loss: 463.6997985839844, p_loss: -4.928989887237549, mean_rew: 0.3796390766339501, variance: 0.6042697268227736, lamda: 1.464327096939087

steps: 539970, episodes: 18000, mean episode reward: 1.7885405963770034, agent episode reward: [-20.711839287781483, 11.250189942079244, 11.250189942079244], time: 46.1
steps: 539970, episodes: 18000, mean episode variance: 2.2698973003700376, agent episode variance: [1.7453686454296111, 0.31398992070555687, 0.2105387342348695], time: 46.101
Running avgs for agent 0: q_loss: 507003.375, p_loss: -1.1746710538864136, mean_rew: -1.393852567406134, variance: 5.817895484765371, lamda: 1.4943320751190186
Running avgs for agent 1: q_loss: 541.1273803710938, p_loss: -5.349247455596924, mean_rew: 0.37992300721455524, variance: 1.0466330690185228, lamda: 1.4952903985977173
Running avgs for agent 2: q_loss: 477.28411865234375, p_loss: -5.233824253082275, mean_rew: 0.37839188293422515, variance: 0.7017957807828983, lamda: 1.4943320751190186

steps: 569970, episodes: 19000, mean episode reward: 1.5435654610130327, agent episode reward: [-22.707686760906206, 12.12562611095962, 12.12562611095962], time: 45.478
steps: 569970, episodes: 19000, mean episode variance: 2.3831563974022867, agent episode variance: [1.8652333735227584, 0.28827197559922935, 0.2296510482802987], time: 45.479
Running avgs for agent 0: q_loss: 459789.5, p_loss: -1.3464939594268799, mean_rew: -1.353453667214872, variance: 6.217444578409195, lamda: 1.5243369340896606
Running avgs for agent 1: q_loss: 531.0542602539062, p_loss: -5.564798831939697, mean_rew: 0.37951697568265985, variance: 0.9609065853307645, lamda: 1.525295376777649
Running avgs for agent 2: q_loss: 506.1441650390625, p_loss: -5.654138088226318, mean_rew: 0.38115394085149573, variance: 0.7655034942676624, lamda: 1.5243369340896606

steps: 599970, episodes: 20000, mean episode reward: 3.0980425705167876, agent episode reward: [-23.47641162958524, 13.287227100051016, 13.287227100051016], time: 45.602
steps: 599970, episodes: 20000, mean episode variance: 2.4845621092580257, agent episode variance: [1.9137881717681884, 0.3190195835232735, 0.2517543539665639], time: 45.602
Running avgs for agent 0: q_loss: 436988.25, p_loss: -0.984615683555603, mean_rew: -1.3218925506888173, variance: 6.379293905893961, lamda: 1.5543420314788818
Running avgs for agent 1: q_loss: 578.5562744140625, p_loss: -5.798435688018799, mean_rew: 0.37982297052505454, variance: 1.0633986117442449, lamda: 1.5553003549575806
Running avgs for agent 2: q_loss: 542.4061279296875, p_loss: -5.999714374542236, mean_rew: 0.3820852393627338, variance: 0.8391811798885465, lamda: 1.5543420314788818

steps: 629970, episodes: 21000, mean episode reward: 3.623513492142899, agent episode reward: [-28.229159358166246, 15.926336425154572, 15.926336425154572], time: 46.095
steps: 629970, episodes: 21000, mean episode variance: 3.1797211480662226, agent episode variance: [2.523028001308441, 0.345878169298172, 0.3108149774596095], time: 46.096
Running avgs for agent 0: q_loss: 678300.875, p_loss: -0.2313496470451355, mean_rew: -1.3077799268664374, variance: 8.410093337694803, lamda: 1.5843470096588135
Running avgs for agent 1: q_loss: 585.087890625, p_loss: -6.2147216796875, mean_rew: 0.3880234516624194, variance: 1.1529272309939067, lamda: 1.5853053331375122
Running avgs for agent 2: q_loss: 628.4359741210938, p_loss: -6.35695743560791, mean_rew: 0.3868370270311657, variance: 1.036049924865365, lamda: 1.5843470096588135

steps: 659970, episodes: 22000, mean episode reward: 2.934718514057385, agent episode reward: [-29.586672107863635, 16.26069531096051, 16.26069531096051], time: 45.601
steps: 659970, episodes: 22000, mean episode variance: 2.3190006988346576, agent episode variance: [1.7291204197406769, 0.3095870248004794, 0.2802932542935014], time: 45.602
Running avgs for agent 0: q_loss: 698326.0625, p_loss: -0.04536794126033783, mean_rew: -1.280540614389189, variance: 5.763734732468923, lamda: 1.6143518686294556
Running avgs for agent 1: q_loss: 614.0187377929688, p_loss: -6.668357849121094, mean_rew: 0.3925566637454288, variance: 1.0319567493349313, lamda: 1.6153103113174438
Running avgs for agent 2: q_loss: 622.1454467773438, p_loss: -6.71494722366333, mean_rew: 0.39157517087734584, variance: 0.9343108476450046, lamda: 1.6143518686294556

steps: 689970, episodes: 23000, mean episode reward: 3.9341540922822347, agent episode reward: [-29.094713628856567, 16.5144338605694, 16.5144338605694], time: 46.655
steps: 689970, episodes: 23000, mean episode variance: 2.0851071481928227, agent episode variance: [1.392196005344391, 0.3544298470839858, 0.33848129576444624], time: 46.655
Running avgs for agent 0: q_loss: 694605.125, p_loss: -0.38698866963386536, mean_rew: -1.2686318953433977, variance: 4.64065335114797, lamda: 1.6443568468093872
Running avgs for agent 1: q_loss: 694.6101684570312, p_loss: -7.338529586791992, mean_rew: 0.4006234309001943, variance: 1.181432823613286, lamda: 1.645315408706665
Running avgs for agent 2: q_loss: 698.9268798828125, p_loss: -7.1156697273254395, mean_rew: 0.3985944648165735, variance: 1.1282709858814874, lamda: 1.6443568468093872

steps: 719970, episodes: 24000, mean episode reward: 4.429982059485034, agent episode reward: [-29.993351531272243, 17.211666795378637, 17.211666795378637], time: 46.09
steps: 719970, episodes: 24000, mean episode variance: 2.061775851748884, agent episode variance: [1.3134442555904389, 0.3551662705540657, 0.39316532560437917], time: 46.09
Running avgs for agent 0: q_loss: 674201.625, p_loss: -1.0789291858673096, mean_rew: -1.2554211035626917, variance: 4.378147518634796, lamda: 1.6743618249893188
Running avgs for agent 1: q_loss: 751.841064453125, p_loss: -8.084818840026855, mean_rew: 0.40842543766078754, variance: 1.1838875685135524, lamda: 1.6753203868865967
Running avgs for agent 2: q_loss: 748.6326293945312, p_loss: -7.707887172698975, mean_rew: 0.40774778660640304, variance: 1.3105510853479305, lamda: 1.6743618249893188

steps: 749970, episodes: 25000, mean episode reward: 4.480760786739082, agent episode reward: [-31.92061135143866, 18.20068606908887, 18.20068606908887], time: 45.948
steps: 749970, episodes: 25000, mean episode variance: 2.344998620338738, agent episode variance: [1.5470518449544906, 0.39489366360008715, 0.40305311178416015], time: 45.949
Running avgs for agent 0: q_loss: 731776.0625, p_loss: -1.516019344329834, mean_rew: -1.2523656795919766, variance: 5.156839483181636, lamda: 1.70436692237854
Running avgs for agent 1: q_loss: 808.28564453125, p_loss: -8.964306831359863, mean_rew: 0.4165652053741072, variance: 1.3163122120002906, lamda: 1.7053253650665283
Running avgs for agent 2: q_loss: 749.84375, p_loss: -8.554984092712402, mean_rew: 0.4152008406599616, variance: 1.343510372613867, lamda: 1.70436692237854

steps: 779970, episodes: 26000, mean episode reward: 6.589730869593946, agent episode reward: [-30.659770695015954, 18.624750782304954, 18.624750782304954], time: 45.92
steps: 779970, episodes: 26000, mean episode variance: 2.603253838289529, agent episode variance: [1.7770227996110917, 0.4113650240264833, 0.41486601465195416], time: 45.921
Running avgs for agent 0: q_loss: 802626.5625, p_loss: -1.4005717039108276, mean_rew: -1.2403010491883966, variance: 5.923409332036972, lamda: 1.7343717813491821
Running avgs for agent 1: q_loss: 779.8698120117188, p_loss: -9.78490924835205, mean_rew: 0.42340048320079576, variance: 1.3712167467549443, lamda: 1.7353302240371704
Running avgs for agent 2: q_loss: 730.8131103515625, p_loss: -9.337129592895508, mean_rew: 0.4227463453635508, variance: 1.3828867155065139, lamda: 1.7343717813491821

steps: 809970, episodes: 27000, mean episode reward: 6.183966560814637, agent episode reward: [-29.901268662686206, 18.042617611750423, 18.042617611750423], time: 46.736
steps: 809970, episodes: 27000, mean episode variance: 2.30934059432894, agent episode variance: [1.5113274682760238, 0.37197251588851216, 0.4260406101644039], time: 46.736
Running avgs for agent 0: q_loss: 720706.75, p_loss: -1.2335922718048096, mean_rew: -1.234837099266773, variance: 5.037758227586746, lamda: 1.7643768787384033
Running avgs for agent 1: q_loss: 741.8856811523438, p_loss: -10.527403831481934, mean_rew: 0.42853975318104914, variance: 1.2399083862950404, lamda: 1.7653353214263916
Running avgs for agent 2: q_loss: 771.0984497070312, p_loss: -10.052560806274414, mean_rew: 0.4287850938470022, variance: 1.4201353672146797, lamda: 1.7643768787384033

steps: 839970, episodes: 28000, mean episode reward: 5.283359833093282, agent episode reward: [-29.005083805550715, 17.144221819321995, 17.144221819321995], time: 46.191
steps: 839970, episodes: 28000, mean episode variance: 2.599516028575599, agent episode variance: [1.7499977499246597, 0.45264445098489525, 0.39687382766604423], time: 46.191
Running avgs for agent 0: q_loss: 717514.625, p_loss: -1.207959771156311, mean_rew: -1.22627130767037, variance: 5.833325833082199, lamda: 1.7943817377090454
Running avgs for agent 1: q_loss: 815.5427856445312, p_loss: -11.195075035095215, mean_rew: 0.43532585584556, variance: 1.5088148366163174, lamda: 1.7953401803970337
Running avgs for agent 2: q_loss: 769.7930297851562, p_loss: -10.795453071594238, mean_rew: 0.4364603886782594, variance: 1.3229127588868141, lamda: 1.7943817377090454

steps: 869970, episodes: 29000, mean episode reward: 4.147581752722575, agent episode reward: [-30.145706229155273, 17.146643990938923, 17.146643990938923], time: 45.563
steps: 869970, episodes: 29000, mean episode variance: 2.078629085853696, agent episode variance: [1.315681691288948, 0.4027074187248945, 0.3602399758398533], time: 45.563
Running avgs for agent 0: q_loss: 703701.5625, p_loss: -1.067959189414978, mean_rew: -1.220563068504331, variance: 4.385605637629827, lamda: 1.8243868350982666
Running avgs for agent 1: q_loss: 757.2989501953125, p_loss: -11.679840087890625, mean_rew: 0.4388610813438808, variance: 1.3423580624163152, lamda: 1.8253452777862549
Running avgs for agent 2: q_loss: 717.4195556640625, p_loss: -11.401001930236816, mean_rew: 0.43955123119675077, variance: 1.2007999194661776, lamda: 1.8243868350982666

steps: 899970, episodes: 30000, mean episode reward: 5.197024540575478, agent episode reward: [-30.276708531551733, 17.736866536063605, 17.736866536063605], time: 45.541
steps: 899970, episodes: 30000, mean episode variance: 3.1450865000411867, agent episode variance: [2.2532572105526922, 0.4170853794142604, 0.474743910074234], time: 45.542
Running avgs for agent 0: q_loss: 788268.5625, p_loss: -0.8768855929374695, mean_rew: -1.2102375141228612, variance: 7.510857368508975, lamda: 1.8543916940689087
Running avgs for agent 1: q_loss: 821.9439697265625, p_loss: -12.193289756774902, mean_rew: 0.4443897328665118, variance: 1.3902845980475347, lamda: 1.855350136756897
Running avgs for agent 2: q_loss: 813.3267822265625, p_loss: -11.998262405395508, mean_rew: 0.4461867423097289, variance: 1.5824797002474467, lamda: 1.8543916940689087

steps: 929970, episodes: 31000, mean episode reward: 6.000090251062907, agent episode reward: [-30.422453760155644, 18.211272005609274, 18.211272005609274], time: 46.146
steps: 929970, episodes: 31000, mean episode variance: 3.155011368595064, agent episode variance: [2.2895571497678757, 0.4522536638379097, 0.4132005549892783], time: 46.147
Running avgs for agent 0: q_loss: 763376.625, p_loss: -0.3082456588745117, mean_rew: -1.1967174932573612, variance: 7.631857165892919, lamda: 1.8843967914581299
Running avgs for agent 1: q_loss: 777.7766723632812, p_loss: -12.663479804992676, mean_rew: 0.4532507735591353, variance: 1.5075122127930323, lamda: 1.8853552341461182
Running avgs for agent 2: q_loss: 755.6898193359375, p_loss: -12.424537658691406, mean_rew: 0.44927398776242244, variance: 1.3773351832975944, lamda: 1.8843967914581299

steps: 959970, episodes: 32000, mean episode reward: 6.4407747845913255, agent episode reward: [-31.123231063369357, 18.78200292398034, 18.78200292398034], time: 46.902
steps: 959970, episodes: 32000, mean episode variance: 2.8066670469865205, agent episode variance: [2.010216682910919, 0.4228265652731061, 0.373623798802495], time: 46.902
Running avgs for agent 0: q_loss: 876383.875, p_loss: 0.33150067925453186, mean_rew: -1.1944889848245899, variance: 6.7007222763697305, lamda: 1.914401650428772
Running avgs for agent 1: q_loss: 783.6461181640625, p_loss: -13.034871101379395, mean_rew: 0.458541905767186, variance: 1.409421884243687, lamda: 1.9153600931167603
Running avgs for agent 2: q_loss: 754.604248046875, p_loss: -12.806629180908203, mean_rew: 0.45519427005408775, variance: 1.2454126626749833, lamda: 1.914401650428772

steps: 989970, episodes: 33000, mean episode reward: 6.888626904057138, agent episode reward: [-32.97627625194979, 19.932451578003462, 19.932451578003462], time: 46.483
steps: 989970, episodes: 33000, mean episode variance: 3.4118368189148605, agent episode variance: [2.626606677889824, 0.4348396901972592, 0.35039045082777737], time: 46.483
Running avgs for agent 0: q_loss: 785255.625, p_loss: 0.9142397046089172, mean_rew: -1.2019315045896783, variance: 8.75535559296608, lamda: 1.9444067478179932
Running avgs for agent 1: q_loss: 827.7207641601562, p_loss: -13.398463249206543, mean_rew: 0.4629073985082518, variance: 1.449465633990864, lamda: 1.9453651905059814
Running avgs for agent 2: q_loss: 779.8492431640625, p_loss: -13.25071907043457, mean_rew: 0.46196947657074344, variance: 1.1679681694259245, lamda: 1.9444067478179932

steps: 1019970, episodes: 34000, mean episode reward: 6.992490393054675, agent episode reward: [-32.284163392360554, 19.638326892707614, 19.638326892707614], time: 46.579
steps: 1019970, episodes: 34000, mean episode variance: 3.0499291663095356, agent episode variance: [2.3234363993406295, 0.3439352431744337, 0.3825575237944722], time: 46.579
Running avgs for agent 0: q_loss: 899764.5, p_loss: 1.4532102346420288, mean_rew: -1.185165281367318, variance: 7.744787997802098, lamda: 1.9744116067886353
Running avgs for agent 1: q_loss: 765.4108276367188, p_loss: -13.767223358154297, mean_rew: 0.4677870699141149, variance: 1.1464508105814457, lamda: 1.9753700494766235
Running avgs for agent 2: q_loss: 832.3748168945312, p_loss: -13.630263328552246, mean_rew: 0.4690650448029803, variance: 1.275191745981574, lamda: 1.9744116067886353

steps: 1049970, episodes: 35000, mean episode reward: 6.83310749489168, agent episode reward: [-33.143037555045765, 19.988072524968725, 19.988072524968725], time: 46.421
steps: 1049970, episodes: 35000, mean episode variance: 3.5104668918550015, agent episode variance: [2.702925352692604, 0.3487274784371257, 0.4588140607252717], time: 46.422
Running avgs for agent 0: q_loss: 733556.8125, p_loss: 2.2513279914855957, mean_rew: -1.1530876535620442, variance: 9.009751175642014, lamda: 2.004409074783325
Running avgs for agent 1: q_loss: 772.5753173828125, p_loss: -14.076821327209473, mean_rew: 0.4734231444289914, variance: 1.1624249281237522, lamda: 2.0053670406341553
Running avgs for agent 2: q_loss: 855.1285400390625, p_loss: -13.932478904724121, mean_rew: 0.4751703471059479, variance: 1.5293802024175724, lamda: 2.004409074783325

steps: 1079970, episodes: 36000, mean episode reward: 6.835104463942704, agent episode reward: [-33.648539237624256, 20.241821850783474, 20.241821850783474], time: 46.996
steps: 1079970, episodes: 36000, mean episode variance: 2.619501640383154, agent episode variance: [1.9560651510953904, 0.3315426241606474, 0.33189386512711644], time: 46.997
Running avgs for agent 0: q_loss: 462060.15625, p_loss: 0.8682350516319275, mean_rew: -0.9921518655432158, variance: 6.520217170317967, lamda: 2.0343804359436035
Running avgs for agent 1: q_loss: 653.8731079101562, p_loss: -14.11523723602295, mean_rew: 0.46253923905977673, variance: 1.1051420805354912, lamda: 2.0353379249572754
Running avgs for agent 2: q_loss: 631.623779296875, p_loss: -13.921751022338867, mean_rew: 0.4618860868871902, variance: 1.1063128837570548, lamda: 2.0343804359436035

steps: 1109970, episodes: 37000, mean episode reward: 7.311519130818688, agent episode reward: [-27.109861820551618, 17.210690475685155, 17.210690475685155], time: 46.529
steps: 1109970, episodes: 37000, mean episode variance: 1.0817632672488688, agent episode variance: [0.4873387354016304, 0.28719688791036607, 0.30722764393687246], time: 46.529
Running avgs for agent 0: q_loss: 33577.60546875, p_loss: 0.05839534103870392, mean_rew: -0.8976767635808922, variance: 1.624462451338768, lamda: 2.064349889755249
Running avgs for agent 1: q_loss: 548.8560180664062, p_loss: -14.085657119750977, mean_rew: 0.46289206412915973, variance: 0.9573229597012202, lamda: 2.0653069019317627
Running avgs for agent 2: q_loss: 524.0274658203125, p_loss: -13.919200897216797, mean_rew: 0.4633080359247212, variance: 1.0240921464562416, lamda: 2.064349889755249

steps: 1139970, episodes: 38000, mean episode reward: 7.83481166516078, agent episode reward: [-21.789779479311058, 14.81229557223592, 14.81229557223592], time: 46.464
steps: 1139970, episodes: 38000, mean episode variance: 1.0429087103791534, agent episode variance: [0.4915594082176685, 0.2929379908591509, 0.2584113113023341], time: 46.465
Running avgs for agent 0: q_loss: 37581.4921875, p_loss: 0.1867656707763672, mean_rew: -0.8817862034766434, variance: 1.6385313607255618, lamda: 2.0943188667297363
Running avgs for agent 1: q_loss: 518.7470703125, p_loss: -13.871285438537598, mean_rew: 0.46774779362463714, variance: 0.976459969530503, lamda: 2.095276355743408
Running avgs for agent 2: q_loss: 485.1875915527344, p_loss: -13.715937614440918, mean_rew: 0.46866777540036625, variance: 0.8613710376744469, lamda: 2.0943188667297363

steps: 1169970, episodes: 39000, mean episode reward: 8.6955676191694, agent episode reward: [-21.578117188667257, 15.136842403918328, 15.136842403918328], time: 47.071
steps: 1169970, episodes: 39000, mean episode variance: 0.9654638952277601, agent episode variance: [0.4235368269830942, 0.242291835822165, 0.29963523242250084], time: 47.072
Running avgs for agent 0: q_loss: 54544.953125, p_loss: 0.4927694797515869, mean_rew: -0.8628954540472081, variance: 1.4117894232769808, lamda: 2.1242880821228027
Running avgs for agent 1: q_loss: 480.5812072753906, p_loss: -13.572301864624023, mean_rew: 0.4698668115286561, variance: 0.80763945274055, lamda: 2.1252453327178955
Running avgs for agent 2: q_loss: 474.8125915527344, p_loss: -13.391608238220215, mean_rew: 0.4698540568864963, variance: 0.9987841080750028, lamda: 2.1242880821228027

steps: 1199970, episodes: 40000, mean episode reward: 9.678557907314673, agent episode reward: [-21.59934592192153, 15.638951914618103, 15.638951914618103], time: 45.996
steps: 1199970, episodes: 40000, mean episode variance: 0.9594353705979883, agent episode variance: [0.4420948444977403, 0.2402519088499248, 0.2770886172503233], time: 45.996
Running avgs for agent 0: q_loss: 7728.90087890625, p_loss: 1.1283739805221558, mean_rew: -0.8499519429932688, variance: 1.4736494816591341, lamda: 2.154257297515869
Running avgs for agent 1: q_loss: 450.1148986816406, p_loss: -13.27846908569336, mean_rew: 0.46929633159671424, variance: 0.800839696166416, lamda: 2.155214786529541
Running avgs for agent 2: q_loss: 440.40020751953125, p_loss: -13.059992790222168, mean_rew: 0.46775674944426177, variance: 0.9236287241677443, lamda: 2.154257297515869

steps: 1229970, episodes: 41000, mean episode reward: 9.46151364857343, agent episode reward: [-20.69210671610637, 15.0768101823399, 15.0768101823399], time: 47.183
steps: 1229970, episodes: 41000, mean episode variance: 0.9014997391700744, agent episode variance: [0.41552137824893, 0.23317473862320184, 0.2528036222979426], time: 47.183
Running avgs for agent 0: q_loss: 4039.55615234375, p_loss: 1.885599970817566, mean_rew: -0.8447835590845055, variance: 1.3850712608297666, lamda: 2.1842265129089355
Running avgs for agent 1: q_loss: 450.8165588378906, p_loss: -13.038212776184082, mean_rew: 0.46950979408320714, variance: 0.7772491287440062, lamda: 2.1851840019226074
Running avgs for agent 2: q_loss: 448.0924987792969, p_loss: -12.835481643676758, mean_rew: 0.4702127774637463, variance: 0.8426787409931421, lamda: 2.1842265129089355

steps: 1259970, episodes: 42000, mean episode reward: 9.687442695546196, agent episode reward: [-20.351624685944238, 15.019533690745215, 15.019533690745215], time: 46.463
steps: 1259970, episodes: 42000, mean episode variance: 0.9102505468614399, agent episode variance: [0.295038494348526, 0.26106546341255304, 0.3541465891003609], time: 46.464
Running avgs for agent 0: q_loss: 3835.62841796875, p_loss: 2.722566843032837, mean_rew: -0.8466162238342053, variance: 0.98346164782842, lamda: 2.214195966720581
Running avgs for agent 1: q_loss: 467.32568359375, p_loss: -12.845151901245117, mean_rew: 0.47278508541036773, variance: 0.8702182113751769, lamda: 2.2151529788970947
Running avgs for agent 2: q_loss: 492.1045227050781, p_loss: -12.624076843261719, mean_rew: 0.4723803502294173, variance: 1.1804886303345363, lamda: 2.214195966720581

steps: 1289970, episodes: 43000, mean episode reward: 9.709862454451356, agent episode reward: [-20.583332358408477, 15.146597406429915, 15.146597406429915], time: 46.433
steps: 1289970, episodes: 43000, mean episode variance: 0.8523365332633257, agent episode variance: [0.33477488680183887, 0.22751789892464877, 0.29004374753683804], time: 46.433
Running avgs for agent 0: q_loss: 3705.160888671875, p_loss: 3.5635576248168945, mean_rew: -0.8471547438826813, variance: 1.115916289339463, lamda: 2.2441649436950684
Running avgs for agent 1: q_loss: 488.3052673339844, p_loss: -12.729071617126465, mean_rew: 0.48113825775363056, variance: 0.7583929964154958, lamda: 2.2451224327087402
Running avgs for agent 2: q_loss: 507.81707763671875, p_loss: -12.496637344360352, mean_rew: 0.48105805647368166, variance: 0.9668124917894602, lamda: 2.2441649436950684

steps: 1319970, episodes: 44000, mean episode reward: 9.444598524869555, agent episode reward: [-21.47798406726515, 15.461291296067353, 15.461291296067353], time: 47.2
steps: 1319970, episodes: 44000, mean episode variance: 0.8090761371552945, agent episode variance: [0.25092190158367156, 0.28107274570316076, 0.2770814898684621], time: 47.2
Running avgs for agent 0: q_loss: 3367.641357421875, p_loss: 4.329334735870361, mean_rew: -0.8546257291221321, variance: 0.8364063386122386, lamda: 2.2741341590881348
Running avgs for agent 1: q_loss: 506.23394775390625, p_loss: -12.643275260925293, mean_rew: 0.4929894831150338, variance: 0.9369091523438692, lamda: 2.2750916481018066
Running avgs for agent 2: q_loss: 520.7337646484375, p_loss: -12.419347763061523, mean_rew: 0.49254634812190734, variance: 0.923604966228207, lamda: 2.2741341590881348

steps: 1349970, episodes: 45000, mean episode reward: 9.986134417209524, agent episode reward: [-21.49199650132052, 15.73906545926502, 15.73906545926502], time: 45.872
steps: 1349970, episodes: 45000, mean episode variance: 0.8702249157354236, agent episode variance: [0.33692990730702876, 0.2498621686473489, 0.2834328397810459], time: 45.873
Running avgs for agent 0: q_loss: 3901.862548828125, p_loss: 4.71744966506958, mean_rew: -0.8596217691125319, variance: 1.1230996910234292, lamda: 2.3041036128997803
Running avgs for agent 1: q_loss: 510.4035339355469, p_loss: -12.58572769165039, mean_rew: 0.5056013155193204, variance: 0.832873895491163, lamda: 2.305060863494873
Running avgs for agent 2: q_loss: 524.8025512695312, p_loss: -12.413141250610352, mean_rew: 0.5062679406183276, variance: 0.9447761326034864, lamda: 2.3041036128997803

steps: 1379970, episodes: 46000, mean episode reward: 10.018701917097708, agent episode reward: [-19.977250864862167, 14.997976390979936, 14.997976390979936], time: 46.546
steps: 1379970, episodes: 46000, mean episode variance: 0.8919719686359167, agent episode variance: [0.31400457325577735, 0.25171542520076035, 0.326251970179379], time: 46.546
Running avgs for agent 0: q_loss: 3187.50634765625, p_loss: 4.957859516143799, mean_rew: -0.8615313462152885, variance: 1.046681910852591, lamda: 2.3340725898742676
Running avgs for agent 1: q_loss: 511.64532470703125, p_loss: -12.556244850158691, mean_rew: 0.520449447685137, variance: 0.8390514173358679, lamda: 2.3350298404693604
Running avgs for agent 2: q_loss: 533.457275390625, p_loss: -12.419707298278809, mean_rew: 0.5207916242733448, variance: 1.0875065672645967, lamda: 2.3340725898742676

steps: 1409970, episodes: 47000, mean episode reward: 10.373788938352886, agent episode reward: [-20.480197301279535, 15.42699311981621, 15.42699311981621], time: 46.457
steps: 1409970, episodes: 47000, mean episode variance: 0.8247765438444913, agent episode variance: [0.3414010248556733, 0.24182197080925108, 0.24155354817956687], time: 46.457
Running avgs for agent 0: q_loss: 2516.694091796875, p_loss: 5.053116798400879, mean_rew: -0.8618139634936787, variance: 1.1380034161855777, lamda: 2.364042043685913
Running avgs for agent 1: q_loss: 507.2376708984375, p_loss: -12.480555534362793, mean_rew: 0.5276735734840647, variance: 0.8060732360308369, lamda: 2.364999294281006
Running avgs for agent 2: q_loss: 528.1027221679688, p_loss: -12.409255981445312, mean_rew: 0.5283938913026105, variance: 0.8051784939318896, lamda: 2.364042043685913

steps: 1439970, episodes: 48000, mean episode reward: 11.013779601624636, agent episode reward: [-19.000415937563325, 15.00709776959398, 15.00709776959398], time: 47.288
steps: 1439970, episodes: 48000, mean episode variance: 0.8582500786818564, agent episode variance: [0.32611937531083823, 0.24145997259393334, 0.29067073077708483], time: 47.289
Running avgs for agent 0: q_loss: 2010.484619140625, p_loss: 5.256213188171387, mean_rew: -0.8610610082418847, variance: 1.0870645843694609, lamda: 2.3940110206604004
Running avgs for agent 1: q_loss: 509.4082336425781, p_loss: -12.425247192382812, mean_rew: 0.5333594892331037, variance: 0.8048665753131111, lamda: 2.3949685096740723
Running avgs for agent 2: q_loss: 540.721923828125, p_loss: -12.407764434814453, mean_rew: 0.5337042223159746, variance: 0.9689024359236161, lamda: 2.3940110206604004

steps: 1469970, episodes: 49000, mean episode reward: 9.59659343333217, agent episode reward: [-16.638105971066427, 13.117349702199299, 13.117349702199299], time: 46.1
steps: 1469970, episodes: 49000, mean episode variance: 0.81497140436247, agent episode variance: [0.34258693673461676, 0.25779759395495055, 0.21458687367290258], time: 46.101
Running avgs for agent 0: q_loss: 1970.206298828125, p_loss: 5.639329433441162, mean_rew: -0.8590958352091063, variance: 1.1419564557820558, lamda: 2.423980236053467
Running avgs for agent 1: q_loss: 522.0794677734375, p_loss: -12.321992874145508, mean_rew: 0.5364875688471245, variance: 0.8593253131831685, lamda: 2.4249374866485596
Running avgs for agent 2: q_loss: 545.5611572265625, p_loss: -12.370502471923828, mean_rew: 0.5372604472225027, variance: 0.7152895789096753, lamda: 2.423980236053467

steps: 1499970, episodes: 50000, mean episode reward: 9.768038215643875, agent episode reward: [-19.10716543750035, 14.437601826572111, 14.437601826572111], time: 46.626
steps: 1499970, episodes: 50000, mean episode variance: 0.89906333136186, agent episode variance: [0.31989676162600517, 0.2797588047161698, 0.299407765019685], time: 46.627
Running avgs for agent 0: q_loss: 1914.5150146484375, p_loss: 5.956577777862549, mean_rew: -0.8570586754795168, variance: 1.0663225387533506, lamda: 2.4539496898651123
Running avgs for agent 1: q_loss: 544.5083618164062, p_loss: -12.153979301452637, mean_rew: 0.5399435284734453, variance: 0.9325293490538994, lamda: 2.454906702041626
Running avgs for agent 2: q_loss: 581.0768432617188, p_loss: -12.258218765258789, mean_rew: 0.5394504056700951, variance: 0.9980258833989502, lamda: 2.4539496898651123

steps: 1529970, episodes: 51000, mean episode reward: 9.997047003078984, agent episode reward: [-23.66242356685168, 16.82973528496533, 16.82973528496533], time: 46.722
steps: 1529970, episodes: 51000, mean episode variance: 0.8638631730116904, agent episode variance: [0.3175431991666555, 0.25215741960704324, 0.2941625542379916], time: 46.723
Running avgs for agent 0: q_loss: 2142.999267578125, p_loss: 6.308460235595703, mean_rew: -0.860219659837169, variance: 1.0584773305555184, lamda: 2.4839186668395996
Running avgs for agent 1: q_loss: 564.220458984375, p_loss: -11.980806350708008, mean_rew: 0.5454315739699479, variance: 0.8405247320234775, lamda: 2.4848763942718506
Running avgs for agent 2: q_loss: 618.0492553710938, p_loss: -12.119428634643555, mean_rew: 0.546134396953533, variance: 0.9805418474599719, lamda: 2.4839186668395996

steps: 1559970, episodes: 52000, mean episode reward: 10.473493659347401, agent episode reward: [-22.64735418937137, 16.560423924359384, 16.560423924359384], time: 46.757
steps: 1559970, episodes: 52000, mean episode variance: 0.8788693244028837, agent episode variance: [0.33760660704225304, 0.24361572165228426, 0.29764699570834635], time: 46.757
Running avgs for agent 0: q_loss: 2425.138427734375, p_loss: 6.613806247711182, mean_rew: -0.8626963301929669, variance: 1.12535535680751, lamda: 2.513887882232666
Running avgs for agent 1: q_loss: 602.8881225585938, p_loss: -11.821527481079102, mean_rew: 0.5513997924121415, variance: 0.8120524055076143, lamda: 2.514845371246338
Running avgs for agent 2: q_loss: 652.9996337890625, p_loss: -11.973431587219238, mean_rew: 0.549608755984684, variance: 0.9921566523611546, lamda: 2.513887882232666

steps: 1589970, episodes: 53000, mean episode reward: 11.24870089328277, agent episode reward: [-22.965730728775092, 17.10721581102893, 17.10721581102893], time: 46.256
steps: 1589970, episodes: 53000, mean episode variance: 0.8205217174887657, agent episode variance: [0.31630391120910645, 0.24335709266364575, 0.26086071361601354], time: 46.256
Running avgs for agent 0: q_loss: 2320.1533203125, p_loss: 6.713081359863281, mean_rew: -0.8635622668311066, variance: 1.0543463706970215, lamda: 2.5438570976257324
Running avgs for agent 1: q_loss: 630.69091796875, p_loss: -11.69393253326416, mean_rew: 0.5555731809854395, variance: 0.8111903088788192, lamda: 2.544814348220825
Running avgs for agent 2: q_loss: 675.38037109375, p_loss: -11.865623474121094, mean_rew: 0.5545075456900164, variance: 0.8695357120533784, lamda: 2.5438570976257324

steps: 1619970, episodes: 54000, mean episode reward: 10.374287736395663, agent episode reward: [-22.18950815114187, 16.281897943768765, 16.281897943768765], time: 47.492
steps: 1619970, episodes: 54000, mean episode variance: 0.8333484885543585, agent episode variance: [0.31631787012517454, 0.266456262357533, 0.25057435607165096], time: 47.493
Running avgs for agent 0: q_loss: 2376.23291015625, p_loss: 6.512141227722168, mean_rew: -0.8605646096429499, variance: 1.0543929004172483, lamda: 2.573826551437378
Running avgs for agent 1: q_loss: 660.5197143554688, p_loss: -11.588141441345215, mean_rew: 0.5593283953061022, variance: 0.8881875411917766, lamda: 2.5747838020324707
Running avgs for agent 2: q_loss: 686.4321899414062, p_loss: -11.794394493103027, mean_rew: 0.5582497730253367, variance: 0.8352478535721699, lamda: 2.573826551437378

steps: 1649970, episodes: 55000, mean episode reward: 10.388328247039697, agent episode reward: [-21.84061423322001, 16.114471240129856, 16.114471240129856], time: 46.167
steps: 1649970, episodes: 55000, mean episode variance: 0.9422225313633681, agent episode variance: [0.33035032099485395, 0.2647918080240488, 0.34708040234446524], time: 46.167
Running avgs for agent 0: q_loss: 2220.17626953125, p_loss: 6.27116584777832, mean_rew: -0.8549661915504035, variance: 1.1011677366495132, lamda: 2.6037955284118652
Running avgs for agent 1: q_loss: 653.2817993164062, p_loss: -11.457355499267578, mean_rew: 0.5583411175435534, variance: 0.8826393600801626, lamda: 2.604753017425537
Running avgs for agent 2: q_loss: 705.6279907226562, p_loss: -11.729743003845215, mean_rew: 0.558066680415765, variance: 1.156934674481551, lamda: 2.6037955284118652

steps: 1679970, episodes: 56000, mean episode reward: 10.517275115106399, agent episode reward: [-22.39835091464952, 16.45781301487796, 16.45781301487796], time: 47.411
steps: 1679970, episodes: 56000, mean episode variance: 0.8321591157317162, agent episode variance: [0.28454722198843957, 0.302072178915143, 0.24553971482813358], time: 47.412
Running avgs for agent 0: q_loss: 2625.955322265625, p_loss: 5.956891059875488, mean_rew: -0.8453220113886498, variance: 0.9484907399614652, lamda: 2.6337647438049316
Running avgs for agent 1: q_loss: 658.3361206054688, p_loss: -11.354215621948242, mean_rew: 0.5583918632181165, variance: 1.0069072630504767, lamda: 2.6347219944000244
Running avgs for agent 2: q_loss: 679.472412109375, p_loss: -11.663228034973145, mean_rew: 0.5586119567801326, variance: 0.8184657160937786, lamda: 2.6337647438049316

steps: 1709970, episodes: 57000, mean episode reward: 10.827300669756232, agent episode reward: [-21.24598082794085, 16.03664074884854, 16.03664074884854], time: 47.351
steps: 1709970, episodes: 57000, mean episode variance: 0.7674348625987768, agent episode variance: [0.2682067651376128, 0.2624576855003834, 0.2367704119607806], time: 47.351
Running avgs for agent 0: q_loss: 2415.32275390625, p_loss: 5.690572261810303, mean_rew: -0.8405676537813355, variance: 0.8940225504587094, lamda: 2.663734197616577
Running avgs for agent 1: q_loss: 622.8729248046875, p_loss: -11.272680282592773, mean_rew: 0.5573411799835901, variance: 0.8748589516679446, lamda: 2.664691209793091
Running avgs for agent 2: q_loss: 673.8019409179688, p_loss: -11.588163375854492, mean_rew: 0.5577571280928061, variance: 0.7892347065359354, lamda: 2.663734197616577

steps: 1739970, episodes: 58000, mean episode reward: 10.665260355469593, agent episode reward: [-22.256684482931654, 16.460972419200626, 16.460972419200626], time: 46.107
steps: 1739970, episodes: 58000, mean episode variance: 0.7932950243987144, agent episode variance: [0.29997120397537946, 0.24200129783153534, 0.2513225225917995], time: 46.108
Running avgs for agent 0: q_loss: 2404.587646484375, p_loss: 5.302496910095215, mean_rew: -0.8321062838019508, variance: 0.9999040132512649, lamda: 2.6937031745910645
Running avgs for agent 1: q_loss: 605.6701049804688, p_loss: -11.242511749267578, mean_rew: 0.5574717546813738, variance: 0.8066709927717844, lamda: 2.6946604251861572
Running avgs for agent 2: q_loss: 667.9246826171875, p_loss: -11.502752304077148, mean_rew: 0.5565601980231151, variance: 0.837741741972665, lamda: 2.6937031745910645

steps: 1769970, episodes: 59000, mean episode reward: 10.49674557394077, agent episode reward: [-21.308351485858577, 15.902548529899672, 15.902548529899672], time: 47.354
steps: 1769970, episodes: 59000, mean episode variance: 0.7440526223294437, agent episode variance: [0.24876003563404084, 0.24179568571597337, 0.2534969009794295], time: 47.354
Running avgs for agent 0: q_loss: 2428.875244140625, p_loss: 4.913125991821289, mean_rew: -0.8219362076756466, variance: 0.8292001187801361, lamda: 2.723672389984131
Running avgs for agent 1: q_loss: 577.927001953125, p_loss: -11.18911361694336, mean_rew: 0.553690875401032, variance: 0.8059856190532446, lamda: 2.7246296405792236
Running avgs for agent 2: q_loss: 635.7036743164062, p_loss: -11.417129516601562, mean_rew: 0.5544623002375896, variance: 0.8449896699314317, lamda: 2.723672389984131/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)


steps: 1799970, episodes: 60000, mean episode reward: 9.504793891791815, agent episode reward: [-20.853021966929386, 15.178907929360602, 15.178907929360602], time: 46.827
steps: 1799970, episodes: 60000, mean episode variance: 0.7138795832879842, agent episode variance: [0.22127837593853475, 0.26214122036471965, 0.23045998698472978], time: 46.827
Running avgs for agent 0: q_loss: 2341.23876953125, p_loss: 4.604904651641846, mean_rew: -0.8120766675583732, variance: 0.7375945864617824, lamda: 2.7536416053771973
Running avgs for agent 1: q_loss: 569.49951171875, p_loss: -11.173015594482422, mean_rew: 0.5521355824525632, variance: 0.8738040678823987, lamda: 2.754599094390869
Running avgs for agent 2: q_loss: 620.9649047851562, p_loss: -11.338274955749512, mean_rew: 0.5523781155778785, variance: 0.7681999566157659, lamda: 2.7536416053771973

...Finished total of 60001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 29970, episodes: 1000, mean episode reward: 9.398724591086044, agent episode reward: [-21.384005688224367, 15.391365139655205, 15.391365139655205], time: 31.52
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 31.521
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 59970, episodes: 2000, mean episode reward: 9.01056370256134, agent episode reward: [-21.26856020074226, 15.139561951651801, 15.139561951651801], time: 40.421
steps: 59970, episodes: 2000, mean episode variance: 0.8866580351628364, agent episode variance: [0.1061458391584456, 0.00018074101209640504, 0.7803314549922943], time: 40.421
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.7127249266953659, variance: 0.3635131478028959, lamda: 2.768676519393921
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.5119625310623712, variance: 0.0006189760688233049, lamda: 2.7696337699890137
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.5117262761150564, variance: 2.672367811203003, lamda: 2.768676519393921

...Finished total of 2001 episodes with the fixed policy.
