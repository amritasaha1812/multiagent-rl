WARNING: Logging before flag parsing goes to stderr.
W0903 16:48:48.413902 4560504256 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0903 16:48:48.414228 4560504256 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-09-03 16:48:48.414691: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0903 16:48:48.417165 4560504256 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0903 16:48:48.419313 4560504256 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0903 16:48:48.419494 4560504256 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0903 16:48:48.419595 4560504256 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0903 16:48:48.811013 4560504256 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0903 16:48:48.970598 4560504256 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0903 16:48:48.977571 4560504256 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0903 16:48:49.592288 4560504256 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation True
adversary agent:  -0.25
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 29970, episodes: 1000, mean episode reward: -23.881041358705502, agent episode reward: [-43.71137747732605, 9.915168059310275, 9.915168059310275], time: 25.081
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 25.081
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 59970, episodes: 2000, mean episode reward: -71.70996664479185, agent episode reward: [-100.43047412494857, 14.360253740078361, 14.360253740078361], time: 48.233
steps: 59970, episodes: 2000, mean episode variance: 1.046911748790415, agent episode variance: [0.8052225096821785, 0.09730099544790573, 0.14438824366033076], time: 48.233
Running avgs for agent 0: q_loss: 12.860795021057129, p_loss: -1.3462480306625366, mean_rew: -1.6113214992189613, variance: 2.7576115131378174, lamda: 1.0130077600479126
Running avgs for agent 1: q_loss: 0.4041300415992737, p_loss: -0.16053152084350586, mean_rew: 0.28339107950422215, variance: 0.3332225871503621, lamda: 1.0087372064590454
Running avgs for agent 2: q_loss: 0.57398921251297, p_loss: -0.7164240479469299, mean_rew: 0.28581174070629145, variance: 0.49448028206825256, lamda: 1.017702341079712

steps: 89970, episodes: 3000, mean episode reward: -187.99493586372242, agent episode reward: [-310.2978532294212, 61.1514586828494, 61.1514586828494], time: 47.598
steps: 89970, episodes: 3000, mean episode variance: 1.0137782143820078, agent episode variance: [0.6056074381172657, 0.20439337572641672, 0.2037774005383253], time: 47.599
Running avgs for agent 0: q_loss: 53.59265899658203, p_loss: -2.475019931793213, mean_rew: -3.9046993022803145, variance: 2.018691460390886, lamda: 1.0428822040557861
Running avgs for agent 1: q_loss: 9.476223945617676, p_loss: -3.1654484272003174, mean_rew: 0.7131706789811925, variance: 0.681311252421389, lamda: 1.0541800260543823
Running avgs for agent 2: q_loss: 11.714717864990234, p_loss: -3.8053760528564453, mean_rew: 0.7119769392757842, variance: 0.6792580017944178, lamda: 1.06232488155365

steps: 119970, episodes: 4000, mean episode reward: -106.05097021771539, agent episode reward: [-171.6874052683876, 32.818217525336124, 32.818217525336124], time: 47.04
steps: 119970, episodes: 4000, mean episode variance: 1.724200475357473, agent episode variance: [1.227191211760044, 0.24046433059871197, 0.25654493299871683], time: 47.04
Running avgs for agent 0: q_loss: 58.28138732910156, p_loss: -1.7740849256515503, mean_rew: -5.475033909614642, variance: 4.09063720703125, lamda: 1.0731065273284912
Running avgs for agent 1: q_loss: 36.271141052246094, p_loss: -7.196135997772217, mean_rew: 1.0316008501251388, variance: 0.8015477686623732, lamda: 1.0908751487731934
Running avgs for agent 2: q_loss: 42.4730224609375, p_loss: -7.931251525878906, mean_rew: 1.0372022809630603, variance: 0.8551497766623894, lamda: 1.097785234451294

steps: 149970, episodes: 5000, mean episode reward: -112.75620052910698, agent episode reward: [-147.341205034549, 17.292502252721018, 17.292502252721018], time: 47.148
steps: 149970, episodes: 5000, mean episode variance: 2.499369940171018, agent episode variance: [1.937984646320343, 0.26124688096530735, 0.30013841288536786], time: 47.148
Running avgs for agent 0: q_loss: 59.74897003173828, p_loss: -0.9258323311805725, mean_rew: -4.934919966012254, variance: 6.459949016571045, lamda: 1.1032272577285767
Running avgs for agent 1: q_loss: 72.38214111328125, p_loss: -8.506786346435547, mean_rew: 0.9060205094061068, variance: 0.8708229365510245, lamda: 1.1239045858383179
Running avgs for agent 2: q_loss: 94.63500213623047, p_loss: -9.476668357849121, mean_rew: 0.9115477291923333, variance: 1.0004613762845596, lamda: 1.1303449869155884

steps: 179970, episodes: 6000, mean episode reward: -163.9598366261991, agent episode reward: [-200.999987367083, 18.52007537044197, 18.52007537044197], time: 47.282
steps: 179970, episodes: 6000, mean episode variance: 2.5966611083373428, agent episode variance: [1.9582061002254485, 0.29511225559562443, 0.34334275251626967], time: 47.282
Running avgs for agent 0: q_loss: 172.0205078125, p_loss: -0.10416670143604279, mean_rew: -5.4652761816046285, variance: 6.527353763580322, lamda: 1.1333036422729492
Running avgs for agent 1: q_loss: 123.56208038330078, p_loss: -10.835840225219727, mean_rew: 0.9057193543075734, variance: 0.9837075186520815, lamda: 1.1556727886199951
Running avgs for agent 2: q_loss: 163.54904174804688, p_loss: -11.862239837646484, mean_rew: 0.9070348133536705, variance: 1.144475841720899, lamda: 1.1618537902832031

steps: 209970, episodes: 7000, mean episode reward: -70.92186091667226, agent episode reward: [-41.346586403396564, -14.787637256637854, -14.787637256637854], time: 46.776
steps: 209970, episodes: 7000, mean episode variance: 2.632251599676907, agent episode variance: [1.9184320826530457, 0.39790814340114594, 0.3159113736227155], time: 46.777
Running avgs for agent 0: q_loss: 357.09307861328125, p_loss: 0.6000961065292358, mean_rew: -5.104774173561831, variance: 6.394773483276367, lamda: 1.1633493900299072
Running avgs for agent 1: q_loss: 179.359130859375, p_loss: -11.791282653808594, mean_rew: 0.7537406167410067, variance: 1.3263604780038198, lamda: 1.1868077516555786
Running avgs for agent 2: q_loss: 215.87318420410156, p_loss: -12.654976844787598, mean_rew: 0.7555763172666433, variance: 1.0530379120757183, lamda: 1.192826747894287

steps: 239970, episodes: 8000, mean episode reward: -51.99296716808615, agent episode reward: [-42.77292708262099, -4.610020042732581, -4.610020042732581], time: 47.975
steps: 239970, episodes: 8000, mean episode variance: 2.137122772708535, agent episode variance: [1.378032108232379, 0.4244337402358651, 0.33465692424029114], time: 47.976
Running avgs for agent 0: q_loss: 1048.59228515625, p_loss: 2.7435383796691895, mean_rew: -4.614410424595941, variance: 4.593440360774596, lamda: 1.1933902502059937
Running avgs for agent 1: q_loss: 245.43959045410156, p_loss: -11.993266105651855, mean_rew: 0.6034044708411731, variance: 1.4147791341195504, lamda: 1.2175734043121338
Running avgs for agent 2: q_loss: 276.3218994140625, p_loss: -12.740402221679688, mean_rew: 0.6052689647187456, variance: 1.1155230808009704, lamda: 1.223484754562378

steps: 269970, episodes: 9000, mean episode reward: -31.20118346682711, agent episode reward: [-42.23561453984335, 5.517215536508115, 5.517215536508115], time: 46.98
steps: 269970, episodes: 9000, mean episode variance: 1.3524695240445435, agent episode variance: [0.5901856244504452, 0.3554309965185821, 0.40685290307551625], time: 46.981
Running avgs for agent 0: q_loss: 6607.63427734375, p_loss: 6.285187721252441, mean_rew: -4.258290614152354, variance: 1.9672854148348173, lamda: 1.2234166860580444
Running avgs for agent 1: q_loss: 314.27557373046875, p_loss: -12.458366394042969, mean_rew: 0.5368755629001895, variance: 1.1847699883952736, lamda: 1.248105525970459
Running avgs for agent 2: q_loss: 388.3072204589844, p_loss: -13.202923774719238, mean_rew: 0.5403533210636738, variance: 1.356176343585054, lamda: 1.253942608833313

steps: 299970, episodes: 10000, mean episode reward: -23.3489006755133, agent episode reward: [-36.000452636844145, 6.325775980665423, 6.325775980665423], time: 48.278
steps: 299970, episodes: 10000, mean episode variance: 1.3817855017706753, agent episode variance: [0.5412350262999535, 0.4127004623562098, 0.427850013114512], time: 48.279
Running avgs for agent 0: q_loss: 18112.64453125, p_loss: 8.736705780029297, mean_rew: -3.913675004353713, variance: 1.8041167543331782, lamda: 1.2534217834472656
Running avgs for agent 1: q_loss: 404.78985595703125, p_loss: -12.75045108795166, mean_rew: 0.5035644036582794, variance: 1.3756682078540325, lamda: 1.2784823179244995
Running avgs for agent 2: q_loss: 522.45703125, p_loss: -13.498806953430176, mean_rew: 0.5007700047693936, variance: 1.4261667103817066, lamda: 1.284267544746399

steps: 329970, episodes: 11000, mean episode reward: -33.10895601417354, agent episode reward: [-40.28149301202563, 3.5862684989260405, 3.5862684989260405], time: 47.016
steps: 329970, episodes: 11000, mean episode variance: 1.3945894858278334, agent episode variance: [0.4142715680003166, 0.3930930499173701, 0.5872248679101467], time: 47.016
Running avgs for agent 0: q_loss: 30464.80078125, p_loss: 11.31240463256836, mean_rew: -3.6938221252252017, variance: 1.380905226667722, lamda: 1.2834267616271973
Running avgs for agent 1: q_loss: 442.7762451171875, p_loss: -12.954524993896484, mean_rew: 0.4758867305971698, variance: 1.3103101663912335, lamda: 1.3087528944015503
Running avgs for agent 2: q_loss: 590.5861206054688, p_loss: -13.662487030029297, mean_rew: 0.47255756008658506, variance: 1.9574162263671557, lamda: 1.3145005702972412

steps: 359970, episodes: 12000, mean episode reward: -23.27267651953671, agent episode reward: [-50.645338705365326, 13.686331092914314, 13.686331092914314], time: 48.459
steps: 359970, episodes: 12000, mean episode variance: 1.81638339715451, agent episode variance: [0.7185858952403068, 0.5777481203898788, 0.5200493815243244], time: 48.46
Running avgs for agent 0: q_loss: 42483.01171875, p_loss: 13.257532119750977, mean_rew: -3.499797013036886, variance: 2.3952863174676895, lamda: 1.3134318590164185
Running avgs for agent 1: q_loss: 532.9417724609375, p_loss: -13.127023696899414, mean_rew: 0.4573002537749212, variance: 1.9258270679662626, lamda: 1.3389482498168945
Running avgs for agent 2: q_loss: 637.7682495117188, p_loss: -13.819376945495605, mean_rew: 0.45242461971438824, variance: 1.7334979384144147, lamda: 1.3446698188781738

steps: 389970, episodes: 13000, mean episode reward: -21.07749191605449, agent episode reward: [-44.43488245136524, 11.678695267655378, 11.678695267655378], time: 47.811
steps: 389970, episodes: 13000, mean episode variance: 2.481254048459232, agent episode variance: [1.5525302361249924, 0.43400121726840735, 0.49472259506583216], time: 47.812
Running avgs for agent 0: q_loss: 51959.140625, p_loss: 14.304427146911621, mean_rew: -3.34405453234024, variance: 5.175100787083308, lamda: 1.34343683719635
Running avgs for agent 1: q_loss: 525.8485717773438, p_loss: -13.43600082397461, mean_rew: 0.45148463430991287, variance: 1.4466707242280246, lamda: 1.3690910339355469
Running avgs for agent 2: q_loss: 654.7666625976562, p_loss: -14.112716674804688, mean_rew: 0.45282264234581626, variance: 1.6490753168861072, lamda: 1.3747929334640503

steps: 419970, episodes: 14000, mean episode reward: -20.27046217918644, agent episode reward: [-46.83156064059484, 13.280549230704198, 13.280549230704198], time: 47.682
steps: 419970, episodes: 14000, mean episode variance: 3.1075915961712597, agent episode variance: [2.0130626044869424, 0.49994320103526113, 0.5945857906490565], time: 47.682
Running avgs for agent 0: q_loss: 57405.02734375, p_loss: 16.068397521972656, mean_rew: -3.205133253541732, variance: 6.710208681623141, lamda: 1.3734418153762817
Running avgs for agent 1: q_loss: 575.8999633789062, p_loss: -13.533147811889648, mean_rew: 0.4536491736315367, variance: 1.6664773367842038, lamda: 1.3991957902908325
Running avgs for agent 2: q_loss: 696.8655395507812, p_loss: -14.239503860473633, mean_rew: 0.45182738792279453, variance: 1.9819526354968549, lamda: 1.4048815965652466

steps: 449970, episodes: 15000, mean episode reward: -20.37988542774708, agent episode reward: [-45.63316815739361, 12.626641364823266, 12.626641364823266], time: 47.874
steps: 449970, episodes: 15000, mean episode variance: 3.0137313023880123, agent episode variance: [2.1340089461207388, 0.49392345832288265, 0.38579889794439076], time: 47.874
Running avgs for agent 0: q_loss: 72449.34375, p_loss: 19.162113189697266, mean_rew: -3.088710391047664, variance: 7.113363153735796, lamda: 1.4034466743469238
Running avgs for agent 1: q_loss: 531.9053344726562, p_loss: -13.459222793579102, mean_rew: 0.4517367453712554, variance: 1.646411527742942, lamda: 1.4292733669281006
Running avgs for agent 2: q_loss: 613.7355346679688, p_loss: -14.213420867919922, mean_rew: 0.444386376178579, variance: 1.2859963264813026, lamda: 1.434950828552246

steps: 479970, episodes: 16000, mean episode reward: -14.701546849544826, agent episode reward: [-43.98296176893666, 14.640707459695918, 14.640707459695918], time: 47.673
steps: 479970, episodes: 16000, mean episode variance: 2.645852490592748, agent episode variance: [1.6511313470602036, 0.49053675888478754, 0.5041843846477568], time: 47.673
Running avgs for agent 0: q_loss: 96585.484375, p_loss: 22.793079376220703, mean_rew: -2.9872591674240874, variance: 5.503771156867345, lamda: 1.433451771736145
Running avgs for agent 1: q_loss: 586.843505859375, p_loss: -13.741181373596191, mean_rew: 0.4490577039487989, variance: 1.6351225296159586, lamda: 1.459326982498169
Running avgs for agent 2: q_loss: 673.379150390625, p_loss: -14.493048667907715, mean_rew: 0.448643145924464, variance: 1.6806146154925228, lamda: 1.4649940729141235

steps: 509970, episodes: 17000, mean episode reward: -11.430081011643143, agent episode reward: [-43.41520964696306, 15.992564317659959, 15.992564317659959], time: 48.426
steps: 509970, episodes: 17000, mean episode variance: 3.883327980324626, agent episode variance: [2.735770798802376, 0.6019514231979847, 0.5456057583242655], time: 48.426
Running avgs for agent 0: q_loss: 148247.6875, p_loss: 25.463001251220703, mean_rew: -2.9027393140710154, variance: 9.11923599600792, lamda: 1.4634567499160767
Running avgs for agent 1: q_loss: 740.0946655273438, p_loss: -14.327712059020996, mean_rew: 0.45178594481711004, variance: 2.006504743993282, lamda: 1.4893677234649658
Running avgs for agent 2: q_loss: 775.3662109375, p_loss: -15.019383430480957, mean_rew: 0.4528563242001329, variance: 1.818685861080885, lamda: 1.4950348138809204

steps: 539970, episodes: 18000, mean episode reward: -12.236264609704055, agent episode reward: [-43.44256800953158, 15.60315169991376, 15.60315169991376], time: 47.889
steps: 539970, episodes: 18000, mean episode variance: 3.1484539084509016, agent episode variance: [2.071454043149948, 0.5736591426581145, 0.5033407226428389], time: 47.89
Running avgs for agent 0: q_loss: 157939.203125, p_loss: 27.85852813720703, mean_rew: -2.8219545031642017, variance: 6.904846810499827, lamda: 1.4934617280960083
Running avgs for agent 1: q_loss: 777.87841796875, p_loss: -14.8853178024292, mean_rew: 0.45942686357418844, variance: 1.9121971421937147, lamda: 1.5194083452224731
Running avgs for agent 2: q_loss: 796.423095703125, p_loss: -15.531201362609863, mean_rew: 0.45379923296304714, variance: 1.6778024088094632, lamda: 1.5250704288482666

steps: 569970, episodes: 19000, mean episode reward: -22.791286279031127, agent episode reward: [-48.56617538684863, 12.887444553908754, 12.887444553908754], time: 47.241
steps: 569970, episodes: 19000, mean episode variance: 3.856061037555337, agent episode variance: [2.6974926871061324, 0.5846148703992367, 0.5739534800499677], time: 47.241
Running avgs for agent 0: q_loss: 181900.078125, p_loss: 28.983564376831055, mean_rew: -2.738165462040424, variance: 8.991642290353775, lamda: 1.5234665870666504
Running avgs for agent 1: q_loss: 786.17626953125, p_loss: -15.446552276611328, mean_rew: 0.46003050596083594, variance: 1.9487162346641222, lamda: 1.5494290590286255
Running avgs for agent 2: q_loss: 902.6096801757812, p_loss: -16.037630081176758, mean_rew: 0.4598444517489789, variance: 1.9131782668332258, lamda: 1.5550791025161743

steps: 599970, episodes: 20000, mean episode reward: -17.299906544682234, agent episode reward: [-49.09415511613663, 15.897124285727202, 15.897124285727202], time: 47.23
steps: 599970, episodes: 20000, mean episode variance: 2.8964868018105627, agent episode variance: [1.626183420419693, 0.6814170194640755, 0.588886361926794], time: 47.23
Running avgs for agent 0: q_loss: 203252.671875, p_loss: 30.18537712097168, mean_rew: -2.667264513193896, variance: 5.420611401398976, lamda: 1.5534716844558716
Running avgs for agent 1: q_loss: 923.3513793945312, p_loss: -15.919746398925781, mean_rew: 0.45742739674973243, variance: 2.271390064880252, lamda: 1.5794341564178467
Running avgs for agent 2: q_loss: 922.9678955078125, p_loss: -16.39229393005371, mean_rew: 0.4572479496953012, variance: 1.9629545397559802, lamda: 1.585084080696106

steps: 629970, episodes: 21000, mean episode reward: -11.795161621522492, agent episode reward: [-47.12930747397388, 17.667072926225693, 17.667072926225693], time: 47.97
steps: 629970, episodes: 21000, mean episode variance: 3.427091459400952, agent episode variance: [2.147031794309616, 0.6767171437144279, 0.6033425213769078], time: 47.97
Running avgs for agent 0: q_loss: 259072.90625, p_loss: 30.953163146972656, mean_rew: -2.632455353490594, variance: 7.156772647698721, lamda: 1.5834765434265137
Running avgs for agent 1: q_loss: 965.1485595703125, p_loss: -16.289804458618164, mean_rew: 0.4613808891489698, variance: 2.2557238123814263, lamda: 1.6094391345977783
Running avgs for agent 2: q_loss: 925.5619506835938, p_loss: -16.692922592163086, mean_rew: 0.4659747615500043, variance: 2.011141737923026, lamda: 1.6150890588760376

steps: 659970, episodes: 22000, mean episode reward: -9.103161846143495, agent episode reward: [-40.90061420445457, 15.898726179155535, 15.898726179155535], time: 50.63
steps: 659970, episodes: 22000, mean episode variance: 2.4993705239146946, agent episode variance: [1.1482789242267608, 0.7111023814976215, 0.6399892181903124], time: 50.63
Running avgs for agent 0: q_loss: 267976.84375, p_loss: 31.249927520751953, mean_rew: -2.573466797197921, variance: 3.8275964140892027, lamda: 1.6134815216064453
Running avgs for agent 1: q_loss: 985.6392822265625, p_loss: -16.574153900146484, mean_rew: 0.4691714834160086, variance: 2.3703412716587384, lamda: 1.6394442319869995
Running avgs for agent 2: q_loss: 1093.4381103515625, p_loss: -16.867992401123047, mean_rew: 0.4663497824319734, variance: 2.133297393967708, lamda: 1.6450940370559692

steps: 689970, episodes: 23000, mean episode reward: -11.614273600338445, agent episode reward: [-47.551928583428, 17.968827491544776, 17.968827491544776], time: 51.462
steps: 689970, episodes: 23000, mean episode variance: 3.637997716382146, agent episode variance: [2.346745119333267, 0.6849017080366612, 0.6063508890122176], time: 51.463
Running avgs for agent 0: q_loss: 333902.65625, p_loss: 31.127290725708008, mean_rew: -2.52908359591426, variance: 7.822483731110891, lamda: 1.6434863805770874
Running avgs for agent 1: q_loss: 961.5460205078125, p_loss: -16.638341903686523, mean_rew: 0.47086762056492254, variance: 2.283005693455537, lamda: 1.6694490909576416
Running avgs for agent 2: q_loss: 1051.59375, p_loss: -16.93863868713379, mean_rew: 0.47252728638782077, variance: 2.021169630040725, lamda: 1.6750991344451904

steps: 719970, episodes: 24000, mean episode reward: -21.221227271326846, agent episode reward: [-73.59414929332488, 26.186461010999025, 26.186461010999025], time: 52.715
steps: 719970, episodes: 24000, mean episode variance: 2.8537425126582385, agent episode variance: [1.7293548886775971, 0.5252240728884935, 0.5991635510921478], time: 52.715
Running avgs for agent 0: q_loss: 368144.6875, p_loss: 30.890676498413086, mean_rew: -2.5168609404720557, variance: 5.76451629559199, lamda: 1.673491358757019
Running avgs for agent 1: q_loss: 895.1932373046875, p_loss: -17.239839553833008, mean_rew: 0.48379811687349994, variance: 1.7507469096283117, lamda: 1.6994541883468628
Running avgs for agent 2: q_loss: 970.4559326171875, p_loss: -17.362314224243164, mean_rew: 0.48482696919609974, variance: 1.997211836973826, lamda: 1.705104112625122

steps: 749970, episodes: 25000, mean episode reward: -20.101782807737152, agent episode reward: [-74.96128171223431, 27.42974945224858, 27.42974945224858], time: 52.657
steps: 749970, episodes: 25000, mean episode variance: 3.0112221935987473, agent episode variance: [1.6744771002531051, 0.6323250412195921, 0.70442005212605], time: 52.657
Running avgs for agent 0: q_loss: 378846.1875, p_loss: 31.115306854248047, mean_rew: -2.505126170348095, variance: 5.581590334177017, lamda: 1.7034964561462402
Running avgs for agent 1: q_loss: 975.719482421875, p_loss: -18.4017333984375, mean_rew: 0.4989136116460535, variance: 2.1077501373986403, lamda: 1.729459285736084
Running avgs for agent 2: q_loss: 1079.8519287109375, p_loss: -18.203731536865234, mean_rew: 0.5022163154374554, variance: 2.3480668404201666, lamda: 1.7351090908050537

steps: 779970, episodes: 26000, mean episode reward: -16.94495500275793, agent episode reward: [-73.05923813815896, 28.05714156770052, 28.05714156770052], time: 51.281
steps: 779970, episodes: 26000, mean episode variance: 3.3849561285078527, agent episode variance: [1.9893775728940963, 0.7979225248992443, 0.5976560307145119], time: 51.281
Running avgs for agent 0: q_loss: 364627.71875, p_loss: 31.0393123626709, mean_rew: -2.5054748165961778, variance: 6.6312585763136545, lamda: 1.7335014343261719
Running avgs for agent 1: q_loss: 1027.49560546875, p_loss: -19.701059341430664, mean_rew: 0.5174287621872706, variance: 2.6597417496641476, lamda: 1.759464144706726
Running avgs for agent 2: q_loss: 1056.6875, p_loss: -19.148717880249023, mean_rew: 0.5155480082633366, variance: 1.992186769048373, lamda: 1.7651139497756958

steps: 809970, episodes: 27000, mean episode reward: -16.30352543151541, agent episode reward: [-74.9650564489955, 29.330765508740043, 29.330765508740043], time: 51.748
steps: 809970, episodes: 27000, mean episode variance: 4.690604111358524, agent episode variance: [3.370607647895813, 0.7073362617045641, 0.6126602017581463], time: 51.748
Running avgs for agent 0: q_loss: 436210.09375, p_loss: 31.16046142578125, mean_rew: -2.488449050218229, variance: 11.235358826319377, lamda: 1.763506293296814
Running avgs for agent 1: q_loss: 1014.83154296875, p_loss: -21.05120086669922, mean_rew: 0.5335353544590452, variance: 2.3577875390152134, lamda: 1.7894690036773682
Running avgs for agent 2: q_loss: 1084.2742919921875, p_loss: -20.245134353637695, mean_rew: 0.5341136065942609, variance: 2.042200672527154, lamda: 1.795118808746338

steps: 839970, episodes: 28000, mean episode reward: -11.097739234604186, agent episode reward: [-66.08087039729028, 27.49156558134305, 27.49156558134305], time: 50.691
steps: 839970, episodes: 28000, mean episode variance: 5.053758697737008, agent episode variance: [3.617442452430725, 0.6986190984025598, 0.7376971469037235], time: 50.691
Running avgs for agent 0: q_loss: 463357.34375, p_loss: 31.807279586791992, mean_rew: -2.5129548006961255, variance: 12.058141508102416, lamda: 1.7935113906860352
Running avgs for agent 1: q_loss: 1066.8731689453125, p_loss: -22.24666404724121, mean_rew: 0.5487287793887505, variance: 2.3287303280085325, lamda: 1.8194738626480103
Running avgs for agent 2: q_loss: 1092.4261474609375, p_loss: -21.241727828979492, mean_rew: 0.548029559669356, variance: 2.458990489679078, lamda: 1.8251241445541382

steps: 869970, episodes: 29000, mean episode reward: -9.548119295863945, agent episode reward: [-62.19368320619388, 26.322781955164974, 26.322781955164974], time: 52.863
steps: 869970, episodes: 29000, mean episode variance: 5.592788262277842, agent episode variance: [4.176872297644615, 0.6678282442837954, 0.748087720349431], time: 52.864
Running avgs for agent 0: q_loss: 529505.375, p_loss: 31.576139450073242, mean_rew: -2.4858255641729907, variance: 13.922907658815383, lamda: 1.8235164880752563
Running avgs for agent 1: q_loss: 1069.080810546875, p_loss: -23.168190002441406, mean_rew: 0.5606358567509466, variance: 2.2260941476126512, lamda: 1.8494789600372314
Running avgs for agent 2: q_loss: 1157.780029296875, p_loss: -22.03919792175293, mean_rew: 0.5620903071593857, variance: 2.4936257344981034, lamda: 1.8551290035247803

steps: 899970, episodes: 30000, mean episode reward: -13.144512308772542, agent episode reward: [-63.022436879170904, 24.93896228519918, 24.93896228519918], time: 52.187
steps: 899970, episodes: 30000, mean episode variance: 4.513316353932023, agent episode variance: [3.161667649626732, 0.6708757151812315, 0.6807729891240597], time: 52.187
Running avgs for agent 0: q_loss: 443453.90625, p_loss: 31.649335861206055, mean_rew: -2.473948949405978, variance: 10.538892165422439, lamda: 1.8535213470458984
Running avgs for agent 1: q_loss: 1183.109375, p_loss: -23.957170486450195, mean_rew: 0.5689388003938942, variance: 2.236252383937438, lamda: 1.8794840574264526
Running avgs for agent 2: q_loss: 1136.345458984375, p_loss: -22.70688819885254, mean_rew: 0.5707432257555763, variance: 2.269243297080199, lamda: 1.8851338624954224

steps: 929970, episodes: 31000, mean episode reward: -11.86556050453401, agent episode reward: [-59.36813233357585, 23.751285914520917, 23.751285914520917], time: 52.616
steps: 929970, episodes: 31000, mean episode variance: 5.893919890858233, agent episode variance: [4.261729716777801, 0.874825572758913, 0.7573646013215184], time: 52.617
Running avgs for agent 0: q_loss: 488532.4375, p_loss: 31.180126190185547, mean_rew: -2.463525584353235, variance: 14.205765722592671, lamda: 1.8835264444351196
Running avgs for agent 1: q_loss: 1289.2047119140625, p_loss: -24.653526306152344, mean_rew: 0.5800503581778372, variance: 2.9160852425297104, lamda: 1.9094889163970947
Running avgs for agent 2: q_loss: 1181.8914794921875, p_loss: -23.303424835205078, mean_rew: 0.5795129584197858, variance: 2.524548671071728, lamda: 1.9151389598846436

steps: 959970, episodes: 32000, mean episode reward: -10.224676000268941, agent episode reward: [-58.862695091530405, 24.31900954563073, 24.31900954563073], time: 58.528
steps: 959970, episodes: 32000, mean episode variance: 5.924478177927434, agent episode variance: [4.489975363969803, 0.6367506804317236, 0.797752133525908], time: 58.528
Running avgs for agent 0: q_loss: 499506.25, p_loss: 30.127880096435547, mean_rew: -2.441004621055248, variance: 14.96658454656601, lamda: 1.9135313034057617
Running avgs for agent 1: q_loss: 1248.5555419921875, p_loss: -25.10911750793457, mean_rew: 0.5861272999407389, variance: 2.1225022681057455, lamda: 1.939494013786316
Running avgs for agent 2: q_loss: 1183.0672607421875, p_loss: -23.72930908203125, mean_rew: 0.5857998554808885, variance: 2.659173778419693, lamda: 1.9451440572738647

steps: 989970, episodes: 33000, mean episode reward: -8.935702501738405, agent episode reward: [-61.17608318404489, 26.120190341153243, 26.120190341153243], time: 62.998
steps: 989970, episodes: 33000, mean episode variance: 5.560615302734077, agent episode variance: [3.9835386908650396, 0.9079114887490869, 0.6691651231199502], time: 62.999
Running avgs for agent 0: q_loss: 446753.1875, p_loss: 29.002187728881836, mean_rew: -2.4257577776139967, variance: 13.278462302883467, lamda: 1.943536400794983
Running avgs for agent 1: q_loss: 1277.8902587890625, p_loss: -25.49138069152832, mean_rew: 0.5924096549304694, variance: 3.0263716291636227, lamda: 1.9694987535476685
Running avgs for agent 2: q_loss: 1221.5294189453125, p_loss: -24.090709686279297, mean_rew: 0.594435652819185, variance: 2.230550410399834, lamda: 1.9751489162445068

steps: 1019970, episodes: 34000, mean episode reward: -6.163090392244736, agent episode reward: [-57.39156578909911, 25.614237698427186, 25.614237698427186], time: 58.736
steps: 1019970, episodes: 34000, mean episode variance: 5.928265709549189, agent episode variance: [4.352819321632385, 0.6452132458686829, 0.9302331420481205], time: 58.737
Running avgs for agent 0: q_loss: 497441.5, p_loss: 28.430843353271484, mean_rew: -2.4193893198595178, variance: 14.509397738774618, lamda: 1.973541259765625
Running avgs for agent 1: q_loss: 1293.2249755859375, p_loss: -25.827098846435547, mean_rew: 0.6016407522203131, variance: 2.1507108195622764, lamda: 1.9994996786117554
Running avgs for agent 2: q_loss: 1304.9681396484375, p_loss: -24.440502166748047, mean_rew: 0.6040872846106499, variance: 3.100777140160402, lamda: 2.005145788192749

steps: 1049970, episodes: 35000, mean episode reward: -9.882997461374579, agent episode reward: [-70.96451718937959, 30.540759864002503, 30.540759864002503], time: 62.457
steps: 1049970, episodes: 35000, mean episode variance: 5.355083193182946, agent episode variance: [3.9331396774053573, 0.7663270627483726, 0.6556164530292153], time: 62.458
Running avgs for agent 0: q_loss: 500650.25, p_loss: 28.487092971801758, mean_rew: -2.4424709034891303, variance: 13.110465591351192, lamda: 2.0035393238067627
Running avgs for agent 1: q_loss: 1148.55712890625, p_loss: -26.527538299560547, mean_rew: 0.6249214246825234, variance: 2.5544235424945754, lamda: 2.0294737815856934
Running avgs for agent 2: q_loss: 1164.5723876953125, p_loss: -25.033327102661133, mean_rew: 0.6250804819013329, variance: 2.185388176764051, lamda: 2.035116672515869

steps: 1079970, episodes: 36000, mean episode reward: -12.8732044851822, agent episode reward: [-74.93285831849873, 31.029826916658266, 31.029826916658266], time: 64.857
steps: 1079970, episodes: 36000, mean episode variance: 6.891237005397677, agent episode variance: [4.8873833725452425, 1.249008183479309, 0.754845449373126], time: 64.857
Running avgs for agent 0: q_loss: 671143.5, p_loss: 26.780664443969727, mean_rew: -2.3649135422085252, variance: 16.29127790848414, lamda: 2.033511161804199
Running avgs for agent 1: q_loss: 1389.455322265625, p_loss: -26.990772247314453, mean_rew: 0.6293724458980446, variance: 4.163360611597697, lamda: 2.0594429969787598
Running avgs for agent 2: q_loss: 1145.9293212890625, p_loss: -25.481481552124023, mean_rew: 0.6275437353816187, variance: 2.5161514979104203, lamda: 2.0650858879089355

steps: 1109970, episodes: 37000, mean episode reward: -12.031452275863279, agent episode reward: [-72.79116982228332, 30.379858773210024, 30.379858773210024], time: 53.07
steps: 1109970, episodes: 37000, mean episode variance: 6.995257164716721, agent episode variance: [4.73633792424202, 1.5107243035584688, 0.7481949369162321], time: 53.071
Running avgs for agent 0: q_loss: 594353.5, p_loss: 24.3664493560791, mean_rew: -2.13678407366952, variance: 15.787793080806733, lamda: 2.0634803771972656
Running avgs for agent 1: q_loss: 1306.7066650390625, p_loss: -26.844335556030273, mean_rew: 0.5983491035239196, variance: 5.03574767852823, lamda: 2.089411973953247
Running avgs for agent 2: q_loss: 884.9249877929688, p_loss: -25.453475952148438, mean_rew: 0.5990889341993192, variance: 2.493983123054107, lamda: 2.095055341720581

steps: 1139970, episodes: 38000, mean episode reward: -11.829773129699344, agent episode reward: [-73.64299819452908, 30.90661253241486, 30.90661253241486], time: 51.009
steps: 1139970, episodes: 38000, mean episode variance: 7.07772612324357, agent episode variance: [4.950687482833862, 1.3112716476023196, 0.8157669928073883], time: 51.01
Running avgs for agent 0: q_loss: 831579.3125, p_loss: 23.228593826293945, mean_rew: -2.071864615696385, variance: 16.502291609446207, lamda: 2.093449592590332
Running avgs for agent 1: q_loss: 1139.9664306640625, p_loss: -27.448287963867188, mean_rew: 0.6073210370591577, variance: 4.370905492007733, lamda: 2.1193811893463135
Running avgs for agent 2: q_loss: 754.5160522460938, p_loss: -26.040390014648438, mean_rew: 0.6092521607002879, variance: 2.719223309357961, lamda: 2.1250243186950684

steps: 1169970, episodes: 39000, mean episode reward: -12.908218847281173, agent episode reward: [-76.30230753654358, 31.697044344631202, 31.697044344631202], time: 51.069
steps: 1169970, episodes: 39000, mean episode variance: 6.771345369681716, agent episode variance: [4.91234417963028, 0.8131028506755829, 1.0458983393758536], time: 51.069
Running avgs for agent 0: q_loss: 418955.6875, p_loss: 18.018407821655273, mean_rew: -1.9694161004694768, variance: 16.374480598767597, lamda: 2.1234188079833984
Running avgs for agent 1: q_loss: 756.2466430664062, p_loss: -28.082626342773438, mean_rew: 0.6183474109211768, variance: 2.7103428355852763, lamda: 2.149350643157959
Running avgs for agent 2: q_loss: 969.7987670898438, p_loss: -26.609466552734375, mean_rew: 0.6167495990743445, variance: 3.486327797919512, lamda: 2.1549935340881348

steps: 1199970, episodes: 40000, mean episode reward: -8.734132570261272, agent episode reward: [-65.78098738274406, 28.523427406241396, 28.523427406241396], time: 54.376
steps: 1199970, episodes: 40000, mean episode variance: 6.862767991900444, agent episode variance: [4.516166724681854, 0.8381352828741073, 1.5084659843444823], time: 54.376
Running avgs for agent 0: q_loss: 111537.546875, p_loss: 13.3618745803833, mean_rew: -1.8776504619123613, variance: 15.053889082272848, lamda: 2.153388261795044
Running avgs for agent 1: q_loss: 692.4503173828125, p_loss: -28.630306243896484, mean_rew: 0.6370123396374844, variance: 2.7937842762470244, lamda: 2.1793198585510254
Running avgs for agent 2: q_loss: 1154.3258056640625, p_loss: -27.14422607421875, mean_rew: 0.6370015082022235, variance: 5.028219947814941, lamda: 2.1849629878997803

steps: 1229970, episodes: 41000, mean episode reward: -7.633562027780184, agent episode reward: [-48.53601374730201, 20.45122585976091, 20.45122585976091], time: 51.137
steps: 1229970, episodes: 41000, mean episode variance: 5.132780331164598, agent episode variance: [2.8930547116100787, 0.794242460489273, 1.4454831590652466], time: 51.137
Running avgs for agent 0: q_loss: 51510.828125, p_loss: 12.063029289245605, mean_rew: -1.8858152613486376, variance: 9.64351570536693, lamda: 2.1833572387695312
Running avgs for agent 1: q_loss: 635.0595092773438, p_loss: -28.954179763793945, mean_rew: 0.6721120714491428, variance: 2.647474868297577, lamda: 2.2092888355255127
Running avgs for agent 2: q_loss: 1114.589111328125, p_loss: -27.521949768066406, mean_rew: 0.6727020279969378, variance: 4.818277196884155, lamda: 2.2149319648742676

steps: 1259970, episodes: 42000, mean episode reward: -10.069373997556484, agent episode reward: [-55.38430738640314, 22.657466694423324, 22.657466694423324], time: 50.723
steps: 1259970, episodes: 42000, mean episode variance: 5.174302900373935, agent episode variance: [2.9400944169163705, 0.8193162124156952, 1.41489227104187], time: 50.724
Running avgs for agent 0: q_loss: 25296.34375, p_loss: 12.091241836547852, mean_rew: -1.8975503003625067, variance: 9.800314723054568, lamda: 2.2133266925811768
Running avgs for agent 1: q_loss: 597.7398681640625, p_loss: -28.706340789794922, mean_rew: 0.6978864437979219, variance: 2.7310540413856508, lamda: 2.239258289337158
Running avgs for agent 2: q_loss: 1100.88818359375, p_loss: -27.422414779663086, mean_rew: 0.6989128956826753, variance: 4.716307570139567, lamda: 2.244901418685913

steps: 1289970, episodes: 43000, mean episode reward: -8.0909127986375, agent episode reward: [-53.53818372069978, 22.72363546103114, 22.72363546103114], time: 52.486
steps: 1289970, episodes: 43000, mean episode variance: 4.8070246529281135, agent episode variance: [2.56426104542613, 0.8204835371971131, 1.4222800703048706], time: 52.486
Running avgs for agent 0: q_loss: 19505.173828125, p_loss: 13.444039344787598, mean_rew: -1.9117488355477152, variance: 8.547536818087101, lamda: 2.243295669555664
Running avgs for agent 1: q_loss: 595.436767578125, p_loss: -28.318321228027344, mean_rew: 0.7155690542701368, variance: 2.734945123990377, lamda: 2.2692272663116455
Running avgs for agent 2: q_loss: 1114.3388671875, p_loss: -27.251487731933594, mean_rew: 0.7148738822068399, variance: 4.740933567682902, lamda: 2.2748703956604004

steps: 1319970, episodes: 44000, mean episode reward: -7.170234103278022, agent episode reward: [-47.29060705065641, 20.060186473689203, 20.060186473689203], time: 50.739
steps: 1319970, episodes: 44000, mean episode variance: 4.5511985135674475, agent episode variance: [2.287603486984968, 0.8086450568139553, 1.4549499697685242], time: 50.739
Running avgs for agent 0: q_loss: 15936.7763671875, p_loss: 15.575775146484375, mean_rew: -1.9270031663123628, variance: 7.62534495661656, lamda: 2.2732648849487305
Running avgs for agent 1: q_loss: 669.0571899414062, p_loss: -28.06756591796875, mean_rew: 0.731510959555907, variance: 2.695483522713184, lamda: 2.299196720123291
Running avgs for agent 2: q_loss: 1131.931396484375, p_loss: -27.1688289642334, mean_rew: 0.7314062697612971, variance: 4.849833232561747, lamda: 2.304839611053467

steps: 1349970, episodes: 45000, mean episode reward: -6.286189963780604, agent episode reward: [-44.614064454638566, 19.16393724542898, 19.16393724542898], time: 51.163
steps: 1349970, episodes: 45000, mean episode variance: 6.094991228461265, agent episode variance: [3.227189152598381, 1.4279466705322266, 1.439855405330658], time: 51.163
Running avgs for agent 0: q_loss: 19241.859375, p_loss: 17.644508361816406, mean_rew: -1.931050470230249, variance: 10.757297175327937, lamda: 2.303234100341797
Running avgs for agent 1: q_loss: 1041.98681640625, p_loss: -27.81649398803711, mean_rew: 0.7428761081596389, variance: 4.759822235107422, lamda: 2.3291659355163574
Running avgs for agent 2: q_loss: 1143.696533203125, p_loss: -27.022428512573242, mean_rew: 0.7433651721200437, variance: 4.79951801776886, lamda: 2.3348090648651123

steps: 1379970, episodes: 46000, mean episode reward: -4.425082182428965, agent episode reward: [-43.237127421274494, 19.406022619422764, 19.406022619422764], time: 52.212
steps: 1379970, episodes: 46000, mean episode variance: 5.5932136052250865, agent episode variance: [2.757749339878559, 1.4460682344436646, 1.3893960309028626], time: 52.213
Running avgs for agent 0: q_loss: 19293.888671875, p_loss: 19.947864532470703, mean_rew: -1.9252592992797724, variance: 9.192497799595197, lamda: 2.3332035541534424
Running avgs for agent 1: q_loss: 1078.85107421875, p_loss: -27.57371711730957, mean_rew: 0.7504345125441576, variance: 4.820227448145548, lamda: 2.3591349124908447
Running avgs for agent 2: q_loss: 1156.16748046875, p_loss: -26.8410587310791, mean_rew: 0.7485308800543874, variance: 4.631320103009542, lamda: 2.3647780418395996

steps: 1409970, episodes: 47000, mean episode reward: -1.7291661171324468, agent episode reward: [-39.66675428632794, 18.968794084597743, 18.968794084597743], time: 51.197
steps: 1409970, episodes: 47000, mean episode variance: 6.362221260309219, agent episode variance: [3.5148183546066285, 1.41530548787117, 1.432097417831421], time: 51.198
Running avgs for agent 0: q_loss: 21097.21484375, p_loss: 22.17388343811035, mean_rew: -1.924005336976702, variance: 11.716061182022095, lamda: 2.3631725311279297
Running avgs for agent 1: q_loss: 1135.9940185546875, p_loss: -27.280868530273438, mean_rew: 0.7577671966727596, variance: 4.717684959570567, lamda: 2.3891043663024902
Running avgs for agent 2: q_loss: 1184.772216796875, p_loss: -26.65066909790039, mean_rew: 0.7571005624213973, variance: 4.77365805943807, lamda: 2.394747257232666

steps: 1439970, episodes: 48000, mean episode reward: -0.5721838375342925, agent episode reward: [-36.53532617270593, 17.98157116758582, 17.98157116758582], time: 51.223
steps: 1439970, episodes: 48000, mean episode variance: 6.702931484222412, agent episode variance: [3.9037011256217955, 1.4172411303520203, 1.3819892282485962], time: 51.224
Running avgs for agent 0: q_loss: 22502.478515625, p_loss: 23.220863342285156, mean_rew: -1.9213988578426329, variance: 13.012337085405985, lamda: 2.393141984939575
Running avgs for agent 1: q_loss: 1207.9100341796875, p_loss: -26.81052589416504, mean_rew: 0.7600847587596036, variance: 4.724137101173401, lamda: 2.4190735816955566
Running avgs for agent 2: q_loss: 1243.654541015625, p_loss: -26.32268524169922, mean_rew: 0.7629108865121177, variance: 4.606630760828654, lamda: 2.4247164726257324

steps: 1469970, episodes: 49000, mean episode reward: -0.867216446477654, agent episode reward: [-35.33463303053167, 17.23370829202701, 17.23370829202701], time: 51.151
steps: 1469970, episodes: 49000, mean episode variance: 7.141118320941925, agent episode variance: [4.3071719751358035, 1.4143970654010773, 1.4195492804050445], time: 51.151
Running avgs for agent 0: q_loss: 23896.859375, p_loss: 24.54065704345703, mean_rew: -1.9037797748858907, variance: 14.357239917119344, lamda: 2.4231112003326416
Running avgs for agent 1: q_loss: 1311.7808837890625, p_loss: -26.362077713012695, mean_rew: 0.7659867601109402, variance: 4.714656884670258, lamda: 2.449042797088623
Running avgs for agent 2: q_loss: 1326.9189453125, p_loss: -25.87149429321289, mean_rew: 0.7663811638314367, variance: 4.731830934683482, lamda: 2.454685926437378

steps: 1499970, episodes: 50000, mean episode reward: -2.431604633921183, agent episode reward: [-38.34495046741432, 17.95667291674657, 17.95667291674657], time: 51.773
steps: 1499970, episodes: 50000, mean episode variance: 6.532574794054032, agent episode variance: [3.6573188638687135, 1.4367488875389098, 1.4385070426464082], time: 51.774
Running avgs for agent 0: q_loss: 28873.92578125, p_loss: 26.135517120361328, mean_rew: -1.8962948017148256, variance: 12.191062879562377, lamda: 2.453080177307129
Running avgs for agent 1: q_loss: 1401.2894287109375, p_loss: -25.948150634765625, mean_rew: 0.7708315507578328, variance: 4.789162958463033, lamda: 2.4790117740631104
Running avgs for agent 2: q_loss: 1414.720458984375, p_loss: -25.556013107299805, mean_rew: 0.7695235318719573, variance: 4.795023475488027, lamda: 2.4846549034118652

steps: 1529970, episodes: 51000, mean episode reward: -1.9177825493473522, agent episode reward: [-36.739265010063725, 17.410741230358187, 17.410741230358187], time: 51.041
steps: 1529970, episodes: 51000, mean episode variance: 7.27113609623909, agent episode variance: [4.4631954917907715, 1.44794788646698, 1.3599927179813385], time: 51.041
Running avgs for agent 0: q_loss: 29659.29296875, p_loss: 28.2509822845459, mean_rew: -1.8859725648644416, variance: 14.877318305969238, lamda: 2.4830493927001953
Running avgs for agent 1: q_loss: 1419.998291015625, p_loss: -25.457353591918945, mean_rew: 0.7719236380501657, variance: 4.826492954889933, lamda: 2.508981227874756
Running avgs for agent 2: q_loss: 1475.0081787109375, p_loss: -25.333274841308594, mean_rew: 0.7723820642902096, variance: 4.533309059937795, lamda: 2.5146241188049316

steps: 1559970, episodes: 52000, mean episode reward: -3.472063857812383, agent episode reward: [-37.24517656462873, 16.886556353408178, 16.886556353408178], time: 52.168
steps: 1559970, episodes: 52000, mean episode variance: 7.096152191508561, agent episode variance: [4.765781275749206, 0.899965250838548, 1.4304056649208068], time: 52.169
Running avgs for agent 0: q_loss: 29875.654296875, p_loss: 28.836210250854492, mean_rew: -1.8810379190549926, variance: 15.885937585830689, lamda: 2.513018846511841
Running avgs for agent 1: q_loss: 1086.2738037109375, p_loss: -25.024948120117188, mean_rew: 0.7743228565980953, variance: 2.9998841694618266, lamda: 2.5389504432678223
Running avgs for agent 2: q_loss: 1500.530029296875, p_loss: -25.105989456176758, mean_rew: 0.7751610212192149, variance: 4.768018883069356, lamda: 2.544593572616577

steps: 1589970, episodes: 53000, mean episode reward: -1.6810561897666607, agent episode reward: [-34.93757123158901, 16.628257520911173, 16.628257520911173], time: 51.752
steps: 1589970, episodes: 53000, mean episode variance: 7.910179508209229, agent episode variance: [4.97460559463501, 1.465884009361267, 1.4696899042129516], time: 51.753
Running avgs for agent 0: q_loss: 31361.654296875, p_loss: 29.529661178588867, mean_rew: -1.871276693045217, variance: 16.582018648783365, lamda: 2.542987823486328
Running avgs for agent 1: q_loss: 1393.1143798828125, p_loss: -24.714553833007812, mean_rew: 0.7776614512639369, variance: 4.886280031204223, lamda: 2.5689194202423096
Running avgs for agent 2: q_loss: 1502.916259765625, p_loss: -24.916696548461914, mean_rew: 0.7782406387468123, variance: 4.898966347376506, lamda: 2.5745625495910645

steps: 1619970, episodes: 54000, mean episode reward: -1.6902504735760904, agent episode reward: [-33.02100190469501, 15.665375715559462, 15.665375715559462], time: 51.072
steps: 1619970, episodes: 54000, mean episode variance: 7.608670035839081, agent episode variance: [4.754908101558685, 1.394175335407257, 1.4595865988731385], time: 51.073
Running avgs for agent 0: q_loss: 31958.7109375, p_loss: 29.977415084838867, mean_rew: -1.856571655054001, variance: 15.849693671862285, lamda: 2.5729570388793945
Running avgs for agent 1: q_loss: 1388.040771484375, p_loss: -24.467693328857422, mean_rew: 0.7770955769394096, variance: 4.64725111802419, lamda: 2.598888635635376
Running avgs for agent 2: q_loss: 1529.9281005859375, p_loss: -24.69749641418457, mean_rew: 0.7782884825210635, variance: 4.865288662910461, lamda: 2.604531764984131

steps: 1649970, episodes: 55000, mean episode reward: -1.7930933920661623, agent episode reward: [-33.63308424457718, 15.91999542625551, 15.91999542625551], time: 52.812
steps: 1649970, episodes: 55000, mean episode variance: 6.4637083903551105, agent episode variance: [3.630371492028236, 1.3937048296928405, 1.4396320686340331], time: 52.813
Running avgs for agent 0: q_loss: 28037.904296875, p_loss: 31.070104598999023, mean_rew: -1.843044212346872, variance: 12.101238306760788, lamda: 2.602926254272461
Running avgs for agent 1: q_loss: 1396.615234375, p_loss: -24.15694808959961, mean_rew: 0.7750185066299069, variance: 4.645682765642802, lamda: 2.6288583278656006
Running avgs for agent 2: q_loss: 1538.9725341796875, p_loss: -24.359577178955078, mean_rew: 0.7746181725806321, variance: 4.798773562113444, lamda: 2.6345009803771973

steps: 1679970, episodes: 56000, mean episode reward: -1.7837415666574388, agent episode reward: [-33.67401072778879, 15.945134580565673, 15.945134580565673], time: 52.245
steps: 1679970, episodes: 56000, mean episode variance: 8.048035706996918, agent episode variance: [5.301833094596863, 1.3926764369010924, 1.3535261754989625], time: 52.245
Running avgs for agent 0: q_loss: 34713.29296875, p_loss: 31.63278579711914, mean_rew: -1.8360963277140399, variance: 17.672776981989543, lamda: 2.6328954696655273
Running avgs for agent 1: q_loss: 1418.088134765625, p_loss: -23.76317024230957, mean_rew: 0.7753383916375369, variance: 4.642254789670308, lamda: 2.658827304840088
Running avgs for agent 2: q_loss: 1566.51025390625, p_loss: -23.98174285888672, mean_rew: 0.775371551278183, variance: 4.511753918329875, lamda: 2.6644701957702637

steps: 1709970, episodes: 57000, mean episode reward: -1.2371640800873365, agent episode reward: [-33.17307327982191, 15.967954599867282, 15.967954599867282], time: 51.896
steps: 1709970, episodes: 57000, mean episode variance: 7.779577085018158, agent episode variance: [5.044144958972931, 1.361037516593933, 1.374394609451294], time: 51.896
Running avgs for agent 0: q_loss: 38735.53515625, p_loss: 31.876604080200195, mean_rew: -1.8138549487189015, variance: 16.81381652990977, lamda: 2.6628646850585938
Running avgs for agent 1: q_loss: 1437.48046875, p_loss: -23.2509822845459, mean_rew: 0.7722582643031931, variance: 4.536791721979777, lamda: 2.688796281814575
Running avgs for agent 2: q_loss: 1562.89794921875, p_loss: -23.462249755859375, mean_rew: 0.7727625946110478, variance: 4.581315364837646, lamda: 2.69443941116333

steps: 1739970, episodes: 58000, mean episode reward: -2.2862546155974908, agent episode reward: [-31.360410685625094, 14.537078035013803, 14.537078035013803], time: 51.9
steps: 1739970, episodes: 58000, mean episode variance: 7.640005167394876, agent episode variance: [5.083753801822662, 1.2832320981025696, 1.2730192674696446], time: 51.901
Running avgs for agent 0: q_loss: 37289.37890625, p_loss: 31.78512954711914, mean_rew: -1.7752485214286253, variance: 16.94584600607554, lamda: 2.6928341388702393
Running avgs for agent 1: q_loss: 1463.8585205078125, p_loss: -22.4716854095459, mean_rew: 0.762202080271409, variance: 4.2774403270085655, lamda: 2.7187657356262207
Running avgs for agent 2: q_loss: 1592.2900390625, p_loss: -22.582138061523438, mean_rew: 0.7599873956269848, variance: 4.243397558232148, lamda: 2.7244088649749756

steps: 1769970, episodes: 59000, mean episode reward: -7.494423658446425, agent episode reward: [-31.624280902943326, 12.064928622248452, 12.064928622248452], time: 51.984
steps: 1769970, episodes: 59000, mean episode variance: 5.976426829218864, agent episode variance: [3.4222565871477126, 1.2685369970798492, 1.2856332449913024], time: 51.985
Running avgs for agent 0: q_loss: 32297.099609375, p_loss: 31.54931640625, mean_rew: -1.732459552479191, variance: 11.407521957159043, lamda: 2.7228031158447266
Running avgs for agent 1: q_loss: 1490.569091796875, p_loss: -21.53618812561035, mean_rew: 0.7478357477138454, variance: 4.228456656932831, lamda: 2.748734951019287
Running avgs for agent 2: q_loss: 1628.98876953125, p_loss: -21.517141342163086, mean_rew: 0.7481494053889571, variance: 4.285444149971008, lamda: 2.754377841949463/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)


steps: 1799970, episodes: 60000, mean episode reward: -6.862092093471994, agent episode reward: [-32.680140362334434, 12.909024134431222, 12.909024134431222], time: 53.335
steps: 1799970, episodes: 60000, mean episode variance: 6.933915056347847, agent episode variance: [4.466126725316047, 1.2013484346866607, 1.2664398963451386], time: 53.335
Running avgs for agent 0: q_loss: 34818.5546875, p_loss: 30.71369743347168, mean_rew: -1.6908192818504257, variance: 14.887089084386826, lamda: 2.752772569656372
Running avgs for agent 1: q_loss: 1481.3575439453125, p_loss: -20.45548439025879, mean_rew: 0.7321213578036724, variance: 4.0044947822888695, lamda: 2.7787039279937744
Running avgs for agent 2: q_loss: 1607.46142578125, p_loss: -20.29410171508789, mean_rew: 0.7286004328862574, variance: 4.221466321150462, lamda: 2.7843472957611084

...Finished total of 60001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 29970, episodes: 1000, mean episode reward: -5.005270375850451, agent episode reward: [-31.406059925085987, 13.200394774617768, 13.200394774617768], time: 34.976
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 34.976
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 59970, episodes: 2000, mean episode reward: -5.417053565664851, agent episode reward: [-31.943396727477108, 13.26317158090613, 13.26317158090613], time: 44.095
steps: 59970, episodes: 2000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 44.095
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -1.049276864993033, variance: 0.0, lamda: 2.7678070068359375
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.44031027150793767, variance: 0.0, lamda: 2.793738842010498
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.44000287797753296, variance: 0.0, lamda: 2.7993814945220947

...Finished total of 2001 episodes with the fixed policy.
