WARNING: Logging before flag parsing goes to stderr.
W0903 14:22:22.805168 4789200320 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0903 14:22:22.805477 4789200320 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-09-03 14:22:22.805932: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0903 14:22:22.812746 4789200320 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0903 14:22:22.814903 4789200320 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0903 14:22:22.815069 4789200320 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0903 14:22:22.815169 4789200320 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0903 14:22:23.171135 4789200320 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0903 14:22:23.254012 4789200320 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0903 14:22:23.260235 4789200320 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0903 14:22:23.634710 4789200320 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation False
adversary agent:  -0.01674283668398857
good agent:  29.040218353271484
good agent:  29.040218353271484
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 29970, episodes: 1000, mean episode reward: -28.751020861214943, agent episode reward: [-42.51837728730512, 6.883678213045088, 6.883678213045088], time: 24.911
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 24.912
steps: 29970, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0], time: 24.912
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 59970, episodes: 2000, mean episode reward: -103.65263341353796, agent episode reward: [-51.30629409165781, -26.17316966094007, -26.17316966094007], time: 47.141
steps: 59970, episodes: 2000, mean episode variance: 0.03209908005036414, agent episode variance: [0.028951975852251052, 0.000353146780282259, 0.0027939574178308248], time: 47.141
steps: 59970, episodes: 2000, mean episode cvar: 15.173081272542477, agent episode cvar: [0.23541844993829728, 7.544546386122703, 7.393116436481476], time: 47.142
Running avgs for agent 0: q_loss: 0.55960613489151, p_loss: 4.169653415679932, mean_rew: -1.593160756374731, variance: 0.09915060223373648, cvar: 0.8062275648117065, v: 0.8062275648117065, mean_q: -4.255712032318115, std_q: 2.4700851440429688, lamda: 1.0001730918884277
Running avgs for agent 1: q_loss: 38.12281799316406, p_loss: -3.7371203899383545, mean_rew: 0.033378731412772596, variance: 0.0012094067817885583, cvar: 25.83749008178711, v: 1.1821478605270386, mean_q: 3.637284755706787, std_q: 1.082484245300293, lamda: 1.0047518014907837
Running avgs for agent 2: q_loss: 52.01648712158203, p_loss: -3.626232385635376, mean_rew: 0.03425960437910563, variance: 0.009568347321338442, cvar: 25.318893432617188, v: 1.1631044149398804, mean_q: 3.5282137393951416, std_q: 1.1880537271499634, lamda: 1.0035909414291382

steps: 89970, episodes: 3000, mean episode reward: -105.94704252703717, agent episode reward: [-16.182263828955964, -44.88238934904061, -44.88238934904061], time: 45.957
steps: 89970, episodes: 3000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 45.958
steps: 89970, episodes: 3000, mean episode cvar: 17.116805331528187, agent episode cvar: [0.12455489283800125, 8.529638296127318, 8.462612142562866], time: 45.958
Running avgs for agent 0: q_loss: 0.16344386339187622, p_loss: 7.534654140472412, mean_rew: -1.3771583598338002, variance: 0.0, cvar: 0.4151829779148102, v: 0.4151829779148102, mean_q: -7.810585975646973, std_q: 5.654763221740723, lamda: 0.9970501661300659
Running avgs for agent 1: q_loss: 1.9301596879959106, p_loss: -3.5840518474578857, mean_rew: -0.5711040139466771, variance: 0.0, cvar: 28.432125091552734, v: 1.5739388465881348, mean_q: 3.500824451446533, std_q: 3.289914131164551, lamda: 1.014894962310791
Running avgs for agent 2: q_loss: 1.8224037885665894, p_loss: -3.2552177906036377, mean_rew: -0.5703521602585676, variance: 0.0, cvar: 28.208707809448242, v: 1.5539034605026245, mean_q: 3.176931858062744, std_q: 3.74941086769104, lamda: 1.0096533298492432

steps: 119970, episodes: 4000, mean episode reward: -56.27343453991741, agent episode reward: [-15.281339982751764, -20.496047278582825, -20.496047278582825], time: 47.214
steps: 119970, episodes: 4000, mean episode variance: 0.4452247364073992, agent episode variance: [0.2582862996309996, 0.0729774011299014, 0.1139610356464982], time: 47.214
steps: 119970, episodes: 4000, mean episode cvar: 17.21667086540209, agent episode cvar: [0.012292033538687974, 8.603685932159424, 8.60069289970398], time: 47.215
Running avgs for agent 0: q_loss: 0.2120116651058197, p_loss: 8.544181823730469, mean_rew: -1.1249330107194642, variance: 0.8609543321033318, cvar: 0.04097343981266022, v: 0.04097343981266022, mean_q: -8.923693656921387, std_q: 8.025330543518066, lamda: 0.995621919631958
Running avgs for agent 1: q_loss: 3.2119688987731934, p_loss: -2.523069143295288, mean_rew: -0.7094139243949275, variance: 0.24325800376633802, cvar: 28.678951263427734, v: 1.948486089706421, mean_q: 2.2977135181427, std_q: 6.000418663024902, lamda: 1.0372018814086914
Running avgs for agent 2: q_loss: 3.174297571182251, p_loss: -2.1442928314208984, mean_rew: -0.7036720909919325, variance: 0.37987011882166066, cvar: 28.668977737426758, v: 1.928440809249878, mean_q: 2.0124080181121826, std_q: 6.401224613189697, lamda: 1.0284717082977295

steps: 149970, episodes: 5000, mean episode reward: -55.016031191101796, agent episode reward: [-14.363500405177943, -20.326265392961922, -20.326265392961922], time: 46.859
steps: 149970, episodes: 5000, mean episode variance: 2.3196773243024946, agent episode variance: [1.7701952663958074, 0.260968107804656, 0.28851395010203124], time: 46.859
steps: 149970, episodes: 5000, mean episode cvar: 17.156818131148814, agent episode cvar: [-0.09420616847276687, 8.60781404876709, 8.643210250854493], time: 46.86
Running avgs for agent 0: q_loss: 0.24848923087120056, p_loss: 8.273492813110352, mean_rew: -0.9840014504314775, variance: 5.900650887986024, cvar: -0.3140205442905426, v: -0.3154936134815216, mean_q: -8.678112983703613, std_q: 9.426129341125488, lamda: 0.9956493973731995
Running avgs for agent 1: q_loss: 3.197707414627075, p_loss: -2.2928006649017334, mean_rew: -0.7042931170760014, variance: 0.8698936926821867, cvar: 28.69271469116211, v: 2.3051271438598633, mean_q: 1.9875643253326416, std_q: 7.4385085105896, lamda: 1.0636484622955322
Running avgs for agent 2: q_loss: 3.747915744781494, p_loss: -1.972870945930481, mean_rew: -0.7042962808358338, variance: 0.9617131670067708, cvar: 28.810699462890625, v: 2.2850778102874756, mean_q: 1.7273368835449219, std_q: 7.902910232543945, lamda: 1.0544520616531372

steps: 179970, episodes: 6000, mean episode reward: -52.46459715443495, agent episode reward: [-14.086254637274815, -19.18917125858006, -19.18917125858006], time: 46.321
steps: 179970, episodes: 6000, mean episode variance: 2.8984555833935737, agent episode variance: [2.465958062887192, 0.21061728642135857, 0.22188023408502341], time: 46.321
steps: 179970, episodes: 6000, mean episode cvar: 17.12254511964321, agent episode cvar: [-0.14587352430820466, 8.645177738189698, 8.623240905761719], time: 46.322
Running avgs for agent 0: q_loss: 0.26417145133018494, p_loss: 7.386734962463379, mean_rew: -0.894237074055458, variance: 8.219860076904297, cvar: -0.48624512553215027, v: -0.6569002866744995, mean_q: -7.773867130279541, std_q: 10.23875904083252, lamda: 0.9957403540611267
Running avgs for agent 1: q_loss: 2.987612724304199, p_loss: -2.5449352264404297, mean_rew: -0.6929480937755165, variance: 0.7020576214045287, cvar: 28.817258834838867, v: 2.6479597091674805, mean_q: 2.2356412410736084, std_q: 8.190417289733887, lamda: 1.0898529291152954
Running avgs for agent 2: q_loss: 2.7327466011047363, p_loss: -2.180825710296631, mean_rew: -0.6967106757421401, variance: 0.7396007802834114, cvar: 28.7441349029541, v: 2.6279075145721436, mean_q: 1.8669079542160034, std_q: 8.673418045043945, lamda: 1.0807256698608398

steps: 209970, episodes: 7000, mean episode reward: -49.864771976267875, agent episode reward: [-14.465277549814175, -17.69974721322685, -17.69974721322685], time: 46.196
steps: 209970, episodes: 7000, mean episode variance: 1.992320349857211, agent episode variance: [1.5741935378015042, 0.22295678427815438, 0.1951700277775526], time: 46.196
steps: 209970, episodes: 7000, mean episode cvar: 17.160156933069228, agent episode cvar: [-0.08370038390159606, 8.628129707336425, 8.6157276096344], time: 46.196
Running avgs for agent 0: q_loss: 0.3311450779438019, p_loss: 6.893450736999512, mean_rew: -0.8268627430060462, variance: 5.24731179267168, cvar: -0.27900129556655884, v: -0.8875068426132202, mean_q: -7.248460292816162, std_q: 10.521288871765137, lamda: 1.0020140409469604
Running avgs for agent 1: q_loss: 3.780066728591919, p_loss: -2.7671244144439697, mean_rew: -0.6833341411530163, variance: 0.7431892809271813, cvar: 28.76043701171875, v: 2.980210781097412, mean_q: 2.463374137878418, std_q: 8.61120319366455, lamda: 1.1155049800872803
Running avgs for agent 2: q_loss: 2.8864448070526123, p_loss: -2.476450204849243, mean_rew: -0.6846734928593208, variance: 0.6505667592585087, cvar: 28.719091415405273, v: 2.960156202316284, mean_q: 2.1082136631011963, std_q: 8.979467391967773, lamda: 1.1060181856155396

steps: 239970, episodes: 8000, mean episode reward: -45.619641910785894, agent episode reward: [-14.114067399420547, -15.752787255682676, -15.752787255682676], time: 46.981
steps: 239970, episodes: 8000, mean episode variance: 1.357390831546858, agent episode variance: [1.0061738107800484, 0.17711418778449298, 0.1741028329823166], time: 46.982
steps: 239970, episodes: 8000, mean episode cvar: 17.120613490521908, agent episode cvar: [-0.046980524599552154, 8.589619979858398, 8.577974035263061], time: 46.982
Running avgs for agent 0: q_loss: 0.34199631214141846, p_loss: 6.799088478088379, mean_rew: -0.7791957316321759, variance: 3.3539127026001614, cvar: -0.15660175681114197, v: -0.9414200782775879, mean_q: -7.125182151794434, std_q: 10.574396133422852, lamda: 1.0148699283599854
Running avgs for agent 1: q_loss: 4.156800746917725, p_loss: -2.9356648921966553, mean_rew: -0.671210652374387, variance: 0.5903806259483099, cvar: 28.63206672668457, v: 3.3044052124023438, mean_q: 2.646190881729126, std_q: 8.930496215820312, lamda: 1.1419297456741333
Running avgs for agent 2: q_loss: 4.074437141418457, p_loss: -2.8234927654266357, mean_rew: -0.6674030648163967, variance: 0.580342776607722, cvar: 28.59324836730957, v: 3.284348964691162, mean_q: 2.4584970474243164, std_q: 9.010212898254395, lamda: 1.1311167478561401

steps: 269970, episodes: 9000, mean episode reward: -40.64979278227631, agent episode reward: [-14.10361984609749, -13.273086468089408, -13.273086468089408], time: 47.585
steps: 269970, episodes: 9000, mean episode variance: 1.0242840529382229, agent episode variance: [0.8785269767940045, 0.031461111560463904, 0.11429596458375454], time: 47.586
steps: 269970, episodes: 9000, mean episode cvar: 17.0759106118083, agent episode cvar: [-0.03979520255327225, 8.538129217147826, 8.577576597213746], time: 47.586
Running avgs for agent 0: q_loss: 0.3371877670288086, p_loss: 6.813534259796143, mean_rew: -0.7450342825521764, variance: 2.928423255980015, cvar: -0.13265067338943481, v: -0.9517417550086975, mean_q: -7.110767364501953, std_q: 10.418562889099121, lamda: 1.0285687446594238
Running avgs for agent 1: q_loss: 3.410517454147339, p_loss: -3.2323923110961914, mean_rew: -0.640892275625313, variance: 0.10487037186821302, cvar: 28.460430145263672, v: 3.622502326965332, mean_q: 2.9372029304504395, std_q: 9.024094581604004, lamda: 1.167251467704773
Running avgs for agent 2: q_loss: 3.938007116317749, p_loss: -3.0819432735443115, mean_rew: -0.6451417379924899, variance: 0.38098654861251513, cvar: 28.591920852661133, v: 3.602445363998413, mean_q: 2.7228453159332275, std_q: 9.1027193069458, lamda: 1.1566091775894165

steps: 299970, episodes: 10000, mean episode reward: -51.840154688232424, agent episode reward: [-14.79800581216297, -18.521074438034727, -18.521074438034727], time: 47.231
steps: 299970, episodes: 10000, mean episode variance: 0.9647927574664354, agent episode variance: [0.9073572217226028, 0.02753126300871372, 0.029904272735118868], time: 47.232
steps: 299970, episodes: 10000, mean episode cvar: 17.09615539070964, agent episode cvar: [-0.03918879607319832, 8.575695230484008, 8.559648956298828], time: 47.232
Running avgs for agent 0: q_loss: 0.3235951364040375, p_loss: 6.95853853225708, mean_rew: -0.7156571742906482, variance: 3.024524072408676, cvar: -0.13062931597232819, v: -0.9646989107131958, mean_q: -7.228848457336426, std_q: 10.472943305969238, lamda: 1.0405486822128296
Running avgs for agent 1: q_loss: 4.342142105102539, p_loss: -3.4263362884521484, mean_rew: -0.6311797341387051, variance: 0.0917708766957124, cvar: 28.585647583007812, v: 3.936006784439087, mean_q: 3.0960168838500977, std_q: 9.280765533447266, lamda: 1.191066026687622
Running avgs for agent 2: q_loss: 4.889278888702393, p_loss: -3.2943475246429443, mean_rew: -0.6278248099984438, variance: 0.09968090911706289, cvar: 28.532161712646484, v: 3.9159483909606934, mean_q: 2.9409866333007812, std_q: 9.280282974243164, lamda: 1.1822326183319092

steps: 329970, episodes: 11000, mean episode reward: -53.936179731701536, agent episode reward: [-13.954511048480551, -19.990834341610494, -19.990834341610494], time: 46.41
steps: 329970, episodes: 11000, mean episode variance: 0.9310846192315221, agent episode variance: [0.8465877126753331, 0.020409395344555378, 0.06408751121163368], time: 46.411
steps: 329970, episodes: 11000, mean episode cvar: 17.06767803257704, agent episode cvar: [-0.04376387912034988, 8.55140535736084, 8.560036554336548], time: 46.411
Running avgs for agent 0: q_loss: 0.31352242827415466, p_loss: 7.161720275878906, mean_rew: -0.6929110053990424, variance: 2.8219590422511103, cvar: -0.1458796113729477, v: -1.0041331052780151, mean_q: -7.420022964477539, std_q: 10.092884063720703, lamda: 1.0517817735671997
Running avgs for agent 1: q_loss: 5.319836139678955, p_loss: -3.71488356590271, mean_rew: -0.6382949144410732, variance: 0.0680313178151846, cvar: 28.504684448242188, v: 4.246066093444824, mean_q: 3.356192111968994, std_q: 9.399112701416016, lamda: 1.2149485349655151
Running avgs for agent 2: q_loss: 4.176529884338379, p_loss: -3.6368777751922607, mean_rew: -0.6313486085885713, variance: 0.21362503737211228, cvar: 28.533456802368164, v: 4.226006984710693, mean_q: 3.2802681922912598, std_q: 9.328719139099121, lamda: 1.2086559534072876

steps: 359970, episodes: 12000, mean episode reward: -39.37151876445787, agent episode reward: [-13.908744892822906, -12.731386935817483, -12.731386935817483], time: 47.138
steps: 359970, episodes: 12000, mean episode variance: 0.9289790438860654, agent episode variance: [0.8673342677354813, 0.035661584258079526, 0.02598319189250469], time: 47.138
steps: 359970, episodes: 12000, mean episode cvar: 17.06612374818325, agent episode cvar: [-0.03455167257785797, 8.542492654800416, 8.558182765960693], time: 47.138
Running avgs for agent 0: q_loss: 0.3301229774951935, p_loss: 7.368545055389404, mean_rew: -0.6782362966002874, variance: 2.8911142257849374, cvar: -0.11517223715782166, v: -1.049034833908081, mean_q: -7.610644340515137, std_q: 10.211776733398438, lamda: 1.0642294883728027
Running avgs for agent 1: q_loss: 4.098400592803955, p_loss: -4.184253692626953, mean_rew: -0.6269953392082526, variance: 0.11887194752693177, cvar: 28.474977493286133, v: 4.553548336029053, mean_q: 3.816847801208496, std_q: 9.285548210144043, lamda: 1.2384746074676514
Running avgs for agent 2: q_loss: 4.3275275230407715, p_loss: -4.151599884033203, mean_rew: -0.6302476861015527, variance: 0.08661063964168231, cvar: 28.52727508544922, v: 4.53348970413208, mean_q: 3.7912631034851074, std_q: 9.09984016418457, lamda: 1.2338507175445557

steps: 389970, episodes: 13000, mean episode reward: -23.44680487852727, agent episode reward: [-14.318763203724583, -4.564020837401344, -4.564020837401344], time: 47.193
steps: 389970, episodes: 13000, mean episode variance: 0.8647468156069518, agent episode variance: [0.8533764701038599, 0.004958884954452515, 0.006411460548639297], time: 47.194
steps: 389970, episodes: 13000, mean episode cvar: 17.116045325100423, agent episode cvar: [-0.025683718860149383, 8.582294733047485, 8.559434310913087], time: 47.194
Running avgs for agent 0: q_loss: 0.3790670335292816, p_loss: 7.353722095489502, mean_rew: -0.6575303083347482, variance: 2.844588233679533, cvar: -0.08561240136623383, v: -1.0749958753585815, mean_q: -7.581506252288818, std_q: 9.824645042419434, lamda: 1.0815565586090088
Running avgs for agent 1: q_loss: 5.457622051239014, p_loss: -4.636447906494141, mean_rew: -0.599807826295462, variance: 0.016529616514841715, cvar: 28.607648849487305, v: 4.859108924865723, mean_q: 4.2692999839782715, std_q: 9.101813316345215, lamda: 1.261081576347351
Running avgs for agent 2: q_loss: 4.7006988525390625, p_loss: -4.608134746551514, mean_rew: -0.597675652721372, variance: 0.021371535162130993, cvar: 28.531444549560547, v: 4.839049339294434, mean_q: 4.260226249694824, std_q: 8.808943748474121, lamda: 1.2565178871154785

steps: 419970, episodes: 14000, mean episode reward: -19.538865271377325, agent episode reward: [-13.431040980264072, -3.0539121455566263, -3.0539121455566263], time: 47.138
steps: 419970, episodes: 14000, mean episode variance: 0.7653022588938475, agent episode variance: [0.7095075081288814, 0.04756230156123638, 0.00823244920372963], time: 47.139
steps: 419970, episodes: 14000, mean episode cvar: 17.12186278104782, agent episode cvar: [-0.020490566730499267, 8.565023946762086, 8.577329401016236], time: 47.139
Running avgs for agent 0: q_loss: 0.36032986640930176, p_loss: 7.260320663452148, mean_rew: -0.6431829642118341, variance: 2.3650250270962716, cvar: -0.06830188632011414, v: -1.077059268951416, mean_q: -7.476481914520264, std_q: 9.681381225585938, lamda: 1.1031233072280884
Running avgs for agent 1: q_loss: 5.243114471435547, p_loss: -4.918313026428223, mean_rew: -0.5613047033053796, variance: 0.15854100520412126, cvar: 28.550077438354492, v: 5.16323709487915, mean_q: 4.5589919090271, std_q: 8.91063117980957, lamda: 1.2858878374099731
Running avgs for agent 2: q_loss: 3.624039649963379, p_loss: -4.899232387542725, mean_rew: -0.5625209764415966, variance: 0.027441497345765432, cvar: 28.591096878051758, v: 5.143177032470703, mean_q: 4.548734664916992, std_q: 8.689888000488281, lamda: 1.279197096824646

steps: 449970, episodes: 15000, mean episode reward: -18.068495881786774, agent episode reward: [-14.118130158917689, -1.975182861434542, -1.975182861434542], time: 47.207
steps: 449970, episodes: 15000, mean episode variance: 0.5321768659949303, agent episode variance: [0.5153187033385038, 0.01638544188439846, 0.00047272077202796937], time: 47.208
steps: 449970, episodes: 15000, mean episode cvar: 17.11559468269348, agent episode cvar: [-0.01628808403015137, 8.551881954193115, 8.580000812530518], time: 47.208
Running avgs for agent 0: q_loss: 0.31989651918411255, p_loss: 7.1280646324157715, mean_rew: -0.6276426447931928, variance: 1.717729011128346, cvar: -0.054293613880872726, v: -1.0509926080703735, mean_q: -7.334067344665527, std_q: 9.413407325744629, lamda: 1.1198701858520508
Running avgs for agent 1: q_loss: 4.91811466217041, p_loss: -5.101097583770752, mean_rew: -0.5345571944044246, variance: 0.05461813961466153, cvar: 28.506275177001953, v: 5.466299533843994, mean_q: 4.745311260223389, std_q: 8.790929794311523, lamda: 1.309993028640747
Running avgs for agent 2: q_loss: 5.046897888183594, p_loss: -5.138762950897217, mean_rew: -0.5334260873888947, variance: 0.0015757359067598978, cvar: 28.600000381469727, v: 5.446240425109863, mean_q: 4.7779436111450195, std_q: 8.5885591506958, lamda: 1.3018176555633545

steps: 479970, episodes: 16000, mean episode reward: -18.67072689578834, agent episode reward: [-13.405892864610415, -2.632417015588963, -2.632417015588963], time: 46.589
steps: 479970, episodes: 16000, mean episode variance: 0.5934771616384387, agent episode variance: [0.5737312254235148, 0.01974593621492386, 0.0], time: 46.589
steps: 479970, episodes: 16000, mean episode cvar: 17.12284158349037, agent episode cvar: [-0.011235450506210326, 8.5745595703125, 8.559517463684083], time: 46.589
Running avgs for agent 0: q_loss: 0.3068654239177704, p_loss: 6.975746154785156, mean_rew: -0.6200603336872423, variance: 1.9124374180783827, cvar: -0.037451498210430145, v: -1.0031720399856567, mean_q: -7.168851852416992, std_q: 9.3881196975708, lamda: 1.1343400478363037
Running avgs for agent 1: q_loss: 4.8314948081970215, p_loss: -5.301616191864014, mean_rew: -0.5019045019997453, variance: 0.06581978738307953, cvar: 28.58186912536621, v: 5.768571853637695, mean_q: 4.936616897583008, std_q: 8.700325965881348, lamda: 1.3362555503845215
Running avgs for agent 2: q_loss: 4.913920879364014, p_loss: -5.309656620025635, mean_rew: -0.5006891252292396, variance: 0.0, cvar: 28.53172492980957, v: 5.748511791229248, mean_q: 4.947312355041504, std_q: 8.607008934020996, lamda: 1.3260492086410522

steps: 509970, episodes: 17000, mean episode reward: -18.10174597996828, agent episode reward: [-13.481208668846218, -2.310268655561029, -2.310268655561029], time: 47.946
steps: 509970, episodes: 17000, mean episode variance: 0.49161175632476806, agent episode variance: [0.4882484036684036, 0.0, 0.003363352656364441], time: 47.946
steps: 509970, episodes: 17000, mean episode cvar: 17.188376552343367, agent episode cvar: [-0.009975025415420532, 8.596936557769775, 8.601415019989014], time: 47.947
Running avgs for agent 0: q_loss: 0.2889915704727173, p_loss: 6.804553031921387, mean_rew: -0.6085430536239468, variance: 1.6274946788946787, cvar: -0.03325008228421211, v: -0.9371622800827026, mean_q: -6.987356662750244, std_q: 9.321280479431152, lamda: 1.1478962898254395
Running avgs for agent 1: q_loss: 5.7928266525268555, p_loss: -5.488037109375, mean_rew: -0.4788329768399321, variance: 0.0, cvar: 28.656452178955078, v: 6.070255756378174, mean_q: 5.123809337615967, std_q: 8.754267692565918, lamda: 1.3613637685775757
Running avgs for agent 2: q_loss: 4.450624942779541, p_loss: -5.4949049949646, mean_rew: -0.47631223074531903, variance: 0.011211175521214803, cvar: 28.671382904052734, v: 6.050195693969727, mean_q: 5.130856990814209, std_q: 8.702033042907715, lamda: 1.3500266075134277

steps: 539970, episodes: 18000, mean episode reward: -16.80389191257254, agent episode reward: [-13.863115285307973, -1.4703883136322806, -1.4703883136322806], time: 47.943
steps: 539970, episodes: 18000, mean episode variance: 0.5453392986431718, agent episode variance: [0.5280241765752435, 0.016753804445266723, 0.0005613176226615906], time: 47.944
steps: 539970, episodes: 18000, mean episode cvar: 17.17277863278985, agent episode cvar: [-0.010518215507268905, 8.613109594345092, 8.570187253952026], time: 47.944
Running avgs for agent 0: q_loss: 0.29068809747695923, p_loss: 6.68733549118042, mean_rew: -0.5997585337161354, variance: 1.7600805885841448, cvar: -0.03506071865558624, v: -0.8829255104064941, mean_q: -6.861448764801025, std_q: 9.386001586914062, lamda: 1.1620885133743286
Running avgs for agent 1: q_loss: 4.4383931159973145, p_loss: -5.709909439086914, mean_rew: -0.4518782934838046, variance: 0.055846014817555745, cvar: 28.710365295410156, v: 6.371503829956055, mean_q: 5.3315749168396, std_q: 8.777999877929688, lamda: 1.3864213228225708
Running avgs for agent 2: q_loss: 4.055642127990723, p_loss: -5.793038845062256, mean_rew: -0.4531484625585539, variance: 0.0018710587422053019, cvar: 28.567291259765625, v: 6.351444721221924, mean_q: 5.4231648445129395, std_q: 8.604289054870605, lamda: 1.3725502490997314

steps: 569970, episodes: 19000, mean episode reward: -17.520906360577964, agent episode reward: [-12.978345221131425, -2.271280569723269, -2.271280569723269], time: 48.036
steps: 569970, episodes: 19000, mean episode variance: 0.40996347223222257, agent episode variance: [0.40996347223222257, 0.0, 0.0], time: 48.037
steps: 569970, episodes: 19000, mean episode cvar: 17.188604288578034, agent episode cvar: [-0.007496255397796631, 8.613473268508912, 8.582627275466919], time: 48.037
Running avgs for agent 0: q_loss: 0.2650308609008789, p_loss: 6.565104961395264, mean_rew: -0.5906163934711766, variance: 1.3665449074407419, cvar: -0.024987518787384033, v: -0.8394399881362915, mean_q: -6.732449531555176, std_q: 9.161147117614746, lamda: 1.1730873584747314
Running avgs for agent 1: q_loss: 5.778055667877197, p_loss: -5.978126525878906, mean_rew: -0.4297521251785477, variance: 0.0, cvar: 28.711578369140625, v: 6.672429084777832, mean_q: 5.603414058685303, std_q: 8.752666473388672, lamda: 1.4123531579971313
Running avgs for agent 2: q_loss: 3.8167097568511963, p_loss: -6.178104877471924, mean_rew: -0.43295970199609013, variance: 0.0, cvar: 28.60875701904297, v: 6.652369976043701, mean_q: 5.792755603790283, std_q: 8.482586860656738, lamda: 1.3964065313339233

steps: 599970, episodes: 20000, mean episode reward: -17.071954611618725, agent episode reward: [-13.043888578216443, -2.0140330167011427, -2.0140330167011427], time: 48.404
steps: 599970, episodes: 20000, mean episode variance: 0.41844995457679035, agent episode variance: [0.41844995457679035, 0.0, 0.0], time: 48.405
steps: 599970, episodes: 20000, mean episode cvar: 17.1668471596539, agent episode cvar: [-0.008320191115140915, 8.57993868637085, 8.595228664398194], time: 48.405
Running avgs for agent 0: q_loss: 0.2618806064128876, p_loss: 6.5159010887146, mean_rew: -0.5841925117219516, variance: 1.3948331819226345, cvar: -0.027733970433473587, v: -0.8233197927474976, mean_q: -6.68074369430542, std_q: 9.069323539733887, lamda: 1.1837687492370605
Running avgs for agent 1: q_loss: 4.488710880279541, p_loss: -6.276569366455078, mean_rew: -0.4104963282278953, variance: 0.0, cvar: 28.599794387817383, v: 6.973114490509033, mean_q: 5.914304256439209, std_q: 8.706780433654785, lamda: 1.4372501373291016
Running avgs for agent 2: q_loss: 4.78375244140625, p_loss: -6.576879978179932, mean_rew: -0.4127476723769647, variance: 0.0, cvar: 28.6507625579834, v: 6.953054428100586, mean_q: 6.2045369148254395, std_q: 8.297808647155762, lamda: 1.4217112064361572

steps: 629970, episodes: 21000, mean episode reward: -16.156627869327167, agent episode reward: [-13.566163814778335, -1.2952320272744156, -1.2952320272744156], time: 47.277
steps: 629970, episodes: 21000, mean episode variance: 0.5281941750496626, agent episode variance: [0.5281941750496626, 0.0, 0.0], time: 47.278
steps: 629970, episodes: 21000, mean episode cvar: 17.156359264463187, agent episode cvar: [-0.009214031130075455, 8.594593826293945, 8.570979469299317], time: 47.278
Running avgs for agent 0: q_loss: 0.30230581760406494, p_loss: 6.438304901123047, mean_rew: -0.5747858643538294, variance: 1.760647250165542, cvar: -0.030713437125086784, v: -0.8160815238952637, mean_q: -6.6016082763671875, std_q: 9.046290397644043, lamda: 1.1986232995986938
Running avgs for agent 1: q_loss: 5.0393242835998535, p_loss: -6.667312145233154, mean_rew: -0.3958249629518139, variance: 0.0, cvar: 28.648645401000977, v: 7.273622035980225, mean_q: 6.291964530944824, std_q: 8.5747709274292, lamda: 1.4617763757705688
Running avgs for agent 2: q_loss: 4.223172187805176, p_loss: -6.994753360748291, mean_rew: -0.3947373141850943, variance: 0.0, cvar: 28.569934844970703, v: 7.253561973571777, mean_q: 6.6161909103393555, std_q: 8.146797180175781, lamda: 1.447718858718872

steps: 659970, episodes: 22000, mean episode reward: -16.386385311870534, agent episode reward: [-12.48423874035383, -1.9510732857583526, -1.9510732857583526], time: 49.219
steps: 659970, episodes: 22000, mean episode variance: 0.36850382831692696, agent episode variance: [0.36850382831692696, 0.0, 0.0], time: 49.22
steps: 659970, episodes: 22000, mean episode cvar: 17.152069988131522, agent episode cvar: [-0.004877077221870423, 8.596467069625854, 8.560479995727539], time: 49.22
Running avgs for agent 0: q_loss: 0.2521553635597229, p_loss: 6.398205757141113, mean_rew: -0.5686344902589644, variance: 1.2283460943897566, cvar: -0.016256922855973244, v: -0.8081890940666199, mean_q: -6.557522773742676, std_q: 9.184786796569824, lamda: 1.2135093212127686
Running avgs for agent 1: q_loss: 4.245088577270508, p_loss: -7.127159595489502, mean_rew: -0.3807576171923096, variance: 0.0, cvar: 28.654890060424805, v: 7.574002265930176, mean_q: 6.751786708831787, std_q: 8.316075325012207, lamda: 1.4871809482574463
Running avgs for agent 2: q_loss: 4.586965560913086, p_loss: -7.4625372886657715, mean_rew: -0.3797708086775348, variance: 0.0, cvar: 28.534934997558594, v: 7.5539422035217285, mean_q: 7.085641860961914, std_q: 7.8489837646484375, lamda: 1.4728566408157349

steps: 689970, episodes: 23000, mean episode reward: -16.05114181573241, agent episode reward: [-12.264339136966788, -1.8934013393828117, -1.8934013393828117], time: 47.688
steps: 689970, episodes: 23000, mean episode variance: 0.4641092411503196, agent episode variance: [0.4641092411503196, 0.0, 0.0], time: 47.689
steps: 689970, episodes: 23000, mean episode cvar: 17.131141773641108, agent episode cvar: [-0.006242137491703033, 8.55567082977295, 8.581713081359863], time: 47.689
Running avgs for agent 0: q_loss: 0.2826443612575531, p_loss: 6.374929428100586, mean_rew: -0.5627125700288939, variance: 1.5470308038343985, cvar: -0.02080712653696537, v: -0.8031198382377625, mean_q: -6.532522678375244, std_q: 9.485795021057129, lamda: 1.2265373468399048
Running avgs for agent 1: q_loss: 4.550530910491943, p_loss: -7.531412601470947, mean_rew: -0.3655441546616684, variance: 0.0, cvar: 28.518903732299805, v: 7.874274253845215, mean_q: 7.1794562339782715, std_q: 8.02441692352295, lamda: 1.511634349822998
Running avgs for agent 2: q_loss: 4.6518402099609375, p_loss: -7.868375778198242, mean_rew: -0.36588678978897404, variance: 0.0, cvar: 28.605709075927734, v: 7.854215621948242, mean_q: 7.506697654724121, std_q: 7.650566577911377, lamda: 1.49845552444458

steps: 719970, episodes: 24000, mean episode reward: -17.502608408160455, agent episode reward: [-12.61107740972143, -2.445765499219512, -2.445765499219512], time: 47.663
steps: 719970, episodes: 24000, mean episode variance: 0.42192376729846, agent episode variance: [0.42192376729846, 0.0, 0.0], time: 47.663
steps: 719970, episodes: 24000, mean episode cvar: 17.180030628442765, agent episode cvar: [-0.010877937078475952, 8.598275142669678, 8.592633422851563], time: 47.664
Running avgs for agent 0: q_loss: 0.2582316994667053, p_loss: 6.317773342132568, mean_rew: -0.558306150837365, variance: 1.4064125576615334, cvar: -0.03625979274511337, v: -0.8030281662940979, mean_q: -6.4776291847229, std_q: 9.410701751708984, lamda: 1.2403171062469482
Running avgs for agent 1: q_loss: 3.4848005771636963, p_loss: -8.014246940612793, mean_rew: -0.35333228957021373, variance: 0.0, cvar: 28.660917282104492, v: 8.174457550048828, mean_q: 7.64621114730835, std_q: 7.745122909545898, lamda: 1.5357218980789185
Running avgs for agent 2: q_loss: 6.096538066864014, p_loss: -8.251341819763184, mean_rew: -0.35551599528940303, variance: 0.0, cvar: 28.642108917236328, v: 8.154406547546387, mean_q: 7.901134490966797, std_q: 7.416933536529541, lamda: 1.524203896522522

steps: 749970, episodes: 25000, mean episode reward: -18.614090388615963, agent episode reward: [-13.149448205854016, -2.7323210913809763, -2.7323210913809763], time: 48.344
steps: 749970, episodes: 25000, mean episode variance: 0.44502225103229287, agent episode variance: [0.44502225103229287, 0.0, 0.0], time: 48.344
steps: 749970, episodes: 25000, mean episode cvar: 17.186476625621317, agent episode cvar: [-0.007882182896137237, 8.592452114105225, 8.601906694412232], time: 48.345
Running avgs for agent 0: q_loss: 0.2760756313800812, p_loss: 6.229389667510986, mean_rew: -0.5522692287038155, variance: 1.4834075034409762, cvar: -0.0262739434838295, v: -0.7810580730438232, mean_q: -6.385156154632568, std_q: 9.465611457824707, lamda: 1.2540836334228516
Running avgs for agent 1: q_loss: 5.043679237365723, p_loss: -8.413359642028809, mean_rew: -0.3433547076435612, variance: 0.0, cvar: 28.641508102416992, v: 8.474578857421875, mean_q: 8.053406715393066, std_q: 7.578881740570068, lamda: 1.5586243867874146
Running avgs for agent 2: q_loss: 6.51826286315918, p_loss: -8.640993118286133, mean_rew: -0.3407637488558919, variance: 0.0, cvar: 28.67302131652832, v: 8.454527854919434, mean_q: 8.303966522216797, std_q: 7.122973442077637, lamda: 1.5507981777191162

steps: 779970, episodes: 26000, mean episode reward: -19.038703925935863, agent episode reward: [-11.944951031428515, -3.5468764472536742, -3.5468764472536742], time: 47.409
steps: 779970, episodes: 26000, mean episode variance: 0.38854874777793885, agent episode variance: [0.38854874777793885, 0.0, 0.0], time: 47.41
steps: 779970, episodes: 26000, mean episode cvar: 17.199511618524788, agent episode cvar: [-0.009422499746084213, 8.606821739196777, 8.602112379074097], time: 47.41
Running avgs for agent 0: q_loss: 0.24960240721702576, p_loss: 6.151175022125244, mean_rew: -0.5453681955422056, variance: 1.2951624925931295, cvar: -0.03140833228826523, v: -0.748624324798584, mean_q: -6.300905704498291, std_q: 8.818131446838379, lamda: 1.2660772800445557
Running avgs for agent 1: q_loss: 4.781029224395752, p_loss: -8.825183868408203, mean_rew: -0.33003355987990346, variance: 0.0, cvar: 28.689403533935547, v: 8.774700164794922, mean_q: 8.478976249694824, std_q: 7.325960159301758, lamda: 1.5838346481323242
Running avgs for agent 2: q_loss: 5.625547409057617, p_loss: -9.006272315979004, mean_rew: -0.33077158562743025, variance: 0.0, cvar: 28.6737117767334, v: 8.754648208618164, mean_q: 8.67794132232666, std_q: 6.895833969116211, lamda: 1.5763417482376099

steps: 809970, episodes: 27000, mean episode reward: -19.562154563491458, agent episode reward: [-12.130413139161744, -3.7158707121648575, -3.7158707121648575], time: 47.862
steps: 809970, episodes: 27000, mean episode variance: 0.4198807716146111, agent episode variance: [0.4198807716146111, 0.0, 0.0], time: 47.862
steps: 809970, episodes: 27000, mean episode cvar: 17.20918731677532, agent episode cvar: [-0.005508514523506165, 8.604149732589722, 8.610546098709106], time: 47.863
Running avgs for agent 0: q_loss: 0.26868560910224915, p_loss: 6.110291004180908, mean_rew: -0.5416819398874991, variance: 1.3996025720487038, cvar: -0.018361715599894524, v: -0.7277739644050598, mean_q: -6.25705099105835, std_q: 8.956876754760742, lamda: 1.2811521291732788
Running avgs for agent 1: q_loss: 6.5480804443359375, p_loss: -9.109503746032715, mean_rew: -0.32518876362055515, variance: 0.0, cvar: 28.68050193786621, v: 9.074820518493652, mean_q: 8.776412010192871, std_q: 7.30837869644165, lamda: 1.6092116832733154
Running avgs for agent 2: q_loss: 4.994426250457764, p_loss: -9.331872940063477, mean_rew: -0.32694862212430725, variance: 0.0, cvar: 28.701820373535156, v: 9.054770469665527, mean_q: 9.021127700805664, std_q: 6.677439212799072, lamda: 1.6021442413330078

steps: 839970, episodes: 28000, mean episode reward: -17.605731951792947, agent episode reward: [-12.285617648881791, -2.660057151455576, -2.660057151455576], time: 47.456
steps: 839970, episodes: 28000, mean episode variance: 0.4055026908740401, agent episode variance: [0.4055026908740401, 0.0, 0.0], time: 47.456
steps: 839970, episodes: 28000, mean episode cvar: 17.270397054225207, agent episode cvar: [-0.002430864781141281, 8.628238395690918, 8.64458952331543], time: 47.456
Running avgs for agent 0: q_loss: 0.25648370385169983, p_loss: 6.046572685241699, mean_rew: -0.5372446870371843, variance: 1.3516756362468003, cvar: -0.008102882653474808, v: -0.7080733776092529, mean_q: -6.186552047729492, std_q: 8.894006729125977, lamda: 1.2938957214355469
Running avgs for agent 1: q_loss: 6.629439353942871, p_loss: -9.436931610107422, mean_rew: -0.31763654116543444, variance: 0.0, cvar: 28.760793685913086, v: 9.3749418258667, mean_q: 9.10839557647705, std_q: 7.195594310760498, lamda: 1.6353949308395386
Running avgs for agent 2: q_loss: 6.779334545135498, p_loss: -9.62353515625, mean_rew: -0.31630876493996424, variance: 0.0, cvar: 28.81529998779297, v: 9.354891777038574, mean_q: 9.32395076751709, std_q: 6.551841735839844, lamda: 1.6277382373809814

steps: 869970, episodes: 29000, mean episode reward: -14.928339338442195, agent episode reward: [-11.142798296255256, -1.892770521093469, -1.892770521093469], time: 46.912
steps: 869970, episodes: 29000, mean episode variance: 0.35729317560046914, agent episode variance: [0.35729317560046914, 0.0, 0.0], time: 46.912
steps: 869970, episodes: 29000, mean episode cvar: 17.202676803320646, agent episode cvar: [-0.005033113747835159, 8.588485912322998, 8.619224004745483], time: 46.912
Running avgs for agent 0: q_loss: 0.2506684362888336, p_loss: 5.950786113739014, mean_rew: -0.526689710595291, variance: 1.1909772520015638, cvar: -0.016777044162154198, v: -0.6872021555900574, mean_q: -6.083621501922607, std_q: 8.472540855407715, lamda: 1.3052853345870972
Running avgs for agent 1: q_loss: 4.394268989562988, p_loss: -9.796854019165039, mean_rew: -0.30710503546953577, variance: 0.0, cvar: 28.628284454345703, v: 9.675063133239746, mean_q: 9.465353965759277, std_q: 6.978085994720459, lamda: 1.6609631776809692
Running avgs for agent 2: q_loss: 5.269418239593506, p_loss: -9.9359712600708, mean_rew: -0.3074017266788501, variance: 0.0, cvar: 28.730749130249023, v: 9.655013084411621, mean_q: 9.64880657196045, std_q: 6.338476657867432, lamda: 1.6531277894973755

steps: 899970, episodes: 30000, mean episode reward: -14.25772878571193, agent episode reward: [-11.892563685112382, -1.1825825502997747, -1.1825825502997747], time: 47.937
steps: 899970, episodes: 30000, mean episode variance: 0.46180106100440027, agent episode variance: [0.46180106100440027, 0.0, 0.0], time: 47.937
steps: 899970, episodes: 30000, mean episode cvar: 17.183791007280348, agent episode cvar: [-0.004561348676681519, 8.596066188812255, 8.592286167144776], time: 47.937
Running avgs for agent 0: q_loss: 0.2732968330383301, p_loss: 5.9283905029296875, mean_rew: -0.526142861966614, variance: 1.5393368700146675, cvar: -0.015204496681690216, v: -0.6702327728271484, mean_q: -6.063383102416992, std_q: 8.911616325378418, lamda: 1.3205344676971436
Running avgs for agent 1: q_loss: 5.239161014556885, p_loss: -10.120758056640625, mean_rew: -0.30010267560347376, variance: 0.0, cvar: 28.653554916381836, v: 9.97518539428711, mean_q: 9.788460731506348, std_q: 6.869702339172363, lamda: 1.6862879991531372
Running avgs for agent 2: q_loss: 4.263506889343262, p_loss: -10.14925765991211, mean_rew: -0.3017825992014814, variance: 0.0, cvar: 28.640953063964844, v: 9.955134391784668, mean_q: 9.873727798461914, std_q: 6.339272975921631, lamda: 1.6783006191253662

steps: 929970, episodes: 31000, mean episode reward: -13.452993342633814, agent episode reward: [-12.155223726816496, -0.6488848079086587, -0.6488848079086587], time: 48.093
steps: 929970, episodes: 31000, mean episode variance: 0.5082161796689033, agent episode variance: [0.5082161796689033, 0.0, 0.0], time: 48.094
steps: 929970, episodes: 31000, mean episode cvar: 17.217816402435304, agent episode cvar: [-0.008324111938476562, 8.612334604263305, 8.613805910110473], time: 48.094
Running avgs for agent 0: q_loss: 0.2903284430503845, p_loss: 5.861426830291748, mean_rew: -0.5226827210761423, variance: 1.694053932229678, cvar: -0.02774704061448574, v: -0.6616289615631104, mean_q: -5.997753143310547, std_q: 9.098554611206055, lamda: 1.337700605392456
Running avgs for agent 1: q_loss: 4.5961809158325195, p_loss: -10.451752662658691, mean_rew: -0.29023441415904083, variance: 0.0, cvar: 28.707780838012695, v: 10.27530574798584, mean_q: 10.129103660583496, std_q: 6.759669780731201, lamda: 1.7108983993530273
Running avgs for agent 2: q_loss: 5.823004722595215, p_loss: -10.453343391418457, mean_rew: -0.2929228581649188, variance: 0.0, cvar: 28.712688446044922, v: 10.255255699157715, mean_q: 10.187703132629395, std_q: 6.238043308258057, lamda: 1.702854037284851

steps: 959970, episodes: 32000, mean episode reward: -13.369953193971895, agent episode reward: [-12.023589944916763, -0.6731816245275668, -0.6731816245275668], time: 46.823
steps: 959970, episodes: 32000, mean episode variance: 0.4718272065743804, agent episode variance: [0.4718272065743804, 0.0, 0.0], time: 46.823
steps: 959970, episodes: 32000, mean episode cvar: 17.195230013012885, agent episode cvar: [-0.009599616885185241, 8.600031175613402, 8.604798454284667], time: 46.824
Running avgs for agent 0: q_loss: 0.26615670323371887, p_loss: 5.736904621124268, mean_rew: -0.5186515742662103, variance: 1.5727573552479346, cvar: -0.03199872374534607, v: -0.6504214406013489, mean_q: -5.867288589477539, std_q: 8.824294090270996, lamda: 1.3562155961990356
Running avgs for agent 1: q_loss: 6.305643558502197, p_loss: -10.7219877243042, mean_rew: -0.284117974471028, variance: 0.0, cvar: 28.666770935058594, v: 10.575427055358887, mean_q: 10.402097702026367, std_q: 6.7673773765563965, lamda: 1.736029028892517
Running avgs for agent 2: q_loss: 5.849043846130371, p_loss: -10.73531723022461, mean_rew: -0.2796566399730251, variance: 0.0, cvar: 28.682662963867188, v: 10.555377960205078, mean_q: 10.480487823486328, std_q: 6.145672798156738, lamda: 1.7288967370986938

steps: 989970, episodes: 33000, mean episode reward: -13.305672142823719, agent episode reward: [-12.381864780067504, -0.46190368137810744, -0.46190368137810744], time: 48.119
steps: 989970, episodes: 33000, mean episode variance: 0.39044718097150327, agent episode variance: [0.39044718097150327, 0.0, 0.0], time: 48.119
steps: 989970, episodes: 33000, mean episode cvar: 17.19611154296994, agent episode cvar: [-0.011551766127347946, 8.60083236503601, 8.60683094406128], time: 48.119
Running avgs for agent 0: q_loss: 0.23744145035743713, p_loss: 5.6429877281188965, mean_rew: -0.5154434403913973, variance: 1.3014906032383442, cvar: -0.03850588947534561, v: -0.638823390007019, mean_q: -5.771236896514893, std_q: 8.725435256958008, lamda: 1.3673725128173828
Running avgs for agent 1: q_loss: 4.908251762390137, p_loss: -11.014249801635742, mean_rew: -0.27486100220960896, variance: 0.0, cvar: 28.6694393157959, v: 10.87554931640625, mean_q: 10.697724342346191, std_q: 6.742834091186523, lamda: 1.7615466117858887
Running avgs for agent 2: q_loss: 5.48330545425415, p_loss: -10.974501609802246, mean_rew: -0.2760689013222476, variance: 0.0, cvar: 28.68943977355957, v: 10.855497360229492, mean_q: 10.725837707519531, std_q: 6.167155742645264, lamda: 1.754744052886963

steps: 1019970, episodes: 34000, mean episode reward: -12.01642295855646, agent episode reward: [-11.918239040930755, -0.04909195881285223, -0.04909195881285223], time: 47.673
steps: 1019970, episodes: 34000, mean episode variance: 0.3976864000186324, agent episode variance: [0.3976864000186324, 0.0, 0.0], time: 47.674
steps: 1019970, episodes: 34000, mean episode cvar: 17.137986435830594, agent episode cvar: [-0.009717091619968414, 8.566422313690186, 8.581281213760375], time: 47.674
Running avgs for agent 0: q_loss: 0.2401207983493805, p_loss: 5.5238189697265625, mean_rew: -0.5075325317447207, variance: 1.3256213333954414, cvar: -0.032390303909778595, v: -0.6145423650741577, mean_q: -5.64231014251709, std_q: 8.54879093170166, lamda: 1.3787956237792969
Running avgs for agent 1: q_loss: 4.656850814819336, p_loss: -11.314894676208496, mean_rew: -0.26728816036090736, variance: 0.0, cvar: 28.554738998413086, v: 11.175670623779297, mean_q: 11.009418487548828, std_q: 6.633999347686768, lamda: 1.7868412733078003
Running avgs for agent 2: q_loss: 5.90052604675293, p_loss: -11.223830223083496, mean_rew: -0.2706374141596179, variance: 0.0, cvar: 28.604270935058594, v: 11.155619621276855, mean_q: 10.975666046142578, std_q: 6.1412482261657715, lamda: 1.7791284322738647

steps: 1049970, episodes: 35000, mean episode reward: -12.319880565719671, agent episode reward: [-12.06345899974027, -0.12821078298970093, -0.12821078298970093], time: 47.618
steps: 1049970, episodes: 35000, mean episode variance: 0.37412811478227376, agent episode variance: [0.37412811478227376, 0.0, 0.0], time: 47.619
steps: 1049970, episodes: 35000, mean episode cvar: 17.12304634487629, agent episode cvar: [-0.012232917666435241, 8.551962696075439, 8.583316566467285], time: 47.619
Running avgs for agent 0: q_loss: 0.2253093421459198, p_loss: 5.220806121826172, mean_rew: -0.47198811915669325, variance: 1.2470937159409126, cvar: -0.04077639430761337, v: -0.600479781627655, mean_q: -5.304011821746826, std_q: 6.831998825073242, lamda: 1.3951061964035034
Running avgs for agent 1: q_loss: 6.560068130493164, p_loss: -11.584136962890625, mean_rew: -0.2755866638516582, variance: 0.0, cvar: 28.506542205810547, v: 11.475790977478027, mean_q: 11.29037857055664, std_q: 6.616586208343506, lamda: 1.811566948890686
Running avgs for agent 2: q_loss: 4.780653476715088, p_loss: -11.528836250305176, mean_rew: -0.2760550438100848, variance: 0.0, cvar: 28.611055374145508, v: 11.455740928649902, mean_q: 11.279747009277344, std_q: 6.037725448608398, lamda: 1.8035485744476318

steps: 1079970, episodes: 36000, mean episode reward: -11.841567821469257, agent episode reward: [-11.453383285788949, -0.19409226784015407, -0.19409226784015407], time: 47.769
steps: 1079970, episodes: 36000, mean episode variance: 0.2357657454572618, agent episode variance: [0.2357657454572618, 0.0, 0.0], time: 47.769
steps: 1079970, episodes: 36000, mean episode cvar: 17.191740381479264, agent episode cvar: [-0.009829208135604858, 8.613121265411378, 8.58844832420349], time: 47.769
Running avgs for agent 0: q_loss: 0.17201358079910278, p_loss: 4.838242053985596, mean_rew: -0.44009860157433534, variance: 0.7858858181908727, cvar: -0.032764025032520294, v: -0.5723900198936462, mean_q: -4.885094165802002, std_q: 4.605167865753174, lamda: 1.4089927673339844
Running avgs for agent 1: q_loss: 4.253728866577148, p_loss: -12.153717041015625, mean_rew: -0.23873459528544164, variance: 0.0, cvar: 28.710407257080078, v: 11.77591323852539, mean_q: 11.913796424865723, std_q: 5.717379570007324, lamda: 1.8369048833847046
Running avgs for agent 2: q_loss: 6.309087753295898, p_loss: -12.05077838897705, mean_rew: -0.2405893409870757, variance: 0.0, cvar: 28.62816047668457, v: 11.75586223602295, mean_q: 11.833135604858398, std_q: 5.288147926330566, lamda: 1.827436089515686

steps: 1109970, episodes: 37000, mean episode reward: -11.578195558093299, agent episode reward: [-11.454385268461001, -0.061905144816149206, -0.061905144816149206], time: 47.83
steps: 1109970, episodes: 37000, mean episode variance: 0.21328873894922434, agent episode variance: [0.21328873894922434, 0.0, 0.0], time: 47.831
steps: 1109970, episodes: 37000, mean episode cvar: 17.179404740810394, agent episode cvar: [-0.013102587223052978, 8.58533473968506, 8.607172588348389], time: 47.831
Running avgs for agent 0: q_loss: 0.16026268899440765, p_loss: 4.6958818435668945, mean_rew: -0.43661874599535905, variance: 0.7109624631640812, cvar: -0.04367528855800629, v: -0.5379841923713684, mean_q: -4.740563869476318, std_q: 4.542534351348877, lamda: 1.4232912063598633
Running avgs for agent 1: q_loss: 5.628177165985107, p_loss: -12.666268348693848, mean_rew: -0.20129552181725818, variance: 0.0, cvar: 28.617782592773438, v: 12.076034545898438, mean_q: 12.459447860717773, std_q: 4.938755989074707, lamda: 1.862789511680603
Running avgs for agent 2: q_loss: 5.933614730834961, p_loss: -12.569469451904297, mean_rew: -0.20062483887084412, variance: 0.0, cvar: 28.69057273864746, v: 12.055983543395996, mean_q: 12.383398056030273, std_q: 4.487872123718262, lamda: 1.8520176410675049

steps: 1139970, episodes: 38000, mean episode reward: -12.83381752977847, agent episode reward: [-11.938988723255243, -0.44741440326161414, -0.44741440326161414], time: 47.129
steps: 1139970, episodes: 38000, mean episode variance: 0.18890492755174637, agent episode variance: [0.18890492755174637, 0.0, 0.0], time: 47.129
steps: 1139970, episodes: 38000, mean episode cvar: 17.199530404895544, agent episode cvar: [-0.009259096294641495, 8.617289150238037, 8.591500350952149], time: 47.13
Running avgs for agent 0: q_loss: 0.16258929669857025, p_loss: 4.563144683837891, mean_rew: -0.4331407257258909, variance: 0.6296830918391546, cvar: -0.030863653868436813, v: -0.5127148628234863, mean_q: -4.606668949127197, std_q: 4.481067657470703, lamda: 1.435853123664856
Running avgs for agent 1: q_loss: 6.181230545043945, p_loss: -13.051843643188477, mean_rew: -0.1804373997368411, variance: 0.0, cvar: 28.72429656982422, v: 12.376155853271484, mean_q: 12.857373237609863, std_q: 4.5425496101379395, lamda: 1.88829505443573
Running avgs for agent 2: q_loss: 4.044992446899414, p_loss: -12.840744972229004, mean_rew: -0.17936691311950462, variance: 0.0, cvar: 28.638336181640625, v: 12.356103897094727, mean_q: 12.663962364196777, std_q: 4.404871463775635, lamda: 1.8762485980987549

steps: 1169970, episodes: 39000, mean episode reward: -13.781561932851695, agent episode reward: [-11.60983999935737, -1.0858609667471633, -1.0858609667471633], time: 48.307
steps: 1169970, episodes: 39000, mean episode variance: 0.18984706483595074, agent episode variance: [0.18984706483595074, 0.0, 0.0], time: 48.308
steps: 1169970, episodes: 39000, mean episode cvar: 17.18035250233114, agent episode cvar: [-0.007984964862465858, 8.612557817459107, 8.575779649734496], time: 48.308
Running avgs for agent 0: q_loss: 0.16165633499622345, p_loss: 4.452610492706299, mean_rew: -0.42995611924282096, variance: 0.6328235494531691, cvar: -0.026616550981998444, v: -0.4806014895439148, mean_q: -4.497213363647461, std_q: 4.437809467315674, lamda: 1.4506473541259766
Running avgs for agent 1: q_loss: 6.988566875457764, p_loss: -13.386014938354492, mean_rew: -0.1612805002604624, variance: 0.0, cvar: 28.708524703979492, v: 12.676277160644531, mean_q: 13.20786190032959, std_q: 4.073046684265137, lamda: 1.91460120677948
Running avgs for agent 2: q_loss: 6.222266674041748, p_loss: -13.16906452178955, mean_rew: -0.1617144623851391, variance: 0.0, cvar: 28.585933685302734, v: 12.656225204467773, mean_q: 13.001567840576172, std_q: 4.091102600097656, lamda: 1.9010424613952637

steps: 1199970, episodes: 40000, mean episode reward: -13.341780322994616, agent episode reward: [-11.456876194240754, -0.9424520643769306, -0.9424520643769306], time: 48.174
steps: 1199970, episodes: 40000, mean episode variance: 0.1802912999726832, agent episode variance: [0.1802912999726832, 0.0, 0.0], time: 48.175
steps: 1199970, episodes: 40000, mean episode cvar: 17.189925028666853, agent episode cvar: [-0.009656912937760354, 8.6262978515625, 8.573284090042113], time: 48.175
Running avgs for agent 0: q_loss: 0.16421322524547577, p_loss: 4.340131759643555, mean_rew: -0.42811381938446386, variance: 0.600970999908944, cvar: -0.032189711928367615, v: -0.4738613963127136, mean_q: -4.385179042816162, std_q: 4.404232501983643, lamda: 1.4655307531356812
Running avgs for agent 1: q_loss: 6.820290565490723, p_loss: -13.724742889404297, mean_rew: -0.14263390758802116, variance: 0.0, cvar: 28.75432586669922, v: 12.976398468017578, mean_q: 13.566583633422852, std_q: 3.6909587383270264, lamda: 1.940086841583252
Running avgs for agent 2: q_loss: 5.45962381362915, p_loss: -13.459906578063965, mean_rew: -0.1431905522257546, variance: 0.0, cvar: 28.577617645263672, v: 12.956347465515137, mean_q: 13.307765007019043, std_q: 3.923064708709717, lamda: 1.9264302253723145

steps: 1229970, episodes: 41000, mean episode reward: -12.448052659526724, agent episode reward: [-11.77334483216254, -0.33735391368209183, -0.33735391368209183], time: 47.344
steps: 1229970, episodes: 41000, mean episode variance: 0.20025531569495797, agent episode variance: [0.20025531569495797, 0.0, 0.0], time: 47.345
steps: 1229970, episodes: 41000, mean episode cvar: 17.18395668916404, agent episode cvar: [-0.011485840514302254, 8.62144400215149, 8.573998527526856], time: 47.345
Running avgs for agent 0: q_loss: 0.1618100255727768, p_loss: 4.206322193145752, mean_rew: -0.42283484445974645, variance: 0.6675177189831932, cvar: -0.03828613460063934, v: -0.4691343307495117, mean_q: -4.251133441925049, std_q: 4.340059280395508, lamda: 1.480849266052246
Running avgs for agent 1: q_loss: 7.017209529876709, p_loss: -14.020519256591797, mean_rew: -0.12605010135428665, variance: 0.0, cvar: 28.738143920898438, v: 13.276519775390625, mean_q: 13.872314453125, std_q: 3.425659656524658, lamda: 1.9659887552261353
Running avgs for agent 2: q_loss: 6.326870441436768, p_loss: -13.750246047973633, mean_rew: -0.1269495566633663, variance: 0.0, cvar: 28.57999610900879, v: 13.256467819213867, mean_q: 13.605996131896973, std_q: 3.8118557929992676, lamda: 1.949232578277588

steps: 1259970, episodes: 42000, mean episode reward: -12.92120736728142, agent episode reward: [-11.048770747388886, -0.9362183099462678, -0.9362183099462678], time: 48.52
steps: 1259970, episodes: 42000, mean episode variance: 0.2015334626249969, agent episode variance: [0.2015334626249969, 0.0, 0.0], time: 48.521
steps: 1259970, episodes: 42000, mean episode cvar: 17.18939743053913, agent episode cvar: [-0.008364999651908874, 8.616884239196777, 8.580878190994262], time: 48.521
Running avgs for agent 0: q_loss: 0.16872715950012207, p_loss: 4.095991611480713, mean_rew: -0.4243259255813788, variance: 0.6717782087499896, cvar: -0.02788333222270012, v: -0.4543200731277466, mean_q: -4.140566349029541, std_q: 4.301475524902344, lamda: 1.4953871965408325
Running avgs for agent 1: q_loss: 4.945442199707031, p_loss: -14.30591106414795, mean_rew: -0.11021774987326471, variance: 0.0, cvar: 28.722949981689453, v: 13.576641082763672, mean_q: 14.175700187683105, std_q: 3.260352611541748, lamda: 1.9910054206848145
Running avgs for agent 2: q_loss: 5.323657035827637, p_loss: -13.99559211730957, mean_rew: -0.1120673190290533, variance: 0.0, cvar: 28.60292625427246, v: 13.556589126586914, mean_q: 13.863748550415039, std_q: 3.794184684753418, lamda: 1.973048448562622

steps: 1289970, episodes: 43000, mean episode reward: -13.338852094040131, agent episode reward: [-11.672534283892421, -0.8331589050738553, -0.8331589050738553], time: 47.348
steps: 1289970, episodes: 43000, mean episode variance: 0.17200973726809024, agent episode variance: [0.17200973726809024, 0.0, 0.0], time: 47.349
steps: 1289970, episodes: 43000, mean episode cvar: 17.23391000699997, agent episode cvar: [-0.006456408023834228, 8.633612520217895, 8.606753894805909], time: 47.349
Running avgs for agent 0: q_loss: 0.15233083069324493, p_loss: 3.9497196674346924, mean_rew: -0.4194913008645817, variance: 0.5733657908936342, cvar: -0.02152135968208313, v: -0.44550827145576477, mean_q: -3.994060754776001, std_q: 4.21905517578125, lamda: 1.511682152748108
Running avgs for agent 1: q_loss: 6.049464702606201, p_loss: -14.575936317443848, mean_rew: -0.09800418844822391, variance: 0.0, cvar: 28.778711318969727, v: 13.876761436462402, mean_q: 14.45654010772705, std_q: 3.21707820892334, lamda: 2.0131735801696777
Running avgs for agent 2: q_loss: 6.735154628753662, p_loss: -14.27701473236084, mean_rew: -0.09902191535367959, variance: 0.0, cvar: 28.689180374145508, v: 13.856710433959961, mean_q: 14.14925765991211, std_q: 3.766493797302246, lamda: 1.9969505071640015

steps: 1319970, episodes: 44000, mean episode reward: -14.398106012071585, agent episode reward: [-12.276278024432333, -1.060913993819626, -1.060913993819626], time: 48.593
steps: 1319970, episodes: 44000, mean episode variance: 0.18393097824417054, agent episode variance: [0.18393097824417054, 0.0, 0.0], time: 48.594
steps: 1319970, episodes: 44000, mean episode cvar: 17.172435064703226, agent episode cvar: [-0.009963046640157699, 8.59602141571045, 8.586376695632934], time: 48.594
Running avgs for agent 0: q_loss: 0.15367934107780457, p_loss: 3.829127311706543, mean_rew: -0.41719291097137573, variance: 0.6131032608139019, cvar: -0.033210158348083496, v: -0.4441802501678467, mean_q: -3.8721415996551514, std_q: 4.162291049957275, lamda: 1.5248404741287231
Running avgs for agent 1: q_loss: 7.222550392150879, p_loss: -14.80542278289795, mean_rew: -0.07897253179758412, variance: 0.0, cvar: 28.653404235839844, v: 14.176884651184082, mean_q: 14.697535514831543, std_q: 3.1453258991241455, lamda: 2.0391790866851807
Running avgs for agent 2: q_loss: 5.157873630523682, p_loss: -14.628169059753418, mean_rew: -0.08064208990903542, variance: 0.0, cvar: 28.62125587463379, v: 14.156832695007324, mean_q: 14.488484382629395, std_q: 3.524704933166504, lamda: 2.020615339279175

steps: 1349970, episodes: 45000, mean episode reward: -15.493406270191494, agent episode reward: [-12.034188018964505, -1.7296091256134947, -1.7296091256134947], time: 47.873
steps: 1349970, episodes: 45000, mean episode variance: 0.1691104485578835, agent episode variance: [0.1691104485578835, 0.0, 0.0], time: 47.874
steps: 1349970, episodes: 45000, mean episode cvar: 17.19411497592926, agent episode cvar: [-0.008071852684020995, 8.60216130065918, 8.600025527954102], time: 47.874
Running avgs for agent 0: q_loss: 0.13785479962825775, p_loss: 3.7114624977111816, mean_rew: -0.4155154967803804, variance: 0.563701495192945, cvar: -0.026906175538897514, v: -0.42612701654434204, mean_q: -3.7555036544799805, std_q: 4.116979122161865, lamda: 1.536574125289917
Running avgs for agent 1: q_loss: 5.915026664733887, p_loss: -15.077252388000488, mean_rew: -0.062330021555148794, variance: 0.0, cvar: 28.673870086669922, v: 14.477005004882812, mean_q: 14.983053207397461, std_q: 3.038321018218994, lamda: 2.064155340194702
Running avgs for agent 2: q_loss: 7.6318278312683105, p_loss: -14.858759880065918, mean_rew: -0.06309273583836587, variance: 0.0, cvar: 28.666751861572266, v: 14.456953048706055, mean_q: 14.742208480834961, std_q: 3.388105630874634, lamda: 2.046480655670166

steps: 1379970, episodes: 46000, mean episode reward: -13.953469675832384, agent episode reward: [-11.767945681849941, -1.0927619969912221, -1.0927619969912221], time: 47.88
steps: 1379970, episodes: 46000, mean episode variance: 0.1814513141065836, agent episode variance: [0.1814513141065836, 0.0, 0.0], time: 47.881
steps: 1379970, episodes: 46000, mean episode cvar: 17.227735999062656, agent episode cvar: [-0.009059744879603386, 8.614616451263428, 8.622179292678833], time: 47.881
Running avgs for agent 0: q_loss: 0.15115022659301758, p_loss: 3.6018033027648926, mean_rew: -0.41234596406963864, variance: 0.6048377136886119, cvar: -0.030199149623513222, v: -0.42358076572418213, mean_q: -3.6447489261627197, std_q: 4.054847717285156, lamda: 1.5509222745895386
Running avgs for agent 1: q_loss: 5.714768409729004, p_loss: -15.323827743530273, mean_rew: -0.053039807482032006, variance: 0.0, cvar: 28.715391159057617, v: 14.777125358581543, mean_q: 15.23772144317627, std_q: 2.9990906715393066, lamda: 2.088310956954956
Running avgs for agent 2: q_loss: 5.351465702056885, p_loss: -15.121230125427246, mean_rew: -0.054477191975254795, variance: 0.0, cvar: 28.740598678588867, v: 14.757075309753418, mean_q: 15.02962875366211, std_q: 3.267038345336914, lamda: 2.0713131427764893

steps: 1409970, episodes: 47000, mean episode reward: -13.313057423491038, agent episode reward: [-11.525689189300161, -0.8936841170954386, -0.8936841170954386], time: 47.855
steps: 1409970, episodes: 47000, mean episode variance: 0.21335422433912754, agent episode variance: [0.21335422433912754, 0.0, 0.0], time: 47.856
steps: 1409970, episodes: 47000, mean episode cvar: 17.198202661827207, agent episode cvar: [-0.008640832588076592, 8.614695657730103, 8.59214783668518], time: 47.856
Running avgs for agent 0: q_loss: 0.17729806900024414, p_loss: 3.5186376571655273, mean_rew: -0.41096232393613513, variance: 0.7111807477970918, cvar: -0.02880277670919895, v: -0.41888484358787537, mean_q: -3.5619819164276123, std_q: 4.001633167266846, lamda: 1.569704532623291
Running avgs for agent 1: q_loss: 5.8056793212890625, p_loss: -15.576517105102539, mean_rew: -0.05045518303591317, variance: 0.0, cvar: 28.71565055847168, v: 15.077247619628906, mean_q: 15.492447853088379, std_q: 2.9621284008026123, lamda: 2.1116349697113037
Running avgs for agent 2: q_loss: 6.124273777008057, p_loss: -15.350143432617188, mean_rew: -0.049956657224259175, variance: 0.0, cvar: 28.640491485595703, v: 15.057195663452148, mean_q: 15.27099323272705, std_q: 3.230483293533325, lamda: 2.0939910411834717

steps: 1439970, episodes: 48000, mean episode reward: -15.27715058738547, agent episode reward: [-11.478130612205026, -1.8995099875902237, -1.8995099875902237], time: 47.487
steps: 1439970, episodes: 48000, mean episode variance: 0.18069813821464778, agent episode variance: [0.18069813821464778, 0.0, 0.0], time: 47.488
steps: 1439970, episodes: 48000, mean episode cvar: 17.208384385332465, agent episode cvar: [-0.008697565808892251, 8.617131532669067, 8.59995041847229], time: 47.488
Running avgs for agent 0: q_loss: 0.17238134145736694, p_loss: 3.4329965114593506, mean_rew: -0.4081458160309986, variance: 0.6023271273821592, cvar: -0.028991887345910072, v: -0.4158734977245331, mean_q: -3.4779129028320312, std_q: 3.963181972503662, lamda: 1.590572714805603
Running avgs for agent 1: q_loss: 5.547816753387451, p_loss: -15.817887306213379, mean_rew: -0.0488580005466517, variance: 0.0, cvar: 28.723772048950195, v: 15.377367973327637, mean_q: 15.740989685058594, std_q: 2.9423582553863525, lamda: 2.135176658630371
Running avgs for agent 2: q_loss: 5.299456596374512, p_loss: -15.583300590515137, mean_rew: -0.047974895216053956, variance: 0.0, cvar: 28.66650390625, v: 15.357317924499512, mean_q: 15.498574256896973, std_q: 3.225257158279419, lamda: 2.118236780166626

steps: 1469970, episodes: 49000, mean episode reward: -15.37899317575812, agent episode reward: [-11.759698599220972, -1.8096472882685735, -1.8096472882685735], time: 48.605
steps: 1469970, episodes: 49000, mean episode variance: 0.13564863748475908, agent episode variance: [0.13564863748475908, 0.0, 0.0], time: 48.605
steps: 1469970, episodes: 49000, mean episode cvar: 17.192806158438326, agent episode cvar: [-0.009113441094756126, 8.608346532821654, 8.593573066711425], time: 48.606
Running avgs for agent 0: q_loss: 0.13552765548229218, p_loss: 3.36189341545105, mean_rew: -0.40581272186253536, variance: 0.452162124949197, cvar: -0.030378134921193123, v: -0.399278849363327, mean_q: -3.4061801433563232, std_q: 3.934361457824707, lamda: 1.6027288436889648
Running avgs for agent 1: q_loss: 4.644643783569336, p_loss: -16.045703887939453, mean_rew: -0.04796253719027233, variance: 0.0, cvar: 28.694488525390625, v: 15.677490234375, mean_q: 15.97437858581543, std_q: 2.936619758605957, lamda: 2.1567423343658447
Running avgs for agent 2: q_loss: 6.621113300323486, p_loss: -15.810942649841309, mean_rew: -0.048920739682596956, variance: 0.0, cvar: 28.645244598388672, v: 15.657438278198242, mean_q: 15.730887413024902, std_q: 3.2227582931518555, lamda: 2.141444444656372

steps: 1499970, episodes: 50000, mean episode reward: -14.328456675526397, agent episode reward: [-11.339412777674545, -1.4945219489259256, -1.4945219489259256], time: 47.483
steps: 1499970, episodes: 50000, mean episode variance: 0.1369228665754199, agent episode variance: [0.1369228665754199, 0.0, 0.0], time: 47.483
steps: 1499970, episodes: 50000, mean episode cvar: 17.141336864948272, agent episode cvar: [-0.009793025493621826, 8.58328482246399, 8.567845067977906], time: 47.484
Running avgs for agent 0: q_loss: 0.1428464949131012, p_loss: 3.2759172916412354, mean_rew: -0.40341481276602953, variance: 0.4564095552513997, cvar: -0.03264341875910759, v: -0.38360220193862915, mean_q: -3.319652557373047, std_q: 3.877368927001953, lamda: 1.6144086122512817
Running avgs for agent 1: q_loss: 8.017916679382324, p_loss: -16.24772834777832, mean_rew: -0.046382855429849525, variance: 0.0, cvar: 28.610950469970703, v: 15.977584838867188, mean_q: 16.17635154724121, std_q: 2.946887493133545, lamda: 2.1797475814819336
Running avgs for agent 2: q_loss: 5.6861443519592285, p_loss: -15.997838973999023, mean_rew: -0.048402594597271496, variance: 0.0, cvar: 28.55948257446289, v: 15.957542419433594, mean_q: 15.925276756286621, std_q: 3.24039363861084, lamda: 2.1651270389556885

steps: 1529970, episodes: 51000, mean episode reward: -14.042748583667416, agent episode reward: [-11.243075398045214, -1.3998365928111025, -1.3998365928111025], time: 48.668
steps: 1529970, episodes: 51000, mean episode variance: 0.140715933047235, agent episode variance: [0.140715933047235, 0.0, 0.0], time: 48.669
steps: 1529970, episodes: 51000, mean episode cvar: 17.114709253534674, agent episode cvar: [-0.006693605199456215, 8.565201417922975, 8.556201440811158], time: 48.669
Running avgs for agent 0: q_loss: 0.14441166818141937, p_loss: 3.2188658714294434, mean_rew: -0.400593691329151, variance: 0.46905311015745005, cvar: -0.022312017157673836, v: -0.36671683192253113, mean_q: -3.2633790969848633, std_q: 3.85886549949646, lamda: 1.626211166381836
Running avgs for agent 1: q_loss: 5.333581447601318, p_loss: -16.451778411865234, mean_rew: -0.04615173252495362, variance: 0.0, cvar: 28.550670623779297, v: 16.277467727661133, mean_q: 16.380361557006836, std_q: 2.9687271118164062, lamda: 2.203235387802124
Running avgs for agent 2: q_loss: 6.216897964477539, p_loss: -16.211532592773438, mean_rew: -0.04534672640246548, variance: 0.0, cvar: 28.52066993713379, v: 16.257434844970703, mean_q: 16.140647888183594, std_q: 3.249208927154541, lamda: 2.1880500316619873

steps: 1559970, episodes: 52000, mean episode reward: -15.614711715421395, agent episode reward: [-10.805055616313561, -2.404828049553918, -2.404828049553918], time: 48.031
steps: 1559970, episodes: 52000, mean episode variance: 0.12292285536602139, agent episode variance: [0.12292285536602139, 0.0, 0.0], time: 48.031
steps: 1559970, episodes: 52000, mean episode cvar: 17.1610456264168, agent episode cvar: [-0.0074676563590765, 8.610868600845336, 8.557644681930542], time: 48.032
Running avgs for agent 0: q_loss: 0.13780298829078674, p_loss: 3.173995018005371, mean_rew: -0.39878351396120876, variance: 0.4097428512200713, cvar: -0.024892188608646393, v: -0.35359084606170654, mean_q: -3.216815233230591, std_q: 3.8228085041046143, lamda: 1.6353026628494263
Running avgs for agent 1: q_loss: 5.174327850341797, p_loss: -16.688879013061523, mean_rew: -0.046838304304366835, variance: 0.0, cvar: 28.702898025512695, v: 16.577302932739258, mean_q: 16.619462966918945, std_q: 3.008286952972412, lamda: 2.225990056991577
Running avgs for agent 2: q_loss: 5.583980083465576, p_loss: -16.44379234313965, mean_rew: -0.047879173872403924, variance: 0.0, cvar: 28.525482177734375, v: 16.55727195739746, mean_q: 16.378808975219727, std_q: 3.2359325885772705, lamda: 2.21136212348938

steps: 1589970, episodes: 53000, mean episode reward: -15.68299389360219, agent episode reward: [-10.812737077551315, -2.435128408025437, -2.435128408025437], time: 48.003
steps: 1589970, episodes: 53000, mean episode variance: 0.12500033378973605, agent episode variance: [0.12500033378973605, 0.0, 0.0], time: 48.004
steps: 1589970, episodes: 53000, mean episode cvar: 17.15863860248029, agent episode cvar: [-0.0066928107887506486, 8.579121536254883, 8.58620987701416], time: 48.004
Running avgs for agent 0: q_loss: 0.13823604583740234, p_loss: 3.1342949867248535, mean_rew: -0.39816519027313746, variance: 0.41666777929912013, cvar: -0.02230936847627163, v: -0.3423958718776703, mean_q: -3.1765851974487305, std_q: 3.8205623626708984, lamda: 1.64875066280365
Running avgs for agent 1: q_loss: 7.144812107086182, p_loss: -16.88499641418457, mean_rew: -0.04715448013276903, variance: 0.0, cvar: 28.597074508666992, v: 16.877138137817383, mean_q: 16.820402145385742, std_q: 3.0093281269073486, lamda: 2.2495105266571045
Running avgs for agent 2: q_loss: 7.256889820098877, p_loss: -16.667236328125, mean_rew: -0.04780102601820093, variance: 0.0, cvar: 28.620698928833008, v: 16.857105255126953, mean_q: 16.604206085205078, std_q: 3.2466325759887695, lamda: 2.234513998031616

steps: 1619970, episodes: 54000, mean episode reward: -15.032467639933877, agent episode reward: [-10.36687458650467, -2.332796526714603, -2.332796526714603], time: 48.61
steps: 1619970, episodes: 54000, mean episode variance: 0.11607311243377626, agent episode variance: [0.11607311243377626, 0.0, 0.0], time: 48.61
steps: 1619970, episodes: 54000, mean episode cvar: 17.171649593696, agent episode cvar: [-0.006303624764084816, 8.584251752853394, 8.593701465606689], time: 48.611
Running avgs for agent 0: q_loss: 0.1313920021057129, p_loss: 3.079531669616699, mean_rew: -0.39359804694618655, variance: 0.3869103747792542, cvar: -0.021012084558606148, v: -0.3315151035785675, mean_q: -3.121767282485962, std_q: 3.7646524906158447, lamda: 1.6598278284072876
Running avgs for agent 1: q_loss: 8.265793800354004, p_loss: -17.054161071777344, mean_rew: -0.048104840456462725, variance: 0.0, cvar: 28.614173889160156, v: 17.176973342895508, mean_q: 16.98858070373535, std_q: 3.0794284343719482, lamda: 2.2741596698760986
Running avgs for agent 2: q_loss: 7.0496954917907715, p_loss: -16.905107498168945, mean_rew: -0.04715118355774315, variance: 0.0, cvar: 28.64566993713379, v: 17.156940460205078, mean_q: 16.841644287109375, std_q: 3.233105182647705, lamda: 2.2576284408569336

steps: 1649970, episodes: 55000, mean episode reward: -15.086903146304616, agent episode reward: [-10.557076776449016, -2.264913184927801, -2.264913184927801], time: 48.255
steps: 1649970, episodes: 55000, mean episode variance: 0.12838561809808016, agent episode variance: [0.12838561809808016, 0.0, 0.0], time: 48.255
steps: 1649970, episodes: 55000, mean episode cvar: 17.17379462067783, agent episode cvar: [-0.006208862140774727, 8.600904638290405, 8.579098844528199], time: 48.256
Running avgs for agent 0: q_loss: 0.13861903548240662, p_loss: 3.033653497695923, mean_rew: -0.39109410897397906, variance: 0.42795206032693384, cvar: -0.020696207880973816, v: -0.33211609721183777, mean_q: -3.076108455657959, std_q: 3.7541751861572266, lamda: 1.6740953922271729
Running avgs for agent 1: q_loss: 6.212378978729248, p_loss: -17.24362564086914, mean_rew: -0.04745685873028371, variance: 0.0, cvar: 28.669681549072266, v: 17.476808547973633, mean_q: 17.181110382080078, std_q: 3.1287543773651123, lamda: 2.297008514404297
Running avgs for agent 2: q_loss: 7.047903537750244, p_loss: -17.09185791015625, mean_rew: -0.047967208028634224, variance: 0.0, cvar: 28.596996307373047, v: 17.456777572631836, mean_q: 17.032262802124023, std_q: 3.243116617202759, lamda: 2.2808871269226074

steps: 1679970, episodes: 56000, mean episode reward: -13.997774780546733, agent episode reward: [-10.374410861212432, -1.8116819596671503, -1.8116819596671503], time: 47.48
steps: 1679970, episodes: 56000, mean episode variance: 0.11657406387478113, agent episode variance: [0.11657406387478113, 0.0, 0.0], time: 47.481
steps: 1679970, episodes: 56000, mean episode cvar: 17.152715348169206, agent episode cvar: [-0.005429663732647896, 8.568190071105956, 8.589954940795899], time: 47.481
Running avgs for agent 0: q_loss: 0.14124014973640442, p_loss: 2.9962539672851562, mean_rew: -0.39085548327640846, variance: 0.3885802129159371, cvar: -0.018098879605531693, v: -0.3224335312843323, mean_q: -3.0397398471832275, std_q: 3.7604081630706787, lamda: 1.6897424459457397
Running avgs for agent 1: q_loss: 8.038189888000488, p_loss: -17.365188598632812, mean_rew: -0.04733792482601112, variance: 0.0, cvar: 28.56063461303711, v: 17.776643753051758, mean_q: 17.306360244750977, std_q: 3.1883668899536133, lamda: 2.319765567779541
Running avgs for agent 2: q_loss: 5.73183012008667, p_loss: -17.305803298950195, mean_rew: -0.048960308526397524, variance: 0.0, cvar: 28.6331844329834, v: 17.756610870361328, mean_q: 17.24171257019043, std_q: 3.2651727199554443, lamda: 2.3041832447052

steps: 1709970, episodes: 57000, mean episode reward: -14.45353483579747, agent episode reward: [-10.340709003039048, -2.0564129163792093, -2.0564129163792093], time: 47.539
steps: 1709970, episodes: 57000, mean episode variance: 0.15070407574251293, agent episode variance: [0.15070407574251293, 0.0, 0.0], time: 47.539
steps: 1709970, episodes: 57000, mean episode cvar: 17.172802897572517, agent episode cvar: [-0.005074627757072449, 8.594805824279785, 8.583071701049805], time: 47.54
Running avgs for agent 0: q_loss: 0.16808326542377472, p_loss: 2.9499144554138184, mean_rew: -0.38711092559745347, variance: 0.5023469191417098, cvar: -0.016915425658226013, v: -0.330585777759552, mean_q: -2.992274284362793, std_q: 3.740241765975952, lamda: 1.7093037366867065
Running avgs for agent 1: q_loss: 6.430518627166748, p_loss: -17.56454086303711, mean_rew: -0.048353071213845536, variance: 0.0, cvar: 28.649354934692383, v: 18.07647705078125, mean_q: 17.501970291137695, std_q: 3.233527421951294, lamda: 2.34433913230896
Running avgs for agent 2: q_loss: 7.7119140625, p_loss: -17.473669052124023, mean_rew: -0.0493982715408087, variance: 0.0, cvar: 28.610240936279297, v: 18.056446075439453, mean_q: 17.418174743652344, std_q: 3.2853589057922363, lamda: 2.3278608322143555

steps: 1739970, episodes: 58000, mean episode reward: -14.022407013888417, agent episode reward: [-10.485942096705699, -1.7682324585913582, -1.7682324585913582], time: 48.793
steps: 1739970, episodes: 58000, mean episode variance: 0.12535369927808643, agent episode variance: [0.12535369927808643, 0.0, 0.0], time: 48.794
steps: 1739970, episodes: 58000, mean episode cvar: 17.156702935501933, agent episode cvar: [-0.0072229144126176835, 8.600076053619384, 8.563849796295166], time: 48.794
Running avgs for agent 0: q_loss: 0.14498890936374664, p_loss: 2.909158229827881, mean_rew: -0.38523467835916975, variance: 0.4178456642602881, cvar: -0.02407638169825077, v: -0.33389341831207275, mean_q: -2.9505105018615723, std_q: 3.7116122245788574, lamda: 1.7272834777832031
Running avgs for agent 1: q_loss: 6.2874603271484375, p_loss: -17.771272659301758, mean_rew: -0.04809420102078646, variance: 0.0, cvar: 28.666919708251953, v: 18.376314163208008, mean_q: 17.704797744750977, std_q: 3.244098663330078, lamda: 2.3669939041137695
Running avgs for agent 2: q_loss: 5.609035491943359, p_loss: -17.626327514648438, mean_rew: -0.04744820404366938, variance: 0.0, cvar: 28.546165466308594, v: 18.35628318786621, mean_q: 17.569629669189453, std_q: 3.3177592754364014, lamda: 2.351842164993286

steps: 1769970, episodes: 59000, mean episode reward: -13.474349453285177, agent episode reward: [-10.337895110285073, -1.568227171500051, -1.568227171500051], time: 48.321
steps: 1769970, episodes: 59000, mean episode variance: 0.10261966904252767, agent episode variance: [0.10261966904252767, 0.0, 0.0], time: 48.321/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

steps: 1769970, episodes: 59000, mean episode cvar: 17.175842568457128, agent episode cvar: [-0.005582108438014984, 8.594977312088012, 8.586447364807128], time: 48.322
Running avgs for agent 0: q_loss: 0.13829775154590607, p_loss: 2.8859000205993652, mean_rew: -0.3819094819032156, variance: 0.34206556347509226, cvar: -0.01860702782869339, v: -0.32729995250701904, mean_q: -2.9268550872802734, std_q: 3.7257845401763916, lamda: 1.7393878698349
Running avgs for agent 1: q_loss: 7.267587661743164, p_loss: -17.950542449951172, mean_rew: -0.04668205372538465, variance: 0.0, cvar: 28.649921417236328, v: 18.676149368286133, mean_q: 17.882680892944336, std_q: 3.2641801834106445, lamda: 2.389195442199707
Running avgs for agent 2: q_loss: 7.883668422698975, p_loss: -17.801618576049805, mean_rew: -0.045816045301515194, variance: 0.0, cvar: 28.621490478515625, v: 18.656116485595703, mean_q: 17.74632453918457, std_q: 3.3576712608337402, lamda: 2.373823404312134

steps: 1799970, episodes: 60000, mean episode reward: -13.236035755188995, agent episode reward: [-10.499477195070332, -1.3682792800593313, -1.3682792800593313], time: 47.628
steps: 1799970, episodes: 60000, mean episode variance: 0.10327074193954468, agent episode variance: [0.10327074193954468, 0.0, 0.0], time: 47.629
steps: 1799970, episodes: 60000, mean episode cvar: 17.191439214214682, agent episode cvar: [-0.007619268909096718, 8.597794050216676, 8.601264432907104], time: 47.629
Running avgs for agent 0: q_loss: 0.1432027369737625, p_loss: 2.896399736404419, mean_rew: -0.38308390496691763, variance: 0.34423580646514895, cvar: -0.025397567078471184, v: -0.33262699842453003, mean_q: -2.9369683265686035, std_q: 3.7617032527923584, lamda: 1.7531471252441406
Running avgs for agent 1: q_loss: 8.168648719787598, p_loss: -18.126981735229492, mean_rew: -0.043819521638843666, variance: 0.0, cvar: 28.659313201904297, v: 18.975984573364258, mean_q: 18.060556411743164, std_q: 3.286055326461792, lamda: 2.412886142730713
Running avgs for agent 2: q_loss: 5.603431701660156, p_loss: -18.04606056213379, mean_rew: -0.04469712317847353, variance: 0.0, cvar: 28.670879364013672, v: 18.95595359802246, mean_q: 17.9881591796875, std_q: 3.344046115875244, lamda: 2.3964619636535645

...Finished total of 60001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 29970, episodes: 1000, mean episode reward: -12.61518199190027, agent episode reward: [-10.454447438693443, -1.0803672766034156, -1.0803672766034156], time: 32.087
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 32.087
steps: 29970, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0], time: 32.088
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 59970, episodes: 2000, mean episode reward: -12.374037863448889, agent episode reward: [-10.270208287217706, -1.0519147881155904, -1.0519147881155904], time: 41.925
steps: 59970, episodes: 2000, mean episode variance: 0.14223425517231225, agent episode variance: [0.14223425517231225, 0.0, 0.0], time: 41.925
steps: 59970, episodes: 2000, mean episode cvar: 16.942962685272096, agent episode cvar: [0.041871205016970635, 8.390164239883422, 8.510927240371704], time: 41.925
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.3434877190748576, variance: 0.4871036136038091, cvar: 0.14339454472064972, v: -0.227829247713089, mean_q: -2.759000062942505, std_q: 3.9585394859313965, lamda: 1.7593419551849365
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.038563941828991546, variance: 0.0, cvar: 28.73343849182129, v: 19.27281951904297, mean_q: 18.44646453857422, std_q: 3.2075085639953613, lamda: 2.424607515335083
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.03675682752751275, variance: 0.0, cvar: 29.147008895874023, v: 19.25278663635254, mean_q: 18.44595718383789, std_q: 3.266594409942627, lamda: 2.4081385135650635

...Finished total of 2001 episodes with the fixed policy.
