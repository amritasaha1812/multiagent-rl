arglist.u_estimation False
adversary agent:  -0.25
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 29970, episodes: 1000, mean episode reward: -27.821152973359073, agent episode reward: [-45.399855680964876, 8.7893513538029, 8.7893513538029], time: 30.521
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 30.522
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan

steps: 59970, episodes: 2000, mean episode reward: -32.367733692609484, agent episode reward: [-42.96819278518272, 5.300229546286617, 5.300229546286617], time: 47.57
steps: 59970, episodes: 2000, mean episode variance: 0.2721128772441298, agent episode variance: [0.19253377421200277, 0.038564739311113955, 0.04101436372101307], time: 47.571
Running avgs for agent 0: q_loss: 0.4152926504611969, p_loss: 2.6820054054260254, mean_rew: -1.590150676792828, variance: 0.6593622404520643, mean_q: -2.7964351177215576, std_q: 2.258388042449951
Running avgs for agent 1: q_loss: 0.09635037183761597, p_loss: -0.34638893604278564, mean_rew: 0.2713509020454057, variance: 0.13207101821899414, mean_q: 0.32664981484413147, std_q: 0.927401602268219
Running avgs for agent 2: q_loss: 0.10337205231189728, p_loss: -0.42189523577690125, mean_rew: 0.27124654595847375, variance: 0.14046016335487366, mean_q: 0.3988892436027527, std_q: 0.9399369955062866

steps: 89970, episodes: 3000, mean episode reward: -0.34653186341333525, agent episode reward: [-14.362666891774476, 7.00806751418057, 7.00806751418057], time: 48.179
steps: 89970, episodes: 3000, mean episode variance: 0.18136942427698524, agent episode variance: [0.13389612022787334, 0.02427874279441312, 0.023194561254698782], time: 48.18
Running avgs for agent 0: q_loss: 0.11982391029596329, p_loss: 4.336162090301514, mean_rew: -1.289245786944538, variance: 0.44632040075957774, mean_q: -4.627586841583252, std_q: 5.3819098472595215
Running avgs for agent 1: q_loss: 0.04015433043241501, p_loss: -0.789227306842804, mean_rew: 0.2286338623188956, variance: 0.08092914264804373, mean_q: 0.7478866577148438, std_q: 1.9910591840744019
Running avgs for agent 2: q_loss: 0.04119313880801201, p_loss: -0.830829381942749, mean_rew: 0.22969915094406806, variance: 0.07731520418232928, mean_q: 0.7734785079956055, std_q: 2.0657200813293457

steps: 119970, episodes: 4000, mean episode reward: 5.44942975135141, agent episode reward: [-11.885185024886525, 8.667307388118967, 8.667307388118967], time: 47.487
steps: 119970, episodes: 4000, mean episode variance: 0.21938068927917628, agent episode variance: [0.16006136336922647, 0.027494803009554744, 0.03182452290039509], time: 47.488
Running avgs for agent 0: q_loss: 0.13986685872077942, p_loss: 4.919729232788086, mean_rew: -1.042288377175115, variance: 0.5335378778974215, mean_q: -5.287338733673096, std_q: 7.416828632354736
Running avgs for agent 1: q_loss: 0.05126399174332619, p_loss: -1.4025299549102783, mean_rew: 0.24213705898258667, variance: 0.09164934336518248, mean_q: 1.3326016664505005, std_q: 2.7045421600341797
Running avgs for agent 2: q_loss: 0.05504067614674568, p_loss: -1.432624340057373, mean_rew: 0.24265989209109326, variance: 0.10608174300131698, mean_q: 1.3395167589187622, std_q: 2.7993826866149902

steps: 149970, episodes: 5000, mean episode reward: 5.7832523335562165, agent episode reward: [-9.92232955345191, 7.852790943504062, 7.852790943504062], time: 47.958
steps: 149970, episodes: 5000, mean episode variance: 0.2860458642207086, agent episode variance: [0.21613781261444093, 0.03488705825340003, 0.035020993352867666], time: 47.958
Running avgs for agent 0: q_loss: 0.15620742738246918, p_loss: 4.956131935119629, mean_rew: -0.8866786987082523, variance: 0.7204593753814698, mean_q: -5.331022262573242, std_q: 8.539997100830078
Running avgs for agent 1: q_loss: 0.06548173725605011, p_loss: -2.0301852226257324, mean_rew: 0.2501222802640188, variance: 0.11629019417800009, mean_q: 1.9449106454849243, std_q: 3.1261377334594727
Running avgs for agent 2: q_loss: 0.06664339452981949, p_loss: -2.000373125076294, mean_rew: 0.2476564177202667, variance: 0.11673664450955888, mean_q: 1.8918286561965942, std_q: 3.1851465702056885

steps: 179970, episodes: 6000, mean episode reward: 4.561213614713986, agent episode reward: [-9.24649046061317, 6.903852037663577, 6.903852037663577], time: 48.343
steps: 179970, episodes: 6000, mean episode variance: 0.280685280142352, agent episode variance: [0.20705713132396342, 0.03728178363479674, 0.03634636518359184], time: 48.343
Running avgs for agent 0: q_loss: 0.16789580881595612, p_loss: 4.786840915679932, mean_rew: -0.784304866194265, variance: 0.6901904377465446, mean_q: -5.147274017333984, std_q: 9.243103981018066
Running avgs for agent 1: q_loss: 0.06959748268127441, p_loss: -2.5365514755249023, mean_rew: 0.24969925328474527, variance: 0.12427261211598913, mean_q: 2.4530677795410156, std_q: 3.3321824073791504
Running avgs for agent 2: q_loss: 0.06993503868579865, p_loss: -2.4760987758636475, mean_rew: 0.24901953522870543, variance: 0.1211545506119728, mean_q: 2.3665518760681152, std_q: 3.3868000507354736

steps: 209970, episodes: 7000, mean episode reward: 3.6333225248699956, agent episode reward: [-9.168425283329045, 6.40087390409952, 6.40087390409952], time: 48.021
steps: 209970, episodes: 7000, mean episode variance: 0.3055479075284675, agent episode variance: [0.22647990144789218, 0.038005334325134754, 0.04106267175544053], time: 48.022
Running avgs for agent 0: q_loss: 0.1696196049451828, p_loss: 4.531541347503662, mean_rew: -0.7105579274981371, variance: 0.7549330048263073, mean_q: -4.868903160095215, std_q: 9.682353973388672
Running avgs for agent 1: q_loss: 0.06876947730779648, p_loss: -2.8662335872650146, mean_rew: 0.24555132040484096, variance: 0.12668444775044918, mean_q: 2.7912979125976562, std_q: 3.447150468826294
Running avgs for agent 2: q_loss: 0.06851126253604889, p_loss: -2.785324811935425, mean_rew: 0.24495712679066983, variance: 0.13687557251813512, mean_q: 2.685396909713745, std_q: 3.4671781063079834

steps: 239970, episodes: 8000, mean episode reward: 2.789945776106431, agent episode reward: [-9.640592627500581, 6.215269201803506, 6.215269201803506], time: 47.296
steps: 239970, episodes: 8000, mean episode variance: 0.34388915889337657, agent episode variance: [0.2548329268395901, 0.04302835530228913, 0.04602787675149739], time: 47.296
Running avgs for agent 0: q_loss: 0.17446836829185486, p_loss: 4.3185906410217285, mean_rew: -0.6598095833957399, variance: 0.8494430894653002, mean_q: -4.626707077026367, std_q: 10.058072090148926
Running avgs for agent 1: q_loss: 0.06613841652870178, p_loss: -3.0643935203552246, mean_rew: 0.24124071556383495, variance: 0.14342785100763042, mean_q: 2.997218132019043, std_q: 3.496220588684082
Running avgs for agent 2: q_loss: 0.06637050956487656, p_loss: -2.996187448501587, mean_rew: 0.24029693001245836, variance: 0.15342625583832462, mean_q: 2.905791997909546, std_q: 3.5263428688049316

steps: 269970, episodes: 9000, mean episode reward: 2.7398510265573224, agent episode reward: [-9.218448034957053, 5.979149530757188, 5.979149530757188], time: 47.431
steps: 269970, episodes: 9000, mean episode variance: 0.34018493824638424, agent episode variance: [0.25362392245978116, 0.043738046946004035, 0.04282296884059906], time: 47.431
Running avgs for agent 0: q_loss: 0.16747374832630157, p_loss: 4.060226917266846, mean_rew: -0.6171875883989612, variance: 0.8454130748659372, mean_q: -4.343911170959473, std_q: 9.817818641662598
Running avgs for agent 1: q_loss: 0.06288821995258331, p_loss: -3.161024808883667, mean_rew: 0.2363697102408141, variance: 0.14579348982001344, mean_q: 3.100553035736084, std_q: 3.489654064178467
Running avgs for agent 2: q_loss: 0.06304492801427841, p_loss: -3.109653949737549, mean_rew: 0.23570571930067602, variance: 0.14274322946866352, mean_q: 3.0289535522460938, std_q: 3.5302629470825195

steps: 299970, episodes: 10000, mean episode reward: 2.5304488846800983, agent episode reward: [-10.226879416944275, 6.378664150812186, 6.378664150812186], time: 46.786
steps: 299970, episodes: 10000, mean episode variance: 0.33940227185282856, agent episode variance: [0.25697914290055635, 0.0404245698209852, 0.04199855913128704], time: 46.786
Running avgs for agent 0: q_loss: 0.16401495039463043, p_loss: 3.8448567390441895, mean_rew: -0.5853573812245148, variance: 0.8565971430018544, mean_q: -4.10853385925293, std_q: 9.700587272644043
Running avgs for agent 1: q_loss: 0.06068415194749832, p_loss: -3.199949264526367, mean_rew: 0.23157659181864554, variance: 0.13474856606995067, mean_q: 3.1456241607666016, std_q: 3.4837207794189453
Running avgs for agent 2: q_loss: 0.061428654938936234, p_loss: -3.169938087463379, mean_rew: 0.23151278037468334, variance: 0.13999519710429012, mean_q: 3.0959949493408203, std_q: 3.516772508621216

steps: 329970, episodes: 11000, mean episode reward: 2.6219011907033973, agent episode reward: [-10.508523674744254, 6.5652124327238255, 6.5652124327238255], time: 48.439
steps: 329970, episodes: 11000, mean episode variance: 0.3698655444541946, agent episode variance: [0.2880241875648499, 0.03989052524603903, 0.04195083164330572], time: 48.44
Running avgs for agent 0: q_loss: 0.172234445810318, p_loss: 3.7403852939605713, mean_rew: -0.5647067891542276, variance: 0.9600806252161662, mean_q: -3.9890356063842773, std_q: 9.6974515914917
Running avgs for agent 1: q_loss: 0.06252098083496094, p_loss: -3.2668683528900146, mean_rew: 0.23174684571828869, variance: 0.13296841748679677, mean_q: 3.2111661434173584, std_q: 3.4873054027557373
Running avgs for agent 2: q_loss: 0.06445859372615814, p_loss: -3.2524452209472656, mean_rew: 0.23226206038733258, variance: 0.13983610547768574, mean_q: 3.1820247173309326, std_q: 3.5138416290283203

steps: 359970, episodes: 12000, mean episode reward: 1.2380840001490951, agent episode reward: [-10.496322745601075, 5.8672033728750845, 5.8672033728750845], time: 46.789
steps: 359970, episodes: 12000, mean episode variance: 0.31814080237876624, agent episode variance: [0.22938424972444774, 0.043995048379525545, 0.04476150427479297], time: 46.789
Running avgs for agent 0: q_loss: 0.17036084830760956, p_loss: 3.7105982303619385, mean_rew: -0.5459329732486727, variance: 0.7646141657481591, mean_q: -3.945089817047119, std_q: 9.660284042358398
Running avgs for agent 1: q_loss: 0.06564849615097046, p_loss: -3.338430881500244, mean_rew: 0.22907609893978217, variance: 0.14665016126508515, mean_q: 3.2805888652801514, std_q: 3.4613583087921143
Running avgs for agent 2: q_loss: 0.06788165867328644, p_loss: -3.339486837387085, mean_rew: 0.2287497239416492, variance: 0.1492050142493099, mean_q: 3.2677559852600098, std_q: 3.536431074142456

steps: 389970, episodes: 13000, mean episode reward: 0.29702044591152593, agent episode reward: [-10.675138235578308, 5.486079340744916, 5.486079340744916], time: 46.306
steps: 389970, episodes: 13000, mean episode variance: 0.345834606423974, agent episode variance: [0.24876037754118444, 0.04804164113476873, 0.04903258774802089], time: 46.307
Running avgs for agent 0: q_loss: 0.17537564039230347, p_loss: 3.6905460357666016, mean_rew: -0.5254945109615101, variance: 0.8292012584706148, mean_q: -3.913356065750122, std_q: 9.424407005310059
Running avgs for agent 1: q_loss: 0.07035521417856216, p_loss: -3.395231008529663, mean_rew: 0.2261302299690206, variance: 0.16013880378256243, mean_q: 3.3367362022399902, std_q: 3.445148229598999
Running avgs for agent 2: q_loss: 0.07279163599014282, p_loss: -3.4517159461975098, mean_rew: 0.22543498151288976, variance: 0.16344195916006962, mean_q: 3.380034923553467, std_q: 3.5183145999908447

steps: 419970, episodes: 14000, mean episode reward: -1.8268866824894194, agent episode reward: [-11.282795711703868, 4.727954514607224, 4.727954514607224], time: 46.964
steps: 419970, episodes: 14000, mean episode variance: 0.35264620210044084, agent episode variance: [0.25287610329687593, 0.04445767081342637, 0.05531242799013853], time: 46.964
Running avgs for agent 0: q_loss: 0.18890640139579773, p_loss: 3.8321781158447266, mean_rew: -0.5197464850933462, variance: 0.8429203443229198, mean_q: -4.046515941619873, std_q: 9.640528678894043
Running avgs for agent 1: q_loss: 0.07611086219549179, p_loss: -3.443030595779419, mean_rew: 0.2218743320514368, variance: 0.1481922360447546, mean_q: 3.3858895301818848, std_q: 3.4860172271728516
Running avgs for agent 2: q_loss: 0.07849550992250443, p_loss: -3.5255305767059326, mean_rew: 0.22160304606097933, variance: 0.18437475996712843, mean_q: 3.452713966369629, std_q: 3.46205735206604

steps: 449970, episodes: 15000, mean episode reward: -1.716592996928799, agent episode reward: [-11.659891642439797, 4.9716493227555, 4.9716493227555], time: 46.651
steps: 449970, episodes: 15000, mean episode variance: 0.35615050677908583, agent episode variance: [0.257710053652525, 0.04618710360117256, 0.05225334952538833], time: 46.652
Running avgs for agent 0: q_loss: 0.1913497895002365, p_loss: 3.9622256755828857, mean_rew: -0.5060684149048962, variance: 0.8590335121750832, mean_q: -4.166289806365967, std_q: 9.374135971069336
Running avgs for agent 1: q_loss: 0.08326452970504761, p_loss: -3.4328625202178955, mean_rew: 0.2187929630568113, variance: 0.15395701200390854, mean_q: 3.3759477138519287, std_q: 3.5356950759887695
Running avgs for agent 2: q_loss: 0.08471601456403732, p_loss: -3.5904810428619385, mean_rew: 0.21829592555235233, variance: 0.17417783175129442, mean_q: 3.5146102905273438, std_q: 3.5243306159973145

steps: 479970, episodes: 16000, mean episode reward: -1.287461656318667, agent episode reward: [-11.938952133157855, 5.325745238419594, 5.325745238419594], time: 47.674
steps: 479970, episodes: 16000, mean episode variance: 0.3458125571068376, agent episode variance: [0.25149805822223426, 0.04671553918998689, 0.0475989596946165], time: 47.675
Running avgs for agent 0: q_loss: 0.1955917626619339, p_loss: 4.137711524963379, mean_rew: -0.5001924730718992, variance: 0.8383268607407808, mean_q: -4.340034008026123, std_q: 9.407350540161133
Running avgs for agent 1: q_loss: 0.08901860564947128, p_loss: -3.4082398414611816, mean_rew: 0.2155908961832604, variance: 0.15571846396662295, mean_q: 3.3491947650909424, std_q: 3.566192150115967
Running avgs for agent 2: q_loss: 0.08877868950366974, p_loss: -3.617368459701538, mean_rew: 0.2134604428836287, variance: 0.158663198982055, mean_q: 3.539641857147217, std_q: 3.5337655544281006

steps: 509970, episodes: 17000, mean episode reward: 0.021053804487228264, agent episode reward: [-12.371068399193659, 6.196061101840444, 6.196061101840444], time: 47.303
steps: 509970, episodes: 17000, mean episode variance: 0.3579193021059036, agent episode variance: [0.2557096753418446, 0.0479588633403182, 0.05425076342374086], time: 47.303
Running avgs for agent 0: q_loss: 0.21454797685146332, p_loss: 4.271578311920166, mean_rew: -0.4923290622242924, variance: 0.8523655844728152, mean_q: -4.469807147979736, std_q: 9.153411865234375
Running avgs for agent 1: q_loss: 0.09559452533721924, p_loss: -3.3872830867767334, mean_rew: 0.2130854543815577, variance: 0.15986287780106068, mean_q: 3.3260574340820312, std_q: 3.5230965614318848
Running avgs for agent 2: q_loss: 0.09538054466247559, p_loss: -3.6264488697052, mean_rew: 0.2126025887393837, variance: 0.1808358780791362, mean_q: 3.5448551177978516, std_q: 3.5642623901367188

steps: 539970, episodes: 18000, mean episode reward: -0.013693115596428598, agent episode reward: [-12.562908718308304, 6.274607801355936, 6.274607801355936], time: 47.108
steps: 539970, episodes: 18000, mean episode variance: 0.3726810722816736, agent episode variance: [0.2631625629663467, 0.052551443563774225, 0.05696706575155258], time: 47.108
Running avgs for agent 0: q_loss: 0.21038302779197693, p_loss: 4.414682388305664, mean_rew: -0.4909072030678781, variance: 0.8772085432211558, mean_q: -4.611894130706787, std_q: 9.136923789978027
Running avgs for agent 1: q_loss: 0.09971273690462112, p_loss: -3.394455909729004, mean_rew: 0.2127774696650876, variance: 0.17517147854591408, mean_q: 3.330496311187744, std_q: 3.589514970779419
Running avgs for agent 2: q_loss: 0.10113851726055145, p_loss: -3.644890069961548, mean_rew: 0.21243906543467106, variance: 0.18989021917184193, mean_q: 3.5572683811187744, std_q: 3.5533785820007324

steps: 569970, episodes: 19000, mean episode reward: 1.0975174400134329, agent episode reward: [-12.582627211589672, 6.840072325801553, 6.840072325801553], time: 47.064
steps: 569970, episodes: 19000, mean episode variance: 0.368162069350481, agent episode variance: [0.2636587208919227, 0.0505552635602653, 0.05394808489829302], time: 47.065
Running avgs for agent 0: q_loss: 0.21070006489753723, p_loss: 4.438535690307617, mean_rew: -0.48990350344233247, variance: 0.8788624029730757, mean_q: -4.6339030265808105, std_q: 9.062311172485352
Running avgs for agent 1: q_loss: 0.10289858281612396, p_loss: -3.4203035831451416, mean_rew: 0.21364538077958495, variance: 0.16851754520088436, mean_q: 3.3524672985076904, std_q: 3.6054537296295166
Running avgs for agent 2: q_loss: 0.10544965416193008, p_loss: -3.6829938888549805, mean_rew: 0.21445851690948256, variance: 0.17982694966097673, mean_q: 3.5924746990203857, std_q: 3.6401500701904297

steps: 599970, episodes: 20000, mean episode reward: 1.732056617407755, agent episode reward: [-12.078247888752637, 6.905152253080196, 6.905152253080196], time: 47.277
steps: 599970, episodes: 20000, mean episode variance: 0.3773578482810408, agent episode variance: [0.26580667136609554, 0.05389046967402101, 0.05766070724092424], time: 47.277
Running avgs for agent 0: q_loss: 0.2066858410835266, p_loss: 4.373164653778076, mean_rew: -0.48318258073318293, variance: 0.8860222378869852, mean_q: -4.563263416290283, std_q: 8.838631629943848
Running avgs for agent 1: q_loss: 0.10580061376094818, p_loss: -3.4881644248962402, mean_rew: 0.21512436468054205, variance: 0.17963489891340334, mean_q: 3.4165754318237305, std_q: 3.654402732849121
Running avgs for agent 2: q_loss: 0.10944589972496033, p_loss: -3.69512939453125, mean_rew: 0.2141861917491244, variance: 0.19220235746974745, mean_q: 3.5999536514282227, std_q: 3.6075968742370605

steps: 629970, episodes: 21000, mean episode reward: 1.4651213629267563, agent episode reward: [-11.317568541452623, 6.391344952189691, 6.391344952189691], time: 49.362
steps: 629970, episodes: 21000, mean episode variance: 0.341877558093518, agent episode variance: [0.23477092630043625, 0.05265103881247342, 0.05445559298060834], time: 49.362
Running avgs for agent 0: q_loss: 0.20137757062911987, p_loss: 4.328592777252197, mean_rew: -0.4782985460464906, variance: 0.7825697543347875, mean_q: -4.512847900390625, std_q: 8.355148315429688
Running avgs for agent 1: q_loss: 0.10745244473218918, p_loss: -3.561539649963379, mean_rew: 0.21429525927467055, variance: 0.1755034627082447, mean_q: 3.4889492988586426, std_q: 3.6420202255249023
Running avgs for agent 2: q_loss: 0.11000581085681915, p_loss: -3.7135839462280273, mean_rew: 0.2135874982897337, variance: 0.1815186432686945, mean_q: 3.617006778717041, std_q: 3.583751678466797

steps: 659970, episodes: 22000, mean episode reward: 1.4513341274746547, agent episode reward: [-11.43256359020382, 6.441948858839236, 6.441948858839236], time: 48.842
steps: 659970, episodes: 22000, mean episode variance: 0.3174434162303805, agent episode variance: [0.21125652940571307, 0.049732435066252946, 0.05645445175841451], time: 48.842
Running avgs for agent 0: q_loss: 0.19832147657871246, p_loss: 4.2689900398254395, mean_rew: -0.47243786824674544, variance: 0.704188431352377, mean_q: -4.451750755310059, std_q: 8.254244804382324
Running avgs for agent 1: q_loss: 0.10789937525987625, p_loss: -3.657686710357666, mean_rew: 0.21436256492079078, variance: 0.1657747835541765, mean_q: 3.581352949142456, std_q: 3.6781368255615234
Running avgs for agent 2: q_loss: 0.10998452454805374, p_loss: -3.7264201641082764, mean_rew: 0.21420889047884506, variance: 0.1881815058613817, mean_q: 3.6277706623077393, std_q: 3.615772008895874

steps: 689970, episodes: 23000, mean episode reward: 1.698263374536371, agent episode reward: [-11.452508106194774, 6.575385740365572, 6.575385740365572], time: 49.195
steps: 689970, episodes: 23000, mean episode variance: 0.33294338571466503, agent episode variance: [0.22803836285322904, 0.05068028165586293, 0.05422474120557308], time: 49.195
Running avgs for agent 0: q_loss: 0.19416409730911255, p_loss: 4.181180953979492, mean_rew: -0.4694707601982029, variance: 0.7601278761774302, mean_q: -4.354329586029053, std_q: 7.945040702819824
Running avgs for agent 1: q_loss: 0.10716275870800018, p_loss: -3.7441234588623047, mean_rew: 0.21371021365121842, variance: 0.16893427218620977, mean_q: 3.6683032512664795, std_q: 3.647113561630249
Running avgs for agent 2: q_loss: 0.10705330222845078, p_loss: -3.759197473526001, mean_rew: 0.2144788640960638, variance: 0.18074913735191028, mean_q: 3.6617813110351562, std_q: 3.626805067062378

steps: 719970, episodes: 24000, mean episode reward: 2.2981432299212274, agent episode reward: [-11.89591792250182, 7.097030576211524, 7.097030576211524], time: 50.489
steps: 719970, episodes: 24000, mean episode variance: 0.30936869991198185, agent episode variance: [0.20327172684669495, 0.054465363942086696, 0.05163160912320018], time: 50.489
Running avgs for agent 0: q_loss: 0.18597178161144257, p_loss: 4.088988304138184, mean_rew: -0.4667994599489555, variance: 0.6775724228223164, mean_q: -4.256965160369873, std_q: 7.590875625610352
Running avgs for agent 1: q_loss: 0.10540574043989182, p_loss: -3.826042890548706, mean_rew: 0.21517140524301193, variance: 0.181551213140289, mean_q: 3.7493953704833984, std_q: 3.666285753250122
Running avgs for agent 2: q_loss: 0.10577747225761414, p_loss: -3.7757723331451416, mean_rew: 0.21456316479448106, variance: 0.1721053637440006, mean_q: 3.680034875869751, std_q: 3.607102870941162

steps: 749970, episodes: 25000, mean episode reward: 2.5831775053003585, agent episode reward: [-11.96461056707483, 7.273894036187594, 7.273894036187594], time: 50.504
steps: 749970, episodes: 25000, mean episode variance: 0.30897592688538134, agent episode variance: [0.19936986382305621, 0.0555618285946548, 0.05404423446767032], time: 50.504
Running avgs for agent 0: q_loss: 0.18464551866054535, p_loss: 4.041415214538574, mean_rew: -0.46438238535011694, variance: 0.6645662127435208, mean_q: -4.2099761962890625, std_q: 7.829593181610107
Running avgs for agent 1: q_loss: 0.10314558446407318, p_loss: -3.892575740814209, mean_rew: 0.21681352129483633, variance: 0.185206095315516, mean_q: 3.816371202468872, std_q: 3.667478084564209
Running avgs for agent 2: q_loss: 0.10412518680095673, p_loss: -3.7858104705810547, mean_rew: 0.2162515241825288, variance: 0.18014744822556775, mean_q: 3.6926424503326416, std_q: 3.5911011695861816

steps: 779970, episodes: 26000, mean episode reward: 3.0725817455928963, agent episode reward: [-11.914996577423203, 7.493789161508048, 7.493789161508048], time: 47.343
steps: 779970, episodes: 26000, mean episode variance: 0.3255161207169294, agent episode variance: [0.22494835778325797, 0.05320009591244161, 0.04736766702122986], time: 47.343
Running avgs for agent 0: q_loss: 0.1826363354921341, p_loss: 3.9601619243621826, mean_rew: -0.4595384341503099, variance: 0.7498278592775265, mean_q: -4.123948574066162, std_q: 7.552513122558594
Running avgs for agent 1: q_loss: 0.100667804479599, p_loss: -3.9397761821746826, mean_rew: 0.2186759678048019, variance: 0.17733365304147203, mean_q: 3.864010810852051, std_q: 3.67911696434021
Running avgs for agent 2: q_loss: 0.10285133868455887, p_loss: -3.7815420627593994, mean_rew: 0.21768100979817945, variance: 0.15789222340409956, mean_q: 3.690305233001709, std_q: 3.5815491676330566

steps: 809970, episodes: 27000, mean episode reward: 3.2368668741210764, agent episode reward: [-12.567025901167554, 7.9019463876443155, 7.9019463876443155], time: 48.443
steps: 809970, episodes: 27000, mean episode variance: 0.3274386459644884, agent episode variance: [0.21659207919239998, 0.05879795797541738, 0.05204860879667103], time: 48.443
Running avgs for agent 0: q_loss: 0.185617133975029, p_loss: 3.9015560150146484, mean_rew: -0.4572166778355779, variance: 0.7219735973079999, mean_q: -4.060478210449219, std_q: 7.353156089782715
Running avgs for agent 1: q_loss: 0.10095842182636261, p_loss: -3.966268301010132, mean_rew: 0.21773237091682057, variance: 0.19599319325139125, mean_q: 3.887637138366699, std_q: 3.660592555999756
Running avgs for agent 2: q_loss: 0.10071440041065216, p_loss: -3.7914257049560547, mean_rew: 0.21956031467181153, variance: 0.1734953626555701, mean_q: 3.700833320617676, std_q: 3.5877137184143066

steps: 839970, episodes: 28000, mean episode reward: 2.461934692679046, agent episode reward: [-11.9366071164764, 7.199270904577723, 7.199270904577723], time: 46.875
steps: 839970, episodes: 28000, mean episode variance: 0.30400374603457747, agent episode variance: [0.18802212449535727, 0.056622064942494034, 0.059359556596726176], time: 46.875
Running avgs for agent 0: q_loss: 0.18446491658687592, p_loss: 3.844191074371338, mean_rew: -0.45493686843334197, variance: 0.6267404149845243, mean_q: -3.997133493423462, std_q: 7.128155708312988
Running avgs for agent 1: q_loss: 0.09880366176366806, p_loss: -4.003727912902832, mean_rew: 0.21969941541201676, variance: 0.18874021647498013, mean_q: 3.9257519245147705, std_q: 3.6244583129882812
Running avgs for agent 2: q_loss: 0.10108621418476105, p_loss: -3.7884302139282227, mean_rew: 0.21977018592632533, variance: 0.19786518865575392, mean_q: 3.700974464416504, std_q: 3.545910596847534

steps: 869970, episodes: 29000, mean episode reward: 2.7522147586239427, agent episode reward: [-12.508622434140626, 7.630418596382285, 7.630418596382285], time: 46.615
steps: 869970, episodes: 29000, mean episode variance: 0.3637126550544053, agent episode variance: [0.25579760468564927, 0.05518506607227027, 0.05272998429648578], time: 46.615
Running avgs for agent 0: q_loss: 0.194487527012825, p_loss: 3.8488199710845947, mean_rew: -0.45562677059434536, variance: 0.8526586822854976, mean_q: -4.00186014175415, std_q: 7.493470191955566
Running avgs for agent 1: q_loss: 0.098343126475811, p_loss: -4.031300067901611, mean_rew: 0.22162065980699272, variance: 0.1839502202409009, mean_q: 3.953658103942871, std_q: 3.5743510723114014
Running avgs for agent 2: q_loss: 0.09835736453533173, p_loss: -3.790005683898926, mean_rew: 0.22127763181089208, variance: 0.17576661432161927, mean_q: 3.7053580284118652, std_q: 3.500899314880371

steps: 899970, episodes: 30000, mean episode reward: 2.467347925013478, agent episode reward: [-12.225568610947267, 7.346458267980371, 7.346458267980371], time: 46.453
steps: 899970, episodes: 30000, mean episode variance: 0.3297727502705529, agent episode variance: [0.2203073993176222, 0.05816381769813597, 0.05130153325479478], time: 46.453
Running avgs for agent 0: q_loss: 0.18986938893795013, p_loss: 3.841464042663574, mean_rew: -0.4538876213374797, variance: 0.7343579977254073, mean_q: -3.9884262084960938, std_q: 7.563912868499756
Running avgs for agent 1: q_loss: 0.0978979766368866, p_loss: -4.0467047691345215, mean_rew: 0.2222655301034922, variance: 0.1938793923271199, mean_q: 3.9688174724578857, std_q: 3.5653035640716553
Running avgs for agent 2: q_loss: 0.09604057669639587, p_loss: -3.8063130378723145, mean_rew: 0.22189843609563875, variance: 0.1710051108493159, mean_q: 3.723827600479126, std_q: 3.4509241580963135

steps: 929970, episodes: 31000, mean episode reward: 2.607349802781705, agent episode reward: [-12.77143489287157, 7.689392347826637, 7.689392347826637], time: 47.096
steps: 929970, episodes: 31000, mean episode variance: 0.3623404132053256, agent episode variance: [0.24845057588070632, 0.05389573734253645, 0.05999409998208284], time: 47.097
Running avgs for agent 0: q_loss: 0.18847031891345978, p_loss: 3.8619654178619385, mean_rew: -0.45411805730806376, variance: 0.828168586269021, mean_q: -4.010204792022705, std_q: 7.418540954589844
Running avgs for agent 1: q_loss: 0.09738985449075699, p_loss: -4.059340476989746, mean_rew: 0.22403211299314554, variance: 0.17965245780845482, mean_q: 3.981173515319824, std_q: 3.5148379802703857
Running avgs for agent 2: q_loss: 0.09785988181829453, p_loss: -3.835153102874756, mean_rew: 0.22329572534908057, variance: 0.19998033327360948, mean_q: 3.7547531127929688, std_q: 3.4627530574798584

steps: 959970, episodes: 32000, mean episode reward: 3.157754518854571, agent episode reward: [-13.034298275938578, 8.096026397396574, 8.096026397396574], time: 47.526
steps: 959970, episodes: 32000, mean episode variance: 0.2692923623863608, agent episode variance: [0.15869040654599667, 0.05192710198648274, 0.05867485385388136], time: 47.527
Running avgs for agent 0: q_loss: 0.17929691076278687, p_loss: 3.8463711738586426, mean_rew: -0.44959733840565386, variance: 0.5289680218199889, mean_q: -3.9889965057373047, std_q: 7.1219401359558105
Running avgs for agent 1: q_loss: 0.09731767326593399, p_loss: -4.05856466293335, mean_rew: 0.22292517489692196, variance: 0.17309033995494247, mean_q: 3.9797744750976562, std_q: 3.428819179534912
Running avgs for agent 2: q_loss: 0.09743749350309372, p_loss: -3.851459264755249, mean_rew: 0.2234630321868263, variance: 0.19558284617960453, mean_q: 3.7735278606414795, std_q: 3.43803334236145

steps: 989970, episodes: 33000, mean episode reward: 3.2609165141635286, agent episode reward: [-12.540516183758111, 7.90071634896082, 7.90071634896082], time: 46.358
steps: 989970, episodes: 33000, mean episode variance: 0.3098247180413455, agent episode variance: [0.20071208224073053, 0.05910119891539216, 0.05001143688522279], time: 46.358
Running avgs for agent 0: q_loss: 0.18786506354808807, p_loss: 3.907299757003784, mean_rew: -0.4532013898301445, variance: 0.6690402741357684, mean_q: -4.050276756286621, std_q: 7.16659688949585
Running avgs for agent 1: q_loss: 0.09790942072868347, p_loss: -4.085405349731445, mean_rew: 0.22611542073908045, variance: 0.19700399638464053, mean_q: 4.004795074462891, std_q: 3.4339661598205566
Running avgs for agent 2: q_loss: 0.09690637141466141, p_loss: -3.897599220275879, mean_rew: 0.22491775460643906, variance: 0.1667047896174093, mean_q: 3.818301200866699, std_q: 3.414339542388916

steps: 1019970, episodes: 34000, mean episode reward: 3.2242533498161023, agent episode reward: [-12.225495889699594, 7.724874619757849, 7.724874619757849], time: 46.971
steps: 1019970, episodes: 34000, mean episode variance: 0.29418913681432607, agent episode variance: [0.18621601301431656, 0.05523643043264747, 0.052736693367362024], time: 46.972
Running avgs for agent 0: q_loss: 0.1841844618320465, p_loss: 3.89082932472229, mean_rew: -0.4435967993152574, variance: 0.6207200433810552, mean_q: -4.025331020355225, std_q: 7.0901336669921875
Running avgs for agent 1: q_loss: 0.09957170486450195, p_loss: -4.119210720062256, mean_rew: 0.22715105713479952, variance: 0.18412143477549156, mean_q: 4.038713455200195, std_q: 3.398061752319336
Running avgs for agent 2: q_loss: 0.09813499450683594, p_loss: -3.915222644805908, mean_rew: 0.22495783969942257, variance: 0.17578897789120673, mean_q: 3.8380231857299805, std_q: 3.3863563537597656

steps: 1049970, episodes: 35000, mean episode reward: 2.801538263154417, agent episode reward: [-12.633123656361416, 7.717330959757915, 7.717330959757915], time: 46.901
steps: 1049970, episodes: 35000, mean episode variance: 0.27343893539439884, agent episode variance: [0.1579084425345063, 0.057970548519864676, 0.05755994434002787], time: 46.901
Running avgs for agent 0: q_loss: 0.17155428230762482, p_loss: 3.7294328212738037, mean_rew: -0.410008072102811, variance: 0.5263614751150211, mean_q: -3.8299739360809326, std_q: 6.056153774261475
Running avgs for agent 1: q_loss: 0.10022464394569397, p_loss: -4.096532344818115, mean_rew: 0.22471585582854625, variance: 0.19323516173288227, mean_q: 4.02058744430542, std_q: 3.219393730163574
Running avgs for agent 2: q_loss: 0.09799043834209442, p_loss: -3.9134862422943115, mean_rew: 0.2249180550608052, variance: 0.19186648113342622, mean_q: 3.841404676437378, std_q: 3.1965301036834717

steps: 1079970, episodes: 36000, mean episode reward: 3.3600926468301995, agent episode reward: [-12.713281724484846, 8.036687185657524, 8.036687185657524], time: 48.95
steps: 1079970, episodes: 36000, mean episode variance: 0.1908287592381239, agent episode variance: [0.08388300213217735, 0.05267491229437292, 0.054270844811573626], time: 48.951
Running avgs for agent 0: q_loss: 0.143925279378891, p_loss: 3.5754590034484863, mean_rew: -0.3865902343712101, variance: 0.27961000710725786, mean_q: -3.6470744609832764, std_q: 4.204420566558838
Running avgs for agent 1: q_loss: 0.09724520146846771, p_loss: -4.14096736907959, mean_rew: 0.22857857606900392, variance: 0.17558304098124305, mean_q: 4.068902969360352, std_q: 2.980570077896118
Running avgs for agent 2: q_loss: 0.09633506089448929, p_loss: -3.9581708908081055, mean_rew: 0.22850380925359715, variance: 0.18090281603857875, mean_q: 3.8941407203674316, std_q: 2.9841232299804688

steps: 1109970, episodes: 37000, mean episode reward: 3.229207704434357, agent episode reward: [-12.304696036322417, 7.766951870378387, 7.766951870378387], time: 46.463
steps: 1109970, episodes: 37000, mean episode variance: 0.18370944690518082, agent episode variance: [0.07874979327991605, 0.05301472314074635, 0.05194493048451841], time: 46.463
Running avgs for agent 0: q_loss: 0.1433313935995102, p_loss: 3.6230480670928955, mean_rew: -0.3822944417230706, variance: 0.2624993109330535, mean_q: -3.6892504692077637, std_q: 4.190392017364502
Running avgs for agent 1: q_loss: 0.09799106419086456, p_loss: -4.171860218048096, mean_rew: 0.2282102066217604, variance: 0.17671574380248786, mean_q: 4.102323532104492, std_q: 2.997586965560913
Running avgs for agent 2: q_loss: 0.09563478082418442, p_loss: -4.023383617401123, mean_rew: 0.22943049874199992, variance: 0.17314976828172804, mean_q: 3.959623098373413, std_q: 3.017519950866699

steps: 1139970, episodes: 38000, mean episode reward: 3.20352222866602, agent episode reward: [-12.551981645843847, 7.877751937254934, 7.877751937254934], time: 48.006
steps: 1139970, episodes: 38000, mean episode variance: 0.19674840245209635, agent episode variance: [0.07673562405258418, 0.05733793741464615, 0.06267484098486602], time: 48.007
Running avgs for agent 0: q_loss: 0.1466636210680008, p_loss: 3.74641489982605, mean_rew: -0.3864823178525672, variance: 0.2557854135086139, mean_q: -3.8157291412353516, std_q: 4.280590057373047
Running avgs for agent 1: q_loss: 0.1001582220196724, p_loss: -4.21834135055542, mean_rew: 0.22793867974577922, variance: 0.1911264580488205, mean_q: 4.149895191192627, std_q: 3.033444404602051
Running avgs for agent 2: q_loss: 0.09779226034879684, p_loss: -4.082571029663086, mean_rew: 0.2281803451139271, variance: 0.20891613661622008, mean_q: 4.017482757568359, std_q: 3.029332160949707

steps: 1169970, episodes: 39000, mean episode reward: 2.8560733136229057, agent episode reward: [-12.675473455345863, 7.765773384484383, 7.765773384484383], time: 46.132
steps: 1169970, episodes: 39000, mean episode variance: 0.1888779020253569, agent episode variance: [0.08193890853598713, 0.05280947911553085, 0.0541295143738389], time: 46.133
Running avgs for agent 0: q_loss: 0.15094494819641113, p_loss: 3.849100351333618, mean_rew: -0.3885399620299617, variance: 0.2731296951199571, mean_q: -3.918469190597534, std_q: 4.317747592926025
Running avgs for agent 1: q_loss: 0.10111463069915771, p_loss: -4.270878314971924, mean_rew: 0.2274952544835869, variance: 0.1760315970517695, mean_q: 4.204927444458008, std_q: 3.0738110542297363
Running avgs for agent 2: q_loss: 0.09917648136615753, p_loss: -4.157212734222412, mean_rew: 0.22873343025880313, variance: 0.180431714579463, mean_q: 4.0921950340271, std_q: 3.0794718265533447

steps: 1199970, episodes: 40000, mean episode reward: 3.2253502857928296, agent episode reward: [-12.365098815631118, 7.795224550711974, 7.795224550711974], time: 46.928
steps: 1199970, episodes: 40000, mean episode variance: 0.19253548946604132, agent episode variance: [0.08383731929585338, 0.052625471036881206, 0.056072699133306744], time: 46.929
Running avgs for agent 0: q_loss: 0.15154589712619781, p_loss: 3.953900098800659, mean_rew: -0.3930406844445204, variance: 0.27945773098617793, mean_q: -4.022456169128418, std_q: 4.387580394744873
Running avgs for agent 1: q_loss: 0.10282515734434128, p_loss: -4.345736503601074, mean_rew: 0.22993651634520973, variance: 0.17541823678960403, mean_q: 4.280358791351318, std_q: 3.1345982551574707
Running avgs for agent 2: q_loss: 0.1006716936826706, p_loss: -4.222349643707275, mean_rew: 0.22876876391546547, variance: 0.18690899711102246, mean_q: 4.1572585105896, std_q: 3.087413787841797

steps: 1229970, episodes: 41000, mean episode reward: 2.77138406762223, agent episode reward: [-12.69373581498978, 7.732559941306007, 7.732559941306007], time: 46.519
steps: 1229970, episodes: 41000, mean episode variance: 0.21424839505925775, agent episode variance: [0.08474213679507375, 0.06587978966161609, 0.06362646860256792], time: 46.52
Running avgs for agent 0: q_loss: 0.15406303107738495, p_loss: 4.019768238067627, mean_rew: -0.3934940022027013, variance: 0.28247378931691247, mean_q: -4.090202808380127, std_q: 4.345676898956299
Running avgs for agent 1: q_loss: 0.10795871913433075, p_loss: -4.402604103088379, mean_rew: 0.23114864341402797, variance: 0.21959929887205362, mean_q: 4.338263988494873, std_q: 3.156062126159668
Running avgs for agent 2: q_loss: 0.1045643761754036, p_loss: -4.284458637237549, mean_rew: 0.23001429359371756, variance: 0.21208822867522636, mean_q: 4.219274997711182, std_q: 3.101870536804199

steps: 1259970, episodes: 42000, mean episode reward: 3.5630467727802237, agent episode reward: [-12.30301120925181, 7.933028991016017, 7.933028991016017], time: 46.658
steps: 1259970, episodes: 42000, mean episode variance: 0.2073530186023563, agent episode variance: [0.08582116575166583, 0.060462875436991456, 0.06106897741369903], time: 46.659
Running avgs for agent 0: q_loss: 0.15990936756134033, p_loss: 4.131311893463135, mean_rew: -0.39991448260883267, variance: 0.28607055250555274, mean_q: -4.20527982711792, std_q: 4.444652557373047
Running avgs for agent 1: q_loss: 0.1107754036784172, p_loss: -4.453629970550537, mean_rew: 0.23261539611732446, variance: 0.20154291812330485, mean_q: 4.392626762390137, std_q: 3.207190990447998
Running avgs for agent 2: q_loss: 0.10782961547374725, p_loss: -4.341897010803223, mean_rew: 0.23135058172120865, variance: 0.20356325804566344, mean_q: 4.277971267700195, std_q: 3.1228442192077637

steps: 1289970, episodes: 43000, mean episode reward: 3.3995337675431916, agent episode reward: [-12.608924962827833, 8.004229365185513, 8.004229365185513], time: 46.833
steps: 1289970, episodes: 43000, mean episode variance: 0.20894925520941615, agent episode variance: [0.08665692084655166, 0.0671591069418937, 0.0551332274209708], time: 46.834
Running avgs for agent 0: q_loss: 0.16719473898410797, p_loss: 4.1999592781066895, mean_rew: -0.402395338712639, variance: 0.28885640282183883, mean_q: -4.275198459625244, std_q: 4.450460910797119
Running avgs for agent 1: q_loss: 0.11465245485305786, p_loss: -4.494562149047852, mean_rew: 0.23537306897794225, variance: 0.22386368980631233, mean_q: 4.434247016906738, std_q: 3.219270944595337
Running avgs for agent 2: q_loss: 0.10965611785650253, p_loss: -4.404524803161621, mean_rew: 0.23402698638551864, variance: 0.18377742473656933, mean_q: 4.340592384338379, std_q: 3.142160654067993

steps: 1319970, episodes: 44000, mean episode reward: 3.1467138807965904, agent episode reward: [-12.802962702921041, 7.974838291858815, 7.974838291858815], time: 47.551
steps: 1319970, episodes: 44000, mean episode variance: 0.21514755596779286, agent episode variance: [0.08686868450976908, 0.06836216521263122, 0.05991670624539256], time: 47.552
Running avgs for agent 0: q_loss: 0.17207354307174683, p_loss: 4.219827651977539, mean_rew: -0.4040679397152451, variance: 0.28956228169923026, mean_q: -4.29793119430542, std_q: 4.457545757293701
Running avgs for agent 1: q_loss: 0.11538553982973099, p_loss: -4.499380111694336, mean_rew: 0.23527054199703543, variance: 0.2278738840421041, mean_q: 4.4406232833862305, std_q: 3.2411816120147705
Running avgs for agent 2: q_loss: 0.11352670937776566, p_loss: -4.4332594871521, mean_rew: 0.23610567995916662, variance: 0.19972235415130854, mean_q: 4.370726585388184, std_q: 3.1512346267700195

steps: 1349970, episodes: 45000, mean episode reward: 3.5213884728567866, agent episode reward: [-12.72519805406798, 8.123293263462383, 8.123293263462383], time: 46.222
steps: 1349970, episodes: 45000, mean episode variance: 0.2166054137572646, agent episode variance: [0.09190655089914798, 0.060627822799608114, 0.06407104005850851], time: 46.223
Running avgs for agent 0: q_loss: 0.17523615062236786, p_loss: 4.197619438171387, mean_rew: -0.4056161377506052, variance: 0.30635516966382664, mean_q: -4.278334140777588, std_q: 4.396563529968262
Running avgs for agent 1: q_loss: 0.11745580285787582, p_loss: -4.542069435119629, mean_rew: 0.23734058535714345, variance: 0.20209274266536037, mean_q: 4.481621265411377, std_q: 3.2627103328704834
Running avgs for agent 2: q_loss: 0.11543601751327515, p_loss: -4.464039325714111, mean_rew: 0.23680046175505998, variance: 0.2135701335283617, mean_q: 4.4032883644104, std_q: 3.1539146900177

steps: 1379970, episodes: 46000, mean episode reward: 3.0503080718458357, agent episode reward: [-12.561839761532312, 7.806073916689073, 7.806073916689073], time: 47.17
steps: 1379970, episodes: 46000, mean episode variance: 0.22341526835039258, agent episode variance: [0.10122794362157583, 0.06664192729257047, 0.05554539743624628], time: 47.171
Running avgs for agent 0: q_loss: 0.17935112118721008, p_loss: 4.14545202255249, mean_rew: -0.4073271256318919, variance: 0.3374264787385861, mean_q: -4.226182460784912, std_q: 4.374107837677002
Running avgs for agent 1: q_loss: 0.11963984370231628, p_loss: -4.570913314819336, mean_rew: 0.23876938020708627, variance: 0.22213975764190158, mean_q: 4.5101470947265625, std_q: 3.280667781829834
Running avgs for agent 2: q_loss: 0.11708470433950424, p_loss: -4.488540172576904, mean_rew: 0.23980058791896056, variance: 0.18515132478748758, mean_q: 4.429641246795654, std_q: 3.156001329421997

steps: 1409970, episodes: 47000, mean episode reward: 4.17945306973518, agent episode reward: [-12.871530350802644, 8.525491710268915, 8.525491710268915], time: 46.829
steps: 1409970, episodes: 47000, mean episode variance: 0.23238098974525928, agent episode variance: [0.09377456315606833, 0.07482388697192073, 0.06378253961727023], time: 46.83
Running avgs for agent 0: q_loss: 0.1843690127134323, p_loss: 4.10022497177124, mean_rew: -0.4094392407476323, variance: 0.3125818771868944, mean_q: -4.180245876312256, std_q: 4.352829456329346
Running avgs for agent 1: q_loss: 0.12137684971094131, p_loss: -4.614928722381592, mean_rew: 0.24188392596224026, variance: 0.24941295657306908, mean_q: 4.555502414703369, std_q: 3.272855520248413
Running avgs for agent 2: q_loss: 0.1185372918844223, p_loss: -4.507494926452637, mean_rew: 0.24178730950723842, variance: 0.21260846539090078, mean_q: 4.448973178863525, std_q: 3.1637556552886963

steps: 1439970, episodes: 48000, mean episode reward: 3.414368884968288, agent episode reward: [-13.126158082536469, 8.27026348375238, 8.27026348375238], time: 46.658
steps: 1439970, episodes: 48000, mean episode variance: 0.2319253731984645, agent episode variance: [0.102017598554492, 0.06246401563473046, 0.06744375900924206], time: 46.658
Running avgs for agent 0: q_loss: 0.1875322014093399, p_loss: 4.0699286460876465, mean_rew: -0.41121537531699764, variance: 0.34005866184830663, mean_q: -4.149993896484375, std_q: 4.347544193267822
Running avgs for agent 1: q_loss: 0.11842779815196991, p_loss: -4.693471908569336, mean_rew: 0.2453597204696944, variance: 0.20821338544910153, mean_q: 4.635800361633301, std_q: 3.22983717918396
Running avgs for agent 2: q_loss: 0.11798626184463501, p_loss: -4.5737810134887695, mean_rew: 0.2459895892700706, variance: 0.22481253003080687, mean_q: 4.518310546875, std_q: 3.149838924407959

steps: 1469970, episodes: 49000, mean episode reward: 4.214218211043721, agent episode reward: [-13.388776091647916, 8.801497151345819, 8.801497151345819], time: 47.627
steps: 1469970, episodes: 49000, mean episode variance: 0.24125878061726688, agent episode variance: [0.101372024692595, 0.0641244022846222, 0.0757623536400497], time: 47.628
Running avgs for agent 0: q_loss: 0.19204024970531464, p_loss: 4.044086456298828, mean_rew: -0.41411067459153306, variance: 0.3379067489753167, mean_q: -4.1265106201171875, std_q: 4.394989967346191
Running avgs for agent 1: q_loss: 0.11828725039958954, p_loss: -4.772480010986328, mean_rew: 0.24890070296118313, variance: 0.2137480076154073, mean_q: 4.714232444763184, std_q: 3.181284189224243
Running avgs for agent 2: q_loss: 0.1199064627289772, p_loss: -4.630897521972656, mean_rew: 0.2497486879729748, variance: 0.25254117880016563, mean_q: 4.577664852142334, std_q: 3.133082628250122

steps: 1499970, episodes: 50000, mean episode reward: 3.681206145712016, agent episode reward: [-13.165783346385908, 8.423494746048961, 8.423494746048961], time: 46.378
steps: 1499970, episodes: 50000, mean episode variance: 0.23868927491549402, agent episode variance: [0.10028275693207979, 0.06388532059732825, 0.074521197386086], time: 46.379
Running avgs for agent 0: q_loss: 0.19663654267787933, p_loss: 4.005856037139893, mean_rew: -0.4150636214968685, variance: 0.334275856440266, mean_q: -4.088007926940918, std_q: 4.418578147888184
Running avgs for agent 1: q_loss: 0.11711547523736954, p_loss: -4.830478191375732, mean_rew: 0.25187944919548766, variance: 0.2129510686577608, mean_q: 4.774620532989502, std_q: 3.1517961025238037
Running avgs for agent 2: q_loss: 0.11896564811468124, p_loss: -4.7202606201171875, mean_rew: 0.2531928330091664, variance: 0.24840399128695329, mean_q: 4.6687774658203125, std_q: 3.1392786502838135

steps: 1529970, episodes: 51000, mean episode reward: 3.507139948045379, agent episode reward: [-13.962489628768541, 8.734814788406961, 8.734814788406961], time: 46.986
steps: 1529970, episodes: 51000, mean episode variance: 0.2349793476499617, agent episode variance: [0.09554236856475472, 0.06610920552536845, 0.07332777355983854], time: 46.987
Running avgs for agent 0: q_loss: 0.20041553676128387, p_loss: 3.987560272216797, mean_rew: -0.41606589756626544, variance: 0.31847456188251577, mean_q: -4.070553302764893, std_q: 4.423762321472168
Running avgs for agent 1: q_loss: 0.11789868772029877, p_loss: -4.899953842163086, mean_rew: 0.2539706627089465, variance: 0.22036401841789485, mean_q: 4.845076560974121, std_q: 3.145315408706665
Running avgs for agent 2: q_loss: 0.11931861937046051, p_loss: -4.778903007507324, mean_rew: 0.25420073732510984, variance: 0.24442591186612844, mean_q: 4.727778911590576, std_q: 3.140430212020874

steps: 1559970, episodes: 52000, mean episode reward: 3.59079082128411, agent episode reward: [-13.865479422809637, 8.728135122046874, 8.728135122046874], time: 47.109
steps: 1559970, episodes: 52000, mean episode variance: 0.2560784973260015, agent episode variance: [0.10784148741513491, 0.08026053048111498, 0.06797647942975163], time: 47.11
Running avgs for agent 0: q_loss: 0.2059870958328247, p_loss: 3.988247871398926, mean_rew: -0.4173505203538979, variance: 0.35947162471711636, mean_q: -4.070425033569336, std_q: 4.500418186187744
Running avgs for agent 1: q_loss: 0.1178206354379654, p_loss: -4.967435836791992, mean_rew: 0.25763353202880335, variance: 0.26753510160371663, mean_q: 4.9142165184021, std_q: 3.1316452026367188
Running avgs for agent 2: q_loss: 0.1197868213057518, p_loss: -4.859480857849121, mean_rew: 0.25625909327091895, variance: 0.2265882647658388, mean_q: 4.807421684265137, std_q: 3.164463758468628

steps: 1589970, episodes: 53000, mean episode reward: 3.8016123140066194, agent episode reward: [-14.189566557451071, 8.995589435728846, 8.995589435728846], time: 46.833
steps: 1589970, episodes: 53000, mean episode variance: 0.2596416369173676, agent episode variance: [0.10552769868448376, 0.061989245384931564, 0.09212469284795224], time: 46.833
Running avgs for agent 0: q_loss: 0.2076040655374527, p_loss: 4.025392055511475, mean_rew: -0.4180289662491622, variance: 0.3517589956149459, mean_q: -4.107629299163818, std_q: 4.553406715393066
Running avgs for agent 1: q_loss: 0.11773193627595901, p_loss: -5.025021076202393, mean_rew: 0.2592379019016309, variance: 0.2066308179497719, mean_q: 4.973241329193115, std_q: 3.1318533420562744
Running avgs for agent 2: q_loss: 0.12309739738702774, p_loss: -4.949196815490723, mean_rew: 0.2577538624473226, variance: 0.30708230949317417, mean_q: 4.8969268798828125, std_q: 3.1697213649749756

steps: 1619970, episodes: 54000, mean episode reward: 3.702921052455998, agent episode reward: [-14.830632330368617, 9.266776691412305, 9.266776691412305], time: 47.013
steps: 1619970, episodes: 54000, mean episode variance: 0.25166162908263506, agent episode variance: [0.11068949108198285, 0.07095811099931598, 0.07001402700133622], time: 47.013
Running avgs for agent 0: q_loss: 0.21015533804893494, p_loss: 4.101061820983887, mean_rew: -0.42114877687006, variance: 0.3689649702732762, mean_q: -4.184196949005127, std_q: 4.639686584472656
Running avgs for agent 1: q_loss: 0.11913121491670609, p_loss: -5.091435432434082, mean_rew: 0.26108768187632353, variance: 0.2365270366643866, mean_q: 5.040384769439697, std_q: 3.1442952156066895
Running avgs for agent 2: q_loss: 0.12369506806135178, p_loss: -5.055188179016113, mean_rew: 0.2609975587239686, variance: 0.23338009000445406, mean_q: 5.0037102699279785, std_q: 3.2087454795837402

steps: 1649970, episodes: 55000, mean episode reward: 3.3571242413318725, agent episode reward: [-15.01505575644665, 9.18608999888926, 9.18608999888926], time: 46.248
steps: 1649970, episodes: 55000, mean episode variance: 0.2662402846310288, agent episode variance: [0.11355395996943116, 0.06808113175816834, 0.08460519290342927], time: 46.248
Running avgs for agent 0: q_loss: 0.21500162780284882, p_loss: 4.18196439743042, mean_rew: -0.42568024954255623, variance: 0.3785131998981039, mean_q: -4.265895843505859, std_q: 4.704263687133789
Running avgs for agent 1: q_loss: 0.11961258202791214, p_loss: -5.187069892883301, mean_rew: 0.26524111139133166, variance: 0.22693710586056112, mean_q: 5.136086463928223, std_q: 3.1870100498199463
Running avgs for agent 2: q_loss: 0.12601621448993683, p_loss: -5.16188383102417, mean_rew: 0.26378038071364723, variance: 0.28201730967809757, mean_q: 5.1095290184021, std_q: 3.232943058013916

steps: 1679970, episodes: 56000, mean episode reward: 3.541385536032786, agent episode reward: [-15.632073205375887, 9.586729370704337, 9.586729370704337], time: 45.482
steps: 1679970, episodes: 56000, mean episode variance: 0.2569895561551675, agent episode variance: [0.105039859816432, 0.07228855334687978, 0.07966114299185574], time: 45.482
Running avgs for agent 0: q_loss: 0.21636570990085602, p_loss: 4.285973072052002, mean_rew: -0.42992632163565814, variance: 0.35013286605477334, mean_q: -4.369766712188721, std_q: 4.785923480987549
Running avgs for agent 1: q_loss: 0.12128448486328125, p_loss: -5.28769063949585, mean_rew: 0.26743237692647415, variance: 0.24096184448959926, mean_q: 5.235042095184326, std_q: 3.2414469718933105
Running avgs for agent 2: q_loss: 0.12612181901931763, p_loss: -5.294662952423096, mean_rew: 0.26801018132140236, variance: 0.2655371433061858, mean_q: 5.240849494934082, std_q: 3.2884457111358643

steps: 1709970, episodes: 57000, mean episode reward: 2.862217986948924, agent episode reward: [-15.980935721674097, 9.421576854311512, 9.421576854311512], time: 45.387
steps: 1709970, episodes: 57000, mean episode variance: 0.2555766194108874, agent episode variance: [0.10579057201743126, 0.08175689558871091, 0.0680291518047452], time: 45.387
Running avgs for agent 0: q_loss: 0.2141864001750946, p_loss: 4.395660877227783, mean_rew: -0.4326315057535723, variance: 0.3526352400581042, mean_q: -4.4784932136535645, std_q: 4.798892498016357
Running avgs for agent 1: q_loss: 0.12300360202789307, p_loss: -5.399640083312988, mean_rew: 0.27088484620471087, variance: 0.272522985295703, mean_q: 5.344531059265137, std_q: 3.2995035648345947
Running avgs for agent 2: q_loss: 0.1253601610660553, p_loss: -5.4246978759765625, mean_rew: 0.26996351080153724, variance: 0.22676383934915065, mean_q: 5.369128227233887, std_q: 3.3198490142822266

steps: 1739970, episodes: 58000, mean episode reward: 3.107994995599006, agent episode reward: [-15.249429621756189, 9.178712308677598, 9.178712308677598], time: 45.941
steps: 1739970, episodes: 58000, mean episode variance: 0.25846817284822465, agent episode variance: [0.11013361503928899, 0.07725439762324095, 0.0710801601856947], time: 45.941
Running avgs for agent 0: q_loss: 0.2216014564037323, p_loss: 4.520116806030273, mean_rew: -0.4371697992903002, variance: 0.3671120501309633, mean_q: -4.603463649749756, std_q: 4.891909122467041
Running avgs for agent 1: q_loss: 0.12397690117359161, p_loss: -5.5205583572387695, mean_rew: 0.2725863320285777, variance: 0.2575146587441365, mean_q: 5.464795112609863, std_q: 3.347111225128174
Running avgs for agent 2: q_loss: 0.12694934010505676, p_loss: -5.568566799163818, mean_rew: 0.27288424985976195, variance: 0.23693386728564897, mean_q: 5.511414051055908, std_q: 3.371657609939575

steps: 1769970, episodes: 59000, mean episode reward: 3.325619909106548, agent episode reward: [-15.288832500370471, 9.30722620473851, 9.30722620473851], time: 46.671
steps: 1769970, episodes: 59000, mean episode variance: 0.29640149197727444, agent episode variance: [0.1123769963979721, 0.08908465297333897, 0.09493984260596335], time: 46.671
Running avgs for agent 0: q_loss: 0.2262842357158661, p_loss: 4.649197578430176, mean_rew: -0.4432759622800797, variance: 0.37458998799324034, mean_q: -4.732957363128662, std_q: 4.98716402053833
Running avgs for agent 1: q_loss: 0.12659922242164612, p_loss: -5.634562969207764, mean_rew: 0.2745707647835307, variance: 0.2969488432444632, mean_q: 5.5771050453186035, std_q: 3.3799352645874023
Running avgs for agent 2: q_loss: 0.126804918050766, p_loss: -5.691106796264648, mean_rew: 0.2743067751561469, variance: 0.3164661420198778, mean_q: 5.63345193862915, std_q: 3.402522325515747

steps: 1799970, episodes: 60000, mean episode reward: 3.294762841843393, agent episode reward: [-15.235024460887242, 9.26489365136532, 9.26489365136532], time: 45.46
steps: 1799970, episodes: 60000, mean episode variance: 0.26464917830377815, agent episode variance: [0.11748210031539202, 0.07662710829079152, 0.07053996969759464], time: 45.46
Running avgs for agent 0: q_loss: 0.22334152460098267, p_loss: 4.742570877075195, mean_rew: -0.4427607846570805, variance: 0.39160700105130675, mean_q: -4.823473930358887, std_q: 4.996242523193359
Running avgs for agent 1: q_loss: 0.12631380558013916, p_loss: -5.752866268157959, mean_rew: 0.2761346116642056, variance: 0.2554236943026384, mean_q: 5.696451187133789, std_q: 3.4277725219726562
Running avgs for agent 2: q_loss: 0.1260158270597458, p_loss: -5.80945348739624, mean_rew: 0.27621311728315723, variance: 0.23513323232531547, mean_q: 5.751011371612549, std_q: 3.4272594451904297

...Finished total of 60001 episodes... Now freezing policy and running for 10000 more episodes to get cvar estimation
steps: 29970, episodes: 1000, mean episode reward: 3.3773730229989063, agent episode reward: [-15.085269155901246, 9.231321089450077, 9.231321089450077], time: 32.774
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 32.774
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 59970, episodes: 2000, mean episode reward: 3.407077119445695, agent episode reward: [-15.79698949746175, 9.602033308453723, 9.602033308453723], time: 41.492
steps: 59970, episodes: 2000, mean episode variance: 0.8266982016712427, agent episode variance: [0.5617460022568703, 0.09196841344237328, 0.17298378597199918], time: 41.493
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5041191756148247, variance: 1.9237875938415527, cvar: 0.46743616461753845, v: 0.43662044405937195, mean_q: -5.951098442077637, std_q: 5.730593681335449
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3091421381118327, variance: 0.3149603200081277, cvar: 54.24017333984375, v: 1.563379168510437, mean_q: 6.79415225982666, std_q: 3.804462194442749
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3083659147954061, variance: 0.592410225931504, cvar: 54.7696647644043, v: 1.563379168510437, mean_q: 6.845499038696289, std_q: 3.8158018589019775

steps: 89970, episodes: 3000, mean episode reward: 3.1152458195109123, agent episode reward: [-15.232491550933277, 9.173868685222095, 9.173868685222095], time: 42.346
steps: 89970, episodes: 3000, mean episode variance: 0.9354409199096263, agent episode variance: [0.6328013393878936, 0.10123962935432791, 0.20139995116740464], time: 42.346
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.512315465593864, variance: 2.1093380451202393, cvar: 0.040114447474479675, v: -0.17020873725414276, mean_q: -6.0149617195129395, std_q: 5.83384895324707
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3113157063959081, variance: 0.33746543118109307, cvar: 48.87644577026367, v: 2.201204776763916, mean_q: 6.801659107208252, std_q: 3.8588199615478516
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31048087868996693, variance: 0.6713331341743469, cvar: 49.51420974731445, v: 2.201204776763916, mean_q: 6.865105152130127, std_q: 3.882866621017456

steps: 119970, episodes: 4000, mean episode reward: 3.3602606987006456, agent episode reward: [-15.35673310168735, 9.358496900193996, 9.358496900193996], time: 41.821
steps: 119970, episodes: 4000, mean episode variance: 0.9810414447709918, agent episode variance: [0.6559085755348205, 0.11106777632981539, 0.21406509290635586], time: 41.821
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5137941758916643, variance: 2.186361789703369, cvar: -0.02869148924946785, v: -0.4217560887336731, mean_q: -6.014908313751221, std_q: 5.797457218170166
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3111407696817974, variance: 0.3702259210993846, cvar: 45.273292541503906, v: 2.6561119556427, mean_q: 6.809154510498047, std_q: 3.849247932434082
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31117824783264875, variance: 0.713550329208374, cvar: 46.07797622680664, v: 2.6561119556427, mean_q: 6.895050525665283, std_q: 3.896529197692871

steps: 149970, episodes: 5000, mean episode reward: 3.3136385299620574, agent episode reward: [-15.0791283450129, 9.19638343748748, 9.19638343748748], time: 43.364
steps: 149970, episodes: 5000, mean episode variance: 0.9667278940565884, agent episode variance: [0.6461506272554398, 0.11169690224900841, 0.20888036455214024], time: 43.364
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5123399773097571, variance: 2.1538355350494385, cvar: -0.03902997076511383, v: -0.507947564125061, mean_q: -6.022177219390869, std_q: 5.757565021514893
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3107784018470838, variance: 0.3723230074966947, cvar: 42.247406005859375, v: 3.048576831817627, mean_q: 6.805402755737305, std_q: 3.830747365951538
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3107640595229683, variance: 0.6962678818404675, cvar: 42.99732971191406, v: 3.048576831817627, mean_q: 6.893305778503418, std_q: 3.86397385597229

steps: 179970, episodes: 6000, mean episode reward: 3.2305977243099577, agent episode reward: [-15.574304704363211, 9.402451214336583, 9.402451214336583], time: 42.767
steps: 179970, episodes: 6000, mean episode variance: 0.996436197726056, agent episode variance: [0.6511651375293732, 0.11563092545978725, 0.22964013473689557], time: 42.768
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.512148769955878, variance: 2.170550584793091, cvar: -0.03198089078068733, v: -0.5319988131523132, mean_q: -6.0232086181640625, std_q: 5.766159534454346
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3111609918644046, variance: 0.3854364181992908, cvar: 39.83325958251953, v: 3.408940076828003, mean_q: 6.826267242431641, std_q: 3.834506034851074
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31247171709564436, variance: 0.7654671157896519, cvar: 40.410118103027344, v: 3.408940076828003, mean_q: 6.898375034332275, std_q: 3.854571580886841

steps: 209970, episodes: 7000, mean episode reward: 2.905377374851283, agent episode reward: [-14.891105576553652, 8.898241475702466, 8.898241475702466], time: 43.181
steps: 209970, episodes: 7000, mean episode variance: 0.9912477030754089, agent episode variance: [0.6521997384428978, 0.10714048969745636, 0.23190747493505479], time: 43.181
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5099999605200768, variance: 2.173999071121216, cvar: -0.03044651634991169, v: -0.5333676934242249, mean_q: -6.0087127685546875, std_q: 5.767724514007568
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3106201991513643, variance: 0.35713496565818786, cvar: 37.51020050048828, v: 3.750152111053467, mean_q: 6.820925712585449, std_q: 3.8426260948181152
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31011036156789773, variance: 0.7730249762535095, cvar: 37.841026306152344, v: 3.750152111053467, mean_q: 6.866620063781738, std_q: 3.8635575771331787

steps: 239970, episodes: 8000, mean episode reward: 3.373378980334988, agent episode reward: [-14.7298856718113, 9.051632326073145, 9.051632326073145], time: 42.797
steps: 239970, episodes: 8000, mean episode variance: 0.9713952025547624, agent episode variance: [0.6582252676486969, 0.10273109798878431, 0.21043883691728116], time: 42.798
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5104171058769922, variance: 2.1940841674804688, cvar: -0.02148035354912281, v: -0.5167286396026611, mean_q: -5.991239547729492, std_q: 5.792542457580566
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30886343452198767, variance: 0.34243699329594773, cvar: 35.20045471191406, v: 4.079039096832275, mean_q: 6.7967071533203125, std_q: 3.8543291091918945
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.308016035541607, variance: 0.7014627897242705, cvar: 35.555938720703125, v: 4.079039096832275, mean_q: 6.847799301147461, std_q: 3.856701135635376

steps: 269970, episodes: 9000, mean episode reward: 3.543757047386136, agent episode reward: [-15.05073304472783, 9.297245046056984, 9.297245046056984], time: 42.802
steps: 269970, episodes: 9000, mean episode variance: 0.9628785763941705, agent episode variance: [0.6411731211543084, 0.10883868001773954, 0.21286677522212266], time: 42.802
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5087728759646339, variance: 2.1372437477111816, cvar: -0.020761674270033836, v: -0.5115586519241333, mean_q: -5.984459400177002, std_q: 5.7709503173828125
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30853392480897374, variance: 0.3627956000591318, cvar: 33.26397705078125, v: 4.399628162384033, mean_q: 6.795719623565674, std_q: 3.85746693611145
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30868177323080737, variance: 0.7095559239387512, cvar: 33.751617431640625, v: 4.399628162384033, mean_q: 6.861074924468994, std_q: 3.8665473461151123

steps: 299970, episodes: 10000, mean episode reward: 3.277261313555081, agent episode reward: [-15.750128537850749, 9.513694925702916, 9.513694925702916], time: 42.902
steps: 299970, episodes: 10000, mean episode variance: 0.9390464761294425, agent episode variance: [0.6221842614412307, 0.10603377069905401, 0.21082844398915768], time: 42.902
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5088628009376407, variance: 2.0739476680755615, cvar: -0.02675408124923706, v: -0.511163055896759, mean_q: -5.979590892791748, std_q: 5.770037651062012
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30867751592263426, variance: 0.35344590233018003, cvar: 31.44957733154297, v: 4.714466571807861, mean_q: 6.792109489440918, std_q: 3.854506731033325
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3091867348515647, variance: 0.7027615308761597, cvar: 31.948701858520508, v: 4.714466571807861, mean_q: 6.860020160675049, std_q: 3.867938756942749

steps: 329970, episodes: 11000, mean episode reward: 3.501361787392723, agent episode reward: [-15.571898744876247, 9.536630266134486, 9.536630266134486], time: 43.668
steps: 329970, episodes: 11000, mean episode variance: 0.9573451493009925, agent episode variance: [0.6310287633538246, 0.11116581135243177, 0.2151505745947361], time: 43.668
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5097949827762441, variance: 2.103429079055786, cvar: -0.017697738483548164, v: -0.5116497874259949, mean_q: -5.983631610870361, std_q: 5.776480197906494
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30924581386503736, variance: 0.3705527045081059, cvar: 29.741289138793945, v: 5.025244235992432, mean_q: 6.786921501159668, std_q: 3.8550853729248047
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31044571392175185, variance: 0.7171685695648193, cvar: 30.251035690307617, v: 5.025244235992432, mean_q: 6.858678340911865, std_q: 3.863842725753784

steps: 359970, episodes: 12000, mean episode reward: 3.410404969609122, agent episode reward: [-14.952948372922881, 9.181676671266002, 9.181676671266002], time: 42.94
steps: 359970, episodes: 12000, mean episode variance: 0.9743499253094197, agent episode variance: [0.6481769773960113, 0.10776191524416208, 0.2184110326692462], time: 42.94
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.50832769985122, variance: 2.1605899333953857, cvar: -0.026981618255376816, v: -0.5169263482093811, mean_q: -5.986730575561523, std_q: 5.778353691101074
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30934102576412636, variance: 0.35920638414720696, cvar: 28.17453384399414, v: 5.333113670349121, mean_q: 6.790770053863525, std_q: 3.8453686237335205
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3096654664005984, variance: 0.7280368208885193, cvar: 28.707969665527344, v: 5.333113670349121, mean_q: 6.861752986907959, std_q: 3.8586978912353516

steps: 389970, episodes: 13000, mean episode reward: 3.3610776591415052, agent episode reward: [-14.226458534365864, 8.793768096753684, 8.793768096753684], time: 43.074
steps: 389970, episodes: 13000, mean episode variance: 0.9644036121144891, agent episode variance: [0.6406987806558609, 0.1112576103284955, 0.21244722113013267], time: 43.075
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5073673826417131, variance: 2.13566255569458, cvar: -0.03814535588026047, v: -0.5269364714622498, mean_q: -5.972938060760498, std_q: 5.719297885894775
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3099796623130201, variance: 0.37085870109498503, cvar: 26.796361923217773, v: 5.638881683349609, mean_q: 6.798441410064697, std_q: 3.851550340652466
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31035984816432166, variance: 0.7081574201583862, cvar: 27.237974166870117, v: 5.638881683349609, mean_q: 6.866436958312988, std_q: 3.8457813262939453

steps: 419970, episodes: 14000, mean episode reward: 3.233816000272299, agent episode reward: [-15.349154386893886, 9.291485193583092, 9.291485193583092], time: 42.389
steps: 419970, episodes: 14000, mean episode variance: 0.9624015915617347, agent episode variance: [0.6371961585283279, 0.10826733334362507, 0.21693809968978167], time: 42.39
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.506517400000743, variance: 2.1239871978759766, cvar: -0.03539887070655823, v: -0.525300920009613, mean_q: -5.965533256530762, std_q: 5.746732711791992
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30823696936569195, variance: 0.3608911111454169, cvar: 25.231822967529297, v: 5.943122386932373, mean_q: 6.774898052215576, std_q: 3.831184148788452
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3089795482721431, variance: 0.7231269478797913, cvar: 25.787967681884766, v: 5.943122386932373, mean_q: 6.857664585113525, std_q: 3.838081121444702

steps: 449970, episodes: 15000, mean episode reward: 3.3586947870613075, agent episode reward: [-15.396147098598767, 9.377420942830039, 9.377420942830039], time: 43.604
steps: 449970, episodes: 15000, mean episode variance: 0.9665975206680596, agent episode variance: [0.6389702919125557, 0.10899742516130209, 0.2186298035942018], time: 43.604
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5058490076859681, variance: 2.1299009323120117, cvar: -0.03642350435256958, v: -0.5285415649414062, mean_q: -5.954908847808838, std_q: 5.716711044311523
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30857766627263367, variance: 0.36332475053767366, cvar: 24.010005950927734, v: 6.246246814727783, mean_q: 6.779669761657715, std_q: 3.8294644355773926
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30832015158900294, variance: 0.7287660241127014, cvar: 24.448835372924805, v: 6.246246814727783, mean_q: 6.845208168029785, std_q: 3.8386943340301514

steps: 479970, episodes: 16000, mean episode reward: 3.3104737560230815, agent episode reward: [-14.833699471622044, 9.072086613822563, 9.072086613822563], time: 43.055
steps: 479970, episodes: 16000, mean episode variance: 0.9655712506063282, agent episode variance: [0.6330888139605522, 0.11354481775686145, 0.21893761888891458], time: 43.055
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5081862269093117, variance: 2.1102960109710693, cvar: -0.03658263757824898, v: -0.5306491851806641, mean_q: -5.977335453033447, std_q: 5.745180606842041
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3097165390371997, variance: 0.37848272585620485, cvar: 22.870269775390625, v: 6.548550128936768, mean_q: 6.787276744842529, std_q: 3.8269848823547363
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3088986499764554, variance: 0.7297920629630487, cvar: 23.24086570739746, v: 6.548550128936768, mean_q: 6.846092224121094, std_q: 3.8308467864990234

steps: 509970, episodes: 17000, mean episode reward: 3.2843839208811065, agent episode reward: [-14.970063084502945, 9.127223502692024, 9.127223502692024], time: 42.976
steps: 509970, episodes: 17000, mean episode variance: 0.9601669593118131, agent episode variance: [0.6407940818667411, 0.10627802754566074, 0.21309484989941122], time: 42.977
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5077235979842015, variance: 2.1359801292419434, cvar: -0.03871094807982445, v: -0.5319076776504517, mean_q: -5.985649585723877, std_q: 5.742440700531006
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30779384795847214, variance: 0.35426009181886914, cvar: 21.702606201171875, v: 6.8502516746521, mean_q: 6.774078845977783, std_q: 3.8125414848327637
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30747345388766734, variance: 0.7103161811828613, cvar: 22.0842227935791, v: 6.8502516746521, mean_q: 6.837413787841797, std_q: 3.81960129737854

steps: 539970, episodes: 18000, mean episode reward: 3.4014698716110834, agent episode reward: [-16.071786080206763, 9.736627975908926, 9.736627975908926], time: 42.487
steps: 539970, episodes: 18000, mean episode variance: 0.9735820843577385, agent episode variance: [0.6588381178677082, 0.10474681535363198, 0.2099971511363983], time: 42.487
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.509553788368909, variance: 2.196126937866211, cvar: -0.03476892039179802, v: -0.5285378098487854, mean_q: -5.998755931854248, std_q: 5.759077072143555
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30842222403207986, variance: 0.34915605117877324, cvar: 20.677570343017578, v: 7.151508808135986, mean_q: 6.77182674407959, std_q: 3.7964136600494385
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30790672545633685, variance: 0.699990451335907, cvar: 21.104887008666992, v: 7.151508808135986, mean_q: 6.843062400817871, std_q: 3.821120500564575

steps: 569970, episodes: 19000, mean episode reward: 3.2688236306257936, agent episode reward: [-15.474170952717445, 9.371497291671618, 9.371497291671618], time: 43.867
steps: 569970, episodes: 19000, mean episode variance: 0.9871816112361849, agent episode variance: [0.656755802989006, 0.11112887665256858, 0.21929693159461022], time: 43.868
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5066726283511878, variance: 2.1891860961914062, cvar: -0.030354805290699005, v: -0.5247014164924622, mean_q: -5.987303256988525, std_q: 5.746687412261963
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30927688464037123, variance: 0.37042958884189525, cvar: 19.8776798248291, v: 7.4524383544921875, mean_q: 6.783528804779053, std_q: 3.80523681640625
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3086328983883437, variance: 0.7309898138046265, cvar: 20.295848846435547, v: 7.4524383544921875, mean_q: 6.855790138244629, std_q: 3.832566261291504

steps: 599970, episodes: 20000, mean episode reward: 3.20275172301973, agent episode reward: [-16.01150327354699, 9.60712749828336, 9.60712749828336], time: 43.303
steps: 599970, episodes: 20000, mean episode variance: 0.9840484227724373, agent episode variance: [0.6661486834287643, 0.10958367202058435, 0.20831606732308863], time: 43.304
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.511905819454438, variance: 2.2204957008361816, cvar: -0.032811760902404785, v: -0.5259695649147034, mean_q: -6.027482509613037, std_q: 5.7911858558654785
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30960273778555075, variance: 0.36527890673528113, cvar: 19.103717803955078, v: 7.753126621246338, mean_q: 6.78896951675415, std_q: 3.8150432109832764
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3102076678018428, variance: 0.6943868910769622, cvar: 19.512413024902344, v: 7.753126621246338, mean_q: 6.868408203125, std_q: 3.8364574909210205

steps: 629970, episodes: 21000, mean episode reward: 3.629608211396623, agent episode reward: [-15.708351347412604, 9.668979779404614, 9.668979779404614], time: 43.028
steps: 629970, episodes: 21000, mean episode variance: 0.9851537474133074, agent episode variance: [0.667300123155117, 0.10450534958019853, 0.21334827467799186], time: 43.029
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5102553165033086, variance: 2.2243337631225586, cvar: -0.036052025854587555, v: -0.5249133706092834, mean_q: -6.017437934875488, std_q: 5.768084526062012
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3114164099899286, variance: 0.3483511652673284, cvar: 18.47532844543457, v: 8.053635597229004, mean_q: 6.805481910705566, std_q: 3.8249425888061523
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30923760065393924, variance: 0.7111608982086182, cvar: 18.742496490478516, v: 8.053635597229004, mean_q: 6.861703872680664, std_q: 3.8445451259613037

steps: 659970, episodes: 22000, mean episode reward: 2.948415303359903, agent episode reward: [-15.510566301910123, 9.229490802635013, 9.229490802635013], time: 43.184
steps: 659970, episodes: 22000, mean episode variance: 0.9979341028220952, agent episode variance: [0.669773988187313, 0.11294554636254907, 0.215214568272233], time: 43.184
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5099560736710032, variance: 2.2325799465179443, cvar: -0.02895425818860531, v: -0.5168909430503845, mean_q: -5.994438648223877, std_q: 5.769192695617676
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30948951288498244, variance: 0.37648515454183024, cvar: 17.708227157592773, v: 8.354044914245605, mean_q: 6.782217025756836, std_q: 3.8183329105377197
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3107496690450527, variance: 0.7173818945884705, cvar: 18.07721710205078, v: 8.354044914245605, mean_q: 6.872653007507324, std_q: 3.8360936641693115

steps: 689970, episodes: 23000, mean episode reward: 3.7791971704221403, agent episode reward: [-14.640260807335515, 9.209728988878826, 9.209728988878826], time: 43.146
steps: 689970, episodes: 23000, mean episode variance: 0.9778266247622669, agent episode variance: [0.6455539197027683, 0.1121817211471498, 0.22009098391234874], time: 43.147
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5074357906962811, variance: 2.151846408843994, cvar: -0.030177904292941093, v: -0.5163732171058655, mean_q: -5.997862339019775, std_q: 5.759456157684326
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31041448584157655, variance: 0.37393907049049935, cvar: 17.2349910736084, v: 8.654350280761719, mean_q: 6.801146030426025, std_q: 3.8302416801452637
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31037158666361175, variance: 0.7336366176605225, cvar: 17.496116638183594, v: 8.654350280761719, mean_q: 6.870469570159912, std_q: 3.844860792160034

steps: 719970, episodes: 24000, mean episode reward: 3.0172892870807293, agent episode reward: [-15.577025060624932, 9.297157173852831, 9.297157173852831], time: 42.622
steps: 719970, episodes: 24000, mean episode variance: 0.9635739177111536, agent episode variance: [0.6452894109785556, 0.10861883478052914, 0.2096656719520688], time: 42.623
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5088481541566091, variance: 2.1509647369384766, cvar: -0.03085292875766754, v: -0.5131242275238037, mean_q: -5.990408420562744, std_q: 5.759498119354248
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3103490951362693, variance: 0.3620627826017638, cvar: 16.67047882080078, v: 8.954475402832031, mean_q: 6.7956013679504395, std_q: 3.824352979660034
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3095566540149052, variance: 0.6988855731735627, cvar: 16.96402931213379, v: 8.954475402832031, mean_q: 6.868840217590332, std_q: 3.8439087867736816

steps: 749970, episodes: 25000, mean episode reward: 3.337161691701431, agent episode reward: [-15.487201504164604, 9.412181597933017, 9.412181597933017], time: 43.745
steps: 749970, episodes: 25000, mean episode variance: 0.9610519066601991, agent episode variance: [0.6481623913645744, 0.09856924253702164, 0.2143202727586031], time: 43.745
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5101376226684069, variance: 2.1605413045485813, cvar: -0.034418895840644836, v: -0.5115371346473694, mean_q: -6.009806156158447, std_q: 5.773876190185547
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3093722976943785, variance: 0.32856414179007215, cvar: 16.2186222076416, v: 9.254596710205078, mean_q: 6.793445110321045, std_q: 3.821441650390625
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3103807656521704, variance: 0.7144008874893188, cvar: 16.53203582763672, v: 9.254596710205078, mean_q: 6.889575004577637, std_q: 3.8436646461486816

steps: 779970, episodes: 26000, mean episode reward: 3.527668368217524, agent episode reward: [-14.555248077100456, 9.04145822265899, 9.04145822265899], time: 42.648
steps: 779970, episodes: 26000, mean episode variance: 0.9783857182189822, agent episode variance: [0.6549040026664734, 0.10728288365155458, 0.21619883190095424], time: 42.648
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5099217743546521, variance: 2.183013439178467, cvar: -0.03197239711880684, v: -0.5128777027130127, mean_q: -6.0062456130981445, std_q: 5.771891117095947
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3103866089055619, variance: 0.3576096121718486, cvar: 15.8679780960083, v: 9.554718017578125, mean_q: 6.800189971923828, std_q: 3.828709363937378
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31048450331243316, variance: 0.7206627130508423, cvar: 16.12336540222168, v: 9.554718017578125, mean_q: 6.877570629119873, std_q: 3.8480656147003174

steps: 809970, episodes: 27000, mean episode reward: 3.8293048679617723, agent episode reward: [-14.630277562373674, 9.229791215167722, 9.229791215167722], time: 43.379
steps: 809970, episodes: 27000, mean episode variance: 1.0002178977429868, agent episode variance: [0.6765844627916813, 0.11030764748901128, 0.2133257874622941], time: 43.379
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5105935796820328, variance: 2.255281686782837, cvar: -0.03531259670853615, v: -0.5100029706954956, mean_q: -6.0190863609313965, std_q: 5.793874740600586
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3095096300668494, variance: 0.3676921582967043, cvar: 15.480937957763672, v: 9.854839324951172, mean_q: 6.785642147064209, std_q: 3.813330888748169
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3103677008185142, variance: 0.711085958207647, cvar: 15.701761245727539, v: 9.854839324951172, mean_q: 6.868844985961914, std_q: 3.83217453956604

steps: 839970, episodes: 28000, mean episode reward: 3.9170508657429544, agent episode reward: [-15.725635419031295, 9.821343142387125, 9.821343142387125], time: 42.555
steps: 839970, episodes: 28000, mean episode variance: 0.9705672722384333, agent episode variance: [0.6502814378142356, 0.10902694354206324, 0.21125889088213443], time: 42.555
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5094336010882147, variance: 2.167604923248291, cvar: -0.0430544838309288, v: -0.5216479301452637, mean_q: -5.998361587524414, std_q: 5.757525444030762
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31029781437698056, variance: 0.3634231451402108, cvar: 15.191224098205566, v: 10.154804229736328, mean_q: 6.795746326446533, std_q: 3.806067705154419
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31009497801321145, variance: 0.7041962742805481, cvar: 15.381552696228027, v: 10.154959678649902, mean_q: 6.863074779510498, std_q: 3.8236522674560547

steps: 869970, episodes: 29000, mean episode reward: 2.519671773837841, agent episode reward: [-15.965890038522703, 9.24278090618027, 9.24278090618027], time: 42.631
steps: 869970, episodes: 29000, mean episode variance: 0.9504914596304297, agent episode variance: [0.6333295232653617, 0.1084883439913392, 0.20867359237372876], time: 42.631
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5079449554307954, variance: 2.111098527908325, cvar: -0.04053158685564995, v: -0.5253725647926331, mean_q: -5.978712558746338, std_q: 5.725344657897949
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30971304221865237, variance: 0.36162781330446403, cvar: 15.013978004455566, v: 10.453841209411621, mean_q: 6.798660755157471, std_q: 3.819242000579834
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31102222657525364, variance: 0.6955786347389221, cvar: 15.251852035522461, v: 10.454900741577148, mean_q: 6.879572868347168, std_q: 3.844839572906494

steps: 899970, episodes: 30000, mean episode reward: 3.174644814047606, agent episode reward: [-14.7201319524462, 8.947388383246905, 8.947388383246905], time: 43.136
steps: 899970, episodes: 30000, mean episode variance: 0.9602198561280966, agent episode variance: [0.645913182169199, 0.1063586869686842, 0.20794798699021338], time: 43.136
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5124927786353427, variance: 2.1530439853668213, cvar: -0.040622297674417496, v: -0.5188673138618469, mean_q: -6.0164947509765625, std_q: 5.777202606201172
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3097994355797952, variance: 0.3545289565622807, cvar: 14.827662467956543, v: 10.745349884033203, mean_q: 6.797865390777588, std_q: 3.808474540710449
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3097551276884708, variance: 0.6931599378585815, cvar: 14.998291015625, v: 10.749831199645996, mean_q: 6.868010997772217, std_q: 3.8304696083068848

steps: 929970, episodes: 31000, mean episode reward: 2.9739801722018258, agent episode reward: [-14.98834400748898, 8.981162089845402, 8.981162089845402], time: 43.755
steps: 929970, episodes: 31000, mean episode variance: 0.9788820355497301, agent episode variance: [0.6565538727641106, 0.10711828399822115, 0.21520987878739833], time: 43.755
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5072829733097638, variance: 2.1885128021240234, cvar: -0.044782377779483795, v: -0.5254966616630554, mean_q: -5.979651927947998, std_q: 5.7451019287109375
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30836183760217295, variance: 0.3570609466607372, cvar: 14.668320655822754, v: 11.006458282470703, mean_q: 6.772411346435547, std_q: 3.805659532546997
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30991451466316405, variance: 0.7173662626246611, cvar: 14.883399963378906, v: 11.02003288269043, mean_q: 6.8571696281433105, std_q: 3.835864782333374

steps: 959970, episodes: 32000, mean episode reward: 3.3824829194359607, agent episode reward: [-14.963500154728633, 9.1729915370823, 9.1729915370823], time: 42.501
steps: 959970, episodes: 32000, mean episode variance: 0.9739426764082164, agent episode variance: [0.6536832266449928, 0.10440842118673027, 0.21585102857649327], time: 42.501
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5099801085260594, variance: 2.1789443492889404, cvar: -0.045931797474622726, v: -0.5313615202903748, mean_q: -5.998685836791992, std_q: 5.738101005554199
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3087179349184503, variance: 0.34802807062243424, cvar: 14.628323554992676, v: 11.220810890197754, mean_q: 6.788413047790527, std_q: 3.8115603923797607
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30954730870180003, variance: 0.7195034285883108, cvar: 14.82563304901123, v: 11.252965927124023, mean_q: 6.868293285369873, std_q: 3.8396174907684326

steps: 989970, episodes: 33000, mean episode reward: 3.236385905730999, agent episode reward: [-15.031187655142253, 9.133786780436624, 9.133786780436624], time: 43.242
steps: 989970, episodes: 33000, mean episode variance: 0.9771785633526743, agent episode variance: [0.6529465769529342, 0.110191808167845, 0.21404017823189497], time: 43.242
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5096970544460727, variance: 2.1764886379241943, cvar: -0.044496405869722366, v: -0.5241209864616394, mean_q: -5.991315841674805, std_q: 5.77285623550415
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3104069549672788, variance: 0.36730602722615, cvar: 14.620110511779785, v: 11.403367042541504, mean_q: 6.796935081481934, std_q: 3.818537712097168
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3098474609878157, variance: 0.7134671807289124, cvar: 14.742698669433594, v: 11.445032119750977, mean_q: 6.856379985809326, std_q: 3.8376805782318115

steps: 1019970, episodes: 34000, mean episode reward: 3.057922672840278, agent episode reward: [-14.994165975691422, 9.026044324265849, 9.026044324265849], time: 43.385
steps: 1019970, episodes: 34000, mean episode variance: 0.943759357791394, agent episode variance: [0.6275339525043965, 0.10842736467346549, 0.20779804061353208], time: 43.385
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5072286867677588, variance: 2.0917797088623047, cvar: -0.046559933573007584, v: -0.5294051170349121, mean_q: -5.977520942687988, std_q: 5.743764877319336
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30955800413361134, variance: 0.3614245489115516, cvar: 14.548929214477539, v: 11.547249794006348, mean_q: 6.787363052368164, std_q: 3.8090450763702393
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30935090302153556, variance: 0.6926601353784402, cvar: 14.68870735168457, v: 11.595403671264648, mean_q: 6.859601020812988, std_q: 3.8300065994262695

steps: 1049970, episodes: 35000, mean episode reward: 3.4757820335244896, agent episode reward: [-15.349800263058961, 9.412791148291726, 9.412791148291726], time: 43.244
steps: 1049970, episodes: 35000, mean episode variance: 0.9675686194766313, agent episode variance: [0.640238051712513, 0.10720637090690434, 0.22012419685721396], time: 43.244
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.507616909719704, variance: 2.134126901626587, cvar: -0.04316585510969162, v: -0.5229525566101074, mean_q: -5.976787090301514, std_q: 5.75886869430542
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3106042440791189, variance: 0.35735456968968116, cvar: 14.552867889404297, v: 11.6655912399292, mean_q: 6.7929768562316895, std_q: 3.813190460205078
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3096271553985625, variance: 0.7337473630905151, cvar: 14.67286491394043, v: 11.725276947021484, mean_q: 6.856985569000244, std_q: 3.8346760272979736

steps: 1079970, episodes: 36000, mean episode reward: 3.684227266013124, agent episode reward: [-16.005959980400743, 9.845093623206932, 9.845093623206932], time: 43.283
steps: 1079970, episodes: 36000, mean episode variance: 0.9506845519114285, agent episode variance: [0.6274388605952262, 0.11195202635414898, 0.2112936649620533], time: 43.284
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5089577132265813, variance: 2.0914628505706787, cvar: -0.045457515865564346, v: -0.5245437026023865, mean_q: -5.985286235809326, std_q: 5.751858711242676
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31023404892048206, variance: 0.3731734211804966, cvar: 14.539578437805176, v: 11.75797176361084, mean_q: 6.7825846672058105, std_q: 3.812197208404541
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30870484991609765, variance: 0.7043122165401776, cvar: 14.636025428771973, v: 11.819804191589355, mean_q: 6.8498616218566895, std_q: 3.8240458965301514

steps: 1109970, episodes: 37000, mean episode reward: 3.3103569863428763, agent episode reward: [-16.44402420542095, 9.877190595881912, 9.877190595881912], time: 43.776
steps: 1109970, episodes: 37000, mean episode variance: 0.9568952002916485, agent episode variance: [0.6393377321958542, 0.10402530496008694, 0.2135321631357074], time: 43.777
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5087130424782407, variance: 2.1311256885528564, cvar: -0.051003698259592056, v: -0.5310608148574829, mean_q: -5.989481449127197, std_q: 5.744451522827148
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31087878156172766, variance: 0.3467510165336231, cvar: 14.515738487243652, v: 11.830326080322266, mean_q: 6.790964126586914, std_q: 3.8089427947998047
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3096785278500569, variance: 0.7117738723754883, cvar: 14.64219856262207, v: 11.900948524475098, mean_q: 6.861020565032959, std_q: 3.823333740234375

steps: 1139970, episodes: 38000, mean episode reward: 3.3671103510170943, agent episode reward: [-15.251673476397835, 9.309391913707465, 9.309391913707465], time: 43.002
steps: 1139970, episodes: 38000, mean episode variance: 0.9569012135900558, agent episode variance: [0.633354533791542, 0.10718613831326365, 0.21636054148525], time: 43.003
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5082935784448216, variance: 2.1111817359924316, cvar: -0.04942401126027107, v: -0.531595766544342, mean_q: -5.995794773101807, std_q: 5.769855976104736
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31146848687548934, variance: 0.35728712771087884, cvar: 14.531230926513672, v: 11.884790420532227, mean_q: 6.8042402267456055, std_q: 3.809033155441284
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.310575782279222, variance: 0.7212017774581909, cvar: 14.673453330993652, v: 11.965826988220215, mean_q: 6.870345115661621, std_q: 3.834597110748291

steps: 1169970, episodes: 39000, mean episode reward: 3.9100431899581727, agent episode reward: [-15.205148875353927, 9.557596032656049, 9.557596032656049], time: 43.419
steps: 1169970, episodes: 39000, mean episode variance: 0.9848577322959899, agent episode variance: [0.6539053951501846, 0.10943865504860878, 0.22151368209719657], time: 43.42
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5100813834979745, variance: 2.179684638977051, cvar: -0.041131164878606796, v: -0.518603503704071, mean_q: -5.999990940093994, std_q: 5.780499458312988
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3113571160314605, variance: 0.36479551682869593, cvar: 14.566139221191406, v: 11.923480987548828, mean_q: 6.805530548095703, std_q: 3.8175032138824463
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31002568489657945, variance: 0.7383788824081421, cvar: 14.619165420532227, v: 12.013275146484375, mean_q: 6.8518195152282715, std_q: 3.8223278522491455

steps: 1199970, episodes: 40000, mean episode reward: 3.527557490061786, agent episode reward: [-15.588052497119865, 9.557804993590826, 9.557804993590826], time: 42.907
steps: 1199970, episodes: 40000, mean episode variance: 0.9592970014847815, agent episode variance: [0.6448492277264595, 0.10221076542511583, 0.2122370083332062], time: 42.907
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5112633770133133, variance: 2.1494975090026855, cvar: -0.0487615205347538, v: -0.52571702003479, mean_q: -5.994543552398682, std_q: 5.778752326965332
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.311388159696409, variance: 0.34070255141705275, cvar: 14.568211555480957, v: 11.958046913146973, mean_q: 6.803089618682861, std_q: 3.8206241130828857
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31207171761664104, variance: 0.707456648349762, cvar: 14.71093463897705, v: 12.039108276367188, mean_q: 6.880051136016846, std_q: 3.842984199523926

steps: 1229970, episodes: 41000, mean episode reward: 3.307766478690759, agent episode reward: [-15.888983148320277, 9.598374813505517, 9.598374813505517], time: 43.484
steps: 1229970, episodes: 41000, mean episode variance: 0.9955334378611297, agent episode variance: [0.6735753358006478, 0.10915718067996204, 0.21280092138051987], time: 43.484
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5105505161395144, variance: 2.245251178741455, cvar: -0.05386646091938019, v: -0.5303083062171936, mean_q: -5.997684478759766, std_q: 5.768770217895508
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31216445601276627, variance: 0.3638572689332068, cvar: 14.557584762573242, v: 11.974381446838379, mean_q: 6.800100326538086, std_q: 3.81538987159729
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31114594931265227, variance: 0.7093364000320435, cvar: 14.688340187072754, v: 12.081478118896484, mean_q: 6.877955913543701, std_q: 3.840941905975342

steps: 1259970, episodes: 42000, mean episode reward: 3.6697041478890373, agent episode reward: [-14.911335661177386, 9.29051990453321, 9.29051990453321], time: 43.069
steps: 1259970, episodes: 42000, mean episode variance: 0.9944344724789261, agent episode variance: [0.6673350320458412, 0.10736820318549871, 0.21973123724758625], time: 43.069
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5125929085184298, variance: 2.2244503498077393, cvar: -0.051925644278526306, v: -0.535431444644928, mean_q: -6.01821756362915, std_q: 5.8013916015625
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31112152885526967, variance: 0.35789401061832904, cvar: 14.535003662109375, v: 11.985487937927246, mean_q: 6.796712398529053, std_q: 3.8104755878448486
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31191678888826974, variance: 0.7324374318122864, cvar: 14.656855583190918, v: 12.096155166625977, mean_q: 6.873079299926758, std_q: 3.8325276374816895

steps: 1289970, episodes: 43000, mean episode reward: 3.1989041911450147, agent episode reward: [-15.367219443529638, 9.283061817337327, 9.283061817337327], time: 42.898
steps: 1289970, episodes: 43000, mean episode variance: 0.9656019046604634, agent episode variance: [0.6460464355945588, 0.10847238749265671, 0.2110830815732479], time: 42.899
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5113157094737415, variance: 2.1534881591796875, cvar: -0.052935633808374405, v: -0.5286225080490112, mean_q: -5.996962070465088, std_q: 5.768229961395264
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31096361538567263, variance: 0.36157462497552234, cvar: 14.568984031677246, v: 11.99632740020752, mean_q: 6.796303749084473, std_q: 3.8228471279144287
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31266605084303417, variance: 0.7036102414131165, cvar: 14.69269847869873, v: 12.111774444580078, mean_q: 6.886488437652588, std_q: 3.837695360183716

steps: 1319970, episodes: 44000, mean episode reward: 3.9417803689491357, agent episode reward: [-15.380902763365329, 9.661341566157233, 9.661341566157233], time: 43.135
steps: 1319970, episodes: 44000, mean episode variance: 0.9705275868922472, agent episode variance: [0.6458425099849701, 0.10522911967337131, 0.2194559572339058], time: 43.135
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5112243696696366, variance: 2.152808427810669, cvar: -0.05328013747930527, v: -0.5275094509124756, mean_q: -6.013099670410156, std_q: 5.770303726196289
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3114059804944308, variance: 0.350763732244571, cvar: 14.595551490783691, v: 11.998230934143066, mean_q: 6.804358005523682, std_q: 3.824840545654297
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3120823203067694, variance: 0.7315197587013245, cvar: 14.67711067199707, v: 12.117877960205078, mean_q: 6.865646839141846, std_q: 3.834503650665283

steps: 1349970, episodes: 45000, mean episode reward: 3.214399855446949, agent episode reward: [-15.249386416028878, 9.231893135737915, 9.231893135737915], time: 44.14
steps: 1349970, episodes: 45000, mean episode variance: 0.963336422353983, agent episode variance: [0.6433130022585392, 0.10870577271282673, 0.211317647382617], time: 44.14
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5124110449009389, variance: 2.144376516342163, cvar: -0.055247701704502106, v: -0.5409901142120361, mean_q: -6.0235185623168945, std_q: 5.788530349731445
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3122303907213452, variance: 0.3623525757094224, cvar: 14.577652931213379, v: 11.990047454833984, mean_q: 6.804444789886475, std_q: 3.8211381435394287
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3122119434665951, variance: 0.7043921579420567, cvar: 14.672438621520996, v: 12.118390083312988, mean_q: 6.878462791442871, std_q: 3.8315274715423584

steps: 1379970, episodes: 46000, mean episode reward: 3.32085722348782, agent episode reward: [-15.368600211150298, 9.344728717319057, 9.344728717319057], time: 42.88
steps: 1379970, episodes: 46000, mean episode variance: 0.9689637026377023, agent episode variance: [0.6596749529838563, 0.10950409660860896, 0.19978465304523707], time: 42.88
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5119924356360872, variance: 2.198916435241699, cvar: -0.049360889941453934, v: -0.5295851826667786, mean_q: -6.008598804473877, std_q: 5.787522315979004
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3117907377581028, variance: 0.36501365536202984, cvar: 14.576080322265625, v: 11.990309715270996, mean_q: 6.800611972808838, std_q: 3.8202669620513916
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3103597550734124, variance: 0.6659488434841235, cvar: 14.650504112243652, v: 12.112567901611328, mean_q: 6.868775844573975, std_q: 3.8261518478393555

steps: 1409970, episodes: 47000, mean episode reward: 3.5591793830503486, agent episode reward: [-15.019731387403313, 9.28945538522683, 9.28945538522683], time: 43.561
steps: 1409970, episodes: 47000, mean episode variance: 0.9770717212744057, agent episode variance: [0.664131574600935, 0.1022403508760035, 0.21069979579746723], time: 43.561
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5133104542823478, variance: 2.2137720584869385, cvar: -0.046902332454919815, v: -0.519980251789093, mean_q: -6.023828983306885, std_q: 5.799803256988525
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3112332882668302, variance: 0.34080116958667833, cvar: 14.569375038146973, v: 11.983614921569824, mean_q: 6.796377182006836, std_q: 3.821079969406128
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3113453969126821, variance: 0.7023326754570007, cvar: 14.674369812011719, v: 12.117620468139648, mean_q: 6.872809886932373, std_q: 3.8362605571746826

steps: 1439970, episodes: 48000, mean episode reward: 3.3390433165232998, agent episode reward: [-15.389855669143847, 9.364449492833574, 9.364449492833574], time: 43.078
steps: 1439970, episodes: 48000, mean episode variance: 0.9627471943348647, agent episode variance: [0.6419554823637008, 0.10769978575408459, 0.21309192621707918], time: 43.079
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5129704838921059, variance: 2.1398515701293945, cvar: -0.05667987838387489, v: -0.5326677560806274, mean_q: -6.027663230895996, std_q: 5.79286527633667
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3127091904799572, variance: 0.3589992858469486, cvar: 14.6212797164917, v: 12.001389503479004, mean_q: 6.816567897796631, std_q: 3.8329825401306152
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31265668115803474, variance: 0.7103064060211182, cvar: 14.718857765197754, v: 12.131843566894531, mean_q: 6.885775566101074, std_q: 3.848630666732788

steps: 1469970, episodes: 49000, mean episode reward: 2.971066903909879, agent episode reward: [-14.877338577469839, 8.924202740689859, 8.924202740689859], time: 43.574
steps: 1469970, episodes: 49000, mean episode variance: 0.9588403039760888, agent episode variance: [0.6347530158162117, 0.10369970806315541, 0.22038758009672166], time: 43.575
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5111510558680172, variance: 2.1158432960510254, cvar: -0.05218968540430069, v: -0.5285358428955078, mean_q: -6.0086989402771, std_q: 5.765010356903076
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3127992949143467, variance: 0.34566569354385135, cvar: 14.59636402130127, v: 12.015267372131348, mean_q: 6.8168182373046875, std_q: 3.8330485820770264
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3118475622609652, variance: 0.7346252202987671, cvar: 14.662779808044434, v: 12.129419326782227, mean_q: 6.875619888305664, std_q: 3.831169366836548

steps: 1499970, episodes: 50000, mean episode reward: 3.5988513478305877, agent episode reward: [-15.295037377258158, 9.446944362544372, 9.446944362544372], time: 43.886
steps: 1499970, episodes: 50000, mean episode variance: 0.9820630286075175, agent episode variance: [0.6467331474423409, 0.11553459287062287, 0.21979528829455375], time: 43.887
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5121390754599957, variance: 2.1557772159576416, cvar: -0.057214152067899704, v: -0.5255265831947327, mean_q: -6.009821891784668, std_q: 5.787606716156006
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3117262073680873, variance: 0.3851153095687429, cvar: 14.588994026184082, v: 12.015122413635254, mean_q: 6.796008110046387, std_q: 3.8256592750549316
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.311313468408086, variance: 0.7326509356498718, cvar: 14.672798156738281, v: 12.132981300354004, mean_q: 6.870131969451904, std_q: 3.836308479309082

steps: 1529970, episodes: 51000, mean episode reward: 3.0170603359613795, agent episode reward: [-14.913495360118446, 8.965277848039912, 8.965277848039912], time: 43.284
steps: 1529970, episodes: 51000, mean episode variance: 0.9836410225965083, agent episode variance: [0.6509505524039269, 0.11305300240591168, 0.21963746778666973], time: 43.284
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5123805276654237, variance: 2.169835090637207, cvar: -0.054014090448617935, v: -0.5306585431098938, mean_q: -6.010740756988525, std_q: 5.785961627960205
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3122867001425411, variance: 0.37684334135303893, cvar: 14.616722106933594, v: 12.01418685913086, mean_q: 6.7996745109558105, std_q: 3.838646650314331
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3117714328592893, variance: 0.7321248926222325, cvar: 14.711819648742676, v: 12.14344596862793, mean_q: 6.873858451843262, std_q: 3.845216989517212

steps: 1559970, episodes: 52000, mean episode reward: 3.5348464887867608, agent episode reward: [-15.097142055258184, 9.315994272022472, 9.315994272022472], time: 44.297
steps: 1559970, episodes: 52000, mean episode variance: 0.9458528245799244, agent episode variance: [0.6310887876749038, 0.1037022150568664, 0.21106182184815406], time: 44.298
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5113117230717529, variance: 2.1036293506622314, cvar: -0.04740302264690399, v: -0.5225443243980408, mean_q: -6.004695415496826, std_q: 5.777622699737549
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30972309042595847, variance: 0.3456740501895547, cvar: 14.574121475219727, v: 11.994513511657715, mean_q: 6.7825398445129395, std_q: 3.820526123046875
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31147339059692275, variance: 0.7035394310951233, cvar: 14.708860397338867, v: 12.141826629638672, mean_q: 6.870909690856934, std_q: 3.842853307723999

steps: 1589970, episodes: 53000, mean episode reward: 3.4547069352213295, agent episode reward: [-14.927527937001429, 9.19111743611138, 9.19111743611138], time: 43.166
steps: 1589970, episodes: 53000, mean episode variance: 0.9385103829801082, agent episode variance: [0.6205038501620292, 0.10584199096262455, 0.21216454185545444], time: 43.167
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5103054993046177, variance: 2.0683462619781494, cvar: -0.051328256726264954, v: -0.5239015221595764, mean_q: -5.987181663513184, std_q: 5.766666412353516
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3108253180509353, variance: 0.35280663654208183, cvar: 14.56046199798584, v: 11.984649658203125, mean_q: 6.780041217803955, std_q: 3.8227474689483643
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3102529967763005, variance: 0.7072151899337769, cvar: 14.670680046081543, v: 12.130187034606934, mean_q: 6.850644588470459, std_q: 3.8351571559906006

steps: 1619970, episodes: 54000, mean episode reward: 3.760379705022333, agent episode reward: [-14.991330415961574, 9.375855060491954, 9.375855060491954], time: 44.277
steps: 1619970, episodes: 54000, mean episode variance: 0.9502189181167633, agent episode variance: [0.6339476401209831, 0.10479793614335359, 0.21147334185242653], time: 44.278
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5096785808272697, variance: 2.113158702850342, cvar: -0.04492705687880516, v: -0.5267316699028015, mean_q: -5.990970134735107, std_q: 5.7459588050842285
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3113832101110695, variance: 0.3493264538111786, cvar: 14.564238548278809, v: 11.979093551635742, mean_q: 6.788149356842041, std_q: 3.8244197368621826
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31181017988622933, variance: 0.7049111723899841, cvar: 14.69042682647705, v: 12.142019271850586, mean_q: 6.864324569702148, std_q: 3.839505910873413

steps: 1649970, episodes: 55000, mean episode reward: 3.3627412469093634, agent episode reward: [-15.504725843951995, 9.43373354543068, 9.43373354543068], time: 43.901
steps: 1649970, episodes: 55000, mean episode variance: 0.9523564472720027, agent episode variance: [0.6289486468434334, 0.10964308749884367, 0.21376471292972565], time: 43.901
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5092150128320534, variance: 2.0964953899383545, cvar: -0.04177476838231087, v: -0.5109583735466003, mean_q: -5.968125820159912, std_q: 5.735288143157959
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31182183729651786, variance: 0.3654769583294789, cvar: 14.582134246826172, v: 11.991524696350098, mean_q: 6.794577598571777, std_q: 3.824091911315918
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31227362463687386, variance: 0.7125490307807922, cvar: 14.684560775756836, v: 12.145190238952637, mean_q: 6.855764389038086, std_q: 3.836221218109131

steps: 1679970, episodes: 56000, mean episode reward: 3.084583463722985, agent episode reward: [-15.214624455077264, 9.149603959400126, 9.149603959400126], time: 43.651
steps: 1679970, episodes: 56000, mean episode variance: 0.9305766714699566, agent episode variance: [0.6126824569106102, 0.10446192995086312, 0.21343228460848332], time: 43.651
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5090787720439621, variance: 2.0422747135162354, cvar: -0.046687833964824677, v: -0.5215930938720703, mean_q: -5.965571880340576, std_q: 5.6976118087768555
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3114061206963575, variance: 0.34820643316954375, cvar: 14.58888053894043, v: 11.985726356506348, mean_q: 6.789170742034912, std_q: 3.830207586288452
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3118412011818538, variance: 0.711440920829773, cvar: 14.691184997558594, v: 12.126007080078125, mean_q: 6.857820510864258, std_q: 3.8374950885772705

steps: 1709970, episodes: 57000, mean episode reward: 3.141103893959712, agent episode reward: [-14.928008756460411, 9.034556325210062, 9.034556325210062], time: 43.194
steps: 1709970, episodes: 57000, mean episode variance: 0.9539390977323056, agent episode variance: [0.6358352673649788, 0.10526728451251983, 0.2128365458548069], time: 43.194
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.50866345859434, variance: 2.119450807571411, cvar: -0.049591559916734695, v: -0.5230327248573303, mean_q: -5.981228828430176, std_q: 5.748945236206055
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31137325229790813, variance: 0.3508909483750661, cvar: 14.591588973999023, v: 11.997074127197266, mean_q: 6.784938812255859, std_q: 3.824997663497925
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3112858576365717, variance: 0.709455132484436, cvar: 14.638206481933594, v: 12.123235702514648, mean_q: 6.8458943367004395, std_q: 3.82100510597229

steps: 1739970, episodes: 58000, mean episode reward: 3.743548310531258, agent episode reward: [-15.066797973492976, 9.405173142012117, 9.405173142012117], time: 44.406
steps: 1739970, episodes: 58000, mean episode variance: 0.9392124444209039, agent episode variance: [0.612788019657135, 0.10938956747576595, 0.21703485728800298], time: 44.406
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5080937356003377, variance: 2.0426266193389893, cvar: -0.03715001046657562, v: -0.5130367279052734, mean_q: -5.961266994476318, std_q: 5.724650859832764
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30973025646836694, variance: 0.3646318915858865, cvar: 14.542743682861328, v: 11.993412971496582, mean_q: 6.772681713104248, std_q: 3.8152127265930176
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31018532213009525, variance: 0.7234495282173157, cvar: 14.649560928344727, v: 12.10624885559082, mean_q: 6.83955717086792, std_q: 3.8291373252868652

steps: 1769970, episodes: 59000, mean episode reward: 3.3055440382575263, agent episode reward: [-15.043783333921656, 9.174663686089591, 9.174663686089591], time: 43.78
steps: 1769970, episodes: 59000, mean episode variance: 0.9520563185028732, agent episode variance: [0.6247081679105758, 0.11071424723789096, 0.21663390335440635], time: 43.78
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5095337302362016, variance: 2.0823607444763184, cvar: -0.04598412662744522, v: -0.5168983936309814, mean_q: -5.970733165740967, std_q: 5.727352619171143
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31062744287754335, variance: 0.3690474907929699, cvar: 14.570701599121094, v: 11.977020263671875, mean_q: 6.774619102478027, std_q: 3.8199462890625
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31054405203180474, variance: 0.7221130111813545, cvar: 14.628434181213379, v: 12.114729881286621, mean_q: 6.838141918182373, std_q: 3.8247945308685303

steps: 1799970, episodes: 60000, mean episode reward: 3.5732851286398777, agent episode reward: [-15.524484961528255, 9.548885045084065, 9.548885045084065], time: 43.755
steps: 1799970, episodes: 60000, mean episode variance: 0.956633565871045, agent episode variance: [0.6309228351712227, 0.11214980907551944, 0.21356092162430287], time: 43.755
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5097637630499658, variance: 2.103076219558716, cvar: -0.040934886783361435, v: -0.5182831883430481, mean_q: -5.967594623565674, std_q: 5.731656074523926
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31052129348860497, variance: 0.37383269691839816, cvar: 14.526028633117676, v: 11.981341361999512, mean_q: 6.771019458770752, std_q: 3.815113067626953
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31110794606772535, variance: 0.7118697762489319, cvar: 14.677946090698242, v: 12.120399475097656, mean_q: 6.846992015838623, std_q: 3.837533473968506

steps: 1829970, episodes: 61000, mean episode reward: 3.5794841190168287, agent episode reward: [-15.592636227311935, 9.586060173164382, 9.586060173164382], time: 43.236
steps: 1829970, episodes: 61000, mean episode variance: 0.9768932417780161, agent episode variance: [0.6484125002026558, 0.11185705560445786, 0.21662368597090245], time: 43.236
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5119413287735343, variance: 2.161374807357788, cvar: -0.04574299231171608, v: -0.527727484703064, mean_q: -5.98600435256958, std_q: 5.735557556152344
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3118091360965476, variance: 0.3728568520148595, cvar: 14.597197532653809, v: 11.982535362243652, mean_q: 6.788455486297607, std_q: 3.8271589279174805
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3112649804145135, variance: 0.7220789790153503, cvar: 14.671396255493164, v: 12.128966331481934, mean_q: 6.852597713470459, std_q: 3.837271213531494

steps: 1859970, episodes: 62000, mean episode reward: 3.2020693475627384, agent episode reward: [-14.916225016204237, 9.059147181883489, 9.059147181883489], time: 44.514
steps: 1859970, episodes: 62000, mean episode variance: 0.9525590972453356, agent episode variance: [0.6271595503389835, 0.11001924163103104, 0.215380305275321], time: 44.514
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5096236754597002, variance: 2.090531826019287, cvar: -0.04272919148206711, v: -0.5242705345153809, mean_q: -5.9716057777404785, std_q: 5.718465328216553
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3120585925144655, variance: 0.3667308054367701, cvar: 14.621989250183105, v: 12.006657600402832, mean_q: 6.788511753082275, std_q: 3.837071180343628
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31104856776297507, variance: 0.7179343700408936, cvar: 14.688994407653809, v: 12.129538536071777, mean_q: 6.8470458984375, std_q: 3.838329315185547

steps: 1889970, episodes: 63000, mean episode reward: 3.3304139175375123, agent episode reward: [-15.541025024067523, 9.435719470802518, 9.435719470802518], time: 43.781
steps: 1889970, episodes: 63000, mean episode variance: 0.9578885466121138, agent episode variance: [0.6352308105826378, 0.1117981973476708, 0.21085953868180513], time: 43.781
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5076619363329277, variance: 2.117436170578003, cvar: -0.03599623963236809, v: -0.5151217579841614, mean_q: -5.937293529510498, std_q: 5.699103832244873
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31054481135449463, variance: 0.3726606578255693, cvar: 14.548431396484375, v: 11.996407508850098, mean_q: 6.772055625915527, std_q: 3.8171346187591553
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3109349472802932, variance: 0.7028650641441345, cvar: 14.634368896484375, v: 12.121830940246582, mean_q: 6.836612224578857, std_q: 3.828706979751587

steps: 1919970, episodes: 64000, mean episode reward: 3.434649789022982, agent episode reward: [-15.333550348038006, 9.384100068530493, 9.384100068530493], time: 43.603
steps: 1919970, episodes: 64000, mean episode variance: 0.9941563184633851, agent episode variance: [0.6663082241415977, 0.10916319290548562, 0.21868490141630173], time: 43.603
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5127852038905777, variance: 2.221027374267578, cvar: -0.046381641179323196, v: -0.5241708755493164, mean_q: -5.996017932891846, std_q: 5.740560531616211
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31127070583832933, variance: 0.3638773096849521, cvar: 14.563896179199219, v: 11.986321449279785, mean_q: 6.7714691162109375, std_q: 3.819869041442871
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3124077786874175, variance: 0.7289496660232544, cvar: 14.681921005249023, v: 12.120553016662598, mean_q: 6.854590892791748, std_q: 3.8397111892700195

steps: 1949970, episodes: 65000, mean episode reward: 3.123396856228498, agent episode reward: [-15.439606416437288, 9.281501636332894, 9.281501636332894], time: 43.714
steps: 1949970, episodes: 65000, mean episode variance: 0.9627955447509885, agent episode variance: [0.6408629900813103, 0.10348978570848703, 0.21844276896119116], time: 43.714
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.509784462028933, variance: 2.1362099647521973, cvar: -0.041449014097452164, v: -0.526608943939209, mean_q: -5.973480224609375, std_q: 5.722477912902832
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3117420352400048, variance: 0.3449659523616235, cvar: 14.574326515197754, v: 11.997588157653809, mean_q: 6.787754535675049, std_q: 3.8220889568328857
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31237508494423283, variance: 0.7281425595283508, cvar: 14.641863822937012, v: 12.135367393493652, mean_q: 6.843988418579102, std_q: 3.828092336654663

steps: 1979970, episodes: 66000, mean episode reward: 3.2598504884358035, agent episode reward: [-15.384611090047114, 9.322230789241459, 9.322230789241459], time: 43.851
steps: 1979970, episodes: 66000, mean episode variance: 1.004691274145618, agent episode variance: [0.6630685427784919, 0.12152473888732493, 0.2200979924798012], time: 43.852
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5111805669294366, variance: 2.210228204727173, cvar: -0.04377637058496475, v: -0.5246698260307312, mean_q: -5.985751152038574, std_q: 5.753959655761719
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31148214832258514, variance: 0.40508246295774974, cvar: 14.577613830566406, v: 12.00842571258545, mean_q: 6.791799545288086, std_q: 3.8246467113494873
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31196192235924164, variance: 0.7336599826812744, cvar: 14.657615661621094, v: 12.12733268737793, mean_q: 6.850403785705566, std_q: 3.8328540325164795

steps: 2009970, episodes: 67000, mean episode reward: 3.389704279724571, agent episode reward: [-15.174186540373357, 9.281945410048964, 9.281945410048964], time: 44.547
steps: 2009970, episodes: 67000, mean episode variance: 0.9891960363090039, agent episode variance: [0.6576655212044716, 0.11234868490695954, 0.2191818301975727], time: 44.547
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5121131147460245, variance: 2.192218542098999, cvar: -0.038387153297662735, v: -0.5186924338340759, mean_q: -5.988232612609863, std_q: 5.744668006896973
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3127146170958599, variance: 0.3744956163565318, cvar: 14.558489799499512, v: 12.009406089782715, mean_q: 6.789748668670654, std_q: 3.822080373764038
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31241522332532007, variance: 0.7306060791015625, cvar: 14.610963821411133, v: 12.110652923583984, mean_q: 6.848087787628174, std_q: 3.8191702365875244

steps: 2039970, episodes: 68000, mean episode reward: 3.002342023841445, agent episode reward: [-14.840374071837413, 8.921358047839428, 8.921358047839428], time: 43.644
steps: 2039970, episodes: 68000, mean episode variance: 0.9845410289615393, agent episode variance: [0.6540779646635055, 0.11663380281627178, 0.21382926148176193], time: 43.645
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5100380443128095, variance: 2.180259943008423, cvar: -0.039190974086523056, v: -0.5151482224464417, mean_q: -5.967694282531738, std_q: 5.7126641273498535
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3119888452720848, variance: 0.3887793427209059, cvar: 14.574112892150879, v: 12.015463829040527, mean_q: 6.789029598236084, std_q: 3.8239364624023438
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31137061555261586, variance: 0.7127642035484314, cvar: 14.653619766235352, v: 12.119874000549316, mean_q: 6.847947597503662, std_q: 3.833927869796753

steps: 2069970, episodes: 69000, mean episode reward: 3.452612210257594, agent episode reward: [-15.1015402104222, 9.277076210339896, 9.277076210339896], time: 44.821
steps: 2069970, episodes: 69000, mean episode variance: 0.9856463395394385, agent episode variance: [0.6551817318499088, 0.11469950624927878, 0.21576510144025088], time: 44.822
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.510730791657982, variance: 2.183939218521118, cvar: -0.03653056174516678, v: -0.5175155997276306, mean_q: -5.983246326446533, std_q: 5.732717514038086
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3124202592286837, variance: 0.38233168749759594, cvar: 14.568092346191406, v: 12.013384819030762, mean_q: 6.799384593963623, std_q: 3.821697235107422
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.31178036744233634, variance: 0.7192170023918152, cvar: 14.658207893371582, v: 12.128742218017578, mean_q: 6.850490093231201, std_q: 3.8346590995788574

steps: 2099970, episodes: 70000, mean episode reward: 3.336081002146757, agent episode reward: [-15.257741625747531, 9.296911313947145, 9.296911313947145], time: 44.047
steps: 2099970, episodes: 70000, mean episode variance: 0.9819712078943849, agent episode variance: [0.6507041530311107, 0.10542116621881724, 0.22584588864445687], time: 44.048
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5087048913545722, variance: 2.1690139770507812, cvar: -0.03936797007918358, v: -0.5211545825004578, mean_q: -5.9749860763549805, std_q: 5.730368614196777
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3110460207880062, variance: 0.35140388739605743, cvar: 14.58609676361084, v: 12.006660461425781, mean_q: 6.77955436706543, std_q: 3.8275532722473145
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.311130456362932, variance: 0.7528195977210999, cvar: 14.664886474609375, v: 12.120704650878906, mean_q: 6.844398021697998, std_q: 3.83793044090271

...Finished total of 70001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 29970, episodes: 1000, mean episode reward: 3.364397987758849, agent episode reward: [-14.813413902862882, 9.088905945310865, 9.088905945310865], time: 32.101
steps: 29970, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 32.101
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 59970, episodes: 2000, mean episode reward: 3.245274903984162, agent episode reward: [-15.582430531403196, 9.413852717693679, 9.413852717693679], time: 41.118
steps: 59970, episodes: 2000, mean episode variance: 0.9813577495031058, agent episode variance: [0.646948086798191, 0.11223895878717303, 0.22217070391774177], time: 41.119
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.5049272573079173, variance: 2.215575695037842, cvar: -0.03348567336797714, v: -0.5254373550415039, mean_q: -6.027682304382324, std_q: 5.634096145629883
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.3091605123861144, variance: 0.38437999584648297, cvar: 14.4725980758667, v: 12.010832786560059, mean_q: 6.857010364532471, std_q: 3.774768590927124
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.30847438562555557, variance: 0.7608585357666016, cvar: 14.567620277404785, v: 12.13167953491211, mean_q: 6.894970893859863, std_q: 3.8023619651794434

...Finished total of 2001 episodes with the fixed policy.
