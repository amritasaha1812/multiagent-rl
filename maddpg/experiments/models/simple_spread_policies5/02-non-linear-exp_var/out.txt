# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 23.72 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies5/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies5/02-non-linear-exp_var/
Job <1090574> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc273>>
arglist.u_estimation True
2019-09-06 04:44:44.687284: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -515.8330002920353, agent episode reward: [-171.94433343067843, -171.94433343067843, -171.94433343067843], time: 45.23
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 45.231
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -780.6947596603628, agent episode reward: [-260.2315865534543, -260.2315865534543, -260.2315865534543], time: 79.815
steps: 49975, episodes: 2000, mean episode variance: 8.658522201299668, agent episode variance: [2.861981756687164, 2.939403468608856, 2.857136976003647], time: 79.816
Running avgs for agent 0: q_loss: 90.77875518798828, p_loss: -5.4553751945495605, mean_rew: -7.931461398422773, variance: 11.729433429045756, lamda: 1.0108745098114014
Running avgs for agent 1: q_loss: 127.62519073486328, p_loss: -5.515380382537842, mean_rew: -7.936483841369897, variance: 12.046735527085476, lamda: 1.0111236572265625
Running avgs for agent 2: q_loss: 115.14420318603516, p_loss: -5.684569358825684, mean_rew: -7.93922736778379, variance: 11.70957777050675, lamda: 1.0112388134002686

steps: 74975, episodes: 3000, mean episode reward: -800.1292571135084, agent episode reward: [-266.7097523711695, -266.7097523711695, -266.7097523711695], time: 79.808
steps: 74975, episodes: 3000, mean episode variance: 6.460723474740982, agent episode variance: [2.1244001071453096, 2.1751662888526915, 2.161157078742981], time: 79.808
Running avgs for agent 0: q_loss: 52.92474365234375, p_loss: -4.817639350891113, mean_rew: -8.932068258998013, variance: 8.497600555419922, lamda: 1.0343601703643799
Running avgs for agent 1: q_loss: 39.663612365722656, p_loss: -4.740474224090576, mean_rew: -8.942363285799415, variance: 8.700665473937988, lamda: 1.0357898473739624
Running avgs for agent 2: q_loss: 45.61019515991211, p_loss: -4.828902244567871, mean_rew: -8.94158592068667, variance: 8.644628524780273, lamda: 1.0351182222366333

steps: 99975, episodes: 4000, mean episode reward: -833.6664914446891, agent episode reward: [-277.888830481563, -277.888830481563, -277.888830481563], time: 80.438
steps: 99975, episodes: 4000, mean episode variance: 6.6992179782390595, agent episode variance: [2.2232832159996034, 2.2418961763381957, 2.2340385859012604], time: 80.439
Running avgs for agent 0: q_loss: 70.3628158569336, p_loss: -4.837929725646973, mean_rew: -9.558477585764638, variance: 8.893133163452148, lamda: 1.0592458248138428
Running avgs for agent 1: q_loss: 53.81662368774414, p_loss: -4.772218704223633, mean_rew: -9.554454572649323, variance: 8.967583656311035, lamda: 1.0607941150665283
Running avgs for agent 2: q_loss: 66.11754608154297, p_loss: -4.819192409515381, mean_rew: -9.547287127435185, variance: 8.93615436553955, lamda: 1.058464765548706

steps: 124975, episodes: 5000, mean episode reward: -853.3846470997543, agent episode reward: [-284.4615490332514, -284.4615490332514, -284.4615490332514], time: 76.653
steps: 124975, episodes: 5000, mean episode variance: 6.9225536941289905, agent episode variance: [2.2952073882818222, 2.3693444766998293, 2.258001829147339], time: 76.653
Running avgs for agent 0: q_loss: 69.60933685302734, p_loss: -5.024730205535889, mean_rew: -9.896020219542343, variance: 9.180830001831055, lamda: 1.084255337715149
Running avgs for agent 1: q_loss: 75.60828399658203, p_loss: -4.975859642028809, mean_rew: -9.89907218377943, variance: 9.477377891540527, lamda: 1.0857982635498047
Running avgs for agent 2: q_loss: 59.148353576660156, p_loss: -5.044616222381592, mean_rew: -9.883596147739222, variance: 9.032008171081543, lamda: 1.0826506614685059

steps: 149975, episodes: 6000, mean episode reward: -884.7326920330931, agent episode reward: [-294.91089734436446, -294.91089734436446, -294.91089734436446], time: 75.876
steps: 149975, episodes: 6000, mean episode variance: 6.979090581893921, agent episode variance: [2.311484236240387, 2.363871831893921, 2.303734513759613], time: 75.877
Running avgs for agent 0: q_loss: 51.325294494628906, p_loss: -5.209383010864258, mean_rew: -10.285578524213193, variance: 9.24593734741211, lamda: 1.1092594861984253
Running avgs for agent 1: q_loss: 57.35905456542969, p_loss: -5.21668815612793, mean_rew: -10.270759995628524, variance: 9.455486297607422, lamda: 1.1108022928237915
Running avgs for agent 2: q_loss: 61.61037826538086, p_loss: -5.1528754234313965, mean_rew: -10.267842113556064, variance: 9.214938163757324, lamda: 1.107680082321167

steps: 174975, episodes: 7000, mean episode reward: -804.6540484413448, agent episode reward: [-268.2180161471149, -268.2180161471149, -268.2180161471149], time: 76.706
steps: 174975, episodes: 7000, mean episode variance: 6.91452184844017, agent episode variance: [2.3181691465377807, 2.303344133615494, 2.2930085682868957], time: 76.707
Running avgs for agent 0: q_loss: 51.535945892333984, p_loss: -5.265389442443848, mean_rew: -10.430004257619963, variance: 9.272676467895508, lamda: 1.1342636346817017
Running avgs for agent 1: q_loss: 64.31044006347656, p_loss: -5.337202072143555, mean_rew: -10.42779551028354, variance: 9.213376998901367, lamda: 1.134710431098938
Running avgs for agent 2: q_loss: 51.9597282409668, p_loss: -5.200668811798096, mean_rew: -10.42868000511633, variance: 9.17203426361084, lamda: 1.1326838731765747

steps: 199975, episodes: 8000, mean episode reward: -763.9407130777697, agent episode reward: [-254.64690435925655, -254.64690435925655, -254.64690435925655], time: 75.96
steps: 199975, episodes: 8000, mean episode variance: 6.825408373594284, agent episode variance: [2.2582016816139223, 2.3016822869777678, 2.2655244050025938], time: 75.961
Running avgs for agent 0: q_loss: 46.33666229248047, p_loss: -5.266757011413574, mean_rew: -10.417530304094758, variance: 9.032806396484375, lamda: 1.1591789722442627
Running avgs for agent 1: q_loss: 58.02732849121094, p_loss: -5.321491241455078, mean_rew: -10.426020756752328, variance: 9.206729888916016, lamda: 1.1568399667739868
Running avgs for agent 2: q_loss: 49.50728225708008, p_loss: -5.259230136871338, mean_rew: -10.423971772811441, variance: 9.062097549438477, lamda: 1.1574954986572266

steps: 224975, episodes: 9000, mean episode reward: -755.6269895952665, agent episode reward: [-251.87566319842225, -251.87566319842225, -251.87566319842225], time: 77.221
steps: 224975, episodes: 9000, mean episode variance: 6.603542710542679, agent episode variance: [2.2117270278930663, 2.204391756296158, 2.1874239263534547], time: 77.221
Running avgs for agent 0: q_loss: 42.94123077392578, p_loss: -5.249595642089844, mean_rew: -10.378924152199513, variance: 8.846908569335938, lamda: 1.1837117671966553
Running avgs for agent 1: q_loss: 45.30506134033203, p_loss: -5.284512996673584, mean_rew: -10.375193710313567, variance: 8.817567825317383, lamda: 1.1765811443328857
Running avgs for agent 2: q_loss: 43.838531494140625, p_loss: -5.265580654144287, mean_rew: -10.377899055966807, variance: 8.749695777893066, lamda: 1.1818783283233643

steps: 249975, episodes: 10000, mean episode reward: -805.375345226179, agent episode reward: [-268.4584484087263, -268.4584484087263, -268.4584484087263], time: 79.028
steps: 249975, episodes: 10000, mean episode variance: 6.484329893112182, agent episode variance: [2.159617136478424, 2.167676691532135, 2.1570360651016234], time: 79.028
Running avgs for agent 0: q_loss: 41.06528091430664, p_loss: -5.243496894836426, mean_rew: -10.3886836136065, variance: 8.638468742370605, lamda: 1.2066570520401
Running avgs for agent 1: q_loss: 40.181053161621094, p_loss: -5.27540922164917, mean_rew: -10.384662598196112, variance: 8.670706748962402, lamda: 1.2003166675567627
Running avgs for agent 2: q_loss: 38.645931243896484, p_loss: -5.266746997833252, mean_rew: -10.383779644816837, variance: 8.628144264221191, lamda: 1.2039703130722046

steps: 274975, episodes: 11000, mean episode reward: -815.0198664988208, agent episode reward: [-271.6732888329403, -271.6732888329403, -271.6732888329403], time: 79.378
steps: 274975, episodes: 11000, mean episode variance: 6.390815135717392, agent episode variance: [2.1278929727077482, 2.139465397834778, 2.1234567651748657], time: 79.379
Running avgs for agent 0: q_loss: 49.643924713134766, p_loss: -5.264355182647705, mean_rew: -10.439131653060972, variance: 8.511571884155273, lamda: 1.228871464729309
Running avgs for agent 1: q_loss: 41.61213302612305, p_loss: -5.2759552001953125, mean_rew: -10.43280411062505, variance: 8.557861328125, lamda: 1.2231571674346924
Running avgs for agent 2: q_loss: 37.29703903198242, p_loss: -5.287557125091553, mean_rew: -10.415410547869325, variance: 8.493825912475586, lamda: 1.2218509912490845

steps: 299975, episodes: 12000, mean episode reward: -772.7842776154196, agent episode reward: [-257.59475920513984, -257.59475920513984, -257.59475920513984], time: 80.288
steps: 299975, episodes: 12000, mean episode variance: 6.286441074490547, agent episode variance: [2.0854003130197527, 2.089902671813965, 2.11113808965683], time: 80.288
Running avgs for agent 0: q_loss: 41.097740173339844, p_loss: -5.281319618225098, mean_rew: -10.45304594423403, variance: 8.341601371765137, lamda: 1.2499381303787231
Running avgs for agent 1: q_loss: 37.57206344604492, p_loss: -5.2665886878967285, mean_rew: -10.433411164252163, variance: 8.359611511230469, lamda: 1.2444721460342407
Running avgs for agent 2: q_loss: 38.28337097167969, p_loss: -5.281539440155029, mean_rew: -10.438752905921712, variance: 8.444552421569824, lamda: 1.2409584522247314

steps: 324975, episodes: 13000, mean episode reward: -767.1157989551366, agent episode reward: [-255.70526631837885, -255.70526631837885, -255.70526631837885], time: 82.208
steps: 324975, episodes: 13000, mean episode variance: 6.224072531700134, agent episode variance: [2.0422928667068483, 2.077425028800964, 2.1043546361923218], time: 82.208
Running avgs for agent 0: q_loss: 37.01493453979492, p_loss: -5.272253513336182, mean_rew: -10.423366931907246, variance: 8.169171333312988, lamda: 1.271673321723938
Running avgs for agent 1: q_loss: 35.481021881103516, p_loss: -5.256591320037842, mean_rew: -10.415581212532965, variance: 8.309700965881348, lamda: 1.2645299434661865
Running avgs for agent 2: q_loss: 52.99766159057617, p_loss: -5.285139560699463, mean_rew: -10.425903362091852, variance: 8.417417526245117, lamda: 1.2528921365737915

steps: 349975, episodes: 14000, mean episode reward: -762.6678111605204, agent episode reward: [-254.22260372017348, -254.22260372017348, -254.22260372017348], time: 86.08
steps: 349975, episodes: 14000, mean episode variance: 6.134973641872406, agent episode variance: [2.0190941824913025, 2.0509818410873413, 2.064897618293762], time: 86.081
Running avgs for agent 0: q_loss: 49.55017852783203, p_loss: -5.261049747467041, mean_rew: -10.396193100851997, variance: 8.076376914978027, lamda: 1.2918063402175903
Running avgs for agent 1: q_loss: 32.55847930908203, p_loss: -5.2730207443237305, mean_rew: -10.413531913376472, variance: 8.203927993774414, lamda: 1.2832520008087158
Running avgs for agent 2: q_loss: 36.353904724121094, p_loss: -5.274263858795166, mean_rew: -10.432545508341395, variance: 8.259590148925781, lamda: 1.269372582435608

steps: 374975, episodes: 15000, mean episode reward: -770.6226608048777, agent episode reward: [-256.8742202682925, -256.8742202682925, -256.8742202682925], time: 86.7
steps: 374975, episodes: 15000, mean episode variance: 6.0317631502151485, agent episode variance: [2.000006364583969, 2.0075741419792177, 2.024182643651962], time: 86.701
Running avgs for agent 0: q_loss: 59.84368133544922, p_loss: -5.249189376831055, mean_rew: -10.401836154900924, variance: 8.000025749206543, lamda: 1.308406949043274
Running avgs for agent 1: q_loss: 31.842700958251953, p_loss: -5.265561580657959, mean_rew: -10.398658354076732, variance: 8.03029727935791, lamda: 1.2998870611190796
Running avgs for agent 2: q_loss: 35.30475997924805, p_loss: -5.252966403961182, mean_rew: -10.397945613494665, variance: 8.096731185913086, lamda: 1.2865345478057861

steps: 399975, episodes: 16000, mean episode reward: -789.2688618835994, agent episode reward: [-263.08962062786645, -263.08962062786645, -263.08962062786645], time: 86.363
steps: 399975, episodes: 16000, mean episode variance: 5.931438455581665, agent episode variance: [1.9661645452976226, 1.986100368976593, 1.9791735413074494], time: 86.363
Running avgs for agent 0: q_loss: 45.631500244140625, p_loss: -5.248405456542969, mean_rew: -10.384975436229583, variance: 7.864658355712891, lamda: 1.3247212171554565
Running avgs for agent 1: q_loss: 30.30272674560547, p_loss: -5.263830184936523, mean_rew: -10.395957020479099, variance: 7.944400787353516, lamda: 1.3160499334335327
Running avgs for agent 2: q_loss: 45.09591293334961, p_loss: -5.245582103729248, mean_rew: -10.398057649406303, variance: 7.916694641113281, lamda: 1.305395483970642

steps: 424975, episodes: 17000, mean episode reward: -829.1306172747193, agent episode reward: [-276.3768724249064, -276.3768724249064, -276.3768724249064], time: 80.244
steps: 424975, episodes: 17000, mean episode variance: 5.930297131776809, agent episode variance: [1.9572331099510192, 1.971271146774292, 2.0017928750514984], time: 80.244
Running avgs for agent 0: q_loss: 51.62861251831055, p_loss: -5.267974376678467, mean_rew: -10.434740515326508, variance: 7.828932762145996, lamda: 1.3462988138198853
Running avgs for agent 1: q_loss: 29.583995819091797, p_loss: -5.264869213104248, mean_rew: -10.410294675918777, variance: 7.88508415222168, lamda: 1.3275810480117798
Running avgs for agent 2: q_loss: 54.726837158203125, p_loss: -5.271852493286133, mean_rew: -10.442394512198184, variance: 8.007171630859375, lamda: 1.3149888515472412

steps: 449975, episodes: 18000, mean episode reward: -854.5550827348077, agent episode reward: [-284.85169424493586, -284.85169424493586, -284.85169424493586], time: 89.668
steps: 449975, episodes: 18000, mean episode variance: 5.880911497354507, agent episode variance: [1.9338463838100433, 1.9617115592956542, 1.9853535542488099], time: 89.668
Running avgs for agent 0: q_loss: 46.32587432861328, p_loss: -5.288936614990234, mean_rew: -10.479532459027286, variance: 7.735385894775391, lamda: 1.3652608394622803
Running avgs for agent 1: q_loss: 47.580711364746094, p_loss: -5.285754680633545, mean_rew: -10.471322456409915, variance: 7.846846580505371, lamda: 1.3355566263198853
Running avgs for agent 2: q_loss: 55.433502197265625, p_loss: -5.288101673126221, mean_rew: -10.474656736256685, variance: 7.941413879394531, lamda: 1.3222849369049072

steps: 474975, episodes: 19000, mean episode reward: -849.1692946080515, agent episode reward: [-283.0564315360171, -283.0564315360171, -283.0564315360171], time: 118.828
steps: 474975, episodes: 19000, mean episode variance: 5.877322381973267, agent episode variance: [1.9162993102073669, 1.9930476710796357, 1.967975400686264], time: 118.828
Running avgs for agent 0: q_loss: 40.990108489990234, p_loss: -5.311588764190674, mean_rew: -10.52621033234335, variance: 7.665197372436523, lamda: 1.3892558813095093
Running avgs for agent 1: q_loss: 49.179405212402344, p_loss: -5.291720867156982, mean_rew: -10.512430881128163, variance: 7.9721903800964355, lamda: 1.3386286497116089
Running avgs for agent 2: q_loss: 54.04641342163086, p_loss: -5.312154293060303, mean_rew: -10.5210268203182, variance: 7.871901988983154, lamda: 1.3321055173873901

steps: 499975, episodes: 20000, mean episode reward: -839.8213653762425, agent episode reward: [-279.9404551254142, -279.9404551254142, -279.9404551254142], time: 109.898
steps: 499975, episodes: 20000, mean episode variance: 5.845868568658829, agent episode variance: [1.863458186864853, 1.996535942554474, 1.985874439239502], time: 109.898
Running avgs for agent 0: q_loss: 58.84135818481445, p_loss: -5.33621072769165, mean_rew: -10.550017248634452, variance: 7.453833103179932, lamda: 1.412231206893921
Running avgs for agent 1: q_loss: 48.305423736572266, p_loss: -5.309787273406982, mean_rew: -10.542903342078711, variance: 7.986144065856934, lamda: 1.3432987928390503
Running avgs for agent 2: q_loss: 39.76064682006836, p_loss: -5.318540573120117, mean_rew: -10.550104287127361, variance: 7.943498134613037, lamda: 1.346904993057251

steps: 524975, episodes: 21000, mean episode reward: -837.6636074106873, agent episode reward: [-279.22120247022906, -279.22120247022906, -279.22120247022906], time: 111.877
steps: 524975, episodes: 21000, mean episode variance: 5.781175405025482, agent episode variance: [1.8707230079174042, 1.956005028963089, 1.954447368144989], time: 111.877
Running avgs for agent 0: q_loss: 60.55821228027344, p_loss: -5.350404739379883, mean_rew: -10.586901840947746, variance: 7.48289155960083, lamda: 1.4271509647369385
Running avgs for agent 1: q_loss: 39.71043395996094, p_loss: -5.343322277069092, mean_rew: -10.586502182838391, variance: 7.8240203857421875, lamda: 1.3480024337768555
Running avgs for agent 2: q_loss: 46.97260665893555, p_loss: -5.328065872192383, mean_rew: -10.585448079392542, variance: 7.817789077758789, lamda: 1.367568850517273

steps: 549975, episodes: 22000, mean episode reward: -850.0084655838639, agent episode reward: [-283.3361551946213, -283.3361551946213, -283.3361551946213], time: 110.158
steps: 549975, episodes: 22000, mean episode variance: 5.7400577490329745, agent episode variance: [1.8631988008022309, 1.9473304109573364, 1.929528537273407], time: 110.159
Running avgs for agent 0: q_loss: 59.06190490722656, p_loss: -5.369542121887207, mean_rew: -10.618428247255226, variance: 7.452795028686523, lamda: 1.4419182538986206
Running avgs for agent 1: q_loss: 30.76961326599121, p_loss: -5.342795372009277, mean_rew: -10.620338581939112, variance: 7.789321422576904, lamda: 1.365403175354004
Running avgs for agent 2: q_loss: 58.88969421386719, p_loss: -5.369320869445801, mean_rew: -10.629734052458486, variance: 7.718114376068115, lamda: 1.383435845375061

steps: 574975, episodes: 23000, mean episode reward: -870.8420109857954, agent episode reward: [-290.2806703285985, -290.2806703285985, -290.2806703285985], time: 112.341
steps: 574975, episodes: 23000, mean episode variance: 5.676361368894577, agent episode variance: [1.8326021256446838, 1.9303445682525635, 1.9134146749973298], time: 112.341
Running avgs for agent 0: q_loss: 45.79261779785156, p_loss: -5.390740871429443, mean_rew: -10.661906282863344, variance: 7.330408096313477, lamda: 1.4591577053070068
Running avgs for agent 1: q_loss: 36.018516540527344, p_loss: -5.386698246002197, mean_rew: -10.659411069187424, variance: 7.721377849578857, lamda: 1.38588285446167
Running avgs for agent 2: q_loss: 60.843936920166016, p_loss: -5.372307777404785, mean_rew: -10.656378123408908, variance: 7.653658866882324, lamda: 1.399879813194275

steps: 599975, episodes: 24000, mean episode reward: -854.2932525042887, agent episode reward: [-284.76441750142953, -284.76441750142953, -284.76441750142953], time: 161.674
steps: 599975, episodes: 24000, mean episode variance: 5.658123664140701, agent episode variance: [1.8177064619064331, 1.927937693834305, 1.9124795083999633], time: 161.674
Running avgs for agent 0: q_loss: 40.055686950683594, p_loss: -5.402353286743164, mean_rew: -10.707158409842796, variance: 7.27082633972168, lamda: 1.482387661933899
Running avgs for agent 1: q_loss: 44.88267135620117, p_loss: -5.406637668609619, mean_rew: -10.71473394364889, variance: 7.7117509841918945, lamda: 1.3946233987808228
Running avgs for agent 2: q_loss: 59.62339782714844, p_loss: -5.385293006896973, mean_rew: -10.684773299155593, variance: 7.649918079376221, lamda: 1.4179778099060059

steps: 624975, episodes: 25000, mean episode reward: -850.4182356569631, agent episode reward: [-283.4727452189877, -283.4727452189877, -283.4727452189877], time: 163.503
steps: 624975, episodes: 25000, mean episode variance: 5.584554126262665, agent episode variance: [1.796123665332794, 1.9183970630168914, 1.870033397912979], time: 163.503
Running avgs for agent 0: q_loss: 39.636966705322266, p_loss: -5.410334587097168, mean_rew: -10.717476335061448, variance: 7.184494972229004, lamda: 1.5059895515441895
Running avgs for agent 1: q_loss: 47.9696159362793, p_loss: -5.408868312835693, mean_rew: -10.718530612293888, variance: 7.673588275909424, lamda: 1.4009406566619873
Running avgs for agent 2: q_loss: 60.552337646484375, p_loss: -5.39765739440918, mean_rew: -10.7105030248392, variance: 7.480134010314941, lamda: 1.4352805614471436

steps: 649975, episodes: 26000, mean episode reward: -862.4764114402669, agent episode reward: [-287.49213714675557, -287.49213714675557, -287.49213714675557], time: 158.219
steps: 649975, episodes: 26000, mean episode variance: 5.558049701452255, agent episode variance: [1.7707544355392455, 1.9170508663654326, 1.8702443995475768], time: 158.22
Running avgs for agent 0: q_loss: 37.91510009765625, p_loss: -5.426604747772217, mean_rew: -10.75510287918835, variance: 7.083017349243164, lamda: 1.527862310409546
Running avgs for agent 1: q_loss: 47.93535232543945, p_loss: -5.41272497177124, mean_rew: -10.741995530996602, variance: 7.668203353881836, lamda: 1.4060817956924438
Running avgs for agent 2: q_loss: 60.152740478515625, p_loss: -5.426738262176514, mean_rew: -10.76125984861815, variance: 7.480977535247803, lamda: 1.4551901817321777

steps: 674975, episodes: 27000, mean episode reward: -867.9733702455508, agent episode reward: [-289.32445674851687, -289.32445674851687, -289.32445674851687], time: 166.357
steps: 674975, episodes: 27000, mean episode variance: 5.493180906772613, agent episode variance: [1.7460474977493285, 1.898789749622345, 1.84834365940094], time: 166.357
Running avgs for agent 0: q_loss: 40.29964828491211, p_loss: -5.454524993896484, mean_rew: -10.7921469773625, variance: 6.984189987182617, lamda: 1.5499788522720337
Running avgs for agent 1: q_loss: 47.74580383300781, p_loss: -5.4281792640686035, mean_rew: -10.771694615383142, variance: 7.59515905380249, lamda: 1.41068434715271
Running avgs for agent 2: q_loss: 61.124122619628906, p_loss: -5.436371326446533, mean_rew: -10.778327303255676, variance: 7.393374919891357, lamda: 1.4752166271209717

steps: 699975, episodes: 28000, mean episode reward: -858.3022488780986, agent episode reward: [-286.1007496260329, -286.1007496260329, -286.1007496260329], time: 167.462
steps: 699975, episodes: 28000, mean episode variance: 5.465499274253845, agent episode variance: [1.7365579295158386, 1.9257483806610107, 1.803192964076996], time: 167.463
Running avgs for agent 0: q_loss: 37.58903884887695, p_loss: -5.452913284301758, mean_rew: -10.80289909615313, variance: 6.946231365203857, lamda: 1.5716890096664429
Running avgs for agent 1: q_loss: 37.58841323852539, p_loss: -5.4306254386901855, mean_rew: -10.778895179350311, variance: 7.702994346618652, lamda: 1.4217801094055176
Running avgs for agent 2: q_loss: 61.15690231323242, p_loss: -5.446435451507568, mean_rew: -10.798147843130568, variance: 7.212771892547607, lamda: 1.4947940111160278

steps: 724975, episodes: 29000, mean episode reward: -872.7856653958382, agent episode reward: [-290.9285551319461, -290.9285551319461, -290.9285551319461], time: 167.429
steps: 724975, episodes: 29000, mean episode variance: 5.368309730052948, agent episode variance: [1.70332483959198, 1.8839486389160156, 1.7810362515449525], time: 167.429
Running avgs for agent 0: q_loss: 36.93882369995117, p_loss: -5.458333969116211, mean_rew: -10.822308884233852, variance: 6.81329870223999, lamda: 1.5922931432724
Running avgs for agent 1: q_loss: 34.03200912475586, p_loss: -5.464519023895264, mean_rew: -10.841621028582209, variance: 7.535794734954834, lamda: 1.4410977363586426
Running avgs for agent 2: q_loss: 60.35285949707031, p_loss: -5.461205959320068, mean_rew: -10.830222303390318, variance: 7.124144554138184, lamda: 1.5148797035217285

steps: 749975, episodes: 30000, mean episode reward: -880.6675374541172, agent episode reward: [-293.555845818039, -293.555845818039, -293.555845818039], time: 164.19
steps: 749975, episodes: 30000, mean episode variance: 5.3720192294120785, agent episode variance: [1.7144198408126832, 1.8689562344551087, 1.788643154144287], time: 164.19
Running avgs for agent 0: q_loss: 47.7006721496582, p_loss: -5.482436656951904, mean_rew: -10.874780233510341, variance: 6.85767936706543, lamda: 1.6102886199951172
Running avgs for agent 1: q_loss: 31.809511184692383, p_loss: -5.483091831207275, mean_rew: -10.871901707396905, variance: 7.475825309753418, lamda: 1.4602221250534058
Running avgs for agent 2: q_loss: 59.846378326416016, p_loss: -5.473357677459717, mean_rew: -10.865721613409558, variance: 7.154572486877441, lamda: 1.534090518951416

steps: 774975, episodes: 31000, mean episode reward: -904.273317877837, agent episode reward: [-301.42443929261236, -301.42443929261236, -301.42443929261236], time: 157.384
steps: 774975, episodes: 31000, mean episode variance: 5.287738676548004, agent episode variance: [1.688563826084137, 1.8368101906776428, 1.7623646597862244], time: 157.384
Running avgs for agent 0: q_loss: 57.91323471069336, p_loss: -5.51068115234375, mean_rew: -10.917275020406517, variance: 6.754255771636963, lamda: 1.6199532747268677
Running avgs for agent 1: q_loss: 33.57295608520508, p_loss: -5.499014854431152, mean_rew: -10.895073134322308, variance: 7.347240447998047, lamda: 1.4794230461120605
Running avgs for agent 2: q_loss: 51.28739929199219, p_loss: -5.486359119415283, mean_rew: -10.885364219134829, variance: 7.049458026885986, lamda: 1.55204176902771

steps: 799975, episodes: 32000, mean episode reward: -894.2291241875351, agent episode reward: [-298.0763747291784, -298.0763747291784, -298.0763747291784], time: 155.398
steps: 799975, episodes: 32000, mean episode variance: 5.28046378159523, agent episode variance: [1.707886058330536, 1.843236060857773, 1.7293416624069213], time: 155.398
Running avgs for agent 0: q_loss: 57.697975158691406, p_loss: -5.521414279937744, mean_rew: -10.942430960296978, variance: 6.831543922424316, lamda: 1.6284055709838867
Running avgs for agent 1: q_loss: 49.07334518432617, p_loss: -5.4896345138549805, mean_rew: -10.919716512336374, variance: 7.372944355010986, lamda: 1.4909205436706543
Running avgs for agent 2: q_loss: 42.53951644897461, p_loss: -5.475042819976807, mean_rew: -10.929619527750111, variance: 6.917366027832031, lamda: 1.5745295286178589

steps: 824975, episodes: 33000, mean episode reward: -892.955328111485, agent episode reward: [-297.6517760371617, -297.6517760371617, -297.6517760371617], time: 156.133
steps: 824975, episodes: 33000, mean episode variance: 5.24260077881813, agent episode variance: [1.6777829563617705, 1.8335137543678284, 1.7313040680885314], time: 156.134
Running avgs for agent 0: q_loss: 48.615081787109375, p_loss: -5.5301079750061035, mean_rew: -10.962928840474822, variance: 6.711132049560547, lamda: 1.634867787361145
Running avgs for agent 1: q_loss: 48.38679504394531, p_loss: -5.5277228355407715, mean_rew: -10.968157729672432, variance: 7.334054470062256, lamda: 1.493611454963684
Running avgs for agent 2: q_loss: 37.56875228881836, p_loss: -5.515829563140869, mean_rew: -10.967674718378746, variance: 6.925216197967529, lamda: 1.597115159034729

steps: 849975, episodes: 34000, mean episode reward: -898.4601895839687, agent episode reward: [-299.4867298613229, -299.4867298613229, -299.4867298613229], time: 156.557
steps: 849975, episodes: 34000, mean episode variance: 5.239483488798141, agent episode variance: [1.6750097031593323, 1.8394462807178498, 1.7250275049209596], time: 156.557
Running avgs for agent 0: q_loss: 41.10078048706055, p_loss: -5.5479655265808105, mean_rew: -10.990698377063309, variance: 6.700038433074951, lamda: 1.6499826908111572
Running avgs for agent 1: q_loss: 47.47785949707031, p_loss: -5.534955978393555, mean_rew: -10.988566261656265, variance: 7.357785224914551, lamda: 1.496854305267334
Running avgs for agent 2: q_loss: 54.470115661621094, p_loss: -5.533323287963867, mean_rew: -10.998060114589128, variance: 6.900110244750977, lamda: 1.6150906085968018

steps: 874975, episodes: 35000, mean episode reward: -896.176375918455, agent episode reward: [-298.72545863948505, -298.72545863948505, -298.72545863948505], time: 155.538
steps: 874975, episodes: 35000, mean episode variance: 5.240201386451721, agent episode variance: [1.6646028513908386, 1.8650887784957886, 1.710509756565094], time: 155.539
Running avgs for agent 0: q_loss: 43.11289978027344, p_loss: -5.555310249328613, mean_rew: -11.020126201978368, variance: 6.658411026000977, lamda: 1.6686222553253174
Running avgs for agent 1: q_loss: 48.58079147338867, p_loss: -5.547667503356934, mean_rew: -11.02476449568221, variance: 7.460354804992676, lamda: 1.5009024143218994
Running avgs for agent 2: q_loss: 44.30829620361328, p_loss: -5.540805339813232, mean_rew: -11.003983831732281, variance: 6.842039108276367, lamda: 1.6257504224777222

steps: 899975, episodes: 36000, mean episode reward: -892.1568983863352, agent episode reward: [-297.385632795445, -297.385632795445, -297.385632795445], time: 160.634
steps: 899975, episodes: 36000, mean episode variance: 5.177215526342392, agent episode variance: [1.6423339586257935, 1.8371408035755157, 1.6977407641410827], time: 160.634
Running avgs for agent 0: q_loss: 56.5395393371582, p_loss: -5.580777645111084, mean_rew: -11.059408891230277, variance: 6.5693359375, lamda: 1.6770635843276978
Running avgs for agent 1: q_loss: 42.367427825927734, p_loss: -5.560004234313965, mean_rew: -11.054831045270062, variance: 7.3485636711120605, lamda: 1.5067224502563477
Running avgs for agent 2: q_loss: 56.603370666503906, p_loss: -5.555403709411621, mean_rew: -11.054306659949251, variance: 6.7909626960754395, lamda: 1.6346670389175415

steps: 924975, episodes: 37000, mean episode reward: -860.6348395495951, agent episode reward: [-286.87827984986507, -286.87827984986507, -286.87827984986507], time: 158.437
steps: 924975, episodes: 37000, mean episode variance: 5.178325700998307, agent episode variance: [1.652137472629547, 1.8308660037517548, 1.6953222246170043], time: 158.437
Running avgs for agent 0: q_loss: 55.46327209472656, p_loss: -5.573356628417969, mean_rew: -11.05712338854691, variance: 6.60854959487915, lamda: 1.6830239295959473
Running avgs for agent 1: q_loss: 36.902069091796875, p_loss: -5.573702335357666, mean_rew: -11.071442932300595, variance: 7.323464393615723, lamda: 1.5237443447113037
Running avgs for agent 2: q_loss: 56.042137145996094, p_loss: -5.574470043182373, mean_rew: -11.075811534698051, variance: 6.781289100646973, lamda: 1.6403090953826904

steps: 949975, episodes: 38000, mean episode reward: -863.1827795267116, agent episode reward: [-287.72759317557046, -287.72759317557046, -287.72759317557046], time: 160.401
steps: 949975, episodes: 38000, mean episode variance: 5.175986389636994, agent episode variance: [1.645907309770584, 1.827168699979782, 1.7029103798866272], time: 160.401
Running avgs for agent 0: q_loss: 40.999202728271484, p_loss: -5.58273458480835, mean_rew: -11.066434112387451, variance: 6.583629131317139, lamda: 1.6920918226242065
Running avgs for agent 1: q_loss: 48.79253005981445, p_loss: -5.568480491638184, mean_rew: -11.070333098259725, variance: 7.3086748123168945, lamda: 1.532193660736084
Running avgs for agent 2: q_loss: 55.59613800048828, p_loss: -5.580199241638184, mean_rew: -11.083310187037776, variance: 6.811641693115234, lamda: 1.645532488822937

steps: 974975, episodes: 39000, mean episode reward: -880.611741014233, agent episode reward: [-293.5372470047443, -293.5372470047443, -293.5372470047443], time: 157.843
steps: 974975, episodes: 39000, mean episode variance: 5.135421720981598, agent episode variance: [1.632571991920471, 1.819080118894577, 1.6837696101665496], time: 157.844
Running avgs for agent 0: q_loss: 36.79829025268555, p_loss: -5.605479717254639, mean_rew: -11.10034900246203, variance: 6.530288219451904, lamda: 1.7095292806625366
Running avgs for agent 1: q_loss: 49.35020065307617, p_loss: -5.596709728240967, mean_rew: -11.114240642683392, variance: 7.276320457458496, lamda: 1.5361640453338623
Running avgs for agent 2: q_loss: 56.00310134887695, p_loss: -5.591489315032959, mean_rew: -11.081130240462066, variance: 6.735078811645508, lamda: 1.6517661809921265

steps: 999975, episodes: 40000, mean episode reward: -892.704456125708, agent episode reward: [-297.56815204190275, -297.56815204190275, -297.56815204190275], time: 156.412
steps: 999975, episodes: 40000, mean episode variance: 5.128488023996353, agent episode variance: [1.6185191917419433, 1.8274232647418975, 1.6825455675125123], time: 156.412
Running avgs for agent 0: q_loss: 35.59757995605469, p_loss: -5.601998805999756, mean_rew: -11.103512831720039, variance: 6.474077224731445, lamda: 1.72574782371521
Running avgs for agent 1: q_loss: 48.077571868896484, p_loss: -5.58331823348999, mean_rew: -11.0996485980337, variance: 7.309693336486816, lamda: 1.5404630899429321
Running avgs for agent 2: q_loss: 55.88681411743164, p_loss: -5.597462177276611, mean_rew: -11.111409425991038, variance: 6.73018217086792, lamda: 1.6565172672271729

steps: 1024975, episodes: 41000, mean episode reward: -883.1091666432312, agent episode reward: [-294.36972221441044, -294.36972221441044, -294.36972221441044], time: 156.227
steps: 1024975, episodes: 41000, mean episode variance: 5.118949763536453, agent episode variance: [1.6022639136314392, 1.8259769508838652, 1.6907088990211487], time: 156.228
Running avgs for agent 0: q_loss: 38.05906677246094, p_loss: -5.636714935302734, mean_rew: -11.17099738288912, variance: 6.409055709838867, lamda: 1.7416391372680664
Running avgs for agent 1: q_loss: 49.115455627441406, p_loss: -5.633915424346924, mean_rew: -11.187658093848553, variance: 7.303907871246338, lamda: 1.54563570022583
Running avgs for agent 2: q_loss: 57.65338897705078, p_loss: -5.637922286987305, mean_rew: -11.182134736000735, variance: 6.762835502624512, lamda: 1.661633014678955

steps: 1049975, episodes: 42000, mean episode reward: -894.8357960439042, agent episode reward: [-298.27859868130145, -298.27859868130145, -298.27859868130145], time: 157.643
steps: 1049975, episodes: 42000, mean episode variance: 5.134653900146485, agent episode variance: [1.6050635251998902, 1.8364268627166749, 1.6931635122299193], time: 157.644
Running avgs for agent 0: q_loss: 34.562042236328125, p_loss: -5.677281379699707, mean_rew: -11.25465638384074, variance: 6.420254230499268, lamda: 1.7571468353271484
Running avgs for agent 1: q_loss: 48.45849609375, p_loss: -5.67032527923584, mean_rew: -11.27533532959973, variance: 7.345706939697266, lamda: 1.5498312711715698
Running avgs for agent 2: q_loss: 56.48942947387695, p_loss: -5.682481288909912, mean_rew: -11.272890802515747, variance: 6.7726545333862305, lamda: 1.6682329177856445

steps: 1074975, episodes: 43000, mean episode reward: -890.3878484787699, agent episode reward: [-296.79594949292334, -296.79594949292334, -296.79594949292334], time: 158.228
steps: 1074975, episodes: 43000, mean episode variance: 5.130726817369461, agent episode variance: [1.6073342428207398, 1.8315856788158418, 1.6918068957328796], time: 158.229
Running avgs for agent 0: q_loss: 36.4769287109375, p_loss: -5.70253324508667, mean_rew: -11.309289724412368, variance: 6.429337024688721, lamda: 1.7712527513504028
Running avgs for agent 1: q_loss: 47.781124114990234, p_loss: -5.679187297821045, mean_rew: -11.295617377478575, variance: 7.326342582702637, lamda: 1.552734136581421
Running avgs for agent 2: q_loss: 56.79059982299805, p_loss: -5.692258834838867, mean_rew: -11.306791293073795, variance: 6.767227649688721, lamda: 1.673443078994751

steps: 1099975, episodes: 44000, mean episode reward: -867.2035945842941, agent episode reward: [-289.06786486143136, -289.06786486143136, -289.06786486143136], time: 156.013
steps: 1099975, episodes: 44000, mean episode variance: 5.119635733127594, agent episode variance: [1.6114077117443084, 1.819006885766983, 1.6892211356163025], time: 156.014
Running avgs for agent 0: q_loss: 48.534969329833984, p_loss: -5.703583717346191, mean_rew: -11.312129006394224, variance: 6.44563102722168, lamda: 1.7847816944122314
Running avgs for agent 1: q_loss: 46.80147933959961, p_loss: -5.692232131958008, mean_rew: -11.314450833019116, variance: 7.276027202606201, lamda: 1.5554417371749878
Running avgs for agent 2: q_loss: 55.536338806152344, p_loss: -5.693345546722412, mean_rew: -11.307631927539157, variance: 6.7568840980529785, lamda: 1.677551031112671

steps: 1124975, episodes: 45000, mean episode reward: -887.8695306041631, agent episode reward: [-295.9565102013877, -295.9565102013877, -295.9565102013877], time: 157.709
steps: 1124975, episodes: 45000, mean episode variance: 5.119051055192948, agent episode variance: [1.5944902477264404, 1.8337574365139007, 1.6908033709526062], time: 157.71
Running avgs for agent 0: q_loss: 57.37977981567383, p_loss: -5.7173309326171875, mean_rew: -11.343280570716175, variance: 6.377961158752441, lamda: 1.7895078659057617
Running avgs for agent 1: q_loss: 45.962337493896484, p_loss: -5.693841457366943, mean_rew: -11.332110423533104, variance: 7.335029602050781, lamda: 1.5569900274276733
Running avgs for agent 2: q_loss: 55.940406799316406, p_loss: -5.7133049964904785, mean_rew: -11.335131638038302, variance: 6.763213157653809, lamda: 1.6809204816818237

steps: 1149975, episodes: 46000, mean episode reward: -904.5498381737806, agent episode reward: [-301.51661272459353, -301.51661272459353, -301.51661272459353], time: 158.25
steps: 1149975, episodes: 46000, mean episode variance: 5.138357707500457, agent episode variance: [1.5855418558120729, 1.8351287219524384, 1.7176871297359466], time: 158.25
Running avgs for agent 0: q_loss: 56.286598205566406, p_loss: -5.710285186767578, mean_rew: -11.324844856809138, variance: 6.342167377471924, lamda: 1.7903141975402832
Running avgs for agent 1: q_loss: 46.36485290527344, p_loss: -5.701359272003174, mean_rew: -11.346441816199638, variance: 7.340514659881592, lamda: 1.5597363710403442
Running avgs for agent 2: q_loss: 54.876617431640625, p_loss: -5.69766092300415, mean_rew: -11.33340027743089, variance: 6.870748519897461, lamda: 1.6854023933410645

steps: 1174975, episodes: 47000, mean episode reward: -906.5075429350212, agent episode reward: [-302.1691809783404, -302.1691809783404, -302.1691809783404], time: 159.999
steps: 1174975, episodes: 47000, mean episode variance: 5.132529538393021, agent episode variance: [1.6020505518913268, 1.8188578565120697, 1.711621129989624], time: 160.0
Running avgs for agent 0: q_loss: 55.7386474609375, p_loss: -5.713475704193115, mean_rew: -11.346433755927489, variance: 6.408202171325684, lamda: 1.7918822765350342
Running avgs for agent 1: q_loss: 45.74492263793945, p_loss: -5.696362495422363, mean_rew: -11.341888656717355, variance: 7.275431156158447, lamda: 1.5620224475860596
Running avgs for agent 2: q_loss: 55.27779006958008, p_loss: -5.705872058868408, mean_rew: -11.342183782897246, variance: 6.846484184265137, lamda: 1.6891120672225952

steps: 1199975, episodes: 48000, mean episode reward: -903.1325668774459, agent episode reward: [-301.04418895914864, -301.04418895914864, -301.04418895914864], time: 154.229
steps: 1199975, episodes: 48000, mean episode variance: 5.149685188293457, agent episode variance: [1.6038760397434235, 1.8485492012500764, 1.6972599472999572], time: 154.229
Running avgs for agent 0: q_loss: 47.9710807800293, p_loss: -5.734341621398926, mean_rew: -11.395064413015866, variance: 6.415503978729248, lamda: 1.7944891452789307
Running avgs for agent 1: q_loss: 45.69140625, p_loss: -5.7179741859436035, mean_rew: -11.387654909358222, variance: 7.3941969871521, lamda: 1.5650595426559448
Running avgs for agent 2: q_loss: 47.41517639160156, p_loss: -5.7437567710876465, mean_rew: -11.403217851151739, variance: 6.7890400886535645, lamda: 1.6944475173950195

steps: 1224975, episodes: 49000, mean episode reward: -899.9263724616545, agent episode reward: [-299.97545748721814, -299.97545748721814, -299.97545748721814], time: 157.657
steps: 1224975, episodes: 49000, mean episode variance: 5.140287864923478, agent episode variance: [1.5913697869777679, 1.8440105657577515, 1.7049075121879578], time: 157.657
Running avgs for agent 0: q_loss: 38.40243911743164, p_loss: -5.781791687011719, mean_rew: -11.447365218562208, variance: 6.365479469299316, lamda: 1.807518720626831
Running avgs for agent 1: q_loss: 45.686439514160156, p_loss: -5.7489237785339355, mean_rew: -11.437598467903804, variance: 7.376042366027832, lamda: 1.5676147937774658
Running avgs for agent 2: q_loss: 49.97343063354492, p_loss: -5.774982452392578, mean_rew: -11.467606821738464, variance: 6.819629669189453, lamda: 1.7080512046813965

steps: 1249975, episodes: 50000, mean episode reward: -893.8834568513662, agent episode reward: [-297.96115228378875, -297.96115228378875, -297.96115228378875], time: 157.613
steps: 1249975, episodes: 50000, mean episode variance: 5.086560748338699, agent episode variance: [1.5875046491622924, 1.8327581312656402, 1.6662979679107666], time: 157.613
Running avgs for agent 0: q_loss: 34.679256439208984, p_loss: -5.801280975341797, mean_rew: -11.494103958443462, variance: 6.350018501281738, lamda: 1.821295976638794
Running avgs for agent 1: q_loss: 45.226531982421875, p_loss: -5.7656354904174805, mean_rew: -11.482189420198054, variance: 7.3310322761535645, lamda: 1.5696606636047363
Running avgs for agent 2: q_loss: 54.424861907958984, p_loss: -5.789633750915527, mean_rew: -11.486330615120725, variance: 6.665192127227783, lamda: 1.710813045501709

steps: 1274975, episodes: 51000, mean episode reward: -890.2817339095554, agent episode reward: [-296.76057796985185, -296.76057796985185, -296.76057796985185], time: 157.84
steps: 1274975, episodes: 51000, mean episode variance: 5.140153315067291, agent episode variance: [1.5879141709804534, 1.8653528628349305, 1.6868862812519074], time: 157.84
Running avgs for agent 0: q_loss: 41.11899948120117, p_loss: -5.8006181716918945, mean_rew: -11.513199774622233, variance: 6.351656436920166, lamda: 1.8287805318832397
Running avgs for agent 1: q_loss: 46.259761810302734, p_loss: -5.795913219451904, mean_rew: -11.526535622655109, variance: 7.461410999298096, lamda: 1.572238564491272
Running avgs for agent 2: q_loss: 54.249732971191406, p_loss: -5.78851842880249, mean_rew: -11.520046754429757, variance: 6.7475457191467285, lamda: 1.712462067604065

steps: 1299975, episodes: 52000, mean episode reward: -887.0794309248519, agent episode reward: [-295.69314364161727, -295.69314364161727, -295.69314364161727], time: 156.423
steps: 1299975, episodes: 52000, mean episode variance: 5.098704053401947, agent episode variance: [1.5670853762626649, 1.8329746835231782, 1.6986439936161042], time: 156.423
Running avgs for agent 0: q_loss: 57.32159423828125, p_loss: -5.814187526702881, mean_rew: -11.543060593093402, variance: 6.268342018127441, lamda: 1.8331021070480347
Running avgs for agent 1: q_loss: 46.2708740234375, p_loss: -5.8088555335998535, mean_rew: -11.543789351341431, variance: 7.3318986892700195, lamda: 1.575631856918335
Running avgs for agent 2: q_loss: 54.104400634765625, p_loss: -5.813563346862793, mean_rew: -11.547089764888408, variance: 6.794576168060303, lamda: 1.7135772705078125

steps: 1324975, episodes: 53000, mean episode reward: -912.9710923007099, agent episode reward: [-304.3236974335699, -304.3236974335699, -304.3236974335699], time: 152.813
steps: 1324975, episodes: 53000, mean episode variance: 5.13502400636673, agent episode variance: [1.582328711748123, 1.8429684002399445, 1.709726894378662], time: 152.813
Running avgs for agent 0: q_loss: 56.68913269042969, p_loss: -5.840752601623535, mean_rew: -11.586979841740016, variance: 6.329314708709717, lamda: 1.833837866783142
Running avgs for agent 1: q_loss: 37.00282669067383, p_loss: -5.817409992218018, mean_rew: -11.572393914722381, variance: 7.37187385559082, lamda: 1.5789082050323486
Running avgs for agent 2: q_loss: 53.36701583862305, p_loss: -5.834940433502197, mean_rew: -11.588967055405151, variance: 6.838907241821289, lamda: 1.7149747610092163

steps: 1349975, episodes: 54000, mean episode reward: -918.1103960022614, agent episode reward: [-306.0367986674205, -306.0367986674205, -306.0367986674205], time: 159.962
steps: 1349975, episodes: 54000, mean episode variance: 5.160363336086273, agent episode variance: [1.5968370618820191, 1.8289889569282531, 1.734537317276001], time: 159.963
Running avgs for agent 0: q_loss: 55.959388732910156, p_loss: -5.85609769821167, mean_rew: -11.628811459473706, variance: 6.387348651885986, lamda: 1.8350532054901123
Running avgs for agent 1: q_loss: 32.65446472167969, p_loss: -5.846868991851807, mean_rew: -11.633563235234215, variance: 7.315956115722656, lamda: 1.5893795490264893
Running avgs for agent 2: q_loss: 54.384281158447266, p_loss: -5.855177402496338, mean_rew: -11.64065483377843, variance: 6.938149452209473, lamda: 1.7167184352874756

steps: 1374975, episodes: 55000, mean episode reward: -917.206862929876, agent episode reward: [-305.73562097662534, -305.73562097662534, -305.73562097662534], time: 165.476
steps: 1374975, episodes: 55000, mean episode variance: 5.154619208812713, agent episode variance: [1.5960855174064636, 1.843938961982727, 1.7145947294235229], time: 165.476
Running avgs for agent 0: q_loss: 55.78315353393555, p_loss: -5.879329681396484, mean_rew: -11.674588281420515, variance: 6.384341716766357, lamda: 1.8361504077911377
Running avgs for agent 1: q_loss: 29.680644989013672, p_loss: -5.886953830718994, mean_rew: -11.709762243251895, variance: 7.375755786895752, lamda: 1.6003018617630005
Running avgs for agent 2: q_loss: 54.58415985107422, p_loss: -5.881945610046387, mean_rew: -11.696837679322815, variance: 6.858378887176514, lamda: 1.718070149421692

steps: 1399975, episodes: 56000, mean episode reward: -901.4614912497061, agent episode reward: [-300.48716374990204, -300.48716374990204, -300.48716374990204], time: 157.247
steps: 1399975, episodes: 56000, mean episode variance: 5.123209895372391, agent episode variance: [1.603576994895935, 1.8228697872161865, 1.6967631132602692], time: 157.248
Running avgs for agent 0: q_loss: 56.672977447509766, p_loss: -5.90749454498291, mean_rew: -11.726588609851689, variance: 6.414308547973633, lamda: 1.8373774290084839
Running avgs for agent 1: q_loss: 29.680383682250977, p_loss: -5.9004902839660645, mean_rew: -11.73436573432247, variance: 7.291479587554932, lamda: 1.6099194288253784
Running avgs for agent 2: q_loss: 53.63249206542969, p_loss: -5.9029645919799805, mean_rew: -11.713399289962707, variance: 6.787052631378174, lamda: 1.7193808555603027

steps: 1424975, episodes: 57000, mean episode reward: -915.4017139795392, agent episode reward: [-305.13390465984645, -305.13390465984645, -305.13390465984645], time: 158.6
steps: 1424975, episodes: 57000, mean episode variance: 5.168913731813431, agent episode variance: [1.615306495666504, 1.8314690761566161, 1.7221381599903107], time: 158.6
Running avgs for agent 0: q_loss: 55.77716064453125, p_loss: -5.91641092300415, mean_rew: -11.758744009188886, variance: 6.461226463317871, lamda: 1.838309645652771
Running avgs for agent 1: q_loss: 29.920347213745117, p_loss: -5.921170711517334, mean_rew: -11.770214843762105, variance: 7.325876235961914, lamda: 1.6172231435775757
Running avgs for agent 2: q_loss: 54.204071044921875, p_loss: -5.923859119415283, mean_rew: -11.781832397592039, variance: 6.888552188873291, lamda: 1.7202634811401367

steps: 1449975, episodes: 58000, mean episode reward: -904.476592845719, agent episode reward: [-301.49219761523966, -301.49219761523966, -301.49219761523966], time: 158.141
steps: 1449975, episodes: 58000, mean episode variance: 5.163245817899704, agent episode variance: [1.6050424675941468, 1.8167445421218873, 1.74145880818367], time: 158.141
Running avgs for agent 0: q_loss: 56.29789733886719, p_loss: -5.937744617462158, mean_rew: -11.793376661213514, variance: 6.420169830322266, lamda: 1.8392033576965332
Running avgs for agent 1: q_loss: 30.72696304321289, p_loss: -5.918089866638184, mean_rew: -11.781951342754, variance: 7.266978740692139, lamda: 1.626626968383789
Running avgs for agent 2: q_loss: 53.29562759399414, p_loss: -5.918282508850098, mean_rew: -11.789742632833791, variance: 6.965835094451904, lamda: 1.720849633216858

steps: 1474975, episodes: 59000, mean episode reward: -898.7903487443056, agent episode reward: [-299.59678291476854, -299.59678291476854, -299.59678291476854], time: 155.228
steps: 1474975, episodes: 59000, mean episode variance: 5.127414376497269, agent episode variance: [1.6125829119682311, 1.8124965481758117, 1.7023349163532258], time: 155.229
Running avgs for agent 0: q_loss: 37.715858459472656, p_loss: -5.944064617156982, mean_rew: -11.80940877942583, variance: 6.450332164764404, lamda: 1.8433074951171875
Running avgs for agent 1: q_loss: 29.187402725219727, p_loss: -5.9285664558410645, mean_rew: -11.803238892761204, variance: 7.249986171722412, lamda: 1.6324220895767212
Running avgs for agent 2: q_loss: 53.47880554199219, p_loss: -5.943216323852539, mean_rew: -11.799432525809397, variance: 6.80933952331543, lamda: 1.7227286100387573

steps: 1499975, episodes: 60000, mean episode reward: -921.8903420091425, agent episode reward: [-307.2967806697142, -307.2967806697142, -307.2967806697142], time: 155.398
steps: 1499975, episodes: 60000, mean episode variance: 5.156344324350357, agent episode variance: [1.6072169375419616, 1.8106224436759948, 1.7385049431324004], time: 155.399
Running avgs for agent 0: q_loss: 32.80405807495117, p_loss: -5.952611446380615, mean_rew: -11.824862915965832, variance: 6.428868293762207, lamda: 1.8498047590255737
Running avgs for agent 1: q_loss: 29.606494903564453, p_loss: -5.941093921661377, mean_rew: -11.832080045361831, variance: 7.242490291595459, lamda: 1.6386655569076538
Running avgs for agent 2: q_loss: 53.248172760009766, p_loss: -5.941586971282959, mean_rew: -11.83871301392458, variance: 6.954019546508789, lamda: 1.7237404584884644

steps: 1524975, episodes: 61000, mean episode reward: -909.2828114054245, agent episode reward: [-303.0942704684748, -303.0942704684748, -303.0942704684748], time: 164.935
steps: 1524975, episodes: 61000, mean episode variance: 5.134593857049942, agent episode variance: [1.6028976225852967, 1.8085451354980469, 1.7231510989665986], time: 164.936
Running avgs for agent 0: q_loss: 35.400421142578125, p_loss: -5.968502521514893, mean_rew: -11.847559447094554, variance: 6.411591053009033, lamda: 1.8556201457977295
Running avgs for agent 1: q_loss: 30.079858779907227, p_loss: -5.955143451690674, mean_rew: -11.852690158459358, variance: 7.234179973602295, lamda: 1.6444987058639526
Running avgs for agent 2: q_loss: 52.78803253173828, p_loss: -5.970060348510742, mean_rew: -11.86508906545608, variance: 6.892603874206543, lamda: 1.725009799003601

steps: 1549975, episodes: 62000, mean episode reward: -918.2121581660506, agent episode reward: [-306.0707193886836, -306.0707193886836, -306.0707193886836], time: 164.633
steps: 1549975, episodes: 62000, mean episode variance: 5.118181742429734, agent episode variance: [1.6044014363288879, 1.7986880946159363, 1.715092211484909], time: 164.633
Running avgs for agent 0: q_loss: 32.45232009887695, p_loss: -5.978672027587891, mean_rew: -11.87567965334612, variance: 6.417605876922607, lamda: 1.8623915910720825
Running avgs for agent 1: q_loss: 30.99135398864746, p_loss: -5.954219818115234, mean_rew: -11.859646787435159, variance: 7.194752216339111, lamda: 1.6518889665603638
Running avgs for agent 2: q_loss: 52.79317855834961, p_loss: -5.9649834632873535, mean_rew: -11.871695766347836, variance: 6.860368728637695, lamda: 1.7262942790985107

steps: 1574975, episodes: 63000, mean episode reward: -885.0449832020079, agent episode reward: [-295.01499440066925, -295.01499440066925, -295.01499440066925], time: 162.206
steps: 1574975, episodes: 63000, mean episode variance: 5.1431946229934695, agent episode variance: [1.6000159094333648, 1.802565041065216, 1.7406136724948884], time: 162.207
Running avgs for agent 0: q_loss: 36.10053634643555, p_loss: -5.989923000335693, mean_rew: -11.892353571808567, variance: 6.400063514709473, lamda: 1.8659435510635376
Running avgs for agent 1: q_loss: 31.545734405517578, p_loss: -5.966209888458252, mean_rew: -11.881584392452766, variance: 7.210260391235352, lamda: 1.6614586114883423
Running avgs for agent 2: q_loss: 52.917240142822266, p_loss: -5.9760661125183105, mean_rew: -11.891494970542443, variance: 6.9624552726745605, lamda: 1.727588176727295

steps: 1599975, episodes: 64000, mean episode reward: -927.9260739016685, agent episode reward: [-309.3086913005562, -309.3086913005562, -309.3086913005562], time: 156.346
steps: 1599975, episodes: 64000, mean episode variance: 5.1076142838001255, agent episode variance: [1.5911991202831268, 1.7825182316303254, 1.733896931886673], time: 156.347
Running avgs for agent 0: q_loss: 56.822513580322266, p_loss: -5.996417999267578, mean_rew: -11.912679499920658, variance: 6.364796161651611, lamda: 1.8684998750686646
Running avgs for agent 1: q_loss: 43.27875900268555, p_loss: -5.970992088317871, mean_rew: -11.899191491900442, variance: 7.130072593688965, lamda: 1.6671260595321655
Running avgs for agent 2: q_loss: 52.0196533203125, p_loss: -5.974040985107422, mean_rew: -11.896493006119483, variance: 6.9355878829956055, lamda: 1.7305550575256348

steps: 1624975, episodes: 65000, mean episode reward: -925.3984387928979, agent episode reward: [-308.4661462642993, -308.4661462642993, -308.4661462642993], time: 157.401
steps: 1624975, episodes: 65000, mean episode variance: 5.125156907320022, agent episode variance: [1.6064043374061585, 1.7917913753986359, 1.7269611945152283], time: 157.402
Running avgs for agent 0: q_loss: 56.4425163269043, p_loss: -6.011743545532227, mean_rew: -11.934479277589142, variance: 6.425617218017578, lamda: 1.869011402130127
Running avgs for agent 1: q_loss: 50.031883239746094, p_loss: -5.975214004516602, mean_rew: -11.909564554213713, variance: 7.167166233062744, lamda: 1.6682485342025757
Running avgs for agent 2: q_loss: 35.89791488647461, p_loss: -5.991252422332764, mean_rew: -11.923337437002978, variance: 6.907844543457031, lamda: 1.7381948232650757

steps: 1649975, episodes: 66000, mean episode reward: -933.8045025997029, agent episode reward: [-311.2681675332343, -311.2681675332343, -311.2681675332343], time: 156.212
steps: 1649975, episodes: 66000, mean episode variance: 5.105555060148239, agent episode variance: [1.5905689868927002, 1.7937078769207, 1.721278196334839], time: 156.213
Running avgs for agent 0: q_loss: 33.77867126464844, p_loss: -6.005924701690674, mean_rew: -11.92415581630297, variance: 6.362276077270508, lamda: 1.872667670249939
Running avgs for agent 1: q_loss: 50.13923263549805, p_loss: -6.013407230377197, mean_rew: -11.981011203514518, variance: 7.174831867218018, lamda: 1.6691571474075317
Running avgs for agent 2: q_loss: 34.92022705078125, p_loss: -6.020659923553467, mean_rew: -11.96499494306134, variance: 6.885112285614014, lamda: 1.7512023448944092

steps: 1674975, episodes: 67000, mean episode reward: -908.3671168123869, agent episode reward: [-302.7890389374623, -302.7890389374623, -302.7890389374623], time: 155.75
steps: 1674975, episodes: 67000, mean episode variance: 5.102258220672607, agent episode variance: [1.6034364862442017, 1.7949490275382995, 1.7038727068901063], time: 155.751
Running avgs for agent 0: q_loss: 35.42286682128906, p_loss: -6.0255656242370605, mean_rew: -11.956632646392253, variance: 6.413745880126953, lamda: 1.8801976442337036
Running avgs for agent 1: q_loss: 49.72710037231445, p_loss: -5.994912147521973, mean_rew: -11.969741989639951, variance: 7.179795742034912, lamda: 1.6698979139328003
Running avgs for agent 2: q_loss: 33.115135192871094, p_loss: -6.0140910148620605, mean_rew: -11.957147236068192, variance: 6.81549072265625, lamda: 1.7621344327926636

steps: 1699975, episodes: 68000, mean episode reward: -904.5125195772964, agent episode reward: [-301.5041731924322, -301.5041731924322, -301.5041731924322], time: 158.816
steps: 1699975, episodes: 68000, mean episode variance: 5.0901624143123625, agent episode variance: [1.5989853281974793, 1.792692703962326, 1.6984843821525575], time: 158.816
Running avgs for agent 0: q_loss: 32.29238510131836, p_loss: -6.032110214233398, mean_rew: -11.975887772626216, variance: 6.395941257476807, lamda: 1.8875441551208496
Running avgs for agent 1: q_loss: 50.212135314941406, p_loss: -5.999886512756348, mean_rew: -11.964415896931047, variance: 7.17077112197876, lamda: 1.6714321374893188
Running avgs for agent 2: q_loss: 32.32777404785156, p_loss: -6.02117395401001, mean_rew: -11.976665934659014, variance: 6.793937683105469, lamda: 1.7697919607162476

steps: 1724975, episodes: 69000, mean episode reward: -897.7402279455937, agent episode reward: [-299.24674264853127, -299.24674264853127, -299.24674264853127], time: 158.803
steps: 1724975, episodes: 69000, mean episode variance: 5.085116887569428, agent episode variance: [1.5907646307945251, 1.786582799434662, 1.7077694573402404], time: 158.803
Running avgs for agent 0: q_loss: 31.654781341552734, p_loss: -6.0397844314575195, mean_rew: -12.003644674256197, variance: 6.363058567047119, lamda: 1.8902876377105713
Running avgs for agent 1: q_loss: 50.2625617980957, p_loss: -6.018808364868164, mean_rew: -11.997843329754552, variance: 7.146330833435059, lamda: 1.6722919940948486
Running avgs for agent 2: q_loss: 33.4168586730957, p_loss: -6.023654460906982, mean_rew: -11.995160926189707, variance: 6.831078052520752, lamda: 1.7753876447677612

steps: 1749975, episodes: 70000, mean episode reward: -902.6749892505293, agent episode reward: [-300.89166308350974, -300.89166308350974, -300.89166308350974], time: 156.09
steps: 1749975, episodes: 70000, mean episode variance: 5.118016285896301, agent episode variance: [1.595102128982544, 1.8151236844062806, 1.7077904725074768], time: 156.091
Running avgs for agent 0: q_loss: 32.355960845947266, p_loss: -6.054033279418945, mean_rew: -12.015674142029809, variance: 6.380408763885498, lamda: 1.8921725749969482
Running avgs for agent 1: q_loss: 50.33484649658203, p_loss: -6.013604640960693, mean_rew: -12.004125838694948, variance: 7.260494709014893, lamda: 1.673671841621399
Running avgs for agent 2: q_loss: 36.056922912597656, p_loss: -6.029007911682129, mean_rew: -12.007969895127012, variance: 6.8311614990234375, lamda: 1.7887524366378784

steps: 1774975, episodes: 71000, mean episode reward: -893.8070104147766, agent episode reward: [-297.93567013825884, -297.93567013825884, -297.93567013825884], time: 157.356
steps: 1774975, episodes: 71000, mean episode variance: 5.073970027208328, agent episode variance: [1.5916935591697692, 1.8035780622959137, 1.6786984057426453], time: 157.356
Running avgs for agent 0: q_loss: 31.90744972229004, p_loss: -6.047581195831299, mean_rew: -12.025515779165902, variance: 6.366774559020996, lamda: 1.893585443496704
Running avgs for agent 1: q_loss: 49.526947021484375, p_loss: -6.014034271240234, mean_rew: -12.009189072272424, variance: 7.214312553405762, lamda: 1.6745624542236328
Running avgs for agent 2: q_loss: 32.95530319213867, p_loss: -6.039916515350342, mean_rew: -12.007867552335906, variance: 6.714794158935547, lamda: 1.7994343042373657

steps: 1799975, episodes: 72000, mean episode reward: -885.6092722404651, agent episode reward: [-295.20309074682166, -295.20309074682166, -295.20309074682166], time: 162.221
steps: 1799975, episodes: 72000, mean episode variance: 5.076392143726349, agent episode variance: [1.5968294587135314, 1.8010035076141357, 1.6785591773986817], time: 162.222
Running avgs for agent 0: q_loss: 38.19367980957031, p_loss: -6.042784214019775, mean_rew: -12.009748194765791, variance: 6.387317657470703, lamda: 1.903688669204712
Running avgs for agent 1: q_loss: 51.170772552490234, p_loss: -6.017080783843994, mean_rew: -12.000345391727858, variance: 7.204013824462891, lamda: 1.6751981973648071
Running avgs for agent 2: q_loss: 32.70986557006836, p_loss: -6.049849510192871, mean_rew: -12.028064165907246, variance: 6.714236736297607, lamda: 1.8070584535598755

steps: 1824975, episodes: 73000, mean episode reward: -893.1483196182699, agent episode reward: [-297.7161065394233, -297.7161065394233, -297.7161065394233], time: 158.185
steps: 1824975, episodes: 73000, mean episode variance: 5.049066095352173, agent episode variance: [1.5848930621147155, 1.8068014199733735, 1.6573716132640839], time: 158.186
Running avgs for agent 0: q_loss: 33.64204788208008, p_loss: -6.041584014892578, mean_rew: -12.004950780021638, variance: 6.339572429656982, lamda: 1.9110934734344482
Running avgs for agent 1: q_loss: 50.233680725097656, p_loss: -6.015995979309082, mean_rew: -11.995687710812856, variance: 7.227205753326416, lamda: 1.6760094165802002
Running avgs for agent 2: q_loss: 49.94702911376953, p_loss: -6.012280464172363, mean_rew: -11.990427769496199, variance: 6.629486560821533, lamda: 1.8136855363845825

steps: 1849975, episodes: 74000, mean episode reward: -880.7891299395739, agent episode reward: [-293.5963766465246, -293.5963766465246, -293.5963766465246], time: 156.386
steps: 1849975, episodes: 74000, mean episode variance: 5.03607852268219, agent episode variance: [1.5671399934291839, 1.8025927617549897, 1.6663457674980164], time: 156.386
Running avgs for agent 0: q_loss: 36.271724700927734, p_loss: -6.052376747131348, mean_rew: -12.012870303720051, variance: 6.268559455871582, lamda: 1.9180058240890503
Running avgs for agent 1: q_loss: 49.50654983520508, p_loss: -6.006932258605957, mean_rew: -11.997041455496834, variance: 7.210371017456055, lamda: 1.6771982908248901
Running avgs for agent 2: q_loss: 53.561214447021484, p_loss: -6.032835006713867, mean_rew: -12.007161910500267, variance: 6.6653828620910645, lamda: 1.8146283626556396

steps: 1874975, episodes: 75000, mean episode reward: -894.6544724728407, agent episode reward: [-298.21815749094685, -298.21815749094685, -298.21815749094685], time: 160.532
steps: 1874975, episodes: 75000, mean episode variance: 5.02942918086052, agent episode variance: [1.550565167427063, 1.813170794725418, 1.6656932187080384], time: 160.533
Running avgs for agent 0: q_loss: 53.170658111572266, p_loss: -6.029406547546387, mean_rew: -11.988160770252433, variance: 6.202260971069336, lamda: 1.9241113662719727
Running avgs for agent 1: q_loss: 49.679195404052734, p_loss: -6.001234531402588, mean_rew: -11.985851199067532, variance: 7.252682685852051, lamda: 1.6777926683425903
Running avgs for agent 2: q_loss: 53.54808807373047, p_loss: -6.026630878448486, mean_rew: -11.987887165496115, variance: 6.662773609161377, lamda: 1.8164339065551758

steps: 1899975, episodes: 76000, mean episode reward: -894.1939848969577, agent episode reward: [-298.0646616323192, -298.0646616323192, -298.0646616323192], time: 158.951
steps: 1899975, episodes: 76000, mean episode variance: 5.043915884494782, agent episode variance: [1.568986598968506, 1.8123659400939942, 1.6625633454322815], time: 158.951
Running avgs for agent 0: q_loss: 34.09239196777344, p_loss: -6.036022663116455, mean_rew: -12.003094029314992, variance: 6.275946617126465, lamda: 1.931725025177002
Running avgs for agent 1: q_loss: 49.17313766479492, p_loss: -6.014258861541748, mean_rew: -12.001644377089287, variance: 7.2494635581970215, lamda: 1.678545355796814
Running avgs for agent 2: q_loss: 48.66737365722656, p_loss: -6.03615665435791, mean_rew: -12.000883633300095, variance: 6.6502532958984375, lamda: 1.8193919658660889

steps: 1924975, episodes: 77000, mean episode reward: -872.6014256098289, agent episode reward: [-290.867141869943, -290.867141869943, -290.867141869943], time: 157.766
steps: 1924975, episodes: 77000, mean episode variance: 5.005992584228515, agent episode variance: [1.5524427618980408, 1.7933679275512695, 1.6601818947792053], time: 157.767
Running avgs for agent 0: q_loss: 33.43914031982422, p_loss: -6.032217502593994, mean_rew: -11.987496574436928, variance: 6.209771156311035, lamda: 1.939016580581665
Running avgs for agent 1: q_loss: 49.062461853027344, p_loss: -6.008143424987793, mean_rew: -11.979235376919423, variance: 7.173471450805664, lamda: 1.679742693901062
Running avgs for agent 2: q_loss: 37.468536376953125, p_loss: -6.0226335525512695, mean_rew: -11.992764848018673, variance: 6.640727519989014, lamda: 1.830285906791687

steps: 1949975, episodes: 78000, mean episode reward: -884.0792598503841, agent episode reward: [-294.6930866167947, -294.6930866167947, -294.6930866167947], time: 158.828
steps: 1949975, episodes: 78000, mean episode variance: 5.019175797700882, agent episode variance: [1.5598360736370087, 1.8017543416023254, 1.6575853824615479], time: 158.829
Running avgs for agent 0: q_loss: 30.811182022094727, p_loss: -6.043148040771484, mean_rew: -12.005372670703501, variance: 6.239344120025635, lamda: 1.9418461322784424
Running avgs for agent 1: q_loss: 50.215213775634766, p_loss: -6.024659156799316, mean_rew: -12.012663836349049, variance: 7.207016944885254, lamda: 1.6825337409973145
Running avgs for agent 2: q_loss: 36.75048065185547, p_loss: -6.04062032699585, mean_rew: -12.003888369330653, variance: 6.630341529846191, lamda: 1.8413007259368896

steps: 1974975, episodes: 79000, mean episode reward: -890.7483325410045, agent episode reward: [-296.91611084700156, -296.91611084700156, -296.91611084700156], time: 164.529
steps: 1974975, episodes: 79000, mean episode variance: 4.990838355779648, agent episode variance: [1.5527923681735993, 1.7995467467308044, 1.6384992408752441], time: 164.53
Running avgs for agent 0: q_loss: 55.62641525268555, p_loss: -6.035434722900391, mean_rew: -12.008454427108285, variance: 6.211169242858887, lamda: 1.9443625211715698
Running avgs for agent 1: q_loss: 48.99374771118164, p_loss: -6.027167797088623, mean_rew: -12.015357945827498, variance: 7.19818639755249, lamda: 1.684445858001709
Running avgs for agent 2: q_loss: 32.55364227294922, p_loss: -6.043413162231445, mean_rew: -11.996471402689295, variance: 6.553997039794922, lamda: 1.8538252115249634

steps: 1999975, episodes: 80000, mean episode reward: -885.2429919325842, agent episode reward: [-295.08099731086133, -295.08099731086133, -295.08099731086133], time: 164.979
steps: 1999975, episodes: 80000, mean episode variance: 4.965968289613723, agent episode variance: [1.5578609733581543, 1.7740806601047516, 1.6340266561508179], time: 164.979
Running avgs for agent 0: q_loss: 58.084930419921875, p_loss: -6.054925918579102, mean_rew: -12.02233179297658, variance: 6.231443405151367, lamda: 1.9455695152282715
Running avgs for agent 1: q_loss: 49.03734588623047, p_loss: -6.016580104827881, mean_rew: -11.979810985378393, variance: 7.096322059631348, lamda: 1.6867773532867432
Running avgs for agent 2: q_loss: 41.42362976074219, p_loss: -6.036467552185059, mean_rew: -12.010720389623149, variance: 6.536106586456299, lamda: 1.860671043395996

steps: 2024975, episodes: 81000, mean episode reward: -904.3158036670939, agent episode reward: [-301.43860122236464, -301.43860122236464, -301.43860122236464], time: 163.012
steps: 2024975, episodes: 81000, mean episode variance: 4.991081990957261, agent episode variance: [1.5595492072105408, 1.7977893979549409, 1.6337433857917785], time: 163.012
Running avgs for agent 0: q_loss: 56.5078125, p_loss: -6.042773246765137, mean_rew: -12.020376257326099, variance: 6.23819637298584, lamda: 1.9463034868240356
Running avgs for agent 1: q_loss: 48.450679779052734, p_loss: -6.021233558654785, mean_rew: -12.018424973330703, variance: 7.191157341003418, lamda: 1.688119649887085
Running avgs for agent 2: q_loss: 53.644012451171875, p_loss: -6.025899410247803, mean_rew: -12.004782458977658, variance: 6.534973621368408, lamda: 1.8635425567626953

steps: 2049975, episodes: 82000, mean episode reward: -892.0177848936196, agent episode reward: [-297.3392616312065, -297.3392616312065, -297.3392616312065], time: 164.609
steps: 2049975, episodes: 82000, mean episode variance: 4.987645854711532, agent episode variance: [1.5493007917404176, 1.7972789256572723, 1.6410661373138429], time: 164.61
Running avgs for agent 0: q_loss: 56.4857177734375, p_loss: -6.04811429977417, mean_rew: -12.015618485296514, variance: 6.197203159332275, lamda: 1.9467887878417969
Running avgs for agent 1: q_loss: 48.80048370361328, p_loss: -6.017735004425049, mean_rew: -11.995608063622438, variance: 7.189115047454834, lamda: 1.689569115638733
Running avgs for agent 2: q_loss: 53.25456237792969, p_loss: -6.0335869789123535, mean_rew: -12.01254295158512, variance: 6.56426477432251, lamda: 1.864653468132019

steps: 2074975, episodes: 83000, mean episode reward: -884.5559505888294, agent episode reward: [-294.8519835296098, -294.8519835296098, -294.8519835296098], time: 166.465
steps: 2074975, episodes: 83000, mean episode variance: 4.985229535579681, agent episode variance: [1.5829001879692077, 1.7763145127296447, 1.6260148348808288], time: 166.465
Running avgs for agent 0: q_loss: 56.4376106262207, p_loss: -6.044197082519531, mean_rew: -12.019178564483918, variance: 6.331600666046143, lamda: 1.9473943710327148
Running avgs for agent 1: q_loss: 38.15693664550781, p_loss: -6.032103538513184, mean_rew: -12.00088951731493, variance: 7.105257987976074, lamda: 1.6983193159103394
Running avgs for agent 2: q_loss: 53.38671112060547, p_loss: -6.029836654663086, mean_rew: -11.990368195791572, variance: 6.504059791564941, lamda: 1.8656911849975586

steps: 2099975, episodes: 84000, mean episode reward: -890.6456368494933, agent episode reward: [-296.88187894983105, -296.88187894983105, -296.88187894983105], time: 164.762
steps: 2099975, episodes: 84000, mean episode variance: 4.939388789892197, agent episode variance: [1.5443855655193328, 1.7669190921783446, 1.628084132194519], time: 164.763
Running avgs for agent 0: q_loss: 57.741580963134766, p_loss: -6.043248653411865, mean_rew: -12.009760662662812, variance: 6.177542209625244, lamda: 1.9490835666656494
Running avgs for agent 1: q_loss: 35.4481201171875, p_loss: -6.02614164352417, mean_rew: -12.020054637971203, variance: 7.067676544189453, lamda: 1.7134026288986206
Running avgs for agent 2: q_loss: 54.539024353027344, p_loss: -6.04136848449707, mean_rew: -12.019769614115228, variance: 6.512335777282715, lamda: 1.867292046546936

steps: 2124975, episodes: 85000, mean episode reward: -890.5103903806404, agent episode reward: [-296.83679679354674, -296.83679679354674, -296.83679679354674], time: 163.356
steps: 2124975, episodes: 85000, mean episode variance: 4.9199021093845365, agent episode variance: [1.5488729646205903, 1.7499595625400544, 1.6210695822238923], time: 163.357
Running avgs for agent 0: q_loss: 43.81167984008789, p_loss: -6.0511698722839355, mean_rew: -12.02660761264496, variance: 6.195492267608643, lamda: 1.9537410736083984
Running avgs for agent 1: q_loss: 43.12033462524414, p_loss: -6.05258321762085, mean_rew: -12.023051340954785, variance: 6.999837875366211, lamda: 1.7294217348098755
Running avgs for agent 2: q_loss: 53.8379020690918, p_loss: -6.032354831695557, mean_rew: -12.021137506718592, variance: 6.484278202056885, lamda: 1.8697690963745117

steps: 2149975, episodes: 86000, mean episode reward: -885.7283709103123, agent episode reward: [-295.24279030343746, -295.24279030343746, -295.24279030343746], time: 166.025
steps: 2149975, episodes: 86000, mean episode variance: 4.908555713176727, agent episode variance: [1.5404447333812714, 1.7493788833618165, 1.6187320964336396], time: 166.025
Running avgs for agent 0: q_loss: 33.194740295410156, p_loss: -6.046901226043701, mean_rew: -12.015547083119689, variance: 6.161779403686523, lamda: 1.9615923166275024
Running avgs for agent 1: q_loss: 46.37910461425781, p_loss: -6.024441242218018, mean_rew: -12.007710822519117, variance: 6.997515678405762, lamda: 1.732682704925537
Running avgs for agent 2: q_loss: 54.09746551513672, p_loss: -6.04090690612793, mean_rew: -12.020029791519098, variance: 6.474928379058838, lamda: 1.871850848197937

steps: 2174975, episodes: 87000, mean episode reward: -908.8921231147943, agent episode reward: [-302.9640410382649, -302.9640410382649, -302.9640410382649], time: 166.253
steps: 2174975, episodes: 87000, mean episode variance: 4.8911258926391605, agent episode variance: [1.5367787172794343, 1.7434452397823335, 1.6109019355773926], time: 166.253
Running avgs for agent 0: q_loss: 40.598873138427734, p_loss: -6.050670623779297, mean_rew: -12.009631803504453, variance: 6.1471147537231445, lamda: 1.9720968008041382
Running avgs for agent 1: q_loss: 47.21249008178711, p_loss: -6.032018661499023, mean_rew: -12.009231703413837, variance: 6.973781108856201, lamda: 1.733731985092163
Running avgs for agent 2: q_loss: 53.967552185058594, p_loss: -6.0436506271362305, mean_rew: -12.02297922328821, variance: 6.443607330322266, lamda: 1.8743149042129517

steps: 2199975, episodes: 88000, mean episode reward: -902.5290077823707, agent episode reward: [-300.8430025941236, -300.8430025941236, -300.8430025941236], time: 163.598
steps: 2199975, episodes: 88000, mean episode variance: 4.870147069454193, agent episode variance: [1.5211609971523286, 1.7534832568168641, 1.5955028154850006], time: 163.599
Running avgs for agent 0: q_loss: 42.291561126708984, p_loss: -6.044576168060303, mean_rew: -12.00740166084688, variance: 6.084643840789795, lamda: 1.9854377508163452
Running avgs for agent 1: q_loss: 46.90314865112305, p_loss: -6.029406547546387, mean_rew: -12.031576975692758, variance: 7.013933181762695, lamda: 1.7352616786956787
Running avgs for agent 2: q_loss: 53.85257339477539, p_loss: -6.038350582122803, mean_rew: -11.99913255429297, variance: 6.382011413574219, lamda: 1.875706434249878

steps: 2224975, episodes: 89000, mean episode reward: -907.9424288597992, agent episode reward: [-302.6474762865998, -302.6474762865998, -302.6474762865998], time: 164.983
steps: 2224975, episodes: 89000, mean episode variance: 4.8750931391716, agent episode variance: [1.511423444747925, 1.7408890144824982, 1.6227806799411775], time: 164.983
Running avgs for agent 0: q_loss: 32.373146057128906, p_loss: -6.040919780731201, mean_rew: -11.999244571797576, variance: 6.045693397521973, lamda: 1.9924449920654297
Running avgs for agent 1: q_loss: 46.478946685791016, p_loss: -6.038467884063721, mean_rew: -12.01825116150002, variance: 6.963555812835693, lamda: 1.736112356185913
Running avgs for agent 2: q_loss: 54.41907501220703, p_loss: -6.039200782775879, mean_rew: -12.014762848340323, variance: 6.491123199462891, lamda: 1.877630352973938

steps: 2249975, episodes: 90000, mean episode reward: -903.7687967472261, agent episode reward: [-301.25626558240873, -301.25626558240873, -301.25626558240873], time: 165.933
steps: 2249975, episodes: 90000, mean episode variance: 4.872106567144394, agent episode variance: [1.5166685366630555, 1.7298064751625062, 1.6256315553188323], time: 165.934
Running avgs for agent 0: q_loss: 36.201663970947266, p_loss: -6.045768737792969, mean_rew: -12.021811569098384, variance: 6.066673755645752, lamda: 1.997682809829712
Running avgs for agent 1: q_loss: 46.86016082763672, p_loss: -6.0376129150390625, mean_rew: -12.019027079785083, variance: 6.919225692749023, lamda: 1.7368696928024292
Running avgs for agent 2: q_loss: 54.129940032958984, p_loss: -6.0484395027160645, mean_rew: -12.021690474166318, variance: 6.50252628326416, lamda: 1.8787391185760498

steps: 2274975, episodes: 91000, mean episode reward: -895.7677050770542, agent episode reward: [-298.5892350256847, -298.5892350256847, -298.5892350256847], time: 167.008
steps: 2274975, episodes: 91000, mean episode variance: 4.883509425163269, agent episode variance: [1.5064288921356201, 1.7618313009738922, 1.6152492320537568], time: 167.009
Running avgs for agent 0: q_loss: 36.30107879638672, p_loss: -6.052994728088379, mean_rew: -12.038380343120277, variance: 6.0257158279418945, lamda: 2.009631395339966
Running avgs for agent 1: q_loss: 46.73393630981445, p_loss: -6.036535263061523, mean_rew: -12.031617758208908, variance: 7.047325134277344, lamda: 1.7375798225402832
Running avgs for agent 2: q_loss: 53.83451843261719, p_loss: -6.046555519104004, mean_rew: -12.02716501738086, variance: 6.460997104644775, lamda: 1.880233645439148

steps: 2299975, episodes: 92000, mean episode reward: -905.5278383637766, agent episode reward: [-301.84261278792553, -301.84261278792553, -301.84261278792553], time: 164.332
steps: 2299975, episodes: 92000, mean episode variance: 4.859970275402069, agent episode variance: [1.4961960260868072, 1.7482312757968903, 1.6155429735183715], time: 164.333
Running avgs for agent 0: q_loss: 34.22278594970703, p_loss: -6.050595760345459, mean_rew: -12.018173197960772, variance: 5.984784126281738, lamda: 2.018792152404785
Running avgs for agent 1: q_loss: 46.88629913330078, p_loss: -6.031126976013184, mean_rew: -12.015419661058893, variance: 6.992924690246582, lamda: 1.7391358613967896
Running avgs for agent 2: q_loss: 54.738826751708984, p_loss: -6.058109760284424, mean_rew: -12.035448580567806, variance: 6.462172031402588, lamda: 1.8827461004257202

steps: 2324975, episodes: 93000, mean episode reward: -902.3786216752396, agent episode reward: [-300.7928738917465, -300.7928738917465, -300.7928738917465], time: 163.989
steps: 2324975, episodes: 93000, mean episode variance: 4.867966981649399, agent episode variance: [1.5021980409622193, 1.7485733079910277, 1.6171956326961516], time: 163.99
Running avgs for agent 0: q_loss: 31.955753326416016, p_loss: -6.053020477294922, mean_rew: -12.02312510323109, variance: 6.008791923522949, lamda: 2.02595853805542
Running avgs for agent 1: q_loss: 46.70881271362305, p_loss: -6.030593395233154, mean_rew: -12.027962983852431, variance: 6.994293689727783, lamda: 1.7404206991195679
Running avgs for agent 2: q_loss: 54.20738220214844, p_loss: -6.046337604522705, mean_rew: -12.032887354838323, variance: 6.468782424926758, lamda: 1.8843342065811157

steps: 2349975, episodes: 94000, mean episode reward: -898.2659966650791, agent episode reward: [-299.4219988883598, -299.4219988883598, -299.4219988883598], time: 164.641
steps: 2349975, episodes: 94000, mean episode variance: 4.849768596410751, agent episode variance: [1.4934701290130614, 1.743460179567337, 1.6128382878303529], time: 164.641
Running avgs for agent 0: q_loss: 32.086143493652344, p_loss: -6.057840347290039, mean_rew: -12.028368871851457, variance: 5.973880767822266, lamda: 2.0292651653289795
Running avgs for agent 1: q_loss: 47.0246467590332, p_loss: -6.031223773956299, mean_rew: -12.00868361372814, variance: 6.973840713500977, lamda: 1.741330862045288
Running avgs for agent 2: q_loss: 54.680633544921875, p_loss: -6.048048496246338, mean_rew: -12.013594787499109, variance: 6.451353549957275, lamda: 1.8864021301269531

steps: 2374975, episodes: 95000, mean episode reward: -903.1372280926577, agent episode reward: [-301.04574269755256, -301.04574269755256, -301.04574269755256], time: 163.311
steps: 2374975, episodes: 95000, mean episode variance: 4.847969189882279, agent episode variance: [1.4988332970142364, 1.7418338027000426, 1.6073020901679993], time: 163.311
Running avgs for agent 0: q_loss: 54.80526351928711, p_loss: -6.0455098152160645, mean_rew: -12.023064076581951, variance: 5.995333194732666, lamda: 2.0346102714538574
Running avgs for agent 1: q_loss: 46.4388542175293, p_loss: -6.035035133361816, mean_rew: -12.033329760853388, variance: 6.967334747314453, lamda: 1.7431601285934448
Running avgs for agent 2: q_loss: 53.47600173950195, p_loss: -6.035348415374756, mean_rew: -12.018210886179482, variance: 6.429207801818848, lamda: 1.889195442199707

steps: 2399975, episodes: 96000, mean episode reward: -910.9909451770495, agent episode reward: [-303.66364839234984, -303.66364839234984, -303.66364839234984], time: 155.947
steps: 2399975, episodes: 96000, mean episode variance: 4.839605026483536, agent episode variance: [1.4842213151454926, 1.738580438375473, 1.61680327296257], time: 155.947
Running avgs for agent 0: q_loss: 56.7015495300293, p_loss: -6.047881126403809, mean_rew: -12.039191908127744, variance: 5.936885356903076, lamda: 2.0362884998321533
Running avgs for agent 1: q_loss: 46.91187286376953, p_loss: -6.038395404815674, mean_rew: -12.032491369427026, variance: 6.954321384429932, lamda: 1.7450617551803589
Running avgs for agent 2: q_loss: 54.294891357421875, p_loss: -6.044559955596924, mean_rew: -12.018414604952984, variance: 6.467213153839111, lamda: 1.891313910484314

steps: 2424975, episodes: 97000, mean episode reward: -897.1991255923041, agent episode reward: [-299.0663751974347, -299.0663751974347, -299.0663751974347], time: 157.941
steps: 2424975, episodes: 97000, mean episode variance: 4.840498483896256, agent episode variance: [1.484270257949829, 1.7422754917144776, 1.6139527342319488], time: 157.941
Running avgs for agent 0: q_loss: 57.09162521362305, p_loss: -6.027474403381348, mean_rew: -12.000520204784284, variance: 5.9370808601379395, lamda: 2.0374836921691895
Running avgs for agent 1: q_loss: 47.0771598815918, p_loss: -6.028112411499023, mean_rew: -12.021830375675872, variance: 6.969101905822754, lamda: 1.7475905418395996
Running avgs for agent 2: q_loss: 54.24795150756836, p_loss: -6.057528495788574, mean_rew: -12.045832976740403, variance: 6.455811023712158, lamda: 1.8944075107574463

steps: 2449975, episodes: 98000, mean episode reward: -895.5380521415822, agent episode reward: [-298.51268404719406, -298.51268404719406, -298.51268404719406], time: 159.839
steps: 2449975, episodes: 98000, mean episode variance: 4.784153286695481, agent episode variance: [1.4782239542007447, 1.727811531305313, 1.5781178011894226], time: 159.839
Running avgs for agent 0: q_loss: 46.24287033081055, p_loss: -6.039368152618408, mean_rew: -12.016693999510931, variance: 5.912895679473877, lamda: 2.041679859161377
Running avgs for agent 1: q_loss: 45.9895133972168, p_loss: -6.027386665344238, mean_rew: -12.018450575198036, variance: 6.911246299743652, lamda: 1.749193549156189
Running avgs for agent 2: q_loss: 34.19233703613281, p_loss: -6.050750255584717, mean_rew: -12.011495838192124, variance: 6.312471866607666, lamda: 1.903064250946045

steps: 2474975, episodes: 99000, mean episode reward: -902.8354819544552, agent episode reward: [-300.945160651485, -300.945160651485, -300.945160651485], time: 157.3
steps: 2474975, episodes: 99000, mean episode variance: 4.793327993154525, agent episode variance: [1.4755958404541016, 1.7448126657009124, 1.5729194869995118], time: 157.301
Running avgs for agent 0: q_loss: 36.586509704589844, p_loss: -6.046535015106201, mean_rew: -12.01378672290898, variance: 5.902383804321289, lamda: 2.0521678924560547
Running avgs for agent 1: q_loss: 47.001121520996094, p_loss: -6.031684398651123, mean_rew: -12.024846296954232, variance: 6.979250431060791, lamda: 1.7506946325302124
Running avgs for agent 2: q_loss: 39.39872741699219, p_loss: -6.043013572692871, mean_rew: -12.009239019225967, variance: 6.291677951812744, lamda: 1.915602445602417

steps: 2499975, episodes: 100000, mean episode reward: -915.3406546216812, agent episode reward: [-305.11355154056037, -305.11355154056037, -305.11355154056037], time: 158.052
steps: 2499975, episodes: 100000, mean episode variance: 4.796747016787529, agent episode variance: [1.4618103669881821, 1.7571773600578309, 1.577759289741516], time: 158.052
Running avgs for agent 0: q_loss: 52.20696258544922, p_loss: -6.052150249481201, mean_rew: -12.038432769206201, variance: 5.847241401672363, lamda: 2.0609872341156006
Running avgs for agent 1: q_loss: 46.54178237915039, p_loss: -6.0322346687316895, mean_rew: -12.014686085192476, variance: 7.028709888458252, lamda: 1.7530426979064941
Running avgs for agent 2: q_loss: 54.002662658691406, p_loss: -6.041512489318848, mean_rew: -12.021521650277402, variance: 6.311037063598633, lamda: 1.9269412755966187

steps: 2524975, episodes: 101000, mean episode reward: -915.7862042006119, agent episode reward: [-305.2620680668706, -305.2620680668706, -305.2620680668706], time: 158.468
steps: 2524975, episodes: 101000, mean episode variance: 4.780702902555466, agent episode variance: [1.4609972078800202, 1.7318598046302796, 1.587845890045166], time: 158.468
Running avgs for agent 0: q_loss: 56.458988189697266, p_loss: -6.035693168640137, mean_rew: -12.008836642987047, variance: 5.84398889541626, lamda: 2.0655570030212402
Running avgs for agent 1: q_loss: 45.571693420410156, p_loss: -6.0222039222717285, mean_rew: -11.996286086690622, variance: 6.9274396896362305, lamda: 1.7545310258865356
Running avgs for agent 2: q_loss: 54.084163665771484, p_loss: -6.03602409362793, mean_rew: -11.999719493084855, variance: 6.351383209228516, lamda: 1.9300942420959473

steps: 2549975, episodes: 102000, mean episode reward: -912.672434413343, agent episode reward: [-304.2241448044476, -304.2241448044476, -304.2241448044476], time: 158.842
steps: 2549975, episodes: 102000, mean episode variance: 4.761050013542175, agent episode variance: [1.464089940071106, 1.71393381690979, 1.5830262565612794], time: 158.842
Running avgs for agent 0: q_loss: 56.19712448120117, p_loss: -6.044482231140137, mean_rew: -12.021518209812404, variance: 5.856359958648682, lamda: 2.0666651725769043
Running avgs for agent 1: q_loss: 45.892860412597656, p_loss: -6.037893772125244, mean_rew: -12.017074141414984, variance: 6.8557353019714355, lamda: 1.755907416343689
Running avgs for agent 2: q_loss: 53.8331413269043, p_loss: -6.033429145812988, mean_rew: -12.006804089749279, variance: 6.332104682922363, lamda: 1.932543158531189

steps: 2574975, episodes: 103000, mean episode reward: -884.7971346651028, agent episode reward: [-294.9323782217009, -294.9323782217009, -294.9323782217009], time: 155.547
steps: 2574975, episodes: 103000, mean episode variance: 4.747482583522797, agent episode variance: [1.458363683462143, 1.727915217399597, 1.5612036826610565], time: 155.548
Running avgs for agent 0: q_loss: 36.161075592041016, p_loss: -6.025110721588135, mean_rew: -12.000213488501704, variance: 5.8334550857543945, lamda: 2.0706241130828857
Running avgs for agent 1: q_loss: 46.306053161621094, p_loss: -6.033916473388672, mean_rew: -12.01672313762832, variance: 6.911661148071289, lamda: 1.757712721824646
Running avgs for agent 2: q_loss: 54.174678802490234, p_loss: -6.0322980880737305, mean_rew: -12.00284735273049, variance: 6.244814395904541, lamda: 1.9351972341537476

steps: 2599975, episodes: 104000, mean episode reward: -898.6398913728336, agent episode reward: [-299.5466304576112, -299.5466304576112, -299.5466304576112], time: 155.927
steps: 2599975, episodes: 104000, mean episode variance: 4.756927566766739, agent episode variance: [1.4659532041549683, 1.7062517609596253, 1.5847226016521454], time: 155.928
Running avgs for agent 0: q_loss: 50.328269958496094, p_loss: -6.030381202697754, mean_rew: -12.002736359863489, variance: 5.863813400268555, lamda: 2.0822391510009766
Running avgs for agent 1: q_loss: 46.053985595703125, p_loss: -6.030704975128174, mean_rew: -12.010131372380293, variance: 6.82500696182251, lamda: 1.7599387168884277
Running avgs for agent 2: q_loss: 54.44681167602539, p_loss: -6.037428855895996, mean_rew: -12.014476885758029, variance: 6.338890075683594, lamda: 1.937781810760498

steps: 2624975, episodes: 105000, mean episode reward: -889.4109183580728, agent episode reward: [-296.4703061193576, -296.4703061193576, -296.4703061193576], time: 166.424
steps: 2624975, episodes: 105000, mean episode variance: 4.722795203208923, agent episode variance: [1.4466268079280853, 1.7239955656528474, 1.5521728296279906], time: 166.425
Running avgs for agent 0: q_loss: 56.95087432861328, p_loss: -6.034315586090088, mean_rew: -11.999354854749624, variance: 5.7865071296691895, lamda: 2.085023880004883
Running avgs for agent 1: q_loss: 42.496803283691406, p_loss: -6.030145168304443, mean_rew: -12.010278446656061, variance: 6.895982265472412, lamda: 1.7624157667160034
Running avgs for agent 2: q_loss: 54.84917449951172, p_loss: -6.038417816162109, mean_rew: -11.990538831093483, variance: 6.208691596984863, lamda: 1.9410786628723145

steps: 2649975, episodes: 106000, mean episode reward: -870.5595807480437, agent episode reward: [-290.1865269160146, -290.1865269160146, -290.1865269160146], time: 162.821
steps: 2649975, episodes: 106000, mean episode variance: 4.697905372381211, agent episode variance: [1.4350178277492522, 1.7093676381111145, 1.5535199065208436], time: 162.822
Running avgs for agent 0: q_loss: 48.06485366821289, p_loss: -6.032629013061523, mean_rew: -11.984520660031212, variance: 5.7400712966918945, lamda: 2.089331865310669
Running avgs for agent 1: q_loss: 32.73398971557617, p_loss: -6.001918792724609, mean_rew: -11.959009914046648, variance: 6.837470531463623, lamda: 1.7743459939956665
Running avgs for agent 2: q_loss: 54.346229553222656, p_loss: -6.020174980163574, mean_rew: -11.971870391228968, variance: 6.2140793800354, lamda: 1.943849802017212

steps: 2674975, episodes: 107000, mean episode reward: -840.1530633111174, agent episode reward: [-280.05102110370586, -280.05102110370586, -280.05102110370586], time: 167.963
steps: 2674975, episodes: 107000, mean episode variance: 4.664249670028687, agent episode variance: [1.437631308555603, 1.6834940662384033, 1.5431242952346802], time: 167.963
Running avgs for agent 0: q_loss: 34.278324127197266, p_loss: -6.0047831535339355, mean_rew: -11.936389440584975, variance: 5.750524997711182, lamda: 2.100456953048706
Running avgs for agent 1: q_loss: 32.52305221557617, p_loss: -6.021191596984863, mean_rew: -11.949444021548947, variance: 6.733975887298584, lamda: 1.7892663478851318
Running avgs for agent 2: q_loss: 54.4945068359375, p_loss: -6.009983539581299, mean_rew: -11.939452714889372, variance: 6.172497272491455, lamda: 1.9476425647735596

steps: 2699975, episodes: 108000, mean episode reward: -861.4736592527634, agent episode reward: [-287.15788641758775, -287.15788641758775, -287.15788641758775], time: 154.499
steps: 2699975, episodes: 108000, mean episode variance: 4.629338303089142, agent episode variance: [1.4254043288230895, 1.645032442331314, 1.5589015319347381], time: 154.5
Running avgs for agent 0: q_loss: 31.4680233001709, p_loss: -6.007193088531494, mean_rew: -11.938905925846859, variance: 5.701617240905762, lamda: 2.1073226928710938
Running avgs for agent 1: q_loss: 40.00839614868164, p_loss: -6.004389762878418, mean_rew: -11.929375007386406, variance: 6.580130100250244, lamda: 1.802809238433838
Running avgs for agent 2: q_loss: 54.85847091674805, p_loss: -6.013541221618652, mean_rew: -11.935491510297236, variance: 6.2356061935424805, lamda: 1.951573133468628

steps: 2724975, episodes: 109000, mean episode reward: -864.3264231542021, agent episode reward: [-288.10880771806734, -288.10880771806734, -288.10880771806734], time: 153.042
steps: 2724975, episodes: 109000, mean episode variance: 4.65197373342514, agent episode variance: [1.4238273689746856, 1.667464524269104, 1.5606818401813507], time: 153.042
Running avgs for agent 0: q_loss: 41.98583221435547, p_loss: -5.993499279022217, mean_rew: -11.908615747025614, variance: 5.695309638977051, lamda: 2.1184775829315186
Running avgs for agent 1: q_loss: 46.033546447753906, p_loss: -5.985309600830078, mean_rew: -11.913099687173416, variance: 6.669857978820801, lamda: 1.8116322755813599
Running avgs for agent 2: q_loss: 54.39701461791992, p_loss: -6.01270055770874, mean_rew: -11.949208983605432, variance: 6.242727756500244, lamda: 1.9552223682403564

steps: 2749975, episodes: 110000, mean episode reward: -835.6953255034306, agent episode reward: [-278.5651085011435, -278.5651085011435, -278.5651085011435], time: 153.532
steps: 2749975, episodes: 110000, mean episode variance: 4.611869542598725, agent episode variance: [1.4082480721473694, 1.6447700119018556, 1.5588514585494995], time: 153.533
Running avgs for agent 0: q_loss: 31.97911262512207, p_loss: -6.002579212188721, mean_rew: -11.917550303919503, variance: 5.632992267608643, lamda: 2.130535125732422
Running avgs for agent 1: q_loss: 44.70061492919922, p_loss: -6.002155303955078, mean_rew: -11.915538134690141, variance: 6.579080104827881, lamda: 1.8146966695785522
Running avgs for agent 2: q_loss: 52.94679641723633, p_loss: -5.998805522918701, mean_rew: -11.928545886674144, variance: 6.235406398773193, lamda: 1.9590966701507568

steps: 2774975, episodes: 111000, mean episode reward: -835.8573181812968, agent episode reward: [-278.6191060604323, -278.6191060604323, -278.6191060604323], time: 156.941
steps: 2774975, episodes: 111000, mean episode variance: 4.572914968013763, agent episode variance: [1.4063750128746033, 1.6483312938213348, 1.5182086613178254], time: 156.942
Running avgs for agent 0: q_loss: 32.73560333251953, p_loss: -5.984739780426025, mean_rew: -11.891679522849838, variance: 5.625500202178955, lamda: 2.135711908340454
Running avgs for agent 1: q_loss: 45.82501983642578, p_loss: -5.977453231811523, mean_rew: -11.879473311011363, variance: 6.593325138092041, lamda: 1.8181350231170654
Running avgs for agent 2: q_loss: 51.2716178894043, p_loss: -5.985038757324219, mean_rew: -11.880944481974641, variance: 6.0728349685668945, lamda: 1.9731786251068115

steps: 2799975, episodes: 112000, mean episode reward: -853.0927343599406, agent episode reward: [-284.36424478664685, -284.36424478664685, -284.36424478664685], time: 156.739
steps: 2799975, episodes: 112000, mean episode variance: 4.537662923812866, agent episode variance: [1.401724725008011, 1.6408261086940765, 1.4951120901107788], time: 156.739
Running avgs for agent 0: q_loss: 38.318641662597656, p_loss: -5.9628376960754395, mean_rew: -11.859991292602313, variance: 5.606899261474609, lamda: 2.1437699794769287
Running avgs for agent 1: q_loss: 43.628929138183594, p_loss: -5.966282367706299, mean_rew: -11.852598406713124, variance: 6.563304424285889, lamda: 1.8202179670333862
Running avgs for agent 2: q_loss: 54.7256965637207, p_loss: -5.984160900115967, mean_rew: -11.87024491351532, variance: 5.980448246002197, lamda: 1.978810429573059

steps: 2824975, episodes: 113000, mean episode reward: -863.2894363386185, agent episode reward: [-287.7631454462061, -287.7631454462061, -287.7631454462061], time: 156.778
steps: 2824975, episodes: 113000, mean episode variance: 4.525061870336533, agent episode variance: [1.385932998418808, 1.6406931869983674, 1.4984356849193572], time: 156.778
Running avgs for agent 0: q_loss: 36.56089401245117, p_loss: -5.966036796569824, mean_rew: -11.856223611670028, variance: 5.543732166290283, lamda: 2.159029483795166
Running avgs for agent 1: q_loss: 44.73207473754883, p_loss: -5.970539569854736, mean_rew: -11.857533450843732, variance: 6.562772274017334, lamda: 1.8211933374404907
Running avgs for agent 2: q_loss: 55.04423522949219, p_loss: -5.9799699783325195, mean_rew: -11.859205259247174, variance: 5.993742942810059, lamda: 1.9831286668777466

steps: 2849975, episodes: 114000, mean episode reward: -821.2371895823998, agent episode reward: [-273.7457298607999, -273.7457298607999, -273.7457298607999], time: 160.471
steps: 2849975, episodes: 114000, mean episode variance: 4.526782960414886, agent episode variance: [1.370944411277771, 1.629379158258438, 1.5264593908786774], time: 160.472
Running avgs for agent 0: q_loss: 58.09648895263672, p_loss: -5.956439971923828, mean_rew: -11.839791642216175, variance: 5.4837775230407715, lamda: 2.167479991912842
Running avgs for agent 1: q_loss: 45.0500373840332, p_loss: -5.972742557525635, mean_rew: -11.854214164100155, variance: 6.517516613006592, lamda: 1.8233153820037842
Running avgs for agent 2: q_loss: 54.80971145629883, p_loss: -5.969116687774658, mean_rew: -11.856169176182465, variance: 6.105837345123291, lamda: 1.9876046180725098

steps: 2874975, episodes: 115000, mean episode reward: -842.835117278555, agent episode reward: [-280.9450390928517, -280.9450390928517, -280.9450390928517], time: 159.17
steps: 2874975, episodes: 115000, mean episode variance: 4.51502656173706, agent episode variance: [1.3698348400592804, 1.63906698179245, 1.5061247398853301], time: 159.17
Running avgs for agent 0: q_loss: 56.04865646362305, p_loss: -5.93641996383667, mean_rew: -11.810010819532968, variance: 5.479339122772217, lamda: 2.1686830520629883
Running avgs for agent 1: q_loss: 44.9269905090332, p_loss: -5.954736709594727, mean_rew: -11.828302092445691, variance: 6.556268215179443, lamda: 1.824865460395813
Running avgs for agent 2: q_loss: 47.373287200927734, p_loss: -5.960180759429932, mean_rew: -11.817798197781947, variance: 6.02449893951416, lamda: 1.992504358291626

steps: 2899975, episodes: 116000, mean episode reward: -831.8551280315937, agent episode reward: [-277.2850426771979, -277.2850426771979, -277.2850426771979], time: 156.933
steps: 2899975, episodes: 116000, mean episode variance: 4.497652574300766, agent episode variance: [1.3839606895446777, 1.6345515241622925, 1.4791403605937958], time: 156.934
Running avgs for agent 0: q_loss: 56.11214065551758, p_loss: -5.937980651855469, mean_rew: -11.818050892287951, variance: 5.5358428955078125, lamda: 2.170747995376587
Running avgs for agent 1: q_loss: 45.16565704345703, p_loss: -5.951278209686279, mean_rew: -11.810475408749195, variance: 6.538206100463867, lamda: 1.8270671367645264
Running avgs for agent 2: q_loss: 46.43563461303711, p_loss: -5.964824199676514, mean_rew: -11.829656629757356, variance: 5.916561603546143, lamda: 2.005659341812134

steps: 2924975, episodes: 117000, mean episode reward: -844.9391105856865, agent episode reward: [-281.6463701952288, -281.6463701952288, -281.6463701952288], time: 157.597
steps: 2924975, episodes: 117000, mean episode variance: 4.462488550662995, agent episode variance: [1.369444642305374, 1.6081678247451783, 1.484876083612442], time: 157.597
Running avgs for agent 0: q_loss: 37.265953063964844, p_loss: -5.940034866333008, mean_rew: -11.796489583558614, variance: 5.477778911590576, lamda: 2.1770424842834473
Running avgs for agent 1: q_loss: 44.40956497192383, p_loss: -5.934172630310059, mean_rew: -11.788940521491094, variance: 6.432671070098877, lamda: 1.8295938968658447
Running avgs for agent 2: q_loss: 55.34382629394531, p_loss: -5.929267406463623, mean_rew: -11.775396763925917, variance: 5.939504623413086, lamda: 2.012742519378662

steps: 2949975, episodes: 118000, mean episode reward: -842.6312647215682, agent episode reward: [-280.87708824052277, -280.87708824052277, -280.87708824052277], time: 162.47
steps: 2949975, episodes: 118000, mean episode variance: 4.437738176822663, agent episode variance: [1.355567692041397, 1.6238549995422362, 1.458315485239029], time: 162.471
Running avgs for agent 0: q_loss: 33.09307098388672, p_loss: -5.927699089050293, mean_rew: -11.776048181044821, variance: 5.422270774841309, lamda: 2.1879327297210693
Running avgs for agent 1: q_loss: 45.05281066894531, p_loss: -5.945239067077637, mean_rew: -11.784151464891158, variance: 6.495419979095459, lamda: 1.831934928894043
Running avgs for agent 2: q_loss: 54.471107482910156, p_loss: -5.942821025848389, mean_rew: -11.784908605573325, variance: 5.833261489868164, lamda: 2.0165622234344482

steps: 2974975, episodes: 119000, mean episode reward: -851.9524462493654, agent episode reward: [-283.9841487497884, -283.9841487497884, -283.9841487497884], time: 163.783
steps: 2974975, episodes: 119000, mean episode variance: 4.429240108966828, agent episode variance: [1.3530181407928468, 1.6095010180473328, 1.466720950126648], time: 163.783
Running avgs for agent 0: q_loss: 34.832637786865234, p_loss: -5.92885160446167, mean_rew: -11.77537552285835, variance: 5.41207218170166, lamda: 2.1958260536193848
Running avgs for agent 1: q_loss: 44.69552993774414, p_loss: -5.92886209487915, mean_rew: -11.763155965143971, variance: 6.438004493713379, lamda: 1.8338488340377808
Running avgs for agent 2: q_loss: 56.009281158447266, p_loss: -5.936008930206299, mean_rew: -11.764476513128482, variance: 5.866883754730225, lamda: 2.0210072994232178

steps: 2999975, episodes: 120000, mean episode reward: -838.355083311554, agent episode reward: [-279.4516944371846, -279.4516944371846, -279.4516944371846], time: 163.573
steps: 2999975, episodes: 120000, mean episode variance: 4.417097887039184, agent episode variance: [1.3388662712574004, 1.6136010258197784, 1.4646305899620056], time: 163.574
Running avgs for agent 0: q_loss: 35.475013732910156, p_loss: -5.9145283699035645, mean_rew: -11.757897992424704, variance: 5.355464935302734, lamda: 2.2089908123016357
Running avgs for agent 1: q_loss: 41.1270866394043, p_loss: -5.9146199226379395, mean_rew: -11.744434640128096, variance: 6.454404354095459, lamda: 1.842069387435913
Running avgs for agent 2: q_loss: 54.50971984863281, p_loss: -5.910154819488525, mean_rew: -11.742799923202634, variance: 5.858522415161133, lamda: 2.0280518531799316

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -843.3340084890086, agent episode reward: [-281.1113361630029, -281.1113361630029, -281.1113361630029], time: 143.598
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 143.598
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -844.4357541249343, agent episode reward: [-281.4785847083115, -281.4785847083115, -281.4785847083115], time: 153.554
steps: 49975, episodes: 2000, mean episode variance: 4.721868223667145, agent episode variance: [1.2043107233047485, 1.6185902771949767, 1.8989672231674195], time: 153.554
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -11.233839075728083, variance: 4.935699939727783, lamda: 2.2173192501068115
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -11.24596159764628, variance: 6.633566856384277, lamda: 1.8498632907867432
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -11.250298626722412, variance: 7.782652854919434, lamda: 2.0346035957336426

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567759473.1090574: line 9: --exp_var_alpha: command not found
