# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation False --constrained \
    False --exp-name MADDPG_simple_spread_uncons_01 --save-dir \
    models/simple_spread_policies5/01-non-linear-uncons/ --plots-dir \
    models/simple_spread_policies5/01-non-linear-uncons/
Job <1084093> is submitted to queue <x86_6h>.
arglist.u_estimation False
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -524.40135556657, agent episode reward: [-174.80045185552333, -174.80045185552333, -174.80045185552333], time: 49.1
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 49.101
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -566.0421796757013, agent episode reward: [-188.68072655856707, -188.68072655856707, -188.68072655856707], time: 73.277
steps: 49975, episodes: 2000, mean episode variance: 2.3415449340194465, agent episode variance: [0.6570827020853758, 0.7243687202632427, 0.9600935116708279], time: 73.278
Running avgs for agent 0: q_loss: 1.9215636253356934, p_loss: 10.772544860839844, mean_rew: -7.28895782841172, variance: 2.692961893792524, mean_q: -10.849425315856934, std_q: 2.813657522201538
Running avgs for agent 1: q_loss: 2.2348549365997314, p_loss: 10.783823013305664, mean_rew: -7.285790140209223, variance: 2.9687242633739457, mean_q: -10.8688325881958, std_q: 2.869110345840454
Running avgs for agent 2: q_loss: 2.132750988006592, p_loss: 10.71798324584961, mean_rew: -7.289023093518137, variance: 3.9348094740607698, mean_q: -10.78426456451416, std_q: 2.722470760345459

steps: 74975, episodes: 3000, mean episode reward: -468.53579096675514, agent episode reward: [-156.17859698891837, -156.17859698891837, -156.17859698891837], time: 72.202
steps: 74975, episodes: 3000, mean episode variance: 5.2102626329958435, agent episode variance: [1.8437672035098076, 1.8332576284408568, 1.5332378010451793], time: 72.202
Running avgs for agent 0: q_loss: 1.1742658615112305, p_loss: 22.10231590270996, mean_rew: -7.1180998288418555, variance: 7.37506881403923, mean_q: -22.25833511352539, std_q: 5.634334564208984
Running avgs for agent 1: q_loss: 1.1011176109313965, p_loss: 22.1182918548584, mean_rew: -7.112091238841351, variance: 7.333030513763427, mean_q: -22.26829719543457, std_q: 5.597753047943115
Running avgs for agent 2: q_loss: 1.0860521793365479, p_loss: 22.377775192260742, mean_rew: -7.1114457351594105, variance: 6.132951204180717, mean_q: -22.552379608154297, std_q: 5.575756072998047

steps: 99975, episodes: 4000, mean episode reward: -444.12163091529965, agent episode reward: [-148.0405436384332, -148.0405436384332, -148.0405436384332], time: 71.876
steps: 99975, episodes: 4000, mean episode variance: 10.47622310256958, agent episode variance: [3.812236466884613, 3.358085137844086, 3.3059014978408814], time: 71.877
Running avgs for agent 0: q_loss: 1.363450527191162, p_loss: 32.411380767822266, mean_rew: -6.802571518058116, variance: 15.248945867538453, mean_q: -32.66183853149414, std_q: 8.355423927307129
Running avgs for agent 1: q_loss: 1.2055432796478271, p_loss: 32.51824951171875, mean_rew: -6.798246226972687, variance: 13.432340551376344, mean_q: -32.77265548706055, std_q: 8.078640937805176
Running avgs for agent 2: q_loss: 1.1748818159103394, p_loss: 32.7009391784668, mean_rew: -6.794692865236285, variance: 13.223605991363526, mean_q: -33.027774810791016, std_q: 8.03662109375

steps: 124975, episodes: 5000, mean episode reward: -402.77128712720287, agent episode reward: [-134.25709570906764, -134.25709570906764, -134.25709570906764], time: 79.077
steps: 124975, episodes: 5000, mean episode variance: 15.521521933078766, agent episode variance: [5.851178453445435, 5.185993560791015, 4.484349918842316], time: 79.077
Running avgs for agent 0: q_loss: 1.5107219219207764, p_loss: 40.64521026611328, mean_rew: -6.5472494528779785, variance: 23.40471381378174, mean_q: -41.023990631103516, std_q: 10.050494194030762
Running avgs for agent 1: q_loss: 1.3564910888671875, p_loss: 40.849998474121094, mean_rew: -6.54849621396023, variance: 20.74397424316406, mean_q: -41.21243667602539, std_q: 9.778770446777344
Running avgs for agent 2: q_loss: 1.3119618892669678, p_loss: 41.026859283447266, mean_rew: -6.557781289628249, variance: 17.937399675369264, mean_q: -41.45602035522461, std_q: 9.851167678833008

steps: 149975, episodes: 6000, mean episode reward: -379.8937185060213, agent episode reward: [-126.63123950200712, -126.63123950200712, -126.63123950200712], time: 74.197
steps: 149975, episodes: 6000, mean episode variance: 19.5476630743742, agent episode variance: [6.855810926318169, 6.584734080314636, 6.107118067741394], time: 74.198
Running avgs for agent 0: q_loss: 1.6858680248260498, p_loss: 47.1056022644043, mean_rew: -6.299531157478314, variance: 27.423243705272675, mean_q: -47.58363723754883, std_q: 11.169282913208008
Running avgs for agent 1: q_loss: 1.5117404460906982, p_loss: 47.229576110839844, mean_rew: -6.301439615716616, variance: 26.338936321258544, mean_q: -47.6568603515625, std_q: 10.944374084472656
Running avgs for agent 2: q_loss: 1.4502050876617432, p_loss: 47.35645294189453, mean_rew: -6.301270864071694, variance: 24.428472270965575, mean_q: -47.84996032714844, std_q: 10.823199272155762

steps: 174975, episodes: 7000, mean episode reward: -370.3257130691725, agent episode reward: [-123.44190435639084, -123.44190435639084, -123.44190435639084], time: 74.708
steps: 174975, episodes: 7000, mean episode variance: 22.37815927028656, agent episode variance: [7.924882767677307, 7.253209531784058, 7.200066970825195], time: 74.709
Running avgs for agent 0: q_loss: 1.832015872001648, p_loss: 52.19092559814453, mean_rew: -6.09682527712757, variance: 31.699531070709227, mean_q: -52.70362091064453, std_q: 11.88853645324707
Running avgs for agent 1: q_loss: 1.6326918601989746, p_loss: 52.280574798583984, mean_rew: -6.100907689962891, variance: 29.01283812713623, mean_q: -52.74110794067383, std_q: 11.664876937866211
Running avgs for agent 2: q_loss: 1.58671236038208, p_loss: 52.495601654052734, mean_rew: -6.103065533053456, variance: 28.80026788330078, mean_q: -52.99637985229492, std_q: 11.434492111206055

steps: 199975, episodes: 8000, mean episode reward: -366.7377948053305, agent episode reward: [-122.24593160177683, -122.24593160177683, -122.24593160177683], time: 71.709
steps: 199975, episodes: 8000, mean episode variance: 26.589065311431884, agent episode variance: [9.452577743530274, 8.588244594573975, 8.548242973327637], time: 71.709
Running avgs for agent 0: q_loss: 1.8686784505844116, p_loss: 56.471710205078125, mean_rew: -5.948576694137534, variance: 37.810310974121094, mean_q: -57.008460998535156, std_q: 12.326008796691895
Running avgs for agent 1: q_loss: 1.7457671165466309, p_loss: 56.375526428222656, mean_rew: -5.945201063847984, variance: 34.3529783782959, mean_q: -56.83388137817383, std_q: 11.977624893188477
Running avgs for agent 2: q_loss: 1.6454178094863892, p_loss: 56.588314056396484, mean_rew: -5.944373911619909, variance: 34.19297189331055, mean_q: -57.06571578979492, std_q: 11.590632438659668

steps: 224975, episodes: 9000, mean episode reward: -357.88696920778807, agent episode reward: [-119.29565640259604, -119.29565640259604, -119.29565640259604], time: 71.842
steps: 224975, episodes: 9000, mean episode variance: 27.72205967569351, agent episode variance: [10.36485291671753, 8.431203169345856, 8.926003589630128], time: 71.843
Running avgs for agent 0: q_loss: 1.920458197593689, p_loss: 59.839969635009766, mean_rew: -5.816101028141546, variance: 41.45941166687012, mean_q: -60.382835388183594, std_q: 12.311474800109863
Running avgs for agent 1: q_loss: 1.882344126701355, p_loss: 59.81299591064453, mean_rew: -5.812181573153825, variance: 33.724812677383426, mean_q: -60.259803771972656, std_q: 12.15383529663086
Running avgs for agent 2: q_loss: 1.6882261037826538, p_loss: 60.013736724853516, mean_rew: -5.808270898634108, variance: 35.70401435852051, mean_q: -60.47243118286133, std_q: 11.787834167480469

steps: 249975, episodes: 10000, mean episode reward: -357.05913710119506, agent episode reward: [-119.019712367065, -119.019712367065, -119.019712367065], time: 72.273
steps: 249975, episodes: 10000, mean episode variance: 30.24596675300598, agent episode variance: [10.941204668045044, 9.604944664001465, 9.699817420959473], time: 72.274
Running avgs for agent 0: q_loss: 2.0030946731567383, p_loss: 62.64154815673828, mean_rew: -5.709341139900756, variance: 43.764818672180176, mean_q: -63.17256546020508, std_q: 12.28782844543457
Running avgs for agent 1: q_loss: 1.9226055145263672, p_loss: 62.73099899291992, mean_rew: -5.704757635080925, variance: 38.41977865600586, mean_q: -63.159027099609375, std_q: 12.239964485168457
Running avgs for agent 2: q_loss: 1.7281370162963867, p_loss: 62.942630767822266, mean_rew: -5.709064158087712, variance: 38.79926968383789, mean_q: -63.4108772277832, std_q: 11.838761329650879

steps: 274975, episodes: 11000, mean episode reward: -349.9594635565579, agent episode reward: [-116.65315451885263, -116.65315451885263, -116.65315451885263], time: 72.578
steps: 274975, episodes: 11000, mean episode variance: 31.64549872684479, agent episode variance: [11.329394676208496, 9.88359326839447, 10.432510782241822], time: 72.578
Running avgs for agent 0: q_loss: 2.0423452854156494, p_loss: 65.0055160522461, mean_rew: -5.610984928098846, variance: 45.317578704833984, mean_q: -65.52739715576172, std_q: 12.385967254638672
Running avgs for agent 1: q_loss: 1.9740527868270874, p_loss: 65.22224426269531, mean_rew: -5.609120065981626, variance: 39.53437307357788, mean_q: -65.62362670898438, std_q: 12.153536796569824
Running avgs for agent 2: q_loss: 1.7513542175292969, p_loss: 65.28070831298828, mean_rew: -5.606495248234487, variance: 41.73004312896729, mean_q: -65.74221801757812, std_q: 11.680951118469238

steps: 299975, episodes: 12000, mean episode reward: -346.33102165431416, agent episode reward: [-115.44367388477136, -115.44367388477136, -115.44367388477136], time: 72.631
steps: 299975, episodes: 12000, mean episode variance: 32.989760994434356, agent episode variance: [11.326105339050294, 10.776752044677734, 10.88690361070633], time: 72.632
Running avgs for agent 0: q_loss: 2.0310990810394287, p_loss: 66.99220275878906, mean_rew: -5.523585544561774, variance: 45.304421356201175, mean_q: -67.50373077392578, std_q: 12.261082649230957
Running avgs for agent 1: q_loss: 1.9937255382537842, p_loss: 67.33631134033203, mean_rew: -5.522022174645436, variance: 43.107008178710934, mean_q: -67.71886444091797, std_q: 12.044685363769531
Running avgs for agent 2: q_loss: 1.7812690734863281, p_loss: 67.22754669189453, mean_rew: -5.527550488719921, variance: 43.54761444282532, mean_q: -67.68392181396484, std_q: 11.746051788330078

steps: 324975, episodes: 13000, mean episode reward: -344.61015277038814, agent episode reward: [-114.8700509234627, -114.8700509234627, -114.8700509234627], time: 71.573
steps: 324975, episodes: 13000, mean episode variance: 34.76170573043823, agent episode variance: [12.379844257354737, 11.465008129119873, 10.916853343963624], time: 71.573
Running avgs for agent 0: q_loss: 1.9821443557739258, p_loss: 68.58673858642578, mean_rew: -5.44924922830016, variance: 49.51937702941895, mean_q: -69.07848358154297, std_q: 11.900236129760742
Running avgs for agent 1: q_loss: 1.986899495124817, p_loss: 69.10624694824219, mean_rew: -5.451701398629244, variance: 45.86003251647949, mean_q: -69.47256469726562, std_q: 11.81151008605957
Running avgs for agent 2: q_loss: 1.767852783203125, p_loss: 68.81259155273438, mean_rew: -5.449537010618831, variance: 43.667413375854494, mean_q: -69.26441955566406, std_q: 11.64389419555664

steps: 349975, episodes: 14000, mean episode reward: -344.77626556380477, agent episode reward: [-114.9254218546016, -114.9254218546016, -114.9254218546016], time: 72.06
steps: 349975, episodes: 14000, mean episode variance: 38.79609276008606, agent episode variance: [11.959849071502685, 15.672682069778443, 11.163561618804932], time: 72.061
Running avgs for agent 0: q_loss: 1.968988299369812, p_loss: 70.07462310791016, mean_rew: -5.385973007190869, variance: 47.83939628601074, mean_q: -70.56200408935547, std_q: 11.71210765838623
Running avgs for agent 1: q_loss: 2.5688118934631348, p_loss: 70.60011291503906, mean_rew: -5.3861932099705, variance: 62.69072827911377, mean_q: -70.95787048339844, std_q: 11.683090209960938
Running avgs for agent 2: q_loss: 1.7824863195419312, p_loss: 70.1978530883789, mean_rew: -5.3972712333145205, variance: 44.65424647521973, mean_q: -70.65390014648438, std_q: 11.653902053833008

steps: 374975, episodes: 15000, mean episode reward: -348.6093395855098, agent episode reward: [-116.20311319516993, -116.20311319516993, -116.20311319516993], time: 72.275
steps: 374975, episodes: 15000, mean episode variance: 45.73574699020386, agent episode variance: [12.127528232574463, 22.060047729492187, 11.548171028137206], time: 72.276
Running avgs for agent 0: q_loss: 1.9365900754928589, p_loss: 71.3421401977539, mean_rew: -5.335343708752564, variance: 48.51011293029785, mean_q: -71.81205749511719, std_q: 11.594078063964844
Running avgs for agent 1: q_loss: 3.1081905364990234, p_loss: 71.8819808959961, mean_rew: -5.338168752867651, variance: 88.24019091796875, mean_q: -72.22438049316406, std_q: 11.352897644042969
Running avgs for agent 2: q_loss: 1.7283087968826294, p_loss: 71.29357147216797, mean_rew: -5.334022374423177, variance: 46.192684112548825, mean_q: -71.73926544189453, std_q: 11.579229354858398

steps: 399975, episodes: 16000, mean episode reward: -347.52659235323614, agent episode reward: [-115.84219745107873, -115.84219745107873, -115.84219745107873], time: 70.692
steps: 399975, episodes: 16000, mean episode variance: 44.86284054946899, agent episode variance: [12.191406906127929, 21.143727821350097, 11.527705821990967], time: 70.692
Running avgs for agent 0: q_loss: 1.8983745574951172, p_loss: 72.47118377685547, mean_rew: -5.295162508634508, variance: 48.765627624511716, mean_q: -72.92778015136719, std_q: 11.39726734161377
Running avgs for agent 1: q_loss: 3.024359703063965, p_loss: 73.03501892089844, mean_rew: -5.287148029342081, variance: 84.57491128540039, mean_q: -73.36296081542969, std_q: 11.013518333435059
Running avgs for agent 2: q_loss: 1.7659448385238647, p_loss: 72.31107330322266, mean_rew: -5.2967773759331065, variance: 46.11082328796387, mean_q: -72.74079895019531, std_q: 11.541788101196289

steps: 424975, episodes: 17000, mean episode reward: -349.91031036917076, agent episode reward: [-116.63677012305692, -116.63677012305692, -116.63677012305692], time: 69.692
steps: 424975, episodes: 17000, mean episode variance: 45.807018351078035, agent episode variance: [12.412827239990234, 21.79446366882324, 11.599727442264557], time: 69.692
Running avgs for agent 0: q_loss: 1.8796582221984863, p_loss: 73.4600601196289, mean_rew: -5.253527804226292, variance: 49.651308959960936, mean_q: -73.8925552368164, std_q: 11.284893035888672
Running avgs for agent 1: q_loss: 2.8923966884613037, p_loss: 74.04064178466797, mean_rew: -5.256687058614639, variance: 87.17785467529296, mean_q: -74.34922790527344, std_q: 10.836902618408203
Running avgs for agent 2: q_loss: 1.7058879137039185, p_loss: 73.14540100097656, mean_rew: -5.253843621569433, variance: 46.39890976905823, mean_q: -73.55523681640625, std_q: 11.200980186462402

steps: 449975, episodes: 18000, mean episode reward: -348.4260078866794, agent episode reward: [-116.14200262889312, -116.14200262889312, -116.14200262889312], time: 72.38
steps: 449975, episodes: 18000, mean episode variance: 45.92240969085693, agent episode variance: [12.536284255981446, 21.399640396118166, 11.986485038757325], time: 72.38
Running avgs for agent 0: q_loss: 1.82497239112854, p_loss: 74.34203338623047, mean_rew: -5.226118680806707, variance: 50.145137023925784, mean_q: -74.75092315673828, std_q: 11.229361534118652
Running avgs for agent 1: q_loss: 2.82737398147583, p_loss: 74.94328308105469, mean_rew: -5.21932379399211, variance: 85.59856158447266, mean_q: -75.23323059082031, std_q: 10.505769729614258
Running avgs for agent 2: q_loss: 1.6730228662490845, p_loss: 74.01583099365234, mean_rew: -5.224521786550813, variance: 47.9459401550293, mean_q: -74.40042877197266, std_q: 11.040067672729492

steps: 474975, episodes: 19000, mean episode reward: -346.79513236730486, agent episode reward: [-115.59837745576829, -115.59837745576829, -115.59837745576829], time: 73.04
steps: 474975, episodes: 19000, mean episode variance: 46.39123878479004, agent episode variance: [12.779673679351806, 21.60255875778198, 12.00900634765625], time: 73.041
Running avgs for agent 0: q_loss: 1.7990021705627441, p_loss: 75.03868865966797, mean_rew: -5.183663164330299, variance: 51.11869471740722, mean_q: -75.43273162841797, std_q: 10.977657318115234
Running avgs for agent 1: q_loss: 2.8266186714172363, p_loss: 75.68472290039062, mean_rew: -5.1964761852303925, variance: 86.41023503112793, mean_q: -75.96340942382812, std_q: 10.42365837097168
Running avgs for agent 2: q_loss: 1.6733417510986328, p_loss: 74.6240234375, mean_rew: -5.187979603179194, variance: 48.036025390625, mean_q: -74.99356842041016, std_q: 10.799208641052246

steps: 499975, episodes: 20000, mean episode reward: -347.74297120810627, agent episode reward: [-115.91432373603543, -115.91432373603543, -115.91432373603543], time: 72.653
steps: 499975, episodes: 20000, mean episode variance: 45.752119190216064, agent episode variance: [12.589530944824219, 21.094928634643555, 12.067659610748292], time: 72.653
Running avgs for agent 0: q_loss: 1.76021409034729, p_loss: 75.70802307128906, mean_rew: -5.157374876535096, variance: 50.358123779296875, mean_q: -76.0860366821289, std_q: 10.775808334350586
Running avgs for agent 1: q_loss: 2.739987373352051, p_loss: 76.34642028808594, mean_rew: -5.156103517228201, variance: 84.37971453857422, mean_q: -76.6146240234375, std_q: 10.192277908325195
Running avgs for agent 2: q_loss: 1.6372768878936768, p_loss: 75.18948364257812, mean_rew: -5.159581284678186, variance: 48.27063844299317, mean_q: -75.5393295288086, std_q: 10.579472541809082

steps: 524975, episodes: 21000, mean episode reward: -348.4108118137893, agent episode reward: [-116.13693727126312, -116.13693727126312, -116.13693727126312], time: 74.548
steps: 524975, episodes: 21000, mean episode variance: 46.49320039367676, agent episode variance: [12.877363693237305, 21.511602897644043, 12.10423380279541], time: 74.548
Running avgs for agent 0: q_loss: 1.7831132411956787, p_loss: 76.35305786132812, mean_rew: -5.141920278982759, variance: 51.50945477294922, mean_q: -76.71945190429688, std_q: 10.849257469177246
Running avgs for agent 1: q_loss: 2.7039201259613037, p_loss: 76.92892456054688, mean_rew: -5.134655307441156, variance: 86.04641159057617, mean_q: -77.18991088867188, std_q: 10.031062126159668
Running avgs for agent 2: q_loss: 1.6050007343292236, p_loss: 75.69581604003906, mean_rew: -5.135123886373556, variance: 48.41693521118164, mean_q: -76.03681182861328, std_q: 10.457324028015137

steps: 549975, episodes: 22000, mean episode reward: -346.46696696798904, agent episode reward: [-115.48898898932967, -115.48898898932967, -115.48898898932967], time: 75.127
steps: 549975, episodes: 22000, mean episode variance: 46.51993955612183, agent episode variance: [12.761071857452393, 21.271136756896972, 12.487730941772462], time: 75.127
Running avgs for agent 0: q_loss: 1.7581716775894165, p_loss: 76.86204528808594, mean_rew: -5.1156658506606, variance: 51.04428742980957, mean_q: -77.21578216552734, std_q: 10.646720886230469
Running avgs for agent 1: q_loss: 2.6785547733306885, p_loss: 77.5174560546875, mean_rew: -5.1161195422047765, variance: 85.08454702758789, mean_q: -77.77129364013672, std_q: 10.013303756713867
Running avgs for agent 2: q_loss: 1.60723876953125, p_loss: 76.1104507446289, mean_rew: -5.111659591492775, variance: 49.95092376708985, mean_q: -76.43657684326172, std_q: 10.335103988647461

steps: 574975, episodes: 23000, mean episode reward: -348.26308307634497, agent episode reward: [-116.08769435878165, -116.08769435878165, -116.08769435878165], time: 75.629
steps: 574975, episodes: 23000, mean episode variance: 46.03037883758545, agent episode variance: [12.850786808013916, 21.370093566894532, 11.809498462677002], time: 75.63
Running avgs for agent 0: q_loss: 1.752862572669983, p_loss: 77.33685302734375, mean_rew: -5.090734112664016, variance: 51.403147232055666, mean_q: -77.68196868896484, std_q: 10.50163459777832
Running avgs for agent 1: q_loss: 2.6030757427215576, p_loss: 77.97212219238281, mean_rew: -5.094195682845779, variance: 85.48037426757813, mean_q: -78.21831512451172, std_q: 9.794178009033203
Running avgs for agent 2: q_loss: 1.5883268117904663, p_loss: 76.54469299316406, mean_rew: -5.086574421547334, variance: 47.23799385070801, mean_q: -76.86495208740234, std_q: 10.144904136657715

steps: 599975, episodes: 24000, mean episode reward: -344.6659327727081, agent episode reward: [-114.88864425756937, -114.88864425756937, -114.88864425756937], time: 74.408
steps: 599975, episodes: 24000, mean episode variance: 46.15754094791412, agent episode variance: [12.024681065559387, 21.739171463012696, 12.393688419342041], time: 74.408
Running avgs for agent 0: q_loss: 1.7713371515274048, p_loss: 77.650634765625, mean_rew: -5.06592317324397, variance: 48.098724262237546, mean_q: -77.99274444580078, std_q: 10.302075386047363
Running avgs for agent 1: q_loss: 2.5736706256866455, p_loss: 78.333740234375, mean_rew: -5.0677257131533855, variance: 86.95668585205078, mean_q: -78.57144165039062, std_q: 9.666364669799805
Running avgs for agent 2: q_loss: 1.5800559520721436, p_loss: 76.89016723632812, mean_rew: -5.07395638447346, variance: 49.574753677368165, mean_q: -77.19762420654297, std_q: 9.928900718688965

steps: 624975, episodes: 25000, mean episode reward: -343.09553446677177, agent episode reward: [-114.3651781555906, -114.3651781555906, -114.3651781555906], time: 72.27
steps: 624975, episodes: 25000, mean episode variance: 45.73818288993836, agent episode variance: [12.62261134338379, 21.442474012374877, 11.673097534179687], time: 72.27
Running avgs for agent 0: q_loss: 1.74507474899292, p_loss: 77.97503662109375, mean_rew: -5.051697007257973, variance: 50.49044537353516, mean_q: -78.31209564208984, std_q: 10.304326057434082
Running avgs for agent 1: q_loss: 2.5946497917175293, p_loss: 78.59880065917969, mean_rew: -5.050077664908762, variance: 85.76989604949951, mean_q: -78.83745574951172, std_q: 9.58121109008789
Running avgs for agent 2: q_loss: 1.6022812128067017, p_loss: 77.2834701538086, mean_rew: -5.049908501859872, variance: 46.69239013671875, mean_q: -77.58320617675781, std_q: 9.822882652282715

steps: 649975, episodes: 26000, mean episode reward: -343.53588271948337, agent episode reward: [-114.51196090649447, -114.51196090649447, -114.51196090649447], time: 72.106
steps: 649975, episodes: 26000, mean episode variance: 46.65344910812378, agent episode variance: [12.969665725708008, 21.434149696350097, 12.249633686065675], time: 72.106
Running avgs for agent 0: q_loss: 1.7241957187652588, p_loss: 78.173095703125, mean_rew: -5.031418381215764, variance: 51.87866290283203, mean_q: -78.50774383544922, std_q: 10.276350021362305
Running avgs for agent 1: q_loss: 2.5228238105773926, p_loss: 78.83171081542969, mean_rew: -5.0275427942440185, variance: 85.73659878540039, mean_q: -79.0621109008789, std_q: 9.567023277282715
Running avgs for agent 2: q_loss: 1.5623688697814941, p_loss: 77.55924987792969, mean_rew: -5.038630984878342, variance: 48.9985347442627, mean_q: -77.8534927368164, std_q: 9.784507751464844

steps: 674975, episodes: 27000, mean episode reward: -340.75211756031064, agent episode reward: [-113.5840391867702, -113.5840391867702, -113.5840391867702], time: 72.064
steps: 674975, episodes: 27000, mean episode variance: 45.713766193389894, agent episode variance: [12.724733032226563, 21.21129960632324, 11.777733554840088], time: 72.065
Running avgs for agent 0: q_loss: 1.7304294109344482, p_loss: 78.28711700439453, mean_rew: -5.013090835431762, variance: 50.89893212890625, mean_q: -78.61469268798828, std_q: 10.022595405578613
Running avgs for agent 1: q_loss: 2.4546451568603516, p_loss: 79.06415557861328, mean_rew: -5.01846380233167, variance: 84.84519842529296, mean_q: -79.295166015625, std_q: 9.460670471191406
Running avgs for agent 2: q_loss: 1.554689645767212, p_loss: 77.8100814819336, mean_rew: -5.018837695608422, variance: 47.11093421936035, mean_q: -78.09771728515625, std_q: 9.76711368560791

steps: 699975, episodes: 28000, mean episode reward: -341.7489769171261, agent episode reward: [-113.91632563904203, -113.91632563904203, -113.91632563904203], time: 69.884
steps: 699975, episodes: 28000, mean episode variance: 44.84430030441284, agent episode variance: [12.477879428863526, 20.66670767211914, 11.699713203430175], time: 69.884
Running avgs for agent 0: q_loss: 1.7426939010620117, p_loss: 78.3975601196289, mean_rew: -5.00062474060481, variance: 49.9115177154541, mean_q: -78.7248306274414, std_q: 10.055292129516602
Running avgs for agent 1: q_loss: 2.459165096282959, p_loss: 79.20280456542969, mean_rew: -5.002807272004836, variance: 82.66683068847657, mean_q: -79.42949676513672, std_q: 9.23771858215332
Running avgs for agent 2: q_loss: 1.5694482326507568, p_loss: 77.95169830322266, mean_rew: -4.998303959934125, variance: 46.7988528137207, mean_q: -78.23471069335938, std_q: 9.604201316833496

steps: 724975, episodes: 29000, mean episode reward: -339.7780220790958, agent episode reward: [-113.25934069303194, -113.25934069303194, -113.25934069303194], time: 72.839
steps: 724975, episodes: 29000, mean episode variance: 48.23939302062988, agent episode variance: [14.837817581176758, 21.25516365814209, 12.146411781311036], time: 72.839
Running avgs for agent 0: q_loss: 2.002507448196411, p_loss: 78.38230895996094, mean_rew: -4.981218653087664, variance: 59.35127032470703, mean_q: -78.70135498046875, std_q: 9.894328117370605
Running avgs for agent 1: q_loss: 2.4216697216033936, p_loss: 79.21031188964844, mean_rew: -4.977975563212502, variance: 85.02065463256837, mean_q: -79.43302917480469, std_q: 9.229567527770996
Running avgs for agent 2: q_loss: 1.536468744277954, p_loss: 78.11388397216797, mean_rew: -4.987542889187453, variance: 48.585647125244144, mean_q: -78.39510345458984, std_q: 9.455967903137207

steps: 749975, episodes: 30000, mean episode reward: -339.04678114710225, agent episode reward: [-113.01559371570075, -113.01559371570075, -113.01559371570075], time: 73.395
steps: 749975, episodes: 30000, mean episode variance: 55.895157104492185, agent episode variance: [22.938259742736815, 20.71988957977295, 12.237007781982422], time: 73.396
Running avgs for agent 0: q_loss: 2.7530922889709473, p_loss: 78.3741226196289, mean_rew: -4.968470072187284, variance: 91.75303897094726, mean_q: -78.68529510498047, std_q: 9.687443733215332
Running avgs for agent 1: q_loss: 2.3931894302368164, p_loss: 79.23758697509766, mean_rew: -4.967573428978847, variance: 82.8795583190918, mean_q: -79.461669921875, std_q: 9.220756530761719
Running avgs for agent 2: q_loss: 1.537736177444458, p_loss: 78.13542175292969, mean_rew: -4.963441745938283, variance: 48.94803112792969, mean_q: -78.4065170288086, std_q: 9.339822769165039

steps: 774975, episodes: 31000, mean episode reward: -341.09321190401675, agent episode reward: [-113.6977373013389, -113.6977373013389, -113.6977373013389], time: 72.944
steps: 774975, episodes: 31000, mean episode variance: 53.63818748474121, agent episode variance: [21.37793418121338, 20.516682205200194, 11.743571098327637], time: 72.945
Running avgs for agent 0: q_loss: 2.662095785140991, p_loss: 78.39707946777344, mean_rew: -4.952479300918452, variance: 85.51173672485352, mean_q: -78.70579528808594, std_q: 9.594583511352539
Running avgs for agent 1: q_loss: 2.3853774070739746, p_loss: 79.258544921875, mean_rew: -4.954875929171472, variance: 82.06672882080078, mean_q: -79.47721862792969, std_q: 9.243202209472656
Running avgs for agent 2: q_loss: 1.5316746234893799, p_loss: 78.21604919433594, mean_rew: -4.952937937315002, variance: 46.97428439331055, mean_q: -78.48028564453125, std_q: 9.36535358428955

steps: 799975, episodes: 32000, mean episode reward: -338.5561394917384, agent episode reward: [-112.85204649724614, -112.85204649724614, -112.85204649724614], time: 72.913
steps: 799975, episodes: 32000, mean episode variance: 53.9671598815918, agent episode variance: [21.63553468322754, 20.320499786376953, 12.011125411987305], time: 72.914
Running avgs for agent 0: q_loss: 2.566004991531372, p_loss: 78.37133026123047, mean_rew: -4.936558772897603, variance: 86.54213873291016, mean_q: -78.66737365722656, std_q: 9.601137161254883
Running avgs for agent 1: q_loss: 2.3536534309387207, p_loss: 79.19276428222656, mean_rew: -4.93973441284181, variance: 81.28199914550781, mean_q: -79.40650939941406, std_q: 9.04702377319336
Running avgs for agent 2: q_loss: 1.50244140625, p_loss: 78.2118148803711, mean_rew: -4.936652700645306, variance: 48.04450164794922, mean_q: -78.47187805175781, std_q: 9.327607154846191

steps: 824975, episodes: 33000, mean episode reward: -341.64913357183195, agent episode reward: [-113.88304452394401, -113.88304452394401, -113.88304452394401], time: 72.476
steps: 824975, episodes: 33000, mean episode variance: 52.98211927413941, agent episode variance: [21.09506908416748, 20.32489934539795, 11.562150844573974], time: 72.476
Running avgs for agent 0: q_loss: 2.5272583961486816, p_loss: 78.35379791259766, mean_rew: -4.926169041987041, variance: 84.38027633666992, mean_q: -78.64183807373047, std_q: 9.499695777893066
Running avgs for agent 1: q_loss: 2.311849594116211, p_loss: 79.10009002685547, mean_rew: -4.925264518489155, variance: 81.2995973815918, mean_q: -79.30938720703125, std_q: 9.083270072937012
Running avgs for agent 2: q_loss: 1.464185357093811, p_loss: 78.24034881591797, mean_rew: -4.927227345486146, variance: 46.2486033782959, mean_q: -78.4990234375, std_q: 9.179597854614258

steps: 849975, episodes: 34000, mean episode reward: -339.2006425186153, agent episode reward: [-113.06688083953846, -113.06688083953846, -113.06688083953846], time: 72.213
steps: 849975, episodes: 34000, mean episode variance: 51.47075531387329, agent episode variance: [20.052326972961424, 19.709464309692382, 11.708964031219482], time: 72.213
Running avgs for agent 0: q_loss: 2.4685113430023193, p_loss: 78.3966293334961, mean_rew: -4.912260563908986, variance: 80.2093078918457, mean_q: -78.67743682861328, std_q: 9.371999740600586
Running avgs for agent 1: q_loss: 2.278298854827881, p_loss: 79.01145935058594, mean_rew: -4.9172455900538905, variance: 78.83785723876953, mean_q: -79.22356414794922, std_q: 9.046225547790527
Running avgs for agent 2: q_loss: 1.5036154985427856, p_loss: 78.1976089477539, mean_rew: -4.916312535219022, variance: 46.83585612487793, mean_q: -78.44925689697266, std_q: 9.227429389953613

steps: 874975, episodes: 35000, mean episode reward: -339.9631804618982, agent episode reward: [-113.32106015396607, -113.32106015396607, -113.32106015396607], time: 81.071
steps: 874975, episodes: 35000, mean episode variance: 51.87523496627808, agent episode variance: [20.579405448913572, 19.79291542816162, 11.50291408920288], time: 81.072
Running avgs for agent 0: q_loss: 2.4059927463531494, p_loss: 78.35011291503906, mean_rew: -4.9052473975053035, variance: 82.31762179565429, mean_q: -78.6290054321289, std_q: 9.500408172607422
Running avgs for agent 1: q_loss: 2.2721073627471924, p_loss: 78.85934448242188, mean_rew: -4.904202001169271, variance: 79.17166171264648, mean_q: -79.06756591796875, std_q: 9.151278495788574
Running avgs for agent 2: q_loss: 1.4911526441574097, p_loss: 78.12416076660156, mean_rew: -4.905403933202741, variance: 46.01165635681152, mean_q: -78.38034057617188, std_q: 9.189836502075195

steps: 899975, episodes: 36000, mean episode reward: -335.1032112738176, agent episode reward: [-111.70107042460585, -111.70107042460585, -111.70107042460585], time: 71.702
steps: 899975, episodes: 36000, mean episode variance: 50.920372173309325, agent episode variance: [19.987672088623047, 19.522633277893068, 11.410066806793212], time: 71.702
Running avgs for agent 0: q_loss: 2.355290412902832, p_loss: 78.28292846679688, mean_rew: -4.890289639848044, variance: 79.95068835449219, mean_q: -78.55440521240234, std_q: 9.39545726776123
Running avgs for agent 1: q_loss: 2.206094741821289, p_loss: 78.68718719482422, mean_rew: -4.890326179897194, variance: 78.09053311157227, mean_q: -78.89360809326172, std_q: 9.026443481445312
Running avgs for agent 2: q_loss: 1.4681694507598877, p_loss: 78.0639877319336, mean_rew: -4.892133021002396, variance: 45.64026722717285, mean_q: -78.31385803222656, std_q: 9.134872436523438

steps: 924975, episodes: 37000, mean episode reward: -334.8876179602513, agent episode reward: [-111.62920598675045, -111.62920598675045, -111.62920598675045], time: 75.863
steps: 924975, episodes: 37000, mean episode variance: 50.49372660064697, agent episode variance: [19.525085487365722, 19.000171035766602, 11.968470077514649], time: 75.864
Running avgs for agent 0: q_loss: 2.3011205196380615, p_loss: 78.2474136352539, mean_rew: -4.88009248544405, variance: 78.10034194946289, mean_q: -78.517578125, std_q: 9.365007400512695
Running avgs for agent 1: q_loss: 2.1936817169189453, p_loss: 78.5989761352539, mean_rew: -4.8820376073265255, variance: 76.00068414306641, mean_q: -78.80281829833984, std_q: 9.020977973937988
Running avgs for agent 2: q_loss: 1.4672653675079346, p_loss: 77.97454833984375, mean_rew: -4.8825739152583845, variance: 47.873880310058595, mean_q: -78.22164916992188, std_q: 9.07548999786377

steps: 949975, episodes: 38000, mean episode reward: -338.01823652892546, agent episode reward: [-112.67274550964183, -112.67274550964183, -112.67274550964183], time: 75.848
steps: 949975, episodes: 38000, mean episode variance: 59.643670413970945, agent episode variance: [19.78832596206665, 19.397335525512695, 20.458008926391603], time: 75.849
Running avgs for agent 0: q_loss: 2.280921697616577, p_loss: 78.15892028808594, mean_rew: -4.871420426698373, variance: 79.1533038482666, mean_q: -78.42330169677734, std_q: 9.421103477478027
Running avgs for agent 1: q_loss: 2.174865245819092, p_loss: 78.38383483886719, mean_rew: -4.866752179701451, variance: 77.58934210205078, mean_q: -78.58634185791016, std_q: 8.987364768981934
Running avgs for agent 2: q_loss: 2.355635166168213, p_loss: 77.84889221191406, mean_rew: -4.869251038924749, variance: 81.83203570556641, mean_q: -78.09439849853516, std_q: 9.006624221801758

steps: 974975, episodes: 39000, mean episode reward: -332.61648090174, agent episode reward: [-110.87216030057999, -110.87216030057999, -110.87216030057999], time: 75.999
steps: 974975, episodes: 39000, mean episode variance: 58.683652717590334, agent episode variance: [19.219006240844728, 18.874182510375977, 20.59046396636963], time: 76.0
Running avgs for agent 0: q_loss: 2.2785556316375732, p_loss: 78.14173126220703, mean_rew: -4.866030490264538, variance: 76.87602496337891, mean_q: -78.39911651611328, std_q: 9.507695198059082
Running avgs for agent 1: q_loss: 2.1233837604522705, p_loss: 78.2457046508789, mean_rew: -4.859563702300474, variance: 75.49673004150391, mean_q: -78.44949340820312, std_q: 8.88035774230957
Running avgs for agent 2: q_loss: 2.2305335998535156, p_loss: 77.70065307617188, mean_rew: -4.863743218855542, variance: 82.36185586547852, mean_q: -77.94282531738281, std_q: 8.84620475769043

steps: 999975, episodes: 40000, mean episode reward: -334.52336314100586, agent episode reward: [-111.50778771366863, -111.50778771366863, -111.50778771366863], time: 103.594
steps: 999975, episodes: 40000, mean episode variance: 56.57333654403686, agent episode variance: [18.788776317596437, 18.26257029724121, 19.52198992919922], time: 103.595
Running avgs for agent 0: q_loss: 2.220942497253418, p_loss: 78.08464050292969, mean_rew: -4.853348220229028, variance: 75.15510527038575, mean_q: -78.34410858154297, std_q: 9.494772911071777
Running avgs for agent 1: q_loss: 2.1092705726623535, p_loss: 78.11144256591797, mean_rew: -4.847954618886073, variance: 73.05028118896485, mean_q: -78.31257629394531, std_q: 8.890359878540039
Running avgs for agent 2: q_loss: 2.1959164142608643, p_loss: 77.65426635742188, mean_rew: -4.849027055457266, variance: 78.08795971679687, mean_q: -77.89556884765625, std_q: 8.951197624206543

steps: 1024975, episodes: 41000, mean episode reward: -332.06756247989773, agent episode reward: [-110.68918749329924, -110.68918749329924, -110.68918749329924], time: 124.379
steps: 1024975, episodes: 41000, mean episode variance: 56.32358155059814, agent episode variance: [18.740236129760742, 18.572378242492675, 19.01096717834473], time: 124.38
Running avgs for agent 0: q_loss: 2.1450235843658447, p_loss: 77.7840347290039, mean_rew: -4.809997841342176, variance: 74.96094451904297, mean_q: -78.0295181274414, std_q: 9.223504066467285
Running avgs for agent 1: q_loss: 2.049579381942749, p_loss: 77.78517150878906, mean_rew: -4.813180562671922, variance: 74.2895129699707, mean_q: -77.98151397705078, std_q: 8.785002708435059
Running avgs for agent 2: q_loss: 2.1243956089019775, p_loss: 77.44571685791016, mean_rew: -4.810414907285513, variance: 76.04386871337891, mean_q: -77.6734390258789, std_q: 8.773416519165039

steps: 1049975, episodes: 42000, mean episode reward: -333.34196293821753, agent episode reward: [-111.11398764607252, -111.11398764607252, -111.11398764607252], time: 83.011
steps: 1049975, episodes: 42000, mean episode variance: 55.099196685791014, agent episode variance: [18.14128923034668, 17.82189575958252, 19.136011695861818], time: 83.011
Running avgs for agent 0: q_loss: 2.0423643589019775, p_loss: 77.27830505371094, mean_rew: -4.74181814664614, variance: 72.56515692138672, mean_q: -77.490234375, std_q: 7.928167343139648
Running avgs for agent 1: q_loss: 1.981738567352295, p_loss: 77.19779968261719, mean_rew: -4.735544745398454, variance: 71.28758303833008, mean_q: -77.37499237060547, std_q: 7.536647319793701
Running avgs for agent 2: q_loss: 2.041937828063965, p_loss: 76.86933135986328, mean_rew: -4.743127733408785, variance: 76.54404678344727, mean_q: -77.06390380859375, std_q: 7.504458427429199

steps: 1074975, episodes: 43000, mean episode reward: -332.19440416323647, agent episode reward: [-110.73146805441216, -110.73146805441216, -110.73146805441216], time: 72.244
steps: 1074975, episodes: 43000, mean episode variance: 53.25310830688476, agent episode variance: [17.521295249938966, 17.465571464538574, 18.266241592407226], time: 72.245
Running avgs for agent 0: q_loss: 1.9755330085754395, p_loss: 76.90924835205078, mean_rew: -4.6789008061694295, variance: 70.08518099975586, mean_q: -77.09913635253906, std_q: 7.2929277420043945
Running avgs for agent 1: q_loss: 1.9225322008132935, p_loss: 76.78157806396484, mean_rew: -4.679140159279201, variance: 69.8622858581543, mean_q: -76.93965911865234, std_q: 6.901445388793945
Running avgs for agent 2: q_loss: 1.9515466690063477, p_loss: 76.47565460205078, mean_rew: -4.6776928858444045, variance: 73.0649663696289, mean_q: -76.64447021484375, std_q: 6.85099983215332

steps: 1099975, episodes: 44000, mean episode reward: -334.78183710778507, agent episode reward: [-111.59394570259501, -111.59394570259501, -111.59394570259501], time: 72.776
steps: 1099975, episodes: 44000, mean episode variance: 52.3192728805542, agent episode variance: [17.451680252075196, 17.156680320739746, 17.710912307739257], time: 72.776
Running avgs for agent 0: q_loss: 1.8886195421218872, p_loss: 76.6844711303711, mean_rew: -4.6372914074649145, variance: 69.80672100830078, mean_q: -76.85546112060547, std_q: 7.209322452545166
Running avgs for agent 1: q_loss: 1.862535834312439, p_loss: 76.56718444824219, mean_rew: -4.6458743809382685, variance: 68.62672128295898, mean_q: -76.71713256835938, std_q: 6.872584342956543
Running avgs for agent 2: q_loss: 1.897944450378418, p_loss: 76.27484893798828, mean_rew: -4.63902552730079, variance: 70.84364923095703, mean_q: -76.43246459960938, std_q: 6.747451305389404

steps: 1124975, episodes: 45000, mean episode reward: -335.6367881651932, agent episode reward: [-111.87892938839772, -111.87892938839772, -111.87892938839772], time: 72.946
steps: 1124975, episodes: 45000, mean episode variance: 51.942231029510495, agent episode variance: [16.891869678497315, 17.254503746032714, 17.79585760498047], time: 72.947
Running avgs for agent 0: q_loss: 1.8573014736175537, p_loss: 76.57936096191406, mean_rew: -4.607177770859945, variance: 67.56747871398926, mean_q: -76.73564147949219, std_q: 7.197376728057861
Running avgs for agent 1: q_loss: 1.8156049251556396, p_loss: 76.2972640991211, mean_rew: -4.609763626792676, variance: 69.01801498413086, mean_q: -76.44636535644531, std_q: 6.805775165557861
Running avgs for agent 2: q_loss: 1.8445827960968018, p_loss: 76.1262435913086, mean_rew: -4.6137776921068205, variance: 71.18343041992188, mean_q: -76.26873779296875, std_q: 6.677868366241455

steps: 1149975, episodes: 46000, mean episode reward: -332.9095512625856, agent episode reward: [-110.96985042086187, -110.96985042086187, -110.96985042086187], time: 72.866
steps: 1149975, episodes: 46000, mean episode variance: 51.0487579574585, agent episode variance: [16.543531547546387, 16.95289804840088, 17.55232836151123], time: 72.867
Running avgs for agent 0: q_loss: 1.8124758005142212, p_loss: 76.46473693847656, mean_rew: -4.5940634680577945, variance: 66.17412619018555, mean_q: -76.61189270019531, std_q: 7.1913228034973145
Running avgs for agent 1: q_loss: 1.7691758871078491, p_loss: 76.10514068603516, mean_rew: -4.591330208416688, variance: 67.81159219360352, mean_q: -76.25155639648438, std_q: 6.79144287109375
Running avgs for agent 2: q_loss: 1.815059781074524, p_loss: 75.97671508789062, mean_rew: -4.593264986226321, variance: 70.20931344604492, mean_q: -76.11593627929688, std_q: 6.656399726867676

steps: 1174975, episodes: 47000, mean episode reward: -333.185722455875, agent episode reward: [-111.06190748529168, -111.06190748529168, -111.06190748529168], time: 73.276
steps: 1174975, episodes: 47000, mean episode variance: 50.61143337249756, agent episode variance: [16.61025321960449, 16.96597900390625, 17.035201148986815], time: 73.276
Running avgs for agent 0: q_loss: 1.7867995500564575, p_loss: 76.34046173095703, mean_rew: -4.574898399260042, variance: 66.44101287841796, mean_q: -76.48332214355469, std_q: 7.189215660095215
Running avgs for agent 1: q_loss: 1.739227533340454, p_loss: 75.88896942138672, mean_rew: -4.577402361467299, variance: 67.863916015625, mean_q: -76.03237915039062, std_q: 6.844330310821533
Running avgs for agent 2: q_loss: 1.7806694507598877, p_loss: 75.8841323852539, mean_rew: -4.583956375358743, variance: 68.14080459594726, mean_q: -76.01691436767578, std_q: 6.661779880523682

steps: 1199975, episodes: 48000, mean episode reward: -333.85121344013436, agent episode reward: [-111.28373781337812, -111.28373781337812, -111.28373781337812], time: 74.425
steps: 1199975, episodes: 48000, mean episode variance: 49.330402256011965, agent episode variance: [16.124063804626466, 16.274645442962647, 16.93169300842285], time: 74.425
Running avgs for agent 0: q_loss: 1.759160041809082, p_loss: 76.25688934326172, mean_rew: -4.56815217103782, variance: 64.49625521850587, mean_q: -76.3942642211914, std_q: 7.239760875701904
Running avgs for agent 1: q_loss: 1.701568603515625, p_loss: 75.74342346191406, mean_rew: -4.569806303596454, variance: 65.09858177185059, mean_q: -75.888671875, std_q: 6.9151434898376465
Running avgs for agent 2: q_loss: 1.7370632886886597, p_loss: 75.7309799194336, mean_rew: -4.566147794648231, variance: 67.7267720336914, mean_q: -75.85808563232422, std_q: 6.656387805938721

steps: 1224975, episodes: 49000, mean episode reward: -332.18626085624226, agent episode reward: [-110.7287536187474, -110.7287536187474, -110.7287536187474], time: 75.341
steps: 1224975, episodes: 49000, mean episode variance: 48.496305953979494, agent episode variance: [15.870557022094726, 15.952050430297852, 16.673698501586912], time: 75.342
Running avgs for agent 0: q_loss: 1.7374203205108643, p_loss: 76.18034362792969, mean_rew: -4.556841229202494, variance: 63.4822280883789, mean_q: -76.31509399414062, std_q: 7.218200206756592
Running avgs for agent 1: q_loss: 1.6903576850891113, p_loss: 75.58243560791016, mean_rew: -4.556105614836406, variance: 63.80820172119141, mean_q: -75.72686004638672, std_q: 6.937212944030762
Running avgs for agent 2: q_loss: 1.684229850769043, p_loss: 75.62632751464844, mean_rew: -4.555650618245841, variance: 66.69479400634765, mean_q: -75.75051879882812, std_q: 6.638411045074463

steps: 1249975, episodes: 50000, mean episode reward: -333.3940458570289, agent episode reward: [-111.13134861900964, -111.13134861900964, -111.13134861900964], time: 80.0
steps: 1249975, episodes: 50000, mean episode variance: 48.13771855545044, agent episode variance: [15.782787071228027, 16.15554557418823, 16.19938591003418], time: 80.0
Running avgs for agent 0: q_loss: 1.7101552486419678, p_loss: 76.07603454589844, mean_rew: -4.55293930053433, variance: 63.13114828491211, mean_q: -76.21014404296875, std_q: 7.228272438049316
Running avgs for agent 1: q_loss: 1.6558271646499634, p_loss: 75.37012481689453, mean_rew: -4.546236757540835, variance: 64.62218229675292, mean_q: -75.51699829101562, std_q: 6.998212814331055
Running avgs for agent 2: q_loss: 1.6709704399108887, p_loss: 75.520751953125, mean_rew: -4.545891006301413, variance: 64.79754364013672, mean_q: -75.64334106445312, std_q: 6.63884162902832

steps: 1274975, episodes: 51000, mean episode reward: -334.9363758029209, agent episode reward: [-111.64545860097365, -111.64545860097365, -111.64545860097365], time: 72.839
steps: 1274975, episodes: 51000, mean episode variance: 47.47868000793457, agent episode variance: [15.711279602050782, 15.763425903320313, 16.003974502563477], time: 72.839
Running avgs for agent 0: q_loss: 1.6758229732513428, p_loss: 75.98892211914062, mean_rew: -4.541761207746509, variance: 62.84511840820313, mean_q: -76.12531280517578, std_q: 7.220012664794922
Running avgs for agent 1: q_loss: 1.629789113998413, p_loss: 75.21183013916016, mean_rew: -4.539585696172932, variance: 63.05370361328125, mean_q: -75.35908508300781, std_q: 7.031021595001221
Running avgs for agent 2: q_loss: 1.6440837383270264, p_loss: 75.48445129394531, mean_rew: -4.541583046701383, variance: 64.0158980102539, mean_q: -75.60649108886719, std_q: 6.708690643310547

steps: 1299975, episodes: 52000, mean episode reward: -331.37438110776975, agent episode reward: [-110.45812703592324, -110.45812703592324, -110.45812703592324], time: 74.59
steps: 1299975, episodes: 52000, mean episode variance: 46.71390059280395, agent episode variance: [15.177946544647217, 15.54296706390381, 15.99298698425293], time: 74.591
Running avgs for agent 0: q_loss: 1.6428751945495605, p_loss: 75.88854217529297, mean_rew: -4.5351670125071415, variance: 60.711786178588866, mean_q: -76.0194091796875, std_q: 7.187790870666504
Running avgs for agent 1: q_loss: 1.603611946105957, p_loss: 75.07931518554688, mean_rew: -4.5367670507021804, variance: 62.17186825561524, mean_q: -75.22488403320312, std_q: 7.130213260650635
Running avgs for agent 2: q_loss: 1.619320034980774, p_loss: 75.47317504882812, mean_rew: -4.538567623370527, variance: 63.97194793701172, mean_q: -75.59381103515625, std_q: 6.734944820404053

steps: 1324975, episodes: 53000, mean episode reward: -327.9319107857649, agent episode reward: [-109.3106369285883, -109.3106369285883, -109.3106369285883], time: 75.262
steps: 1324975, episodes: 53000, mean episode variance: 46.55491555786133, agent episode variance: [15.287615310668945, 15.446997604370118, 15.820302642822266], time: 75.262
Running avgs for agent 0: q_loss: 1.6173304319381714, p_loss: 75.88346862792969, mean_rew: -4.531746909444681, variance: 61.15046124267578, mean_q: -76.00936126708984, std_q: 7.248851776123047
Running avgs for agent 1: q_loss: 1.5848913192749023, p_loss: 74.9232177734375, mean_rew: -4.533069880977733, variance: 61.78799041748047, mean_q: -75.07415771484375, std_q: 7.226840972900391
Running avgs for agent 2: q_loss: 1.5833441019058228, p_loss: 75.39997863769531, mean_rew: -4.530505937208969, variance: 63.281210571289066, mean_q: -75.52056121826172, std_q: 6.721848964691162

steps: 1349975, episodes: 54000, mean episode reward: -331.10188981736684, agent episode reward: [-110.36729660578897, -110.36729660578897, -110.36729660578897], time: 77.04
steps: 1349975, episodes: 54000, mean episode variance: 46.134755657196045, agent episode variance: [15.14650721359253, 15.220616806030273, 15.767631637573242], time: 77.041
Running avgs for agent 0: q_loss: 1.586462378501892, p_loss: 75.86148071289062, mean_rew: -4.526094680426933, variance: 60.58602885437012, mean_q: -75.9856185913086, std_q: 7.239443302154541
Running avgs for agent 1: q_loss: 1.5805755853652954, p_loss: 74.7455062866211, mean_rew: -4.525999296969916, variance: 60.88246722412109, mean_q: -74.90226745605469, std_q: 7.272627830505371
Running avgs for agent 2: q_loss: 1.566067099571228, p_loss: 75.37520599365234, mean_rew: -4.5275919499533845, variance: 63.07052655029297, mean_q: -75.49930572509766, std_q: 6.704551696777344

steps: 1374975, episodes: 55000, mean episode reward: -331.6907565997691, agent episode reward: [-110.56358553325639, -110.56358553325639, -110.56358553325639], time: 75.309
steps: 1374975, episodes: 55000, mean episode variance: 45.326901920318605, agent episode variance: [14.803077850341797, 14.901192588806152, 15.622631481170654], time: 75.31
Running avgs for agent 0: q_loss: 1.5472787618637085, p_loss: 75.85889434814453, mean_rew: -4.52196532001207, variance: 59.21231140136719, mean_q: -75.97914123535156, std_q: 7.248591899871826
Running avgs for agent 1: q_loss: 1.5760656595230103, p_loss: 74.54634094238281, mean_rew: -4.525005074365427, variance: 59.60477035522461, mean_q: -74.70552825927734, std_q: 7.299849033355713
Running avgs for agent 2: q_loss: 1.5580190420150757, p_loss: 75.33039855957031, mean_rew: -4.523177272981533, variance: 62.490525924682615, mean_q: -75.45670318603516, std_q: 6.673461437225342

steps: 1399975, episodes: 56000, mean episode reward: -328.399214969198, agent episode reward: [-109.46640498973267, -109.46640498973267, -109.46640498973267], time: 74.761
steps: 1399975, episodes: 56000, mean episode variance: 45.27641201400757, agent episode variance: [14.850121398925781, 14.936650024414062, 15.489640590667724], time: 74.761
Running avgs for agent 0: q_loss: 1.5126806497573853, p_loss: 75.79761505126953, mean_rew: -4.515328395173679, variance: 59.400485595703124, mean_q: -75.91221618652344, std_q: 7.260366916656494
Running avgs for agent 1: q_loss: 1.5345358848571777, p_loss: 74.25711059570312, mean_rew: -4.510719207138998, variance: 59.74660009765625, mean_q: -74.41352844238281, std_q: 7.322202205657959
Running avgs for agent 2: q_loss: 1.5260212421417236, p_loss: 75.22714233398438, mean_rew: -4.508505942201922, variance: 61.9585623626709, mean_q: -75.35296630859375, std_q: 6.561067581176758

steps: 1424975, episodes: 57000, mean episode reward: -327.04188422458435, agent episode reward: [-109.01396140819479, -109.01396140819479, -109.01396140819479], time: 74.237
steps: 1424975, episodes: 57000, mean episode variance: 44.454165699005124, agent episode variance: [14.552150871276856, 14.620615306854248, 15.281399520874023], time: 74.238
Running avgs for agent 0: q_loss: 1.4839353561401367, p_loss: 75.72510528564453, mean_rew: -4.506250128965314, variance: 58.208603485107425, mean_q: -75.83943176269531, std_q: 7.257054805755615
Running avgs for agent 1: q_loss: 1.523222804069519, p_loss: 74.086181640625, mean_rew: -4.5053180636505274, variance: 58.48246122741699, mean_q: -74.23890686035156, std_q: 7.392869472503662
Running avgs for agent 2: q_loss: 1.5052456855773926, p_loss: 75.20596313476562, mean_rew: -4.506066993908809, variance: 61.125598083496094, mean_q: -75.33570098876953, std_q: 6.495616436004639

steps: 1449975, episodes: 58000, mean episode reward: -326.04683582100387, agent episode reward: [-108.6822786070013, -108.6822786070013, -108.6822786070013], time: 75.812
steps: 1449975, episodes: 58000, mean episode variance: 43.81841090393066, agent episode variance: [14.479195892333985, 14.17868448638916, 15.16053052520752], time: 75.813
Running avgs for agent 0: q_loss: 1.4697152376174927, p_loss: 75.68547058105469, mean_rew: -4.506668542262185, variance: 57.91678356933594, mean_q: -75.79808044433594, std_q: 7.294308662414551
Running avgs for agent 1: q_loss: 1.5143052339553833, p_loss: 73.94678497314453, mean_rew: -4.49842239848687, variance: 56.71473794555664, mean_q: -74.09722137451172, std_q: 7.4801154136657715
Running avgs for agent 2: q_loss: 1.491870641708374, p_loss: 75.16865539550781, mean_rew: -4.501107028161962, variance: 60.64212210083008, mean_q: -75.29660034179688, std_q: 6.461671829223633

steps: 1474975, episodes: 59000, mean episode reward: -326.6662877788486, agent episode reward: [-108.88876259294953, -108.88876259294953, -108.88876259294953], time: 71.688
steps: 1474975, episodes: 59000, mean episode variance: 43.35181332397461, agent episode variance: [14.30341834640503, 14.373772533416748, 14.674622444152831], time: 71.689
Running avgs for agent 0: q_loss: 1.4404951333999634, p_loss: 75.59903717041016, mean_rew: -4.498578480972142, variance: 57.21367338562012, mean_q: -75.71208190917969, std_q: 7.312265396118164
Running avgs for agent 1: q_loss: 1.4981110095977783, p_loss: 73.7625961303711, mean_rew: -4.490320935506114, variance: 57.49509013366699, mean_q: -73.90577697753906, std_q: 7.5483622550964355
Running avgs for agent 2: q_loss: 1.4582186937332153, p_loss: 75.14056396484375, mean_rew: -4.495303364082621, variance: 58.698489776611325, mean_q: -75.26728057861328, std_q: 6.451760768890381

steps: 1499975, episodes: 60000, mean episode reward: -327.18753793551804, agent episode reward: [-109.06251264517267, -109.06251264517267, -109.06251264517267], time: 69.545
steps: 1499975, episodes: 60000, mean episode variance: 43.47761557388306, agent episode variance: [14.122310199737548, 14.30606050491333, 15.049244869232178], time: 69.546
Running avgs for agent 0: q_loss: 1.4206069707870483, p_loss: 75.49490356445312, mean_rew: -4.4902847843045715, variance: 56.48924079895019, mean_q: -75.60806274414062, std_q: 7.359402179718018
Running avgs for agent 1: q_loss: 1.4837242364883423, p_loss: 73.64282989501953, mean_rew: -4.488398707431518, variance: 57.22424201965332, mean_q: -73.7796859741211, std_q: 7.64552116394043
Running avgs for agent 2: q_loss: 1.4523388147354126, p_loss: 75.03926849365234, mean_rew: -4.487444788317089, variance: 60.19697947692871, mean_q: -75.16584777832031, std_q: 6.401508808135986

...Finished total of 60001 episodes... Now freezing policy and running for 10000 more episodes to get cvar estimation
steps: 24975, episodes: 1000, mean episode reward: -328.1655054245519, agent episode reward: [-109.38850180818395, -109.38850180818395, -109.38850180818395], time: 53.824
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 53.825
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -325.2170997616146, agent episode reward: [-108.4056999205382, -108.4056999205382, -108.4056999205382], time: 76.608
steps: 49975, episodes: 2000, mean episode variance: 38.29438275909424, agent episode variance: [17.21918441772461, 21.075198341369628, 0.0], time: 76.609
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371735733479404, variance: 70.57042694091797, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -74.55026245117188, std_q: 7.158022880554199
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.36945449611286, variance: 86.37376403808594, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -72.42973327636719, std_q: 7.583098888397217
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.376861748119091, variance: 0.0, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -75.43656921386719, std_q: 6.460460662841797

steps: 74975, episodes: 3000, mean episode reward: -325.6900485443021, agent episode reward: [-108.56334951476738, -108.56334951476738, -108.56334951476738], time: 75.992
steps: 74975, episodes: 3000, mean episode variance: 38.478217498779294, agent episode variance: [17.28179852294922, 21.196418975830078, 0.0], time: 75.992
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.353435788362319, variance: 69.127197265625, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -74.53448486328125, std_q: 7.12014102935791
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.357279915059359, variance: 84.78567504882812, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -72.47411346435547, std_q: 7.604447841644287
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.353094510026143, variance: 0.0, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -75.36236572265625, std_q: 6.367825984954834

steps: 99975, episodes: 4000, mean episode reward: -327.8791893276706, agent episode reward: [-109.29306310922354, -109.29306310922354, -109.29306310922354], time: 74.868
steps: 99975, episodes: 4000, mean episode variance: 38.29942459106445, agent episode variance: [17.167041229248046, 21.132383361816405, 0.0], time: 74.868
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.354128231664418, variance: 68.66816711425781, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -74.5470199584961, std_q: 7.12385368347168
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.356214202219927, variance: 84.52953338623047, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -72.48820495605469, std_q: 7.566964626312256
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.36296933431344, variance: 0.0, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -75.39259338378906, std_q: 6.377302646636963

steps: 124975, episodes: 5000, mean episode reward: -327.5962133943373, agent episode reward: [-109.19873779811245, -109.19873779811245, -109.19873779811245], time: 75.683
steps: 124975, episodes: 5000, mean episode variance: 38.37826499938965, agent episode variance: [17.21752554321289, 21.160739456176756, 0.0], time: 75.684
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3615288797449665, variance: 68.87010192871094, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -74.53105163574219, std_q: 7.092309951782227
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.360178987293636, variance: 84.64295959472656, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -72.44326782226562, std_q: 7.544036388397217
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3583161440293345, variance: 0.0, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -75.38009643554688, std_q: 6.373520851135254

steps: 149975, episodes: 6000, mean episode reward: -327.02763032871997, agent episode reward: [-109.0092101095733, -109.0092101095733, -109.0092101095733], time: 75.386
steps: 149975, episodes: 6000, mean episode variance: 38.55181390380859, agent episode variance: [17.344346115112305, 21.207467788696288, 0.0], time: 75.386
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.359672330658339, variance: 69.37738037109375, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -74.51560974121094, std_q: 7.114149570465088
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.35925323273522, variance: 84.82987213134766, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -72.4290542602539, std_q: 7.544491767883301
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.359551262213006, variance: 0.0, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -75.37104797363281, std_q: 6.363857269287109

steps: 174975, episodes: 7000, mean episode reward: -328.8916898447948, agent episode reward: [-109.63056328159826, -109.63056328159826, -109.63056328159826], time: 75.842
steps: 174975, episodes: 7000, mean episode variance: 38.85240545272827, agent episode variance: [17.389153095245362, 21.46325235748291, 0.0], time: 75.843
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3641097418419035, variance: 69.5566177368164, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -74.50482177734375, std_q: 7.085992813110352
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.364439230552958, variance: 85.85301208496094, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -72.44579315185547, std_q: 7.543031692504883
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.368106655517977, variance: 0.0, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -75.37517547607422, std_q: 6.364121913909912

steps: 199975, episodes: 8000, mean episode reward: -326.6165130105257, agent episode reward: [-108.87217100350856, -108.87217100350856, -108.87217100350856], time: 67.308
steps: 199975, episodes: 8000, mean episode variance: 39.1548445930481, agent episode variance: [17.63265026473999, 21.522194328308107, 0.0], time: 67.309
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.36470012523676, variance: 70.53060150146484, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -74.4835205078125, std_q: 7.095871925354004
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.364322414196017, variance: 86.08878326416016, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -72.431884765625, std_q: 7.568085193634033
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.37055284137052, variance: 0.0, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -75.39008331298828, std_q: 6.374176979064941

steps: 224975, episodes: 9000, mean episode reward: -329.57457670530783, agent episode reward: [-109.85819223510259, -109.85819223510259, -109.85819223510259], time: 67.467
steps: 224975, episodes: 9000, mean episode variance: 38.7882928314209, agent episode variance: [17.394406562805177, 21.393886268615724, 0.0], time: 67.467
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.366815153966889, variance: 69.57762145996094, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -74.5102310180664, std_q: 7.122168064117432
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.366140432134822, variance: 85.57554626464844, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -72.41437530517578, std_q: 7.536484241485596
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.367225433676244, variance: 0.0, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -75.37899780273438, std_q: 6.360612392425537

steps: 249975, episodes: 10000, mean episode reward: -327.75234576666287, agent episode reward: [-109.25078192222097, -109.25078192222097, -109.25078192222097], time: 74.098
steps: 249975, episodes: 10000, mean episode variance: 38.83600144195557, agent episode variance: [17.34760595703125, 21.488395484924318, 0.0], time: 74.099
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.374324904178592, variance: 69.39041900634766, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -74.5597152709961, std_q: 7.1409525871276855
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3712358379913105, variance: 85.95357513427734, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -72.46381378173828, std_q: 7.555050849914551
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.36625720899262, variance: 0.0, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -75.37052917480469, std_q: 6.361100673675537

steps: 274975, episodes: 11000, mean episode reward: -327.4046908376977, agent episode reward: [-109.13489694589924, -109.13489694589924, -109.13489694589924], time: 69.866
steps: 274975, episodes: 11000, mean episode variance: 38.502043296813966, agent episode variance: [17.177610725402833, 21.324432571411133, 0.0], time: 69.867
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.363609411851001, variance: 68.71043395996094, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -74.52722930908203, std_q: 7.117777347564697
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.36531885875416, variance: 85.29773712158203, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -72.45584106445312, std_q: 7.531891822814941
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.36968402435695, variance: 0.0, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -75.40193176269531, std_q: 6.402029991149902

steps: 299975, episodes: 12000, mean episode reward: -327.42794929512786, agent episode reward: [-109.14264976504263, -109.14264976504263, -109.14264976504263], time: 70.566
steps: 299975, episodes: 12000, mean episode variance: 38.76069728088379, agent episode variance: [17.356047355651857, 21.404649925231933, 0.0], time: 70.566
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371138028289329, variance: 69.42418670654297, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -74.54148864746094, std_q: 7.115725517272949
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.368901265207136, variance: 85.61859893798828, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -72.46514129638672, std_q: 7.534989833831787
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.368441256233946, variance: 0.0, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -75.39433288574219, std_q: 6.378715991973877

steps: 324975, episodes: 13000, mean episode reward: -327.9150980811675, agent episode reward: [-109.3050326937225, -109.3050326937225, -109.3050326937225], time: 70.13
steps: 324975, episodes: 13000, mean episode variance: 38.89542971420288, agent episode variance: [17.380674823760987, 21.514754890441896, 0.0], time: 70.131
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3690712186264, variance: 69.522705078125, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -74.53829956054688, std_q: 7.112273693084717
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.366175834407303, variance: 86.05901336669922, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -72.4891586303711, std_q: 7.548603534698486
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.367405873955968, variance: 0.0, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -75.38370513916016, std_q: 6.3695292472839355

steps: 349975, episodes: 14000, mean episode reward: -327.4511971311251, agent episode reward: [-109.15039904370838, -109.15039904370838, -109.15039904370838], time: 69.777
steps: 349975, episodes: 14000, mean episode variance: 38.83792018508911, agent episode variance: [17.37066376876831, 21.4672564163208, 0.0], time: 69.777
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371365386843776, variance: 69.48265838623047, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -74.56793975830078, std_q: 7.123565196990967
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370296388760671, variance: 85.86903381347656, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -72.48992919921875, std_q: 7.548112392425537
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370487372697398, variance: 0.0, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -75.40404510498047, std_q: 6.374988079071045

steps: 374975, episodes: 15000, mean episode reward: -325.9871424071918, agent episode reward: [-108.66238080239727, -108.66238080239727, -108.66238080239727], time: 72.656
steps: 374975, episodes: 15000, mean episode variance: 38.87735790252685, agent episode variance: [17.447328521728515, 21.43002938079834, 0.0], time: 72.657
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3722811363780405, variance: 69.78931427001953, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -74.55937957763672, std_q: 7.128913402557373
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.364171078831001, variance: 85.7201156616211, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -72.46420288085938, std_q: 7.52931547164917
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.369671806169202, variance: 0.0, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -75.40235900878906, std_q: 6.382449150085449

steps: 399975, episodes: 16000, mean episode reward: -327.75386150889267, agent episode reward: [-109.25128716963088, -109.25128716963088, -109.25128716963088], time: 70.059
steps: 399975, episodes: 16000, mean episode variance: 38.83116775894165, agent episode variance: [17.348502658843994, 21.482665100097655, 0.0], time: 70.059
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3664498527847675, variance: 69.39400482177734, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -74.53839874267578, std_q: 7.087255001068115
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370489778030222, variance: 85.93065643310547, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -72.49759674072266, std_q: 7.55177116394043
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.37333282450088, variance: 0.0, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -75.4088134765625, std_q: 6.4016804695129395

steps: 424975, episodes: 17000, mean episode reward: -327.1653541643415, agent episode reward: [-109.05511805478051, -109.05511805478051, -109.05511805478051], time: 68.907
steps: 424975, episodes: 17000, mean episode variance: 39.059826393127445, agent episode variance: [17.456310737609865, 21.60351565551758, 0.0], time: 68.908
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373009409851949, variance: 69.82524108886719, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -74.56204986572266, std_q: 7.12014102935791
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373962103826901, variance: 86.4140625, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -72.50540924072266, std_q: 7.549880027770996
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.3704974528569025, variance: 0.0, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -75.40465545654297, std_q: 6.367126941680908

steps: 449975, episodes: 18000, mean episode reward: -326.38592636729476, agent episode reward: [-108.79530878909826, -108.79530878909826, -108.79530878909826], time: 69.31
steps: 449975, episodes: 18000, mean episode variance: 39.02663551712036, agent episode variance: [17.485405216217043, 21.54123030090332, 0.0], time: 69.311
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.37644337540004, variance: 69.9416275024414, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -74.5678482055664, std_q: 7.1254119873046875
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.367848356707833, variance: 86.16492462158203, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -72.47101593017578, std_q: 7.518913269042969
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.364937869702995, variance: 0.0, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -75.38396453857422, std_q: 6.3484978675842285

steps: 474975, episodes: 19000, mean episode reward: -328.9385116552653, agent episode reward: [-109.64617055175512, -109.64617055175512, -109.64617055175512], time: 71.42
steps: 474975, episodes: 19000, mean episode variance: 39.05466748809815, agent episode variance: [17.491592880249023, 21.56307460784912, 0.0], time: 71.421
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.373282486131622, variance: 69.96637725830078, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -74.56180572509766, std_q: 7.146237373352051
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.370338986623432, variance: 86.2522964477539, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -72.46237182617188, std_q: 7.553311347961426
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.367746077793625, variance: 0.0, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -75.39557647705078, std_q: 6.370230674743652

steps: 499975, episodes: 20000, mean episode reward: -326.4471583699371, agent episode reward: [-108.8157194566457, -108.8157194566457, -108.8157194566457], time: 73.098
steps: 499975, episodes: 20000, mean episode variance: 39.09940868377686, agent episode variance: [17.55508741760254, 21.544321266174315, 0.0], time: 73.098
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.371201385288019, variance: 70.22035217285156, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -74.54756164550781, std_q: 7.123620986938477
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.372828202878821, variance: 86.17728424072266, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -72.46981048583984, std_q: 7.5686235427856445
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.369059355713003, variance: 0.0, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -75.4075927734375, std_q: 6.370630264282227

steps: 524975, episodes: 21000, mean episode reward: -326.96569140140866, agent episode reward: [-108.9885638004695, -108.9885638004695, -108.9885638004695], time: 70.166
steps: 524975, episodes: 21000, mean episode variance: 38.78076521682739, agent episode variance: [17.466818019866942, 21.31394719696045, 0.0], time: 70.167
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.368368458542532, variance: 69.86727142333984, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -74.54254913330078, std_q: 7.1215009689331055
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.366535414928475, variance: 85.25578308105469, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -72.46588134765625, std_q: 7.55258321762085
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.367649005352087, variance: 0.0, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -75.4051284790039, std_q: 6.370928764343262

Traceback (most recent call last):
  File "train.py", line 493, in <module>
    train(arglist)
  File "train.py", line 216, in train
    obs_n = env.reset()
  File "../../multiagent-particle-envs/multiagent/environment.py", line 108, in reset
    self.reset_callback(self.world)
  File "../../multiagent-particle-envs/multiagent/scenarios/simple_spread.py", line 66, in reset_world
    save([world.agents, world.landmarks], 'ss_state')
  File "../../multiagent-particle-envs/multiagent/scenarios/simple_spread.py", line 9, in save
    with open('obj/' + filename + '.pkl', 'wb') as handle:
KeyboardInterrupt
