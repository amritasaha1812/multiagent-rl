# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 40.0 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies8/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies8/02-non-linear-exp_var/
Job <1091626> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc048>>
arglist.u_estimation True
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -518.923674471169, agent episode reward: [-172.97455815705635, -172.97455815705635, -172.97455815705635], time: 97.237
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 97.237
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -632.4360467719804, agent episode reward: [-210.81201559066014, -210.81201559066014, -210.81201559066014], time: 116.853
steps: 49975, episodes: 2000, mean episode variance: 6.733507896564901, agent episode variance: [2.999866408854723, 1.067789179675281, 2.665852308034897], time: 116.853
Running avgs for agent 0: q_loss: 117.61203002929688, p_loss: -5.599776268005371, mean_rew: -7.525962131357995, variance: 12.294534683227539, lamda: 1.0107274055480957
Running avgs for agent 1: q_loss: 441.2679443359375, p_loss: 11.493048667907715, mean_rew: -7.527105935267669, variance: 4.376185162603611, lamda: 1.0114456415176392
Running avgs for agent 2: q_loss: 80.02583312988281, p_loss: -5.0411248207092285, mean_rew: -7.518804431271582, variance: 10.925624213257773, lamda: 1.0109003782272339

steps: 74975, episodes: 3000, mean episode reward: -620.4986229098141, agent episode reward: [-206.83287430327135, -206.83287430327135, -206.83287430327135], time: 117.749
steps: 74975, episodes: 3000, mean episode variance: 7.753923806190491, agent episode variance: [1.9343157601356507, 3.8787913665771483, 1.9408166794776915], time: 117.749
Running avgs for agent 0: q_loss: 22.55313491821289, p_loss: -4.321664810180664, mean_rew: -7.818540854758416, variance: 7.737263202667236, lamda: 1.0349435806274414
Running avgs for agent 1: q_loss: 5045.9169921875, p_loss: 27.202672958374023, mean_rew: -7.831122586269976, variance: 15.515165466308593, lamda: 1.0361820459365845
Running avgs for agent 2: q_loss: 23.52549171447754, p_loss: -4.2501397132873535, mean_rew: -7.831224666700838, variance: 7.7632670402526855, lamda: 1.0353376865386963

steps: 99975, episodes: 4000, mean episode reward: -566.3067533991559, agent episode reward: [-188.76891779971868, -188.76891779971868, -188.76891779971868], time: 115.207
steps: 99975, episodes: 4000, mean episode variance: 8.700003016233444, agent episode variance: [1.8758964314460755, 4.935709769487381, 1.8883968152999877], time: 115.208
Running avgs for agent 0: q_loss: 18.381946563720703, p_loss: -4.0748491287231445, mean_rew: -7.849993051713496, variance: 7.5035858154296875, lamda: 1.0592412948608398
Running avgs for agent 1: q_loss: 14973.568359375, p_loss: 42.82909393310547, mean_rew: -7.857492812939854, variance: 19.742839077949522, lamda: 1.0611965656280518
Running avgs for agent 2: q_loss: 19.804597854614258, p_loss: -4.0959696769714355, mean_rew: -7.8592855408958915, variance: 7.553587436676025, lamda: 1.0596715211868286

steps: 124975, episodes: 5000, mean episode reward: -514.9479049535105, agent episode reward: [-171.6493016511702, -171.6493016511702, -171.6493016511702], time: 118.092
steps: 124975, episodes: 5000, mean episode variance: 13.01306941127777, agent episode variance: [1.7986545357704162, 9.405848044872284, 1.8085668306350708], time: 118.092
Running avgs for agent 0: q_loss: 18.224458694458008, p_loss: -3.9916656017303467, mean_rew: -7.712411281941631, variance: 7.194618225097656, lamda: 1.0795634984970093
Running avgs for agent 1: q_loss: 36573.20703125, p_loss: 56.426048278808594, mean_rew: -7.714025573204413, variance: 37.62339217948914, lamda: 1.0862008333206177
Running avgs for agent 2: q_loss: 15.873815536499023, p_loss: -3.969987630844116, mean_rew: -7.715470911316382, variance: 7.234267711639404, lamda: 1.0821894407272339

steps: 149975, episodes: 6000, mean episode reward: -493.3745612127527, agent episode reward: [-164.45818707091757, -164.45818707091757, -164.45818707091757], time: 163.286
steps: 149975, episodes: 6000, mean episode variance: 16.755831486701965, agent episode variance: [1.7393562383651733, 13.292384283065797, 1.7240909652709961], time: 163.287
Running avgs for agent 0: q_loss: 18.16899299621582, p_loss: -3.890315532684326, mean_rew: -7.5264846163268535, variance: 6.957424640655518, lamda: 1.0857104063034058
Running avgs for agent 1: q_loss: 57070.65234375, p_loss: 66.92818450927734, mean_rew: -7.525174879777519, variance: 53.16953713226319, lamda: 1.111204981803894
Running avgs for agent 2: q_loss: 14.951594352722168, p_loss: -3.8829286098480225, mean_rew: -7.52813544659517, variance: 6.896363735198975, lamda: 1.1003345251083374

steps: 174975, episodes: 7000, mean episode reward: -477.34137590685475, agent episode reward: [-159.11379196895157, -159.11379196895157, -159.11379196895157], time: 162.658
steps: 174975, episodes: 7000, mean episode variance: 28.208742656469344, agent episode variance: [1.7076057982444763, 24.839296293258666, 1.6618405649662018], time: 162.659
Running avgs for agent 0: q_loss: 16.2247257232666, p_loss: -3.795464277267456, mean_rew: -7.3656470349295065, variance: 6.830423355102539, lamda: 1.0870025157928467
Running avgs for agent 1: q_loss: 134082.203125, p_loss: 74.1502456665039, mean_rew: -7.357442088084925, variance: 99.35718517303467, lamda: 1.1362091302871704
Running avgs for agent 2: q_loss: 14.382503509521484, p_loss: -3.8143186569213867, mean_rew: -7.3599440688915205, variance: 6.647362232208252, lamda: 1.1219371557235718

steps: 199975, episodes: 8000, mean episode reward: -475.2292504186551, agent episode reward: [-158.4097501395517, -158.4097501395517, -158.4097501395517], time: 158.219
steps: 199975, episodes: 8000, mean episode variance: 34.10354477953911, agent episode variance: [1.6770676364898682, 30.817503372192384, 1.6089737708568572], time: 158.22
Running avgs for agent 0: q_loss: 11.212238311767578, p_loss: -3.7031614780426025, mean_rew: -7.229455940746489, variance: 6.708271026611328, lamda: 1.0950076580047607
Running avgs for agent 1: q_loss: 186272.28125, p_loss: 79.24803924560547, mean_rew: -7.235011915348298, variance: 123.27001348876954, lamda: 1.1612132787704468
Running avgs for agent 2: q_loss: 14.083636283874512, p_loss: -3.7298436164855957, mean_rew: -7.226320273321563, variance: 6.435894966125488, lamda: 1.1367112398147583

steps: 224975, episodes: 9000, mean episode reward: -474.65748337245844, agent episode reward: [-158.21916112415283, -158.21916112415283, -158.21916112415283], time: 168.047
steps: 224975, episodes: 9000, mean episode variance: 36.223764050006864, agent episode variance: [1.6299603161811829, 33.03111382293701, 1.5626899108886718], time: 168.047
Running avgs for agent 0: q_loss: 11.600347518920898, p_loss: -3.643836259841919, mean_rew: -7.132225331536314, variance: 6.519841194152832, lamda: 1.114540696144104
Running avgs for agent 1: q_loss: 220553.953125, p_loss: 83.75282287597656, mean_rew: -7.12778706947705, variance: 132.12445529174803, lamda: 1.1862174272537231
Running avgs for agent 2: q_loss: 12.746949195861816, p_loss: -3.6738243103027344, mean_rew: -7.117337523626415, variance: 6.250759601593018, lamda: 1.1524587869644165

steps: 249975, episodes: 10000, mean episode reward: -482.6162726424408, agent episode reward: [-160.8720908808136, -160.8720908808136, -160.8720908808136], time: 166.561
steps: 249975, episodes: 10000, mean episode variance: 38.87284478330612, agent episode variance: [1.5846804251670839, 35.745266502380375, 1.542897855758667], time: 166.561
Running avgs for agent 0: q_loss: 11.99683952331543, p_loss: -3.57734751701355, mean_rew: -7.047331771527532, variance: 6.338721752166748, lamda: 1.1366815567016602
Running avgs for agent 1: q_loss: 241756.9375, p_loss: 87.64909362792969, mean_rew: -7.045019065976599, variance: 142.9810660095215, lamda: 1.21122145652771
Running avgs for agent 2: q_loss: 9.971435546875, p_loss: -3.6240220069885254, mean_rew: -7.049122943078272, variance: 6.171591281890869, lamda: 1.165658950805664

steps: 274975, episodes: 11000, mean episode reward: -486.54855316254765, agent episode reward: [-162.18285105418258, -162.18285105418258, -162.18285105418258], time: 163.549
steps: 274975, episodes: 11000, mean episode variance: 39.93624398231506, agent episode variance: [1.5373003854751588, 36.89755848693848, 1.5013851099014281], time: 163.55
Running avgs for agent 0: q_loss: 13.218791007995605, p_loss: -3.5826947689056396, mean_rew: -6.994746404325382, variance: 6.1492018699646, lamda: 1.1504709720611572
Running avgs for agent 1: q_loss: 272747.15625, p_loss: 90.82162475585938, mean_rew: -6.982531531750649, variance: 147.59023394775392, lamda: 1.2362257242202759
Running avgs for agent 2: q_loss: 9.934337615966797, p_loss: -3.60396146774292, mean_rew: -6.988134083875243, variance: 6.005540370941162, lamda: 1.1867121458053589

steps: 299975, episodes: 12000, mean episode reward: -488.10941787552974, agent episode reward: [-162.70313929184326, -162.70313929184326, -162.70313929184326], time: 164.783
steps: 299975, episodes: 12000, mean episode variance: 40.86896153211594, agent episode variance: [1.548963675737381, 37.85522242736816, 1.4647754290103912], time: 164.784
Running avgs for agent 0: q_loss: 12.230972290039062, p_loss: -3.546074867248535, mean_rew: -6.951575562892701, variance: 6.195854663848877, lamda: 1.15324068069458
Running avgs for agent 1: q_loss: 303522.9375, p_loss: 94.30856323242188, mean_rew: -6.9530503619682085, variance: 151.42088970947265, lamda: 1.2612298727035522
Running avgs for agent 2: q_loss: 11.84642219543457, p_loss: -3.5797200202941895, mean_rew: -6.95105345473888, variance: 5.85910177230835, lamda: 1.202858567237854

steps: 324975, episodes: 13000, mean episode reward: -490.02557243811845, agent episode reward: [-163.3418574793728, -163.3418574793728, -163.3418574793728], time: 163.557
steps: 324975, episodes: 13000, mean episode variance: 43.50626291084289, agent episode variance: [1.5228622899055482, 40.53851005554199, 1.4448905653953552], time: 163.558
Running avgs for agent 0: q_loss: 10.936590194702148, p_loss: -3.5186944007873535, mean_rew: -6.91056818564007, variance: 6.09144926071167, lamda: 1.1588146686553955
Running avgs for agent 1: q_loss: 322662.90625, p_loss: 97.68402099609375, mean_rew: -6.915848616200261, variance: 162.15404022216796, lamda: 1.286233901977539
Running avgs for agent 2: q_loss: 9.07142162322998, p_loss: -3.5498130321502686, mean_rew: -6.9138009432886625, variance: 5.779562473297119, lamda: 1.2207798957824707

steps: 349975, episodes: 14000, mean episode reward: -488.14980979970363, agent episode reward: [-162.71660326656786, -162.71660326656786, -162.71660326656786], time: 163.759
steps: 349975, episodes: 14000, mean episode variance: 43.952329095363616, agent episode variance: [1.498676052570343, 41.02906199645996, 1.424591046333313], time: 163.76
Running avgs for agent 0: q_loss: 8.8734130859375, p_loss: -3.5107946395874023, mean_rew: -6.892248750773251, variance: 5.994704246520996, lamda: 1.1742346286773682
Running avgs for agent 1: q_loss: 349628.21875, p_loss: 100.87893676757812, mean_rew: -6.889079137575625, variance: 164.11624798583983, lamda: 1.3112380504608154
Running avgs for agent 2: q_loss: 8.905498504638672, p_loss: -3.525277614593506, mean_rew: -6.88037834183065, variance: 5.6983642578125, lamda: 1.2416547536849976

steps: 374975, episodes: 15000, mean episode reward: -493.82126528332117, agent episode reward: [-164.60708842777373, -164.60708842777373, -164.60708842777373], time: 162.518
steps: 374975, episodes: 15000, mean episode variance: 44.39685520029068, agent episode variance: [1.469520046710968, 41.53252008056641, 1.3948150730133058], time: 162.518
Running avgs for agent 0: q_loss: 9.207916259765625, p_loss: -3.5015623569488525, mean_rew: -6.8651365046620265, variance: 5.878079891204834, lamda: 1.1922849416732788
Running avgs for agent 1: q_loss: 377285.34375, p_loss: 103.52481079101562, mean_rew: -6.860633969930716, variance: 166.13008032226563, lamda: 1.3362423181533813
Running avgs for agent 2: q_loss: 8.234903335571289, p_loss: -3.5193419456481934, mean_rew: -6.865816733951593, variance: 5.579260349273682, lamda: 1.2643905878067017

steps: 399975, episodes: 16000, mean episode reward: -488.5989180007796, agent episode reward: [-162.86630600025987, -162.86630600025987, -162.86630600025987], time: 165.215
steps: 399975, episodes: 16000, mean episode variance: 46.025825410604476, agent episode variance: [1.4608600070476532, 43.190464401245116, 1.3745010023117066], time: 165.215
Running avgs for agent 0: q_loss: 10.04335880279541, p_loss: -3.487104892730713, mean_rew: -6.837454493917472, variance: 5.843440532684326, lamda: 1.200132131576538
Running avgs for agent 1: q_loss: 394754.59375, p_loss: 105.91224670410156, mean_rew: -6.841210626945727, variance: 172.76185760498046, lamda: 1.3612463474273682
Running avgs for agent 2: q_loss: 9.11418342590332, p_loss: -3.507511615753174, mean_rew: -6.842929660405434, variance: 5.498003959655762, lamda: 1.281305193901062

steps: 424975, episodes: 17000, mean episode reward: -491.68331166051513, agent episode reward: [-163.8944372201717, -163.8944372201717, -163.8944372201717], time: 168.332
steps: 424975, episodes: 17000, mean episode variance: 45.61737131977081, agent episode variance: [1.4548964123725892, 42.81577384948731, 1.346701057910919], time: 168.332
Running avgs for agent 0: q_loss: 7.793808460235596, p_loss: -3.466097116470337, mean_rew: -6.820353196023803, variance: 5.81958532333374, lamda: 1.2060935497283936
Running avgs for agent 1: q_loss: 414857.09375, p_loss: 108.33152770996094, mean_rew: -6.827914791914391, variance: 171.26309539794923, lamda: 1.3862504959106445
Running avgs for agent 2: q_loss: 8.49345874786377, p_loss: -3.5122010707855225, mean_rew: -6.825750112357522, variance: 5.386804103851318, lamda: 1.2971439361572266

steps: 449975, episodes: 18000, mean episode reward: -494.9296684118541, agent episode reward: [-164.97655613728466, -164.97655613728466, -164.97655613728466], time: 167.907
steps: 449975, episodes: 18000, mean episode variance: 45.76103762459755, agent episode variance: [1.4482646403312682, 42.978418144226076, 1.334354840040207], time: 167.907
Running avgs for agent 0: q_loss: 7.497859477996826, p_loss: -3.4659738540649414, mean_rew: -6.811717111047507, variance: 5.793058395385742, lamda: 1.219433069229126
Running avgs for agent 1: q_loss: 448085.0625, p_loss: 110.47832489013672, mean_rew: -6.811261596307909, variance: 171.9136725769043, lamda: 1.4112547636032104
Running avgs for agent 2: q_loss: 8.430171966552734, p_loss: -3.4951860904693604, mean_rew: -6.813284184767577, variance: 5.337419509887695, lamda: 1.3125914335250854

steps: 474975, episodes: 19000, mean episode reward: -495.46726044923474, agent episode reward: [-165.15575348307826, -165.15575348307826, -165.15575348307826], time: 165.793
steps: 474975, episodes: 19000, mean episode variance: 48.35504340624809, agent episode variance: [1.4128909542560577, 45.63380271911621, 1.308349732875824], time: 165.794
Running avgs for agent 0: q_loss: 7.600147247314453, p_loss: -3.4678685665130615, mean_rew: -6.801947281110615, variance: 5.651564598083496, lamda: 1.231809139251709
Running avgs for agent 1: q_loss: 459070.15625, p_loss: 112.2021255493164, mean_rew: -6.808353426586484, variance: 182.53521087646484, lamda: 1.4362587928771973
Running avgs for agent 2: q_loss: 7.579808712005615, p_loss: -3.490892171859741, mean_rew: -6.80170339993453, variance: 5.233398914337158, lamda: 1.3318486213684082

steps: 499975, episodes: 20000, mean episode reward: -505.77842018648295, agent episode reward: [-168.59280672882764, -168.59280672882764, -168.59280672882764], time: 164.667
steps: 499975, episodes: 20000, mean episode variance: 46.85803919410706, agent episode variance: [1.4033207187652588, 44.16921879577637, 1.2854996795654297], time: 164.668
Running avgs for agent 0: q_loss: 7.738133907318115, p_loss: -3.462580680847168, mean_rew: -6.798350865102081, variance: 5.613283157348633, lamda: 1.2344083786010742
Running avgs for agent 1: q_loss: 494211.90625, p_loss: 113.84162139892578, mean_rew: -6.7884061154912585, variance: 176.67687518310547, lamda: 1.4612630605697632
Running avgs for agent 2: q_loss: 7.233671188354492, p_loss: -3.4905717372894287, mean_rew: -6.793249002683359, variance: 5.141998767852783, lamda: 1.3507397174835205

steps: 524975, episodes: 21000, mean episode reward: -514.4189677535603, agent episode reward: [-171.47298925118676, -171.47298925118676, -171.47298925118676], time: 165.924
steps: 524975, episodes: 21000, mean episode variance: 47.60792916870117, agent episode variance: [1.407937399148941, 44.92286395263672, 1.277127816915512], time: 165.925
Running avgs for agent 0: q_loss: 7.040452003479004, p_loss: -3.4529247283935547, mean_rew: -6.79174683733422, variance: 5.631750106811523, lamda: 1.2385514974594116
Running avgs for agent 1: q_loss: 520881.875, p_loss: 115.59172058105469, mean_rew: -6.79440130355987, variance: 179.69145581054687, lamda: 1.4862672090530396
Running avgs for agent 2: q_loss: 6.729111671447754, p_loss: -3.486199378967285, mean_rew: -6.788001256297288, variance: 5.108511924743652, lamda: 1.3651131391525269

steps: 549975, episodes: 22000, mean episode reward: -519.7407416825654, agent episode reward: [-173.24691389418848, -173.24691389418848, -173.24691389418848], time: 165.406
steps: 549975, episodes: 22000, mean episode variance: 50.58569485163689, agent episode variance: [1.3983497858047484, 47.9346468963623, 1.2526981694698334], time: 165.407
Running avgs for agent 0: q_loss: 6.281445503234863, p_loss: -3.4512646198272705, mean_rew: -6.794456192393582, variance: 5.5933990478515625, lamda: 1.2485331296920776
Running avgs for agent 1: q_loss: 535425.1875, p_loss: 117.0309829711914, mean_rew: -6.794963691620938, variance: 191.7385875854492, lamda: 1.5112712383270264
Running avgs for agent 2: q_loss: 6.2302656173706055, p_loss: -3.4900107383728027, mean_rew: -6.796992720576664, variance: 5.0107927322387695, lamda: 1.3792805671691895

steps: 574975, episodes: 23000, mean episode reward: -532.1576054797071, agent episode reward: [-177.38586849323568, -177.38586849323568, -177.38586849323568], time: 165.575
steps: 574975, episodes: 23000, mean episode variance: 48.57464985990524, agent episode variance: [1.3850142347812653, 45.93256958007812, 1.2570660450458526], time: 165.576
Running avgs for agent 0: q_loss: 6.444492340087891, p_loss: -3.4488461017608643, mean_rew: -6.808699639701101, variance: 5.5400567054748535, lamda: 1.2538378238677979
Running avgs for agent 1: q_loss: 556480.1875, p_loss: 118.44034576416016, mean_rew: -6.813675019050908, variance: 183.7302783203125, lamda: 1.5362753868103027
Running avgs for agent 2: q_loss: 7.691375255584717, p_loss: -3.5077030658721924, mean_rew: -6.8100015185743805, variance: 5.028264045715332, lamda: 1.3871203660964966

steps: 599975, episodes: 24000, mean episode reward: -543.3520652748595, agent episode reward: [-181.11735509161983, -181.11735509161983, -181.11735509161983], time: 167.775
steps: 599975, episodes: 24000, mean episode variance: 48.02253167295456, agent episode variance: [1.3960846290588378, 45.37837094116211, 1.248076102733612], time: 167.775
Running avgs for agent 0: q_loss: 6.468184471130371, p_loss: -3.4609673023223877, mean_rew: -6.821213422094102, variance: 5.584338665008545, lamda: 1.2554643154144287
Running avgs for agent 1: q_loss: 576529.9375, p_loss: 119.5910873413086, mean_rew: -6.821393729126334, variance: 181.51348376464844, lamda: 1.5612796545028687
Running avgs for agent 2: q_loss: 6.94077730178833, p_loss: -3.507704496383667, mean_rew: -6.8244271632986715, variance: 4.992304801940918, lamda: 1.3896714448928833

steps: 624975, episodes: 25000, mean episode reward: -544.431092539452, agent episode reward: [-181.477030846484, -181.477030846484, -181.477030846484], time: 165.772
steps: 624975, episodes: 25000, mean episode variance: 48.29834688711166, agent episode variance: [1.3879558053016663, 45.664234115600586, 1.2461569662094116], time: 165.772
Running avgs for agent 0: q_loss: 5.8922200202941895, p_loss: -3.4744105339050293, mean_rew: -6.837996461802724, variance: 5.551823139190674, lamda: 1.2579537630081177
Running avgs for agent 1: q_loss: 599006.5625, p_loss: 120.95540618896484, mean_rew: -6.838379161129623, variance: 182.65693646240234, lamda: 1.586283802986145
Running avgs for agent 2: q_loss: 7.001996040344238, p_loss: -3.5103745460510254, mean_rew: -6.834341035399929, variance: 4.984627723693848, lamda: 1.3952869176864624

steps: 649975, episodes: 26000, mean episode reward: -545.561201549159, agent episode reward: [-181.8537338497197, -181.8537338497197, -181.8537338497197], time: 166.245
steps: 649975, episodes: 26000, mean episode variance: 48.680611375570294, agent episode variance: [1.3794771246910096, 46.055987602233884, 1.245146648645401], time: 166.246
Running avgs for agent 0: q_loss: 5.396368026733398, p_loss: -3.484388589859009, mean_rew: -6.862483670812425, variance: 5.517908096313477, lamda: 1.2668699026107788
Running avgs for agent 1: q_loss: 620879.375, p_loss: 122.05574035644531, mean_rew: -6.859074971309944, variance: 184.22395040893554, lamda: 1.6112878322601318
Running avgs for agent 2: q_loss: 6.651206016540527, p_loss: -3.5135927200317383, mean_rew: -6.864614791069179, variance: 4.980586528778076, lamda: 1.4085749387741089

steps: 674975, episodes: 27000, mean episode reward: -553.4487907626992, agent episode reward: [-184.48293025423308, -184.48293025423308, -184.48293025423308], time: 164.287
steps: 674975, episodes: 27000, mean episode variance: 48.93600401496887, agent episode variance: [1.3806591885089874, 46.3174944152832, 1.2378504111766815], time: 164.288
Running avgs for agent 0: q_loss: 5.159616470336914, p_loss: -3.4901363849639893, mean_rew: -6.8786612877068345, variance: 5.522636890411377, lamda: 1.276903510093689
Running avgs for agent 1: q_loss: 627877.125, p_loss: 123.01490020751953, mean_rew: -6.87431431639612, variance: 185.2699776611328, lamda: 1.6362920999526978
Running avgs for agent 2: q_loss: 6.358910083770752, p_loss: -3.5234057903289795, mean_rew: -6.875189236065155, variance: 4.951401233673096, lamda: 1.424194097518921

steps: 699975, episodes: 28000, mean episode reward: -549.1958095510726, agent episode reward: [-183.06526985035748, -183.06526985035748, -183.06526985035748], time: 169.302
steps: 699975, episodes: 28000, mean episode variance: 52.17255668997765, agent episode variance: [1.3763239357471466, 49.572550445556644, 1.2236823086738586], time: 169.302
Running avgs for agent 0: q_loss: 5.064595699310303, p_loss: -3.5059781074523926, mean_rew: -6.899487327216683, variance: 5.505295753479004, lamda: 1.2821649312973022
Running avgs for agent 1: q_loss: 639970.5625, p_loss: 123.99282836914062, mean_rew: -6.897968639862322, variance: 198.29020178222657, lamda: 1.6612961292266846
Running avgs for agent 2: q_loss: 7.417698383331299, p_loss: -3.5430233478546143, mean_rew: -6.894581796465945, variance: 4.8947296142578125, lamda: 1.433956503868103

steps: 724975, episodes: 29000, mean episode reward: -552.9961258929113, agent episode reward: [-184.3320419643038, -184.3320419643038, -184.3320419643038], time: 168.918
steps: 724975, episodes: 29000, mean episode variance: 49.4590755879879, agent episode variance: [1.3703159074783324, 46.86346826171875, 1.2252914187908173], time: 168.918
Running avgs for agent 0: q_loss: 5.334316730499268, p_loss: -3.5095298290252686, mean_rew: -6.90645538470905, variance: 5.481263637542725, lamda: 1.2914855480194092
Running avgs for agent 1: q_loss: 666604.3125, p_loss: 125.34820556640625, mean_rew: -6.907962604376679, variance: 187.453873046875, lamda: 1.686300277709961
Running avgs for agent 2: q_loss: 7.4011759757995605, p_loss: -3.543165683746338, mean_rew: -6.912627640611539, variance: 4.901165962219238, lamda: 1.4361169338226318

steps: 749975, episodes: 30000, mean episode reward: -555.659351166649, agent episode reward: [-185.2197837222164, -185.2197837222164, -185.2197837222164], time: 168.678
steps: 749975, episodes: 30000, mean episode variance: 50.0274096300602, agent episode variance: [1.3727323732376098, 47.431158401489256, 1.2235188553333283], time: 168.679
Running avgs for agent 0: q_loss: 5.275023460388184, p_loss: -3.5255939960479736, mean_rew: -6.931209916167251, variance: 5.49092960357666, lamda: 1.2952545881271362
Running avgs for agent 1: q_loss: 694505.375, p_loss: 126.75092315673828, mean_rew: -6.930747930638039, variance: 189.72463360595702, lamda: 1.7113045454025269
Running avgs for agent 2: q_loss: 6.623610973358154, p_loss: -3.548736810684204, mean_rew: -6.92540386685999, variance: 4.894075393676758, lamda: 1.4393110275268555

steps: 774975, episodes: 31000, mean episode reward: -557.9516971210804, agent episode reward: [-185.98389904036014, -185.98389904036014, -185.98389904036014], time: 162.952
steps: 774975, episodes: 31000, mean episode variance: 50.62124036478996, agent episode variance: [1.368947295188904, 48.03098245239258, 1.2213106172084809], time: 162.952
Running avgs for agent 0: q_loss: 5.379579067230225, p_loss: -3.5243585109710693, mean_rew: -6.944596810293837, variance: 5.475789546966553, lamda: 1.3009649515151978
Running avgs for agent 1: q_loss: 702462.4375, p_loss: 127.89017486572266, mean_rew: -6.936912392632692, variance: 192.1239298095703, lamda: 1.7363085746765137
Running avgs for agent 2: q_loss: 6.997982501983643, p_loss: -3.562974691390991, mean_rew: -6.943119935961, variance: 4.885242462158203, lamda: 1.4457783699035645

steps: 799975, episodes: 32000, mean episode reward: -566.3885769478087, agent episode reward: [-188.79619231593622, -188.79619231593622, -188.79619231593622], time: 162.859
steps: 799975, episodes: 32000, mean episode variance: 52.427276874303814, agent episode variance: [1.3638196244239806, 49.84049432373047, 1.2229629261493682], time: 162.859
Running avgs for agent 0: q_loss: 5.144379138946533, p_loss: -3.544402837753296, mean_rew: -6.959466083124417, variance: 5.4552788734436035, lamda: 1.3033394813537598
Running avgs for agent 1: q_loss: 735927.125, p_loss: 129.25851440429688, mean_rew: -6.954340062761569, variance: 199.36197729492187, lamda: 1.76131272315979
Running avgs for agent 2: q_loss: 7.604605674743652, p_loss: -3.582282066345215, mean_rew: -6.964869276383687, variance: 4.891851425170898, lamda: 1.4488791227340698

steps: 824975, episodes: 33000, mean episode reward: -564.1031815617212, agent episode reward: [-188.034393853907, -188.034393853907, -188.034393853907], time: 165.945
steps: 824975, episodes: 33000, mean episode variance: 52.24274357128143, agent episode variance: [1.3697176306247711, 49.65528219604492, 1.21774374461174], time: 165.945
Running avgs for agent 0: q_loss: 4.906505107879639, p_loss: -3.5579135417938232, mean_rew: -6.979423262121637, variance: 5.478870391845703, lamda: 1.3039137125015259
Running avgs for agent 1: q_loss: 766741.875, p_loss: 130.33656311035156, mean_rew: -6.980318619938247, variance: 198.6211287841797, lamda: 1.7863168716430664
Running avgs for agent 2: q_loss: 5.987945079803467, p_loss: -3.5816006660461426, mean_rew: -6.979613869966631, variance: 4.870974540710449, lamda: 1.4601445198059082

steps: 849975, episodes: 34000, mean episode reward: -570.9036789708458, agent episode reward: [-190.30122632361523, -190.30122632361523, -190.30122632361523], time: 163.175
steps: 849975, episodes: 34000, mean episode variance: 51.030449170827865, agent episode variance: [1.3714767966270447, 48.44208897399902, 1.2168834002017974], time: 163.175
Running avgs for agent 0: q_loss: 5.352819442749023, p_loss: -3.548830509185791, mean_rew: -7.002013810075557, variance: 5.485907077789307, lamda: 1.3091398477554321
Running avgs for agent 1: q_loss: 777932.9375, p_loss: 131.26869201660156, mean_rew: -6.999307304775709, variance: 193.76835589599608, lamda: 1.8113210201263428
Running avgs for agent 2: q_loss: 5.193379878997803, p_loss: -3.58571457862854, mean_rew: -6.996474240691514, variance: 4.867533206939697, lamda: 1.4706134796142578

steps: 874975, episodes: 35000, mean episode reward: -574.5244270778417, agent episode reward: [-191.5081423592806, -191.5081423592806, -191.5081423592806], time: 167.004
steps: 874975, episodes: 35000, mean episode variance: 51.79803528547287, agent episode variance: [1.3565806794166564, 49.23213854980469, 1.209316056251526], time: 167.004
Running avgs for agent 0: q_loss: 5.203646659851074, p_loss: -3.572937488555908, mean_rew: -7.019132159952252, variance: 5.426323413848877, lamda: 1.318697214126587
Running avgs for agent 1: q_loss: 811032.0625, p_loss: 132.23118591308594, mean_rew: -7.013297185145281, variance: 196.92855419921875, lamda: 1.8363251686096191
Running avgs for agent 2: q_loss: 6.11374044418335, p_loss: -3.596452236175537, mean_rew: -7.014108467548543, variance: 4.837264060974121, lamda: 1.478803277015686

steps: 899975, episodes: 36000, mean episode reward: -578.2743801416925, agent episode reward: [-192.7581267138975, -192.7581267138975, -192.7581267138975], time: 172.627
steps: 899975, episodes: 36000, mean episode variance: 52.3163697245121, agent episode variance: [1.3499052383899688, 49.75347706604004, 1.2129874200820923], time: 172.628
Running avgs for agent 0: q_loss: 5.221868991851807, p_loss: -3.5825376510620117, mean_rew: -7.033012486568364, variance: 5.39962100982666, lamda: 1.3288823366165161
Running avgs for agent 1: q_loss: 834744.125, p_loss: 133.0609893798828, mean_rew: -7.03305717542548, variance: 199.01390826416016, lamda: 1.861329436302185
Running avgs for agent 2: q_loss: 6.4629645347595215, p_loss: -3.6088173389434814, mean_rew: -7.038512728263888, variance: 4.851949691772461, lamda: 1.480662226676941

steps: 924975, episodes: 37000, mean episode reward: -570.0742231568258, agent episode reward: [-190.0247410522753, -190.0247410522753, -190.0247410522753], time: 166.063
steps: 924975, episodes: 37000, mean episode variance: 53.64613684868812, agent episode variance: [1.349064774274826, 51.08898921203613, 1.2080828623771667], time: 166.063
Running avgs for agent 0: q_loss: 4.986528396606445, p_loss: -3.5910537242889404, mean_rew: -7.051616494315133, variance: 5.39625883102417, lamda: 1.333721399307251
Running avgs for agent 1: q_loss: 849770.3125, p_loss: 133.7674102783203, mean_rew: -7.051134023165841, variance: 204.35595684814453, lamda: 1.8863335847854614
Running avgs for agent 2: q_loss: 6.455572128295898, p_loss: -3.6194090843200684, mean_rew: -7.055071854704651, variance: 4.83233118057251, lamda: 1.4819878339767456

steps: 949975, episodes: 38000, mean episode reward: -572.11761304948, agent episode reward: [-190.70587101649332, -190.70587101649332, -190.70587101649332], time: 168.806
steps: 949975, episodes: 38000, mean episode variance: 53.91040798997879, agent episode variance: [1.3505552277565003, 51.353697814941405, 1.2061549472808837], time: 168.806
Running avgs for agent 0: q_loss: 4.74092960357666, p_loss: -3.596696376800537, mean_rew: -7.066640074053516, variance: 5.402220726013184, lamda: 1.3415141105651855
Running avgs for agent 1: q_loss: 872263.625, p_loss: 134.31434631347656, mean_rew: -7.068255377926085, variance: 205.41479125976562, lamda: 1.9113376140594482
Running avgs for agent 2: q_loss: 5.671016216278076, p_loss: -3.6239209175109863, mean_rew: -7.06480923248368, variance: 4.824620246887207, lamda: 1.4883060455322266

steps: 974975, episodes: 39000, mean episode reward: -571.6346344391832, agent episode reward: [-190.54487814639435, -190.54487814639435, -190.54487814639435], time: 167.45
steps: 974975, episodes: 39000, mean episode variance: 52.056806744098665, agent episode variance: [1.341455150604248, 49.5027571105957, 1.212594482898712], time: 167.45
Running avgs for agent 0: q_loss: 5.10830545425415, p_loss: -3.607501983642578, mean_rew: -7.0866662443244985, variance: 5.365820407867432, lamda: 1.3503690958023071
Running avgs for agent 1: q_loss: 897379.875, p_loss: 135.1820068359375, mean_rew: -7.087243768861563, variance: 198.0110284423828, lamda: 1.9363417625427246
Running avgs for agent 2: q_loss: 6.479247093200684, p_loss: -3.635810136795044, mean_rew: -7.084868034989319, variance: 4.850378036499023, lamda: 1.4919140338897705

steps: 999975, episodes: 40000, mean episode reward: -570.8970624442724, agent episode reward: [-190.29902081475745, -190.29902081475745, -190.29902081475745], time: 164.544
steps: 999975, episodes: 40000, mean episode variance: 55.45928437399864, agent episode variance: [1.3440703403949739, 52.91089381408691, 1.2043202195167542], time: 164.545
Running avgs for agent 0: q_loss: 4.905146598815918, p_loss: -3.6190600395202637, mean_rew: -7.094708172375603, variance: 5.376281261444092, lamda: 1.353447437286377
Running avgs for agent 1: q_loss: 916834.5625, p_loss: 135.6032257080078, mean_rew: -7.096041295257995, variance: 211.64357525634765, lamda: 1.961345911026001
Running avgs for agent 2: q_loss: 5.659787654876709, p_loss: -3.6374576091766357, mean_rew: -7.100724060852692, variance: 4.8172807693481445, lamda: 1.5022797584533691

steps: 1024975, episodes: 41000, mean episode reward: -573.116521990231, agent episode reward: [-191.03884066341033, -191.03884066341033, -191.03884066341033], time: 165.415
steps: 1024975, episodes: 41000, mean episode variance: 53.28881299591065, agent episode variance: [1.335412079334259, 50.75221740722656, 1.201183509349823], time: 165.416
Running avgs for agent 0: q_loss: 4.657712936401367, p_loss: -3.6223819255828857, mean_rew: -7.115560844805432, variance: 5.341648578643799, lamda: 1.3556777238845825
Running avgs for agent 1: q_loss: 931416.4375, p_loss: 136.11190795898438, mean_rew: -7.111542641362723, variance: 203.00886962890624, lamda: 1.9863502979278564
Running avgs for agent 2: q_loss: 4.97733211517334, p_loss: -3.646848678588867, mean_rew: -7.111885162045307, variance: 4.804734230041504, lamda: 1.5138556957244873

steps: 1049975, episodes: 42000, mean episode reward: -573.3247512450025, agent episode reward: [-191.10825041500084, -191.10825041500084, -191.10825041500084], time: 165.998
steps: 1049975, episodes: 42000, mean episode variance: 54.00767031025887, agent episode variance: [1.3329486606121064, 51.47988090515137, 1.194840744495392], time: 165.999
Running avgs for agent 0: q_loss: 4.637882709503174, p_loss: -3.6223998069763184, mean_rew: -7.111356928431962, variance: 5.331794738769531, lamda: 1.360412359237671
Running avgs for agent 1: q_loss: 934166.9375, p_loss: 136.58213806152344, mean_rew: -7.109498429039782, variance: 205.91952362060547, lamda: 2.011340856552124
Running avgs for agent 2: q_loss: 4.127877712249756, p_loss: -3.638155698776245, mean_rew: -7.105455076128614, variance: 4.779362678527832, lamda: 1.5235588550567627

steps: 1074975, episodes: 43000, mean episode reward: -573.352339297143, agent episode reward: [-191.117446432381, -191.117446432381, -191.117446432381], time: 164.331
steps: 1074975, episodes: 43000, mean episode variance: 54.374662254571916, agent episode variance: [1.3362986209392547, 51.854114776611326, 1.1842488570213319], time: 164.331
Running avgs for agent 0: q_loss: 4.0240654945373535, p_loss: -3.616575002670288, mean_rew: -7.090474301411641, variance: 5.345194339752197, lamda: 1.3606293201446533
Running avgs for agent 1: q_loss: 934601.875, p_loss: 136.6971435546875, mean_rew: -7.090843147206369, variance: 207.4164591064453, lamda: 2.0363152027130127
Running avgs for agent 2: q_loss: 3.945352554321289, p_loss: -3.631657123565674, mean_rew: -7.083581054552619, variance: 4.736995220184326, lamda: 1.5305050611495972

steps: 1099975, episodes: 44000, mean episode reward: -569.3292078143671, agent episode reward: [-189.77640260478907, -189.77640260478907, -189.77640260478907], time: 163.479
steps: 1099975, episodes: 44000, mean episode variance: 52.79912698483467, agent episode variance: [1.333608412027359, 50.28377571105957, 1.1817428617477417], time: 163.479
Running avgs for agent 0: q_loss: 3.926191568374634, p_loss: -3.612128496170044, mean_rew: -7.083141405449329, variance: 5.334433555603027, lamda: 1.3608512878417969
Running avgs for agent 1: q_loss: 922711.6875, p_loss: 136.86593627929688, mean_rew: -7.087976149193301, variance: 201.13510284423828, lamda: 2.0612895488739014
Running avgs for agent 2: q_loss: 5.6285271644592285, p_loss: -3.635942220687866, mean_rew: -7.089300344459503, variance: 4.726971626281738, lamda: 1.5342228412628174

steps: 1124975, episodes: 45000, mean episode reward: -577.37648449838, agent episode reward: [-192.45882816612666, -192.45882816612666, -192.45882816612666], time: 164.972
steps: 1124975, episodes: 45000, mean episode variance: 52.99152869915962, agent episode variance: [1.333062607526779, 50.478986206054685, 1.1794798855781554], time: 164.972
Running avgs for agent 0: q_loss: 3.8531336784362793, p_loss: -3.611879825592041, mean_rew: -7.097531007571014, variance: 5.332250118255615, lamda: 1.3636364936828613
Running avgs for agent 1: q_loss: 927341.125, p_loss: 136.93275451660156, mean_rew: -7.098183180829391, variance: 201.91594482421874, lamda: 2.086263656616211
Running avgs for agent 2: q_loss: 5.936717987060547, p_loss: -3.6401937007904053, mean_rew: -7.096270677613763, variance: 4.71791934967041, lamda: 1.5344408750534058

steps: 1149975, episodes: 46000, mean episode reward: -583.4461985965548, agent episode reward: [-194.48206619885156, -194.48206619885156, -194.48206619885156], time: 165.62
steps: 1149975, episodes: 46000, mean episode variance: 54.7467951130867, agent episode variance: [1.3313846476078033, 52.23153948974609, 1.1838709757328034], time: 165.621
Running avgs for agent 0: q_loss: 4.0418219566345215, p_loss: -3.6242289543151855, mean_rew: -7.11849109976026, variance: 5.325538635253906, lamda: 1.3676831722259521
Running avgs for agent 1: q_loss: 928173.9375, p_loss: 136.9683837890625, mean_rew: -7.12254421676671, variance: 208.92615795898436, lamda: 2.1112382411956787
Running avgs for agent 2: q_loss: 6.064016819000244, p_loss: -3.6580002307891846, mean_rew: -7.124584460930402, variance: 4.735484600067139, lamda: 1.5344916582107544

steps: 1174975, episodes: 47000, mean episode reward: -588.1914591715367, agent episode reward: [-196.0638197238456, -196.0638197238456, -196.0638197238456], time: 166.286
steps: 1174975, episodes: 47000, mean episode variance: 52.93892283630371, agent episode variance: [1.3365765490531922, 50.41512399291992, 1.187222294330597], time: 166.286
Running avgs for agent 0: q_loss: 4.090602397918701, p_loss: -3.643798351287842, mean_rew: -7.15585429699807, variance: 5.346306324005127, lamda: 1.3677456378936768
Running avgs for agent 1: q_loss: 929397.625, p_loss: 137.10305786132812, mean_rew: -7.152384427702137, variance: 201.6604959716797, lamda: 2.1362123489379883
Running avgs for agent 2: q_loss: 5.91830587387085, p_loss: -3.673405170440674, mean_rew: -7.1566242089084815, variance: 4.748888969421387, lamda: 1.534773826599121

steps: 1199975, episodes: 48000, mean episode reward: -589.6251267210748, agent episode reward: [-196.54170890702494, -196.54170890702494, -196.54170890702494], time: 165.076
steps: 1199975, episodes: 48000, mean episode variance: 49.95744245505333, agent episode variance: [1.3428853921890258, 47.418032760620115, 1.1965243022441865], time: 165.076
Running avgs for agent 0: q_loss: 4.138875484466553, p_loss: -3.6683969497680664, mean_rew: -7.1948493309058525, variance: 5.371541976928711, lamda: 1.3677455186843872
Running avgs for agent 1: q_loss: 945386.5, p_loss: 137.36441040039062, mean_rew: -7.197097976679179, variance: 189.67213104248046, lamda: 2.161186933517456
Running avgs for agent 2: q_loss: 5.809125900268555, p_loss: -3.690411329269409, mean_rew: -7.190981131344066, variance: 4.786097049713135, lamda: 1.5350383520126343

steps: 1224975, episodes: 49000, mean episode reward: -590.8340761338577, agent episode reward: [-196.94469204461925, -196.94469204461925, -196.94469204461925], time: 165.576
steps: 1224975, episodes: 49000, mean episode variance: 51.7151440372467, agent episode variance: [1.3514936275482177, 49.161118240356444, 1.202532169342041], time: 165.577
Running avgs for agent 0: q_loss: 4.19022798538208, p_loss: -3.688988208770752, mean_rew: -7.233021318515536, variance: 5.405974388122559, lamda: 1.3678416013717651
Running avgs for agent 1: q_loss: 944822.75, p_loss: 137.57379150390625, mean_rew: -7.23920803427285, variance: 196.64447296142578, lamda: 2.1861610412597656
Running avgs for agent 2: q_loss: 5.93985652923584, p_loss: -3.714736223220825, mean_rew: -7.230750843912662, variance: 4.810128211975098, lamda: 1.5353740453720093

steps: 1249975, episodes: 50000, mean episode reward: -601.9468891783793, agent episode reward: [-200.64896305945973, -200.64896305945973, -200.64896305945973], time: 163.825
steps: 1249975, episodes: 50000, mean episode variance: 49.26499126315117, agent episode variance: [1.3547011260986328, 46.709435470581056, 1.2008546664714814], time: 163.826
Running avgs for agent 0: q_loss: 4.2299346923828125, p_loss: -3.71075177192688, mean_rew: -7.278179054157872, variance: 5.41880464553833, lamda: 1.3679461479187012
Running avgs for agent 1: q_loss: 953725.0625, p_loss: 137.84376525878906, mean_rew: -7.260992589624617, variance: 186.83774188232422, lamda: 2.2111356258392334
Running avgs for agent 2: q_loss: 5.993471145629883, p_loss: -3.7303013801574707, mean_rew: -7.265515163141808, variance: 4.8034186363220215, lamda: 1.5355349779129028

steps: 1274975, episodes: 51000, mean episode reward: -594.9022418943041, agent episode reward: [-198.30074729810138, -198.30074729810138, -198.30074729810138], time: 164.493
steps: 1274975, episodes: 51000, mean episode variance: 49.64210342240334, agent episode variance: [1.3661747665405273, 47.06536715698242, 1.2105614988803863], time: 164.494
Running avgs for agent 0: q_loss: 4.306572914123535, p_loss: -3.7275466918945312, mean_rew: -7.315130587815996, variance: 5.4646992683410645, lamda: 1.367946743965149
Running avgs for agent 1: q_loss: 971474.6875, p_loss: 138.14468383789062, mean_rew: -7.304219434916091, variance: 188.26146862792967, lamda: 2.236109972000122
Running avgs for agent 2: q_loss: 4.557666778564453, p_loss: -3.7464218139648438, mean_rew: -7.308466436784691, variance: 4.842246055603027, lamda: 1.54020357131958

steps: 1299975, episodes: 52000, mean episode reward: -597.464763776862, agent episode reward: [-199.15492125895406, -199.15492125895406, -199.15492125895406], time: 165.516
steps: 1299975, episodes: 52000, mean episode variance: 47.94432864618302, agent episode variance: [1.3632558290958405, 45.369650604248044, 1.2114222128391265], time: 165.516
Running avgs for agent 0: q_loss: 4.509885787963867, p_loss: -3.740361213684082, mean_rew: -7.341998989206598, variance: 5.453023433685303, lamda: 1.368902325630188
Running avgs for agent 1: q_loss: 985361.625, p_loss: 138.6715850830078, mean_rew: -7.346184790954163, variance: 181.47860241699217, lamda: 2.2610843181610107
Running avgs for agent 2: q_loss: 4.460926055908203, p_loss: -3.7733070850372314, mean_rew: -7.350630047934633, variance: 4.845688819885254, lamda: 1.5473624467849731

steps: 1324975, episodes: 53000, mean episode reward: -596.5537460758379, agent episode reward: [-198.85124869194598, -198.85124869194598, -198.85124869194598], time: 166.535
steps: 1324975, episodes: 53000, mean episode variance: 49.18507750344276, agent episode variance: [1.3682610774040223, 46.60676750183106, 1.2100489242076873], time: 166.535
Running avgs for agent 0: q_loss: 4.135858535766602, p_loss: -3.7609755992889404, mean_rew: -7.382540800154765, variance: 5.473044395446777, lamda: 1.3737330436706543
Running avgs for agent 1: q_loss: 994997.5625, p_loss: 138.92584228515625, mean_rew: -7.375751749820918, variance: 186.42707000732423, lamda: 2.2860586643218994
Running avgs for agent 2: q_loss: 5.246539115905762, p_loss: -3.787580966949463, mean_rew: -7.383852285209183, variance: 4.840195178985596, lamda: 1.5524390935897827

steps: 1349975, episodes: 54000, mean episode reward: -598.7132640663132, agent episode reward: [-199.57108802210442, -199.57108802210442, -199.57108802210442], time: 161.957
steps: 1349975, episodes: 54000, mean episode variance: 48.39561199975014, agent episode variance: [1.3780449390411378, 45.800712615966795, 1.2168544447422027], time: 161.958
Running avgs for agent 0: q_loss: 4.3655290603637695, p_loss: -3.780489206314087, mean_rew: -7.418864310138418, variance: 5.512179851531982, lamda: 1.3750104904174805
Running avgs for agent 1: q_loss: 1011509.375, p_loss: 139.31280517578125, mean_rew: -7.4206816914992775, variance: 183.20285046386718, lamda: 2.311033010482788
Running avgs for agent 2: q_loss: 5.946773529052734, p_loss: -3.8063697814941406, mean_rew: -7.417961312920203, variance: 4.867417812347412, lamda: 1.5539008378982544

steps: 1374975, episodes: 55000, mean episode reward: -603.3085500924645, agent episode reward: [-201.10285003082151, -201.10285003082151, -201.10285003082151], time: 163.374
steps: 1374975, episodes: 55000, mean episode variance: 50.12700741410256, agent episode variance: [1.3842685742378236, 47.522005889892576, 1.2207329499721526], time: 163.375
Running avgs for agent 0: q_loss: 4.474913120269775, p_loss: -3.7953951358795166, mean_rew: -7.447233969048275, variance: 5.537074089050293, lamda: 1.3752849102020264
Running avgs for agent 1: q_loss: 1025477.375, p_loss: 139.69337463378906, mean_rew: -7.451260919158212, variance: 190.0880235595703, lamda: 2.3360073566436768
Running avgs for agent 2: q_loss: 6.157618045806885, p_loss: -3.83194637298584, mean_rew: -7.461277284294065, variance: 4.882931709289551, lamda: 1.5540930032730103

steps: 1399975, episodes: 56000, mean episode reward: -611.5178604421143, agent episode reward: [-203.83928681403805, -203.83928681403805, -203.83928681403805], time: 163.85
steps: 1399975, episodes: 56000, mean episode variance: 49.55130378746986, agent episode variance: [1.38042053771019, 46.94802116394043, 1.2228620858192445], time: 163.851
Running avgs for agent 0: q_loss: 4.35088586807251, p_loss: -3.82303786277771, mean_rew: -7.502727307903311, variance: 5.5216827392578125, lamda: 1.3807884454727173
Running avgs for agent 1: q_loss: 1045609.875, p_loss: 140.08860778808594, mean_rew: -7.493228218715148, variance: 187.79208465576173, lamda: 2.3609814643859863
Running avgs for agent 2: q_loss: 6.199298858642578, p_loss: -3.8423430919647217, mean_rew: -7.490065548701596, variance: 4.891448020935059, lamda: 1.554093837738037

steps: 1424975, episodes: 57000, mean episode reward: -608.1859906553537, agent episode reward: [-202.7286635517846, -202.7286635517846, -202.7286635517846], time: 161.68
steps: 1424975, episodes: 57000, mean episode variance: 47.91922276425362, agent episode variance: [1.390225733757019, 45.29361318969727, 1.2353838407993316], time: 161.68
Running avgs for agent 0: q_loss: 4.27395486831665, p_loss: -3.8429014682769775, mean_rew: -7.531704965214284, variance: 5.5609025955200195, lamda: 1.3837624788284302
Running avgs for agent 1: q_loss: 1066061.625, p_loss: 140.54580688476562, mean_rew: -7.53227168796692, variance: 181.17445275878907, lamda: 2.385956048965454
Running avgs for agent 2: q_loss: 6.385629177093506, p_loss: -3.8667163848876953, mean_rew: -7.536460187394179, variance: 4.941534996032715, lamda: 1.5542327165603638

steps: 1449975, episodes: 58000, mean episode reward: -604.2121547212837, agent episode reward: [-201.40405157376125, -201.40405157376125, -201.40405157376125], time: 164.96
steps: 1449975, episodes: 58000, mean episode variance: 49.131853138923645, agent episode variance: [1.3970189552307128, 46.48922270202637, 1.2456114816665649], time: 164.96
Running avgs for agent 0: q_loss: 4.593078136444092, p_loss: -3.858854293823242, mean_rew: -7.573330522597178, variance: 5.588075160980225, lamda: 1.3859020471572876
Running avgs for agent 1: q_loss: 1077304.125, p_loss: 140.88104248046875, mean_rew: -7.572203422815112, variance: 185.95689080810547, lamda: 2.4109301567077637
Running avgs for agent 2: q_loss: 4.964837074279785, p_loss: -3.872667074203491, mean_rew: -7.566074857707999, variance: 4.98244571685791, lamda: 1.5575674772262573

steps: 1474975, episodes: 59000, mean episode reward: -608.5817608185524, agent episode reward: [-202.86058693951748, -202.86058693951748, -202.86058693951748], time: 170.924
steps: 1474975, episodes: 59000, mean episode variance: 49.98638621354103, agent episode variance: [1.396306866168976, 47.34662406921387, 1.2434552781581878], time: 170.925
Running avgs for agent 0: q_loss: 4.459634780883789, p_loss: -3.880129098892212, mean_rew: -7.605075787596703, variance: 5.5852274894714355, lamda: 1.3863011598587036
Running avgs for agent 1: q_loss: 1115221.75, p_loss: 141.28904724121094, mean_rew: -7.608538657084154, variance: 189.38649627685547, lamda: 2.4359047412872314
Running avgs for agent 2: q_loss: 6.177504539489746, p_loss: -3.8923373222351074, mean_rew: -7.603543099844712, variance: 4.97382116317749, lamda: 1.5626763105392456

steps: 1499975, episodes: 60000, mean episode reward: -604.0206493443883, agent episode reward: [-201.34021644812944, -201.34021644812944, -201.34021644812944], time: 169.591
steps: 1499975, episodes: 60000, mean episode variance: 48.82794407272339, agent episode variance: [1.4050630621910096, 46.17550830078125, 1.2473727097511291], time: 169.591
Running avgs for agent 0: q_loss: 4.26765251159668, p_loss: -3.8975069522857666, mean_rew: -7.643586603327401, variance: 5.6202521324157715, lamda: 1.3933074474334717
Running avgs for agent 1: q_loss: 1136512.75, p_loss: 141.78599548339844, mean_rew: -7.645483297017766, variance: 184.702033203125, lamda: 2.460878849029541
Running avgs for agent 2: q_loss: 6.330789566040039, p_loss: -3.9101171493530273, mean_rew: -7.642318738065496, variance: 4.989490985870361, lamda: 1.5633203983306885

steps: 1524975, episodes: 61000, mean episode reward: -607.1069182135111, agent episode reward: [-202.36897273783703, -202.36897273783703, -202.36897273783703], time: 167.441
steps: 1524975, episodes: 61000, mean episode variance: 49.53731751203537, agent episode variance: [1.3980324988365174, 46.88707147216797, 1.252213541030884], time: 167.441
Running avgs for agent 0: q_loss: 4.888108730316162, p_loss: -3.9214634895324707, mean_rew: -7.679898491711698, variance: 5.592129707336426, lamda: 1.398263692855835
Running avgs for agent 1: q_loss: 1168378.375, p_loss: 142.1957244873047, mean_rew: -7.679581461748148, variance: 187.54828588867187, lamda: 2.4858531951904297
Running avgs for agent 2: q_loss: 6.507164001464844, p_loss: -3.925001859664917, mean_rew: -7.6732919543934655, variance: 5.008854389190674, lamda: 1.5634479522705078

steps: 1549975, episodes: 62000, mean episode reward: -607.786535088694, agent episode reward: [-202.59551169623128, -202.59551169623128, -202.59551169623128], time: 167.84
steps: 1549975, episodes: 62000, mean episode variance: 50.379089272260664, agent episode variance: [1.4051577684879304, 47.721081420898436, 1.252850082874298], time: 167.841
Running avgs for agent 0: q_loss: 4.495585918426514, p_loss: -3.932835340499878, mean_rew: -7.706290001155648, variance: 5.620630741119385, lamda: 1.398606777191162
Running avgs for agent 1: q_loss: 1183833.75, p_loss: 142.52906799316406, mean_rew: -7.707266729320914, variance: 190.88432568359374, lamda: 2.5108277797698975
Running avgs for agent 2: q_loss: 6.565929412841797, p_loss: -3.9451234340667725, mean_rew: -7.701106691814819, variance: 5.01140022277832, lamda: 1.5638242959976196

steps: 1574975, episodes: 63000, mean episode reward: -605.1084279050226, agent episode reward: [-201.70280930167422, -201.70280930167422, -201.70280930167422], time: 168.704
steps: 1574975, episodes: 63000, mean episode variance: 48.73786008262634, agent episode variance: [1.4118991441726685, 46.07298831176758, 1.252972626686096], time: 168.704
Running avgs for agent 0: q_loss: 4.910112380981445, p_loss: -3.9456989765167236, mean_rew: -7.732735934776434, variance: 5.647596836090088, lamda: 1.399420976638794
Running avgs for agent 1: q_loss: 1225604.375, p_loss: 142.9154052734375, mean_rew: -7.729370894061909, variance: 184.29195324707032, lamda: 2.535801887512207
Running avgs for agent 2: q_loss: 5.963164806365967, p_loss: -3.9616799354553223, mean_rew: -7.731452632221868, variance: 5.011890411376953, lamda: 1.564786434173584

steps: 1599975, episodes: 64000, mean episode reward: -610.5232398847866, agent episode reward: [-203.50774662826223, -203.50774662826223, -203.50774662826223], time: 170.159
steps: 1599975, episodes: 64000, mean episode variance: 50.773171211719514, agent episode variance: [1.4141781272888183, 48.10441572570801, 1.2545773587226867], time: 170.16
Running avgs for agent 0: q_loss: 5.004098415374756, p_loss: -3.9599530696868896, mean_rew: -7.752834642804965, variance: 5.656713008880615, lamda: 1.3994213342666626
Running avgs for agent 1: q_loss: 1247925.0, p_loss: 143.36468505859375, mean_rew: -7.758839789245108, variance: 192.41766290283203, lamda: 2.560776472091675
Running avgs for agent 2: q_loss: 6.662176132202148, p_loss: -3.97220778465271, mean_rew: -7.758515649761828, variance: 5.018309593200684, lamda: 1.5673905611038208

steps: 1624975, episodes: 65000, mean episode reward: -610.5485867645817, agent episode reward: [-203.51619558819394, -203.51619558819394, -203.51619558819394], time: 167.69
steps: 1624975, episodes: 65000, mean episode variance: 49.85399862933159, agent episode variance: [1.4170145492553712, 47.172546463012694, 1.2644376170635223], time: 167.69
Running avgs for agent 0: q_loss: 5.041211128234863, p_loss: -3.969832181930542, mean_rew: -7.7724059536690095, variance: 5.668057441711426, lamda: 1.3996484279632568
Running avgs for agent 1: q_loss: 1271920.375, p_loss: 143.8468475341797, mean_rew: -7.777451097097544, variance: 188.69018585205077, lamda: 2.5857508182525635
Running avgs for agent 2: q_loss: 5.774545669555664, p_loss: -3.9801783561706543, mean_rew: -7.776249827257316, variance: 5.057750701904297, lamda: 1.5685549974441528

steps: 1649975, episodes: 66000, mean episode reward: -609.3097955652471, agent episode reward: [-203.1032651884157, -203.1032651884157, -203.1032651884157], time: 166.273
steps: 1649975, episodes: 66000, mean episode variance: 49.27197592163086, agent episode variance: [1.421164400100708, 46.587533660888674, 1.2632778606414794], time: 166.274
Running avgs for agent 0: q_loss: 5.111498832702637, p_loss: -3.9886622428894043, mean_rew: -7.800194750048862, variance: 5.684657573699951, lamda: 1.3997385501861572
Running avgs for agent 1: q_loss: 1310914.25, p_loss: 144.38699340820312, mean_rew: -7.79868439015858, variance: 186.3501346435547, lamda: 2.610725164413452
Running avgs for agent 2: q_loss: 6.065495014190674, p_loss: -3.9972832202911377, mean_rew: -7.799972811761283, variance: 5.053111553192139, lamda: 1.5751121044158936

steps: 1674975, episodes: 67000, mean episode reward: -602.9298589166675, agent episode reward: [-200.97661963888916, -200.97661963888916, -200.97661963888916], time: 164.749
steps: 1674975, episodes: 67000, mean episode variance: 50.040603509664535, agent episode variance: [1.4251796329021453, 47.352931900024416, 1.262491976737976], time: 164.749
Running avgs for agent 0: q_loss: 4.614777088165283, p_loss: -3.994846820831299, mean_rew: -7.818114668675615, variance: 5.700718879699707, lamda: 1.4021018743515015
Running avgs for agent 1: q_loss: 1337850.375, p_loss: 144.81333923339844, mean_rew: -7.815141218660437, variance: 189.41172760009766, lamda: 2.635699510574341
Running avgs for agent 2: q_loss: 5.97402811050415, p_loss: -4.006731986999512, mean_rew: -7.817669652902401, variance: 5.0499677658081055, lamda: 1.5779520273208618

steps: 1699975, episodes: 68000, mean episode reward: -605.7304587323063, agent episode reward: [-201.91015291076874, -201.91015291076874, -201.91015291076874], time: 164.874
steps: 1699975, episodes: 68000, mean episode variance: 50.5246169526577, agent episode variance: [1.4298651127815247, 47.82720237731934, 1.267549462556839], time: 164.874
Running avgs for agent 0: q_loss: 5.248635768890381, p_loss: -3.9966676235198975, mean_rew: -7.832290781927265, variance: 5.719460964202881, lamda: 1.4051982164382935
Running avgs for agent 1: q_loss: 1343380.0, p_loss: 145.11962890625, mean_rew: -7.8360390728050495, variance: 191.30880950927735, lamda: 2.6606738567352295
Running avgs for agent 2: q_loss: 6.67841100692749, p_loss: -4.021479606628418, mean_rew: -7.843529584048763, variance: 5.070198059082031, lamda: 1.5815678834915161

steps: 1724975, episodes: 69000, mean episode reward: -594.4237380415158, agent episode reward: [-198.1412460138386, -198.1412460138386, -198.1412460138386], time: 164.244
steps: 1724975, episodes: 69000, mean episode variance: 50.424764754295346, agent episode variance: [1.4268731718063354, 47.72615730285644, 1.2717342796325684], time: 164.245
Running avgs for agent 0: q_loss: 5.301315784454346, p_loss: -4.012894630432129, mean_rew: -7.852285788259869, variance: 5.707493305206299, lamda: 1.4051992893218994
Running avgs for agent 1: q_loss: 1360458.25, p_loss: 145.40628051757812, mean_rew: -7.853093481367332, variance: 190.90462921142577, lamda: 2.685648202896118
Running avgs for agent 2: q_loss: 6.201694488525391, p_loss: -4.02633810043335, mean_rew: -7.851294464028793, variance: 5.086936950683594, lamda: 1.5820977687835693

steps: 1749975, episodes: 70000, mean episode reward: -604.0804383161781, agent episode reward: [-201.36014610539272, -201.36014610539272, -201.36014610539272], time: 163.969
steps: 1749975, episodes: 70000, mean episode variance: 49.347167184829715, agent episode variance: [1.4303059096336366, 46.64925411987305, 1.2676071553230286], time: 163.969
Running avgs for agent 0: q_loss: 5.406493186950684, p_loss: -4.02085542678833, mean_rew: -7.861590469013114, variance: 5.721223831176758, lamda: 1.405199408531189
Running avgs for agent 1: q_loss: 1367904.25, p_loss: 145.6501007080078, mean_rew: -7.861861931603507, variance: 186.5970164794922, lamda: 2.710622549057007
Running avgs for agent 2: q_loss: 6.7751994132995605, p_loss: -4.035396099090576, mean_rew: -7.87212097458096, variance: 5.070428848266602, lamda: 1.5828073024749756

steps: 1774975, episodes: 71000, mean episode reward: -600.9568115564845, agent episode reward: [-200.3189371854948, -200.3189371854948, -200.3189371854948], time: 165.396
steps: 1774975, episodes: 71000, mean episode variance: 51.678082883119586, agent episode variance: [1.4300764536857604, 48.97792706298828, 1.2700793664455414], time: 165.397
Running avgs for agent 0: q_loss: 5.524788856506348, p_loss: -4.028865337371826, mean_rew: -7.8802586270453325, variance: 5.720305442810059, lamda: 1.4051991701126099
Running avgs for agent 1: q_loss: 1397594.75, p_loss: 146.08447265625, mean_rew: -7.886809771820883, variance: 195.91170825195312, lamda: 2.7355968952178955
Running avgs for agent 2: q_loss: 6.9328789710998535, p_loss: -4.044293403625488, mean_rew: -7.884132953432166, variance: 5.080317497253418, lamda: 1.5828381776809692

steps: 1799975, episodes: 72000, mean episode reward: -604.3573011130557, agent episode reward: [-201.4524337043519, -201.4524337043519, -201.4524337043519], time: 165.449
steps: 1799975, episodes: 72000, mean episode variance: 50.962298686742784, agent episode variance: [1.4330081148147582, 48.26156211853027, 1.2677284533977509], time: 165.449
Running avgs for agent 0: q_loss: 5.595386505126953, p_loss: -4.040835380554199, mean_rew: -7.893531680373747, variance: 5.732032775878906, lamda: 1.4051992893218994
Running avgs for agent 1: q_loss: 1419262.75, p_loss: 146.62924194335938, mean_rew: -7.896698045024693, variance: 193.0462484741211, lamda: 2.760571002960205
Running avgs for agent 2: q_loss: 6.710490703582764, p_loss: -4.054368019104004, mean_rew: -7.894152629704608, variance: 5.070914268493652, lamda: 1.583056926727295

steps: 1824975, episodes: 73000, mean episode reward: -597.5818118651456, agent episode reward: [-199.19393728838185, -199.19393728838185, -199.19393728838185], time: 162.448
steps: 1824975, episodes: 73000, mean episode variance: 49.56037407517433, agent episode variance: [1.442620864391327, 46.84911149597168, 1.2686417148113251], time: 162.449
Running avgs for agent 0: q_loss: 5.519177436828613, p_loss: -4.047177791595459, mean_rew: -7.912910409728001, variance: 5.77048397064209, lamda: 1.4051992893218994
Running avgs for agent 1: q_loss: 1437066.75, p_loss: 147.0595245361328, mean_rew: -7.912915376243, variance: 187.3964459838867, lamda: 2.785545587539673
Running avgs for agent 2: q_loss: 6.205524444580078, p_loss: -4.066162586212158, mean_rew: -7.9160593812815225, variance: 5.0745673179626465, lamda: 1.5851496458053589

steps: 1849975, episodes: 74000, mean episode reward: -602.59148062331, agent episode reward: [-200.86382687443663, -200.86382687443663, -200.86382687443663], time: 160.562
steps: 1849975, episodes: 74000, mean episode variance: 48.87386370968819, agent episode variance: [1.4483544988632202, 46.151592041015626, 1.2739171698093414], time: 160.563
Running avgs for agent 0: q_loss: 5.576891899108887, p_loss: -4.055648326873779, mean_rew: -7.9194499744787885, variance: 5.793417930603027, lamda: 1.4051991701126099
Running avgs for agent 1: q_loss: 1454983.5, p_loss: 147.2580108642578, mean_rew: -7.913724389769061, variance: 184.6063681640625, lamda: 2.8105199337005615
Running avgs for agent 2: q_loss: 6.9437947273254395, p_loss: -4.065476894378662, mean_rew: -7.91767400193609, variance: 5.095668792724609, lamda: 1.5857702493667603

steps: 1874975, episodes: 75000, mean episode reward: -606.0786619099707, agent episode reward: [-202.02622063665692, -202.02622063665692, -202.02622063665692], time: 162.108
steps: 1874975, episodes: 75000, mean episode variance: 49.368307225227355, agent episode variance: [1.4431351547241211, 46.650830963134766, 1.2743411073684692], time: 162.108
Running avgs for agent 0: q_loss: 5.4499335289001465, p_loss: -4.059967994689941, mean_rew: -7.928694097274408, variance: 5.772540092468262, lamda: 1.4051991701126099
Running avgs for agent 1: q_loss: 1469077.125, p_loss: 147.449951171875, mean_rew: -7.934824838354036, variance: 186.60332385253906, lamda: 2.835494041442871
Running avgs for agent 2: q_loss: 6.877072334289551, p_loss: -4.0758280754089355, mean_rew: -7.929314772729135, variance: 5.09736442565918, lamda: 1.5859651565551758

steps: 1899975, episodes: 76000, mean episode reward: -595.9681717440824, agent episode reward: [-198.65605724802742, -198.65605724802742, -198.65605724802742], time: 163.838
steps: 1899975, episodes: 76000, mean episode variance: 50.45716576218605, agent episode variance: [1.4472498683929442, 47.73232405090332, 1.2775918428897857], time: 163.839
Running avgs for agent 0: q_loss: 5.588875770568848, p_loss: -4.063353061676025, mean_rew: -7.937541097392769, variance: 5.788999557495117, lamda: 1.4052690267562866
Running avgs for agent 1: q_loss: 1470092.625, p_loss: 147.8261260986328, mean_rew: -7.9498958356695235, variance: 190.9292962036133, lamda: 2.860468864440918
Running avgs for agent 2: q_loss: 6.939783096313477, p_loss: -4.069993495941162, mean_rew: -7.929606029520071, variance: 5.110367298126221, lamda: 1.5860402584075928

steps: 1924975, episodes: 77000, mean episode reward: -601.6224068509249, agent episode reward: [-200.54080228364163, -200.54080228364163, -200.54080228364163], time: 163.947
steps: 1924975, episodes: 77000, mean episode variance: 49.66124509334564, agent episode variance: [1.4514210665225982, 46.928505935668944, 1.2813180911540984], time: 163.947
Running avgs for agent 0: q_loss: 5.35635232925415, p_loss: -4.070845603942871, mean_rew: -7.953081893603758, variance: 5.805684566497803, lamda: 1.405450701713562
Running avgs for agent 1: q_loss: 1481593.375, p_loss: 148.02410888671875, mean_rew: -7.943538743109749, variance: 187.71402374267578, lamda: 2.8854429721832275
Running avgs for agent 2: q_loss: 6.078006267547607, p_loss: -4.083524703979492, mean_rew: -7.9478120442275655, variance: 5.125272274017334, lamda: 1.5865178108215332

steps: 1949975, episodes: 78000, mean episode reward: -603.6461589586493, agent episode reward: [-201.21538631954982, -201.21538631954982, -201.21538631954982], time: 175.225
steps: 1949975, episodes: 78000, mean episode variance: 48.00870799708366, agent episode variance: [1.4509490275382995, 45.285295013427735, 1.27246395611763], time: 175.226
Running avgs for agent 0: q_loss: 5.159786224365234, p_loss: -4.072242736816406, mean_rew: -7.960430712883377, variance: 5.80379581451416, lamda: 1.4089102745056152
Running avgs for agent 1: q_loss: 1499322.875, p_loss: 148.46884155273438, mean_rew: -7.959995662491064, variance: 181.14118005371094, lamda: 2.910417318344116
Running avgs for agent 2: q_loss: 5.398934364318848, p_loss: -4.085739612579346, mean_rew: -7.954896905311838, variance: 5.089855670928955, lamda: 1.5892912149429321

steps: 1974975, episodes: 79000, mean episode reward: -598.2492296384952, agent episode reward: [-199.41640987949842, -199.41640987949842, -199.41640987949842], time: 180.917
steps: 1974975, episodes: 79000, mean episode variance: 48.777994778633115, agent episode variance: [1.4436287388801574, 46.058978012084964, 1.2753880276679992], time: 180.917
Running avgs for agent 0: q_loss: 5.6722235679626465, p_loss: -4.076669692993164, mean_rew: -7.964571452741562, variance: 5.774514675140381, lamda: 1.4097732305526733
Running avgs for agent 1: q_loss: 1508597.0, p_loss: 148.78921508789062, mean_rew: -7.967997220349568, variance: 184.23591204833986, lamda: 2.935391664505005
Running avgs for agent 2: q_loss: 6.751941680908203, p_loss: -4.080710411071777, mean_rew: -7.963972272107358, variance: 5.1015520095825195, lamda: 1.5928153991699219

steps: 1999975, episodes: 80000, mean episode reward: -599.6387447274651, agent episode reward: [-199.87958157582167, -199.87958157582167, -199.87958157582167], time: 178.394
steps: 1999975, episodes: 80000, mean episode variance: 48.376150802373886, agent episode variance: [1.4482210173606873, 45.650788360595705, 1.2771414244174957], time: 178.395
Running avgs for agent 0: q_loss: 5.59281063079834, p_loss: -4.08144998550415, mean_rew: -7.969109145239482, variance: 5.79288387298584, lamda: 1.4097732305526733
Running avgs for agent 1: q_loss: 1515293.5, p_loss: 149.1221923828125, mean_rew: -7.9703980995122565, variance: 182.60315344238282, lamda: 2.9603660106658936
Running avgs for agent 2: q_loss: 6.84293794631958, p_loss: -4.093769550323486, mean_rew: -7.973261119807511, variance: 5.108565330505371, lamda: 1.5928620100021362

steps: 2024975, episodes: 81000, mean episode reward: -601.750092116226, agent episode reward: [-200.58336403874196, -200.58336403874196, -200.58336403874196], time: 170.873
steps: 2024975, episodes: 81000, mean episode variance: 46.8500764977932, agent episode variance: [1.448328830718994, 44.11979460144043, 1.2819530656337739], time: 170.873
Running avgs for agent 0: q_loss: 5.631821155548096, p_loss: -4.083789348602295, mean_rew: -7.9817132411974985, variance: 5.793315410614014, lamda: 1.4097732305526733
Running avgs for agent 1: q_loss: 1536446.875, p_loss: 149.5190887451172, mean_rew: -7.989659095801815, variance: 176.47917840576173, lamda: 2.9853403568267822
Running avgs for agent 2: q_loss: 6.73301362991333, p_loss: -4.10185432434082, mean_rew: -7.989883598973934, variance: 5.127812385559082, lamda: 1.593809723854065

steps: 2049975, episodes: 82000, mean episode reward: -603.3401270228948, agent episode reward: [-201.11337567429828, -201.11337567429828, -201.11337567429828], time: 167.565
steps: 2049975, episodes: 82000, mean episode variance: 47.91453270316124, agent episode variance: [1.4510046057701111, 45.17402694702148, 1.289501150369644], time: 167.565
Running avgs for agent 0: q_loss: 5.614895343780518, p_loss: -4.087218284606934, mean_rew: -7.99500613123819, variance: 5.804018020629883, lamda: 1.4097774028778076
Running avgs for agent 1: q_loss: 1528133.875, p_loss: 149.5874786376953, mean_rew: -7.990042322720422, variance: 180.69610778808592, lamda: 3.010314702987671
Running avgs for agent 2: q_loss: 6.785215854644775, p_loss: -4.101564407348633, mean_rew: -7.99828834627481, variance: 5.1580047607421875, lamda: 1.5941481590270996

steps: 2074975, episodes: 83000, mean episode reward: -603.542155751613, agent episode reward: [-201.18071858387103, -201.18071858387103, -201.18071858387103], time: 165.458
steps: 2074975, episodes: 83000, mean episode variance: 48.75718340754509, agent episode variance: [1.4478050975799561, 46.027987274169924, 1.2813910357952119], time: 165.458
Running avgs for agent 0: q_loss: 5.3875861167907715, p_loss: -4.096746444702148, mean_rew: -8.008265331174389, variance: 5.791220664978027, lamda: 1.413253664970398
Running avgs for agent 1: q_loss: 1543476.0, p_loss: 149.96910095214844, mean_rew: -8.015505960951536, variance: 184.1119490966797, lamda: 3.0352890491485596
Running avgs for agent 2: q_loss: 5.038609504699707, p_loss: -4.106223106384277, mean_rew: -8.002144905631289, variance: 5.1255645751953125, lamda: 1.5975154638290405

steps: 2099975, episodes: 84000, mean episode reward: -610.6565677623246, agent episode reward: [-203.55218925410819, -203.55218925410819, -203.55218925410819], time: 175.202
steps: 2099975, episodes: 84000, mean episode variance: 47.9369150364399, agent episode variance: [1.4488034029006958, 45.21049943542481, 1.277612198114395], time: 175.203
Running avgs for agent 0: q_loss: 5.555632591247559, p_loss: -4.102944374084473, mean_rew: -8.0164252763841, variance: 5.795213222503662, lamda: 1.41471266746521
Running avgs for agent 1: q_loss: 1554512.125, p_loss: 150.31271362304688, mean_rew: -8.019897827273125, variance: 180.84199774169923, lamda: 3.060263156890869
Running avgs for agent 2: q_loss: 4.945931911468506, p_loss: -4.1063313484191895, mean_rew: -8.00557339186011, variance: 5.110448360443115, lamda: 1.6033421754837036

steps: 2124975, episodes: 85000, mean episode reward: -607.1526191063936, agent episode reward: [-202.38420636879783, -202.38420636879783, -202.38420636879783], time: 178.961
steps: 2124975, episodes: 85000, mean episode variance: 48.61452105259895, agent episode variance: [1.4617715964317322, 45.876996353149416, 1.275753103017807], time: 178.961
Running avgs for agent 0: q_loss: 5.5568437576293945, p_loss: -4.111271858215332, mean_rew: -8.037544513643514, variance: 5.847085952758789, lamda: 1.4148370027542114
Running avgs for agent 1: q_loss: 1571311.0, p_loss: 150.7932586669922, mean_rew: -8.03058683278811, variance: 183.50798541259766, lamda: 3.085237741470337
Running avgs for agent 2: q_loss: 4.642102241516113, p_loss: -4.1182355880737305, mean_rew: -8.027533600502952, variance: 5.103012561798096, lamda: 1.6093266010284424

steps: 2149975, episodes: 86000, mean episode reward: -606.368997805412, agent episode reward: [-202.1229992684707, -202.1229992684707, -202.1229992684707], time: 173.465
steps: 2149975, episodes: 86000, mean episode variance: 48.460539381504056, agent episode variance: [1.4528778462409973, 45.73579006958008, 1.2718714656829835], time: 173.465
Running avgs for agent 0: q_loss: 5.593540191650391, p_loss: -4.115105628967285, mean_rew: -8.040685797073898, variance: 5.811511039733887, lamda: 1.4148794412612915
Running avgs for agent 1: q_loss: 1574197.5, p_loss: 151.050537109375, mean_rew: -8.025275730793744, variance: 182.9431602783203, lamda: 3.1102118492126465
Running avgs for agent 2: q_loss: 4.716493129730225, p_loss: -4.122931003570557, mean_rew: -8.037119887180426, variance: 5.087485313415527, lamda: 1.6180591583251953

steps: 2174975, episodes: 87000, mean episode reward: -603.671303821075, agent episode reward: [-201.22376794035836, -201.22376794035836, -201.22376794035836], time: 164.012
steps: 2174975, episodes: 87000, mean episode variance: 46.30291808104515, agent episode variance: [1.4565124897956847, 43.57606842041016, 1.2703371708393096], time: 164.012
Running avgs for agent 0: q_loss: 4.974976539611816, p_loss: -4.119022369384766, mean_rew: -8.043094559521247, variance: 5.8260498046875, lamda: 1.4156074523925781
Running avgs for agent 1: q_loss: 1600119.875, p_loss: 151.52699279785156, mean_rew: -8.044227971229873, variance: 174.30427368164064, lamda: 3.1351864337921143
Running avgs for agent 2: q_loss: 5.672395706176758, p_loss: -4.127066135406494, mean_rew: -8.050331464022644, variance: 5.081348419189453, lamda: 1.6235698461532593

steps: 2199975, episodes: 88000, mean episode reward: -604.2664314622921, agent episode reward: [-201.4221438207641, -201.4221438207641, -201.4221438207641], time: 161.77
steps: 2199975, episodes: 88000, mean episode variance: 47.307716478586194, agent episode variance: [1.4567667207717896, 44.580638641357424, 1.2703111164569856], time: 161.77
Running avgs for agent 0: q_loss: 5.719620704650879, p_loss: -4.126871109008789, mean_rew: -8.05505913890278, variance: 5.827066898345947, lamda: 1.416610598564148
Running avgs for agent 1: q_loss: 1624179.375, p_loss: 152.2509002685547, mean_rew: -8.049500341190843, variance: 178.3225545654297, lamda: 3.160160541534424
Running avgs for agent 2: q_loss: 6.760519504547119, p_loss: -4.131954193115234, mean_rew: -8.051525127200541, variance: 5.081243991851807, lamda: 1.6245603561401367

steps: 2224975, episodes: 89000, mean episode reward: -606.4618920241692, agent episode reward: [-202.15396400805642, -202.15396400805642, -202.15396400805642], time: 162.147
steps: 2224975, episodes: 89000, mean episode variance: 47.153114066839215, agent episode variance: [1.4576841583251954, 44.425990982055666, 1.2694389264583588], time: 162.148
Running avgs for agent 0: q_loss: 5.677588939666748, p_loss: -4.119218826293945, mean_rew: -8.050569576737717, variance: 5.83073616027832, lamda: 1.4172495603561401
Running avgs for agent 1: q_loss: 1640053.75, p_loss: 152.7796630859375, mean_rew: -8.05799314528257, variance: 177.70396392822266, lamda: 3.1851353645324707
Running avgs for agent 2: q_loss: 5.427053451538086, p_loss: -4.126625061035156, mean_rew: -8.044946715921485, variance: 5.077755928039551, lamda: 1.6262328624725342

steps: 2249975, episodes: 90000, mean episode reward: -606.9750972692701, agent episode reward: [-202.32503242309005, -202.32503242309005, -202.32503242309005], time: 163.103
steps: 2249975, episodes: 90000, mean episode variance: 45.34429791760444, agent episode variance: [1.4484287314414979, 42.628814880371095, 1.2670543057918549], time: 163.104
Running avgs for agent 0: q_loss: 4.967577934265137, p_loss: -4.130791664123535, mean_rew: -8.057784985076374, variance: 5.793715000152588, lamda: 1.419653296470642
Running avgs for agent 1: q_loss: 1685414.875, p_loss: 153.36196899414062, mean_rew: -8.059127626781098, variance: 170.51525952148438, lamda: 3.2101097106933594
Running avgs for agent 2: q_loss: 5.694436550140381, p_loss: -4.129859447479248, mean_rew: -8.050651312883842, variance: 5.0682172775268555, lamda: 1.6314948797225952

steps: 2274975, episodes: 91000, mean episode reward: -607.41372585072, agent episode reward: [-202.47124195024, -202.47124195024, -202.47124195024], time: 168.002
steps: 2274975, episodes: 91000, mean episode variance: 45.90510881233215, agent episode variance: [1.4526871829032897, 43.18720440673828, 1.2652172226905822], time: 168.002
Running avgs for agent 0: q_loss: 5.280500888824463, p_loss: -4.127747058868408, mean_rew: -8.057602039882491, variance: 5.810749053955078, lamda: 1.420714020729065
Running avgs for agent 1: q_loss: 1693815.0, p_loss: 154.02850341796875, mean_rew: -8.061310577264674, variance: 172.74881762695313, lamda: 3.235083818435669
Running avgs for agent 2: q_loss: 4.948882579803467, p_loss: -4.141814231872559, mean_rew: -8.062432903891024, variance: 5.060868740081787, lamda: 1.635249137878418

steps: 2299975, episodes: 92000, mean episode reward: -606.2084310384008, agent episode reward: [-202.0694770128003, -202.0694770128003, -202.0694770128003], time: 170.726
steps: 2299975, episodes: 92000, mean episode variance: 46.43557525491715, agent episode variance: [1.4564601926803589, 43.71702798461914, 1.2620870776176452], time: 170.726
Running avgs for agent 0: q_loss: 5.531352519989014, p_loss: -4.1349358558654785, mean_rew: -8.066915101780932, variance: 5.825840950012207, lamda: 1.4213573932647705
Running avgs for agent 1: q_loss: 1733794.125, p_loss: 154.45655822753906, mean_rew: -8.06624868028244, variance: 174.86811193847657, lamda: 3.2600581645965576
Running avgs for agent 2: q_loss: 4.630341053009033, p_loss: -4.137697219848633, mean_rew: -8.064508156967984, variance: 5.048348426818848, lamda: 1.6409265995025635

steps: 2324975, episodes: 93000, mean episode reward: -611.7917694142724, agent episode reward: [-203.93058980475743, -203.93058980475743, -203.93058980475743], time: 175.043
steps: 2324975, episodes: 93000, mean episode variance: 45.36992008590698, agent episode variance: [1.4570945749282838, 42.65608137512207, 1.2567441358566285], time: 175.043
Running avgs for agent 0: q_loss: 5.585309028625488, p_loss: -4.133222579956055, mean_rew: -8.061362593927997, variance: 5.828378200531006, lamda: 1.4214575290679932
Running avgs for agent 1: q_loss: 1749362.0, p_loss: 154.9733428955078, mean_rew: -8.072734772796622, variance: 170.62432550048828, lamda: 3.2850325107574463
Running avgs for agent 2: q_loss: 4.617391109466553, p_loss: -4.139875411987305, mean_rew: -8.057725210335205, variance: 5.026976585388184, lamda: 1.6466723680496216

steps: 2349975, episodes: 94000, mean episode reward: -608.2516794491779, agent episode reward: [-202.75055981639264, -202.75055981639264, -202.75055981639264], time: 172.409
steps: 2349975, episodes: 94000, mean episode variance: 47.575749435424804, agent episode variance: [1.4513991827964783, 44.874174255371095, 1.2501759972572326], time: 172.409
Running avgs for agent 0: q_loss: 4.985599994659424, p_loss: -4.139077186584473, mean_rew: -8.07055265613477, variance: 5.805596828460693, lamda: 1.4229174852371216
Running avgs for agent 1: q_loss: 1778094.875, p_loss: 155.32855224609375, mean_rew: -8.081333869734264, variance: 179.49669702148438, lamda: 3.310006856918335
Running avgs for agent 2: q_loss: 4.734137535095215, p_loss: -4.145116806030273, mean_rew: -8.072514367396034, variance: 5.000704288482666, lamda: 1.653964638710022

steps: 2374975, episodes: 95000, mean episode reward: -607.765966055252, agent episode reward: [-202.58865535175067, -202.58865535175067, -202.58865535175067], time: 168.996
steps: 2374975, episodes: 95000, mean episode variance: 46.205499039649965, agent episode variance: [1.45737344455719, 43.501891052246094, 1.2462345428466797], time: 168.997
Running avgs for agent 0: q_loss: 5.583887577056885, p_loss: -4.132180690765381, mean_rew: -8.073673010325685, variance: 5.829493999481201, lamda: 1.4258898496627808
Running avgs for agent 1: q_loss: 1822828.875, p_loss: 155.6172332763672, mean_rew: -8.081643954151042, variance: 174.00756420898438, lamda: 3.3349809646606445
Running avgs for agent 2: q_loss: 6.294931411743164, p_loss: -4.143648624420166, mean_rew: -8.076666980248872, variance: 4.984938621520996, lamda: 1.6612097024917603

steps: 2399975, episodes: 96000, mean episode reward: -604.181842716651, agent episode reward: [-201.39394757221694, -201.39394757221694, -201.39394757221694], time: 178.214
steps: 2399975, episodes: 96000, mean episode variance: 45.26663427591324, agent episode variance: [1.444572154045105, 42.577644546508786, 1.2444175753593445], time: 178.215
Running avgs for agent 0: q_loss: 5.49154806137085, p_loss: -4.140382289886475, mean_rew: -8.07429358119877, variance: 5.7782883644104, lamda: 1.4268361330032349
Running avgs for agent 1: q_loss: 1858500.25, p_loss: 155.8006591796875, mean_rew: -8.071759892581404, variance: 170.31057818603514, lamda: 3.3599555492401123
Running avgs for agent 2: q_loss: 6.84650993347168, p_loss: -4.142886161804199, mean_rew: -8.073490051028735, variance: 4.977670192718506, lamda: 1.6620110273361206

steps: 2424975, episodes: 97000, mean episode reward: -604.8685549221279, agent episode reward: [-201.62285164070934, -201.62285164070934, -201.62285164070934], time: 178.509
steps: 2424975, episodes: 97000, mean episode variance: 46.92752031135559, agent episode variance: [1.4524031281471252, 44.229083572387694, 1.2460336108207704], time: 178.509
Running avgs for agent 0: q_loss: 5.580026149749756, p_loss: -4.13317346572876, mean_rew: -8.071204234935637, variance: 5.809612274169922, lamda: 1.428948998451233
Running avgs for agent 1: q_loss: 1886322.875, p_loss: 156.02430725097656, mean_rew: -8.075835476683558, variance: 176.91633428955078, lamda: 3.384929656982422
Running avgs for agent 2: q_loss: 5.171598434448242, p_loss: -4.143702030181885, mean_rew: -8.070221009383234, variance: 4.984134197235107, lamda: 1.6654058694839478

steps: 2449975, episodes: 98000, mean episode reward: -603.1460029126177, agent episode reward: [-201.04866763753924, -201.04866763753924, -201.04866763753924], time: 177.276
steps: 2449975, episodes: 98000, mean episode variance: 46.54226142168045, agent episode variance: [1.4523590121269225, 43.85079333496094, 1.2391090745925903], time: 177.276
Running avgs for agent 0: q_loss: 5.660478591918945, p_loss: -4.13749361038208, mean_rew: -8.07737822668043, variance: 5.809435844421387, lamda: 1.4290224313735962
Running avgs for agent 1: q_loss: 1869607.75, p_loss: 155.890869140625, mean_rew: -8.0601266390938, variance: 175.40317333984376, lamda: 3.4099042415618896
Running avgs for agent 2: q_loss: 5.5533857345581055, p_loss: -4.128358364105225, mean_rew: -8.066355603822045, variance: 4.956436634063721, lamda: 1.6724371910095215

steps: 2474975, episodes: 99000, mean episode reward: -599.4827091998407, agent episode reward: [-199.82756973328023, -199.82756973328023, -199.82756973328023], time: 173.062
steps: 2474975, episodes: 99000, mean episode variance: 45.5714709687233, agent episode variance: [1.44326664352417, 42.895633224487305, 1.2325711007118225], time: 173.062
Running avgs for agent 0: q_loss: 5.454198360443115, p_loss: -4.139787197113037, mean_rew: -8.070667393359754, variance: 5.773066520690918, lamda: 1.429103136062622
Running avgs for agent 1: q_loss: 1901287.25, p_loss: 155.88027954101562, mean_rew: -8.073733700592884, variance: 171.58253289794922, lamda: 3.4348785877227783
Running avgs for agent 2: q_loss: 4.9394989013671875, p_loss: -4.134275436401367, mean_rew: -8.065782650804753, variance: 4.930284023284912, lamda: 1.6775202751159668

steps: 2499975, episodes: 100000, mean episode reward: -597.6042368681733, agent episode reward: [-199.2014122893911, -199.2014122893911, -199.2014122893911], time: 162.056
steps: 2499975, episodes: 100000, mean episode variance: 46.04900335192681, agent episode variance: [1.4473919034004212, 43.37359831237793, 1.2280131361484528], time: 162.057
Running avgs for agent 0: q_loss: 4.247920036315918, p_loss: -4.1232099533081055, mean_rew: -8.064863132884584, variance: 5.789567470550537, lamda: 1.4318617582321167
Running avgs for agent 1: q_loss: 1893285.625, p_loss: 155.81130981445312, mean_rew: -8.068408453887557, variance: 173.49439324951172, lamda: 3.459852695465088
Running avgs for agent 2: q_loss: 4.749311447143555, p_loss: -4.131111145019531, mean_rew: -8.058034591284528, variance: 4.912052154541016, lamda: 1.682281255722046

steps: 2524975, episodes: 101000, mean episode reward: -603.7576501739921, agent episode reward: [-201.2525500579974, -201.2525500579974, -201.2525500579974], time: 121.643
steps: 2524975, episodes: 101000, mean episode variance: 45.92793097352982, agent episode variance: [1.441952203989029, 43.259531677246095, 1.226447092294693], time: 121.644
Running avgs for agent 0: q_loss: 5.405142784118652, p_loss: -4.131155490875244, mean_rew: -8.069198492029559, variance: 5.767808437347412, lamda: 1.4346009492874146
Running avgs for agent 1: q_loss: 1892767.25, p_loss: 155.63296508789062, mean_rew: -8.057758580445215, variance: 173.03812670898438, lamda: 3.4848272800445557
Running avgs for agent 2: q_loss: 6.7317609786987305, p_loss: -4.134270668029785, mean_rew: -8.061135675109053, variance: 4.905787944793701, lamda: 1.6857163906097412

steps: 2549975, episodes: 102000, mean episode reward: -606.4534824976406, agent episode reward: [-202.15116083254688, -202.15116083254688, -202.15116083254688], time: 114.86
steps: 2549975, episodes: 102000, mean episode variance: 43.746199615240094, agent episode variance: [1.4386408529281616, 41.08265293884277, 1.2249058234691619], time: 114.86
Running avgs for agent 0: q_loss: 4.3455376625061035, p_loss: -4.128907680511475, mean_rew: -8.068272585377086, variance: 5.754563331604004, lamda: 1.436120867729187
Running avgs for agent 1: q_loss: 1904246.25, p_loss: 155.6645050048828, mean_rew: -8.071073522109451, variance: 164.33061175537108, lamda: 3.5098016262054443
Running avgs for agent 2: q_loss: 6.54464864730835, p_loss: -4.1360955238342285, mean_rew: -8.06243557398147, variance: 4.899622917175293, lamda: 1.6863125562667847

steps: 2574975, episodes: 103000, mean episode reward: -607.3176638946018, agent episode reward: [-202.4392212982006, -202.4392212982006, -202.4392212982006], time: 117.749
steps: 2574975, episodes: 103000, mean episode variance: 44.5668836004734, agent episode variance: [1.4373704929351807, 41.903122879028324, 1.2263902285099029], time: 117.75
Running avgs for agent 0: q_loss: 5.4882049560546875, p_loss: -4.123021125793457, mean_rew: -8.06094729857552, variance: 5.749482154846191, lamda: 1.436558723449707
Running avgs for agent 1: q_loss: 1896175.875, p_loss: 155.47158813476562, mean_rew: -8.067778632699063, variance: 167.6124915161133, lamda: 3.534775972366333
Running avgs for agent 2: q_loss: 6.192875862121582, p_loss: -4.126112937927246, mean_rew: -8.059052576778969, variance: 4.905560493469238, lamda: 1.68752121925354

steps: 2599975, episodes: 104000, mean episode reward: -607.9298429802868, agent episode reward: [-202.64328099342896, -202.64328099342896, -202.64328099342896], time: 113.842
steps: 2599975, episodes: 104000, mean episode variance: 44.84576895976067, agent episode variance: [1.4425954933166505, 42.17956854248047, 1.2236049239635467], time: 113.842
Running avgs for agent 0: q_loss: 5.4965434074401855, p_loss: -4.123743534088135, mean_rew: -8.063487739326673, variance: 5.770381927490234, lamda: 1.4365586042404175
Running avgs for agent 1: q_loss: 1918778.5, p_loss: 155.4180450439453, mean_rew: -8.058234116212077, variance: 168.71827416992187, lamda: 3.559750556945801
Running avgs for agent 2: q_loss: 7.159379005432129, p_loss: -4.130645275115967, mean_rew: -8.06070622037975, variance: 4.894420146942139, lamda: 1.6897273063659668

steps: 2624975, episodes: 105000, mean episode reward: -602.3632592441718, agent episode reward: [-200.78775308139058, -200.78775308139058, -200.78775308139058], time: 98.963
steps: 2624975, episodes: 105000, mean episode variance: 44.45631021690369, agent episode variance: [1.430607931137085, 41.80746223449707, 1.2182400512695313], time: 98.964
Running avgs for agent 0: q_loss: 5.564398765563965, p_loss: -4.126551151275635, mean_rew: -8.060488333144288, variance: 5.722431659698486, lamda: 1.4367884397506714
Running avgs for agent 1: q_loss: 1917560.375, p_loss: 155.3325653076172, mean_rew: -8.06742095098868, variance: 167.2298489379883, lamda: 3.5847246646881104
Running avgs for agent 2: q_loss: 7.297201156616211, p_loss: -4.125229358673096, mean_rew: -8.05543204702983, variance: 4.872960567474365, lamda: 1.6899372339248657

steps: 2649975, episodes: 106000, mean episode reward: -603.6012974327259, agent episode reward: [-201.20043247757525, -201.20043247757525, -201.20043247757525], time: 99.444
steps: 2649975, episodes: 106000, mean episode variance: 43.99972693252563, agent episode variance: [1.4230924367904663, 41.35940351867676, 1.2172309770584107], time: 99.444
Running avgs for agent 0: q_loss: 5.537640571594238, p_loss: -4.124934673309326, mean_rew: -8.069031883853473, variance: 5.6923699378967285, lamda: 1.4373422861099243
Running avgs for agent 1: q_loss: 1934715.125, p_loss: 155.44631958007812, mean_rew: -8.068479091172936, variance: 165.43761407470703, lamda: 3.609699010848999
Running avgs for agent 2: q_loss: 5.747714042663574, p_loss: -4.130322456359863, mean_rew: -8.057708971734481, variance: 4.868923664093018, lamda: 1.6913717985153198

steps: 2674975, episodes: 107000, mean episode reward: -603.4130504868251, agent episode reward: [-201.13768349560834, -201.13768349560834, -201.13768349560834], time: 101.866
steps: 2674975, episodes: 107000, mean episode variance: 43.73494911599159, agent episode variance: [1.430912872314453, 41.091306610107424, 1.2127296335697173], time: 101.867
Running avgs for agent 0: q_loss: 5.435679912567139, p_loss: -4.123244762420654, mean_rew: -8.063410483846143, variance: 5.72365140914917, lamda: 1.437597393989563
Running avgs for agent 1: q_loss: 1954501.625, p_loss: 155.4880828857422, mean_rew: -8.058616881083386, variance: 164.3652264404297, lamda: 3.6346733570098877
Running avgs for agent 2: q_loss: 5.589582443237305, p_loss: -4.126739978790283, mean_rew: -8.05352383729472, variance: 4.850918292999268, lamda: 1.6969177722930908

steps: 2699975, episodes: 108000, mean episode reward: -606.3294966231897, agent episode reward: [-202.10983220772985, -202.10983220772985, -202.10983220772985], time: 103.44
steps: 2699975, episodes: 108000, mean episode variance: 43.5730656247139, agent episode variance: [1.4356676959991455, 40.92421293640137, 1.213184992313385], time: 103.441
Running avgs for agent 0: q_loss: 5.592672348022461, p_loss: -4.124212265014648, mean_rew: -8.068726589929211, variance: 5.742671012878418, lamda: 1.4378308057785034
Running avgs for agent 1: q_loss: 1976132.375, p_loss: 155.6724395751953, mean_rew: -8.057882310953987, variance: 163.69685174560547, lamda: 3.6596474647521973
Running avgs for agent 2: q_loss: 7.093800067901611, p_loss: -4.122178077697754, mean_rew: -8.058164041364533, variance: 4.8527398109436035, lamda: 1.700348138809204

steps: 2724975, episodes: 109000, mean episode reward: -601.8275576735884, agent episode reward: [-200.60918589119612, -200.60918589119612, -200.60918589119612], time: 95.206
steps: 2724975, episodes: 109000, mean episode variance: 43.73039934921265, agent episode variance: [1.4276951904296875, 41.09396324157715, 1.2087409172058106], time: 95.206
Running avgs for agent 0: q_loss: 4.330193519592285, p_loss: -4.117866039276123, mean_rew: -8.065759936641022, variance: 5.710780620574951, lamda: 1.444059133529663
Running avgs for agent 1: q_loss: 1955988.125, p_loss: 155.70069885253906, mean_rew: -8.059874273674918, variance: 164.3758529663086, lamda: 3.684622049331665
Running avgs for agent 2: q_loss: 5.204998970031738, p_loss: -4.12775993347168, mean_rew: -8.060187414103465, variance: 4.834963798522949, lamda: 1.7027428150177002

steps: 2749975, episodes: 110000, mean episode reward: -599.935139493095, agent episode reward: [-199.97837983103167, -199.97837983103167, -199.97837983103167], time: 94.638
steps: 2749975, episodes: 110000, mean episode variance: 43.50498811793327, agent episode variance: [1.4243829669952393, 40.88152079772949, 1.1990843532085418], time: 94.639
Running avgs for agent 0: q_loss: 5.290245056152344, p_loss: -4.1180644035339355, mean_rew: -8.06184974954963, variance: 5.6975321769714355, lamda: 1.446916937828064
Running avgs for agent 1: q_loss: 1963639.5, p_loss: 155.71856689453125, mean_rew: -8.056401299885366, variance: 163.52608319091797, lamda: 3.7095963954925537
Running avgs for agent 2: q_loss: 5.79808235168457, p_loss: -4.134860515594482, mean_rew: -8.0600440378146, variance: 4.796337604522705, lamda: 1.7078616619110107

steps: 2774975, episodes: 111000, mean episode reward: -609.2088985393075, agent episode reward: [-203.0696328464358, -203.0696328464358, -203.0696328464358], time: 96.15
steps: 2774975, episodes: 111000, mean episode variance: 42.57769412016869, agent episode variance: [1.4221628179550172, 39.95764962768555, 1.197881674528122], time: 96.15
Running avgs for agent 0: q_loss: 5.04799222946167, p_loss: -4.120303153991699, mean_rew: -8.061367771368818, variance: 5.6886515617370605, lamda: 1.4494822025299072
Running avgs for agent 1: q_loss: 1987765.25, p_loss: 155.94618225097656, mean_rew: -8.060525019442856, variance: 159.8305985107422, lamda: 3.7345705032348633
Running avgs for agent 2: q_loss: 7.331698417663574, p_loss: -4.126721382141113, mean_rew: -8.057963694316454, variance: 4.7915263175964355, lamda: 1.7113772630691528

steps: 2799975, episodes: 112000, mean episode reward: -605.722495619819, agent episode reward: [-201.9074985399397, -201.9074985399397, -201.9074985399397], time: 96.488
steps: 2799975, episodes: 112000, mean episode variance: 42.93870630264282, agent episode variance: [1.4152335720062257, 40.32755346679688, 1.1959192638397216], time: 96.488
Running avgs for agent 0: q_loss: 5.738722801208496, p_loss: -4.118500232696533, mean_rew: -8.062931488545024, variance: 5.660933971405029, lamda: 1.4521715641021729
Running avgs for agent 1: q_loss: 1999958.75, p_loss: 156.20260620117188, mean_rew: -8.065803433053553, variance: 161.3102138671875, lamda: 3.759545087814331
Running avgs for agent 2: q_loss: 7.290550708770752, p_loss: -4.135804176330566, mean_rew: -8.065003551807667, variance: 4.783676624298096, lamda: 1.7117910385131836

steps: 2824975, episodes: 113000, mean episode reward: -601.0337725720143, agent episode reward: [-200.3445908573381, -200.3445908573381, -200.3445908573381], time: 94.843
steps: 2824975, episodes: 113000, mean episode variance: 43.035318484783176, agent episode variance: [1.409648540019989, 40.4243031616211, 1.2013667831420898], time: 94.844
Running avgs for agent 0: q_loss: 5.2659454345703125, p_loss: -4.115942478179932, mean_rew: -8.057962004787967, variance: 5.638594150543213, lamda: 1.4530134201049805
Running avgs for agent 1: q_loss: 2091349.25, p_loss: 156.44126892089844, mean_rew: -8.06890002493527, variance: 161.6972126464844, lamda: 3.7845191955566406
Running avgs for agent 2: q_loss: 5.910279750823975, p_loss: -4.126008987426758, mean_rew: -8.064443916232637, variance: 4.805467128753662, lamda: 1.7124619483947754

steps: 2849975, episodes: 114000, mean episode reward: -602.723917175148, agent episode reward: [-200.90797239171604, -200.90797239171604, -200.90797239171604], time: 102.637
steps: 2849975, episodes: 114000, mean episode variance: 42.52633255624771, agent episode variance: [1.4080873663425446, 39.91631956481934, 1.2019256250858308], time: 102.637
Running avgs for agent 0: q_loss: 5.320030212402344, p_loss: -4.123135566711426, mean_rew: -8.068479232436117, variance: 5.632349491119385, lamda: 1.4573957920074463
Running avgs for agent 1: q_loss: 2085427.125, p_loss: 156.62777709960938, mean_rew: -8.064434894005734, variance: 159.66527825927736, lamda: 3.8094935417175293
Running avgs for agent 2: q_loss: 7.366434574127197, p_loss: -4.136542320251465, mean_rew: -8.06948051394241, variance: 4.80770206451416, lamda: 1.714016079902649

steps: 2874975, episodes: 115000, mean episode reward: -600.8705171929023, agent episode reward: [-200.29017239763408, -200.29017239763408, -200.29017239763408], time: 104.433
steps: 2874975, episodes: 115000, mean episode variance: 42.52505243206024, agent episode variance: [1.4104077425003052, 39.91971383666992, 1.1949308528900147], time: 104.434
Running avgs for agent 0: q_loss: 5.54498815536499, p_loss: -4.120991230010986, mean_rew: -8.064737607357001, variance: 5.64163064956665, lamda: 1.4579057693481445
Running avgs for agent 1: q_loss: 2083913.0, p_loss: 157.01852416992188, mean_rew: -8.063950374822257, variance: 159.67885534667968, lamda: 3.834468364715576
Running avgs for agent 2: q_loss: 5.225508689880371, p_loss: -4.1287384033203125, mean_rew: -8.065487247777419, variance: 4.779723644256592, lamda: 1.7188137769699097

steps: 2899975, episodes: 116000, mean episode reward: -606.3223823592799, agent episode reward: [-202.10746078642669, -202.10746078642669, -202.10746078642669], time: 90.868
steps: 2899975, episodes: 116000, mean episode variance: 42.996423184871674, agent episode variance: [1.4114388599395753, 40.39359768676758, 1.1913866381645202], time: 90.869
Running avgs for agent 0: q_loss: 5.620552062988281, p_loss: -4.121038913726807, mean_rew: -8.06640135055771, variance: 5.645755767822266, lamda: 1.4579100608825684
Running avgs for agent 1: q_loss: 2120787.5, p_loss: 157.3717041015625, mean_rew: -8.070011345498472, variance: 161.5743907470703, lamda: 3.8594424724578857
Running avgs for agent 2: q_loss: 4.7553629875183105, p_loss: -4.132718086242676, mean_rew: -8.068609960529507, variance: 4.765546798706055, lamda: 1.728170394897461

steps: 2924975, episodes: 117000, mean episode reward: -596.7089430122245, agent episode reward: [-198.9029810040748, -198.9029810040748, -198.9029810040748], time: 86.623
steps: 2924975, episodes: 117000, mean episode variance: 42.15070279860497, agent episode variance: [1.409814374446869, 39.54787556457519, 1.193012859582901], time: 86.624
Running avgs for agent 0: q_loss: 5.615087985992432, p_loss: -4.127465724945068, mean_rew: -8.068370147041959, variance: 5.639257431030273, lamda: 1.4581252336502075
Running avgs for agent 1: q_loss: 2118674.75, p_loss: 157.64199829101562, mean_rew: -8.073664034300272, variance: 158.19150225830077, lamda: 3.8844170570373535
Running avgs for agent 2: q_loss: 7.293857097625732, p_loss: -4.128871917724609, mean_rew: -8.065121381618065, variance: 4.7720513343811035, lamda: 1.7325348854064941

steps: 2949975, episodes: 118000, mean episode reward: -598.8072683484429, agent episode reward: [-199.60242278281424, -199.60242278281424, -199.60242278281424], time: 81.272
steps: 2949975, episodes: 118000, mean episode variance: 43.53240842294693, agent episode variance: [1.4025563230514526, 40.93998635864258, 1.1898657412528992], time: 81.273
Running avgs for agent 0: q_loss: 5.333607196807861, p_loss: -4.126490116119385, mean_rew: -8.068962233725822, variance: 5.610225677490234, lamda: 1.458901047706604
Running avgs for agent 1: q_loss: 2118850.0, p_loss: 157.80734252929688, mean_rew: -8.065652392104319, variance: 163.7599454345703, lamda: 3.909391164779663
Running avgs for agent 2: q_loss: 6.43040657043457, p_loss: -4.127285480499268, mean_rew: -8.063066763415819, variance: 4.759462833404541, lamda: 1.733946681022644

steps: 2974975, episodes: 119000, mean episode reward: -597.4312495182012, agent episode reward: [-199.14374983940047, -199.14374983940047, -199.14374983940047], time: 83.908
steps: 2974975, episodes: 119000, mean episode variance: 42.05387333631516, agent episode variance: [1.404532241821289, 39.45818995666504, 1.1911511378288269], time: 83.909
Running avgs for agent 0: q_loss: 4.874675750732422, p_loss: -4.127524375915527, mean_rew: -8.071885057022305, variance: 5.618128776550293, lamda: 1.46104097366333
Running avgs for agent 1: q_loss: 2149710.25, p_loss: 158.0152587890625, mean_rew: -8.059603921465333, variance: 157.83275982666015, lamda: 3.9343655109405518
Running avgs for agent 2: q_loss: 7.330062389373779, p_loss: -4.130568981170654, mean_rew: -8.065460570520608, variance: 4.764604568481445, lamda: 1.7346301078796387

steps: 2999975, episodes: 120000, mean episode reward: -597.5033665514848, agent episode reward: [-199.16778885049496, -199.16778885049496, -199.16778885049496], time: 81.513
steps: 2999975, episodes: 120000, mean episode variance: 41.68925377035141, agent episode variance: [1.414191967010498, 39.08506275939941, 1.1899990439414978], time: 81.514
Running avgs for agent 0: q_loss: 5.081536293029785, p_loss: -4.117330074310303, mean_rew: -8.063069421427185, variance: 5.656767845153809, lamda: 1.4620836973190308
Running avgs for agent 1: q_loss: 2188744.25, p_loss: 158.1571807861328, mean_rew: -8.063797768223298, variance: 156.34025103759765, lamda: 3.9593398571014404
Running avgs for agent 2: q_loss: 4.693669319152832, p_loss: -4.133260726928711, mean_rew: -8.070195258327216, variance: 4.759995937347412, lamda: 1.7364997863769531

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -592.4915600593486, agent episode reward: [-197.4971866864495, -197.4971866864495, -197.4971866864495], time: 54.826
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 54.827
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -598.7872050001387, agent episode reward: [-199.59573500004626, -199.59573500004626, -199.59573500004626], time: 66.659
steps: 49975, episodes: 2000, mean episode variance: 74.80752148485183, agent episode variance: [1.1248847489356995, 72.5389548034668, 1.1436819324493408], time: 66.659
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.926499669154791, variance: 4.610183238983154, lamda: 1.462463140487671
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.922629964150052, variance: 297.2908020019531, lamda: 3.971877098083496
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.9221569785057655, variance: 4.687221050262451, lamda: 1.7381287813186646

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567760875.1091626: line 9: --exp_var_alpha: command not found
