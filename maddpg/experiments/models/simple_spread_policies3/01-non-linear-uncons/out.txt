# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation False --constrained \
    False --exp-name MADDPG_simple_spread_uncons_01 --save-dir \
    models/simple_spread_policies3/01-non-linear-uncons/ --plots-dir \
    models/simple_spread_policies3/01-non-linear-uncons/
Job <1084033> is submitted to queue <x86_6h>.
arglist.u_estimation False
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -517.7832970937422, agent episode reward: [-172.59443236458077, -172.59443236458077, -172.59443236458077], time: 44.441
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 44.441
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -592.7494298849858, agent episode reward: [-197.58314329499524, -197.58314329499524, -197.58314329499524], time: 67.234
steps: 49975, episodes: 2000, mean episode variance: 2.601621643051505, agent episode variance: [0.8516976411044598, 0.7225422829538584, 1.0273817189931869], time: 67.235
Running avgs for agent 0: q_loss: 2.233628988265991, p_loss: 10.879877090454102, mean_rew: -7.381723942785057, variance: 3.4905641028871304, mean_q: -10.962098121643066, std_q: 2.885464668273926
Running avgs for agent 1: q_loss: 1.9818819761276245, p_loss: 10.511953353881836, mean_rew: -7.3774030157346, variance: 2.961238864564993, mean_q: -10.6105318069458, std_q: 2.8177096843719482
Running avgs for agent 2: q_loss: 2.4730842113494873, p_loss: 11.07657527923584, mean_rew: -7.382385675088252, variance: 4.210580815545848, mean_q: -11.1778564453125, std_q: 2.9559237957000732

steps: 74975, episodes: 3000, mean episode reward: -464.81306412806487, agent episode reward: [-154.93768804268828, -154.93768804268828, -154.93768804268828], time: 72.164
steps: 74975, episodes: 3000, mean episode variance: 4.916555577456951, agent episode variance: [1.5912931424081325, 1.8194987996816636, 1.505763635367155], time: 72.164
Running avgs for agent 0: q_loss: 1.2672048807144165, p_loss: 22.389183044433594, mean_rew: -7.219747994921974, variance: 6.36517256963253, mean_q: -22.548181533813477, std_q: 5.984259605407715
Running avgs for agent 1: q_loss: 1.1801437139511108, p_loss: 21.893972396850586, mean_rew: -7.219596052964085, variance: 7.2779951987266545, mean_q: -22.08661651611328, std_q: 5.908487319946289
Running avgs for agent 2: q_loss: 1.2961430549621582, p_loss: 22.804649353027344, mean_rew: -7.214477539022307, variance: 6.02305454146862, mean_q: -22.967985153198242, std_q: 6.100435733795166

steps: 99975, episodes: 4000, mean episode reward: -430.52990100010146, agent episode reward: [-143.5099670000338, -143.5099670000338, -143.5099670000338], time: 71.821
steps: 99975, episodes: 4000, mean episode variance: 13.876177004933357, agent episode variance: [5.371030155658722, 3.7972281799316407, 4.707918669342995], time: 71.821
Running avgs for agent 0: q_loss: 1.7335575819015503, p_loss: 32.576446533203125, mean_rew: -6.854018658004963, variance: 21.484120622634887, mean_q: -32.85293960571289, std_q: 8.892460823059082
Running avgs for agent 1: q_loss: 1.3207576274871826, p_loss: 32.06763458251953, mean_rew: -6.84905781326374, variance: 15.188912719726563, mean_q: -32.415016174316406, std_q: 8.661643028259277
Running avgs for agent 2: q_loss: 1.6094890832901, p_loss: 32.950313568115234, mean_rew: -6.846087425242865, variance: 18.83167467737198, mean_q: -33.19218444824219, std_q: 8.712393760681152

steps: 124975, episodes: 5000, mean episode reward: -405.1333340383921, agent episode reward: [-135.04444467946405, -135.04444467946405, -135.04444467946405], time: 72.163
steps: 124975, episodes: 5000, mean episode variance: 21.538401601672174, agent episode variance: [6.438557971835136, 5.596847372055054, 9.502996257781982], time: 72.163
Running avgs for agent 0: q_loss: 1.9045214653015137, p_loss: 40.854331970214844, mean_rew: -6.564532902572995, variance: 25.754231887340545, mean_q: -41.263553619384766, std_q: 10.722650527954102
Running avgs for agent 1: q_loss: 1.4751821756362915, p_loss: 40.168949127197266, mean_rew: -6.555448712180964, variance: 22.387389488220215, mean_q: -40.62997817993164, std_q: 10.439743995666504
Running avgs for agent 2: q_loss: 2.1238462924957275, p_loss: 41.28835678100586, mean_rew: -6.561940347218755, variance: 38.01198503112793, mean_q: -41.57174301147461, std_q: 10.465105056762695

steps: 149975, episodes: 6000, mean episode reward: -393.0909829798751, agent episode reward: [-131.03032765995837, -131.03032765995837, -131.03032765995837], time: 72.46
steps: 149975, episodes: 6000, mean episode variance: 30.35439622116089, agent episode variance: [11.850213203430176, 6.865578117370606, 11.638604900360107], time: 72.461
Running avgs for agent 0: q_loss: 2.388725757598877, p_loss: 47.30811309814453, mean_rew: -6.335162191829346, variance: 47.400852813720704, mean_q: -47.796043395996094, std_q: 11.731298446655273
Running avgs for agent 1: q_loss: 1.6581578254699707, p_loss: 46.66876983642578, mean_rew: -6.3326185194311115, variance: 27.462312469482423, mean_q: -47.20454788208008, std_q: 11.589506149291992
Running avgs for agent 2: q_loss: 2.257474899291992, p_loss: 48.18561553955078, mean_rew: -6.339957420859338, variance: 46.55441960144043, mean_q: -48.492942810058594, std_q: 11.630928993225098

steps: 174975, episodes: 7000, mean episode reward: -384.6540867503746, agent episode reward: [-128.21802891679152, -128.21802891679152, -128.21802891679152], time: 79.145
steps: 174975, episodes: 7000, mean episode variance: 35.09292278766632, agent episode variance: [13.612067017555237, 7.922597335815429, 13.558258434295654], time: 79.145
Running avgs for agent 0: q_loss: 2.53977370262146, p_loss: 52.60801315307617, mean_rew: -6.156702717954702, variance: 54.44826807022095, mean_q: -53.12843704223633, std_q: 12.40868091583252
Running avgs for agent 1: q_loss: 1.804003119468689, p_loss: 51.99803161621094, mean_rew: -6.145957970861636, variance: 31.690389343261717, mean_q: -52.54731369018555, std_q: 12.084318161010742
Running avgs for agent 2: q_loss: 2.3622286319732666, p_loss: 53.74558639526367, mean_rew: -6.15441222797334, variance: 54.233033737182616, mean_q: -54.064727783203125, std_q: 12.203874588012695

steps: 199975, episodes: 8000, mean episode reward: -382.2776309483323, agent episode reward: [-127.42587698277741, -127.42587698277741, -127.42587698277741], time: 75.302
steps: 199975, episodes: 8000, mean episode variance: 39.029457662582395, agent episode variance: [14.839132369995117, 8.900497171401977, 15.289828121185304], time: 75.303
Running avgs for agent 0: q_loss: 2.597304344177246, p_loss: 57.003387451171875, mean_rew: -6.015370602737789, variance: 59.356529479980466, mean_q: -57.518226623535156, std_q: 12.906935691833496
Running avgs for agent 1: q_loss: 1.9486714601516724, p_loss: 56.602516174316406, mean_rew: -6.021690714284286, variance: 35.60198868560791, mean_q: -57.15610885620117, std_q: 12.568031311035156
Running avgs for agent 2: q_loss: 2.449373960494995, p_loss: 58.39962387084961, mean_rew: -6.01569315188655, variance: 61.159312484741214, mean_q: -58.71726989746094, std_q: 12.437614440917969

steps: 224975, episodes: 9000, mean episode reward: -381.2757568601804, agent episode reward: [-127.09191895339346, -127.09191895339346, -127.09191895339346], time: 73.318
steps: 224975, episodes: 9000, mean episode variance: 42.86484253692627, agent episode variance: [16.107599437713624, 9.9534772605896, 16.803765838623047], time: 73.318
Running avgs for agent 0: q_loss: 2.6470627784729004, p_loss: 60.75406265258789, mean_rew: -5.9067486019823265, variance: 64.4303977508545, mean_q: -61.251075744628906, std_q: 13.006731033325195
Running avgs for agent 1: q_loss: 2.0075840950012207, p_loss: 60.36086654663086, mean_rew: -5.906138480663242, variance: 39.8139090423584, mean_q: -60.9079475402832, std_q: 12.749149322509766
Running avgs for agent 2: q_loss: 2.5231614112854004, p_loss: 62.36243438720703, mean_rew: -5.908576854403913, variance: 67.21506335449219, mean_q: -62.69562530517578, std_q: 12.647028923034668

steps: 249975, episodes: 10000, mean episode reward: -377.9330610408272, agent episode reward: [-125.97768701360907, -125.97768701360907, -125.97768701360907], time: 72.613
steps: 249975, episodes: 10000, mean episode variance: 44.88309467697144, agent episode variance: [17.16063204574585, 10.438382186889648, 17.284080444335938], time: 72.614
Running avgs for agent 0: q_loss: 2.6974833011627197, p_loss: 63.93034362792969, mean_rew: -5.821374817574006, variance: 68.6425281829834, mean_q: -64.42061614990234, std_q: 13.0817289352417
Running avgs for agent 1: q_loss: 2.038933515548706, p_loss: 63.631893157958984, mean_rew: -5.820567795038148, variance: 41.75352874755859, mean_q: -64.15937805175781, std_q: 12.754446983337402
Running avgs for agent 2: q_loss: 2.572800397872925, p_loss: 65.82198333740234, mean_rew: -5.8259888271536635, variance: 69.13632177734375, mean_q: -66.14987182617188, std_q: 12.642792701721191

steps: 274975, episodes: 11000, mean episode reward: -374.6791720569287, agent episode reward: [-124.89305735230955, -124.89305735230955, -124.89305735230955], time: 72.267
steps: 274975, episodes: 11000, mean episode variance: 47.4648104763031, agent episode variance: [17.602803047180174, 11.083870794296265, 18.77813663482666], time: 72.267
Running avgs for agent 0: q_loss: 2.6843626499176025, p_loss: 66.73018646240234, mean_rew: -5.741359640321878, variance: 70.4112121887207, mean_q: -67.20369720458984, std_q: 12.913482666015625
Running avgs for agent 1: q_loss: 2.034471273422241, p_loss: 66.48468780517578, mean_rew: -5.741179582683221, variance: 44.33548317718506, mean_q: -67.00579833984375, std_q: 12.520541191101074
Running avgs for agent 2: q_loss: 2.5928337574005127, p_loss: 68.71717834472656, mean_rew: -5.749073587830349, variance: 75.11254653930663, mean_q: -69.02948760986328, std_q: 12.50903034210205

steps: 299975, episodes: 12000, mean episode reward: -373.16758897924177, agent episode reward: [-124.38919632641394, -124.38919632641394, -124.38919632641394], time: 73.45
steps: 299975, episodes: 12000, mean episode variance: 48.689033664703366, agent episode variance: [18.093115524291992, 11.751895572662354, 18.844022567749022], time: 73.451
Running avgs for agent 0: q_loss: 2.684049367904663, p_loss: 69.17593383789062, mean_rew: -5.677980763474184, variance: 72.37246209716797, mean_q: -69.6348876953125, std_q: 12.895682334899902
Running avgs for agent 1: q_loss: 2.0212955474853516, p_loss: 68.97583770751953, mean_rew: -5.675325326124168, variance: 47.00758229064942, mean_q: -69.48313903808594, std_q: 12.368779182434082
Running avgs for agent 2: q_loss: 2.5793604850769043, p_loss: 71.3088150024414, mean_rew: -5.683395874295656, variance: 75.37609027099609, mean_q: -71.61112213134766, std_q: 12.34559154510498

steps: 324975, episodes: 13000, mean episode reward: -370.63060559563496, agent episode reward: [-123.543535198545, -123.543535198545, -123.543535198545], time: 72.873
steps: 324975, episodes: 13000, mean episode variance: 50.99389973068237, agent episode variance: [19.092825942993166, 12.41509280014038, 19.485980987548828], time: 72.874
Running avgs for agent 0: q_loss: 2.638424873352051, p_loss: 71.2756576538086, mean_rew: -5.622858084187916, variance: 76.37130377197266, mean_q: -71.72259521484375, std_q: 12.544828414916992
Running avgs for agent 1: q_loss: 2.033210277557373, p_loss: 71.15998077392578, mean_rew: -5.623106613080194, variance: 49.66037120056152, mean_q: -71.65323638916016, std_q: 12.14732551574707
Running avgs for agent 2: q_loss: 2.61863374710083, p_loss: 73.4260482788086, mean_rew: -5.616301038487428, variance: 77.94392395019531, mean_q: -73.7189712524414, std_q: 12.007052421569824

steps: 349975, episodes: 14000, mean episode reward: -371.39937548754017, agent episode reward: [-123.79979182918007, -123.79979182918007, -123.79979182918007], time: 72.167
steps: 349975, episodes: 14000, mean episode variance: 51.5686324839592, agent episode variance: [18.889097465515135, 12.482085952281953, 20.19744906616211], time: 72.168
Running avgs for agent 0: q_loss: 2.6195502281188965, p_loss: 73.11970520019531, mean_rew: -5.566849256580045, variance: 75.55638986206054, mean_q: -73.56159210205078, std_q: 12.253480911254883
Running avgs for agent 1: q_loss: 2.042325019836426, p_loss: 72.93207550048828, mean_rew: -5.566093931867334, variance: 49.92834380912781, mean_q: -73.40471649169922, std_q: 11.757988929748535
Running avgs for agent 2: q_loss: 2.5829336643218994, p_loss: 75.28660583496094, mean_rew: -5.571623213560557, variance: 80.78979626464844, mean_q: -75.57412719726562, std_q: 11.956255912780762

steps: 374975, episodes: 15000, mean episode reward: -367.8166677795757, agent episode reward: [-122.60555592652521, -122.60555592652521, -122.60555592652521], time: 72.641
steps: 374975, episodes: 15000, mean episode variance: 52.56148996734619, agent episode variance: [19.33894920349121, 12.843816352844238, 20.378724411010744], time: 72.642
Running avgs for agent 0: q_loss: 2.610187530517578, p_loss: 74.7822494506836, mean_rew: -5.52679720467395, variance: 77.35579681396484, mean_q: -75.21503448486328, std_q: 12.018399238586426
Running avgs for agent 1: q_loss: 2.0294151306152344, p_loss: 74.5638427734375, mean_rew: -5.527475678134372, variance: 51.37526541137695, mean_q: -75.0162582397461, std_q: 11.662135124206543
Running avgs for agent 2: q_loss: 2.568725347518921, p_loss: 76.88500213623047, mean_rew: -5.527493266620618, variance: 81.51489764404297, mean_q: -77.17356872558594, std_q: 11.877252578735352

steps: 399975, episodes: 16000, mean episode reward: -367.373555080564, agent episode reward: [-122.45785169352132, -122.45785169352132, -122.45785169352132], time: 71.72
steps: 399975, episodes: 16000, mean episode variance: 54.1648192024231, agent episode variance: [19.63753694152832, 13.446970371246337, 21.08031188964844], time: 71.721
Running avgs for agent 0: q_loss: 2.568606376647949, p_loss: 76.19833374023438, mean_rew: -5.487675964485851, variance: 78.55014776611328, mean_q: -76.61894989013672, std_q: 11.71530818939209
Running avgs for agent 1: q_loss: 2.006145477294922, p_loss: 76.057861328125, mean_rew: -5.489841119413796, variance: 53.78788148498535, mean_q: -76.49042510986328, std_q: 11.472355842590332
Running avgs for agent 2: q_loss: 2.5738234519958496, p_loss: 78.15278625488281, mean_rew: -5.491563823893421, variance: 84.32124755859375, mean_q: -78.43681335449219, std_q: 11.641348838806152

steps: 424975, episodes: 17000, mean episode reward: -365.6266051461028, agent episode reward: [-121.87553504870093, -121.87553504870093, -121.87553504870093], time: 72.34
steps: 424975, episodes: 17000, mean episode variance: 54.69338221359253, agent episode variance: [20.287529739379885, 13.50660460281372, 20.899247871398927], time: 72.34
Running avgs for agent 0: q_loss: 2.5431876182556152, p_loss: 77.37632751464844, mean_rew: -5.448438504953657, variance: 81.15011895751954, mean_q: -77.7884750366211, std_q: 11.462689399719238
Running avgs for agent 1: q_loss: 1.9574205875396729, p_loss: 77.26264190673828, mean_rew: -5.446498724854064, variance: 54.02641841125488, mean_q: -77.67782592773438, std_q: 11.058619499206543
Running avgs for agent 2: q_loss: 2.5155951976776123, p_loss: 79.24955749511719, mean_rew: -5.452575162901301, variance: 83.59699148559571, mean_q: -79.52686309814453, std_q: 11.31430435180664

steps: 449975, episodes: 18000, mean episode reward: -363.4752397221266, agent episode reward: [-121.15841324070887, -121.15841324070887, -121.15841324070887], time: 71.271
steps: 449975, episodes: 18000, mean episode variance: 54.98407932662964, agent episode variance: [20.225474655151366, 13.636378688812256, 21.122225982666016], time: 71.271
Running avgs for agent 0: q_loss: 2.501593589782715, p_loss: 78.3585205078125, mean_rew: -5.412910378810373, variance: 80.90189862060546, mean_q: -78.75605010986328, std_q: 11.249720573425293
Running avgs for agent 1: q_loss: 1.9894826412200928, p_loss: 78.37820434570312, mean_rew: -5.4200647144621525, variance: 54.54551475524902, mean_q: -78.78766632080078, std_q: 10.999802589416504
Running avgs for agent 2: q_loss: 2.5323171615600586, p_loss: 80.24581146240234, mean_rew: -5.42321994176471, variance: 84.48890393066407, mean_q: -80.52342224121094, std_q: 11.116991996765137

steps: 474975, episodes: 19000, mean episode reward: -363.9941732725777, agent episode reward: [-121.33139109085926, -121.33139109085926, -121.33139109085926], time: 69.804
steps: 474975, episodes: 19000, mean episode variance: 54.76821266174316, agent episode variance: [20.309293991088868, 13.645573432922363, 20.813345237731934], time: 69.804
Running avgs for agent 0: q_loss: 2.4822442531585693, p_loss: 79.18128967285156, mean_rew: -5.385306103777446, variance: 81.23717596435547, mean_q: -79.57575988769531, std_q: 11.04000473022461
Running avgs for agent 1: q_loss: 1.9098234176635742, p_loss: 79.18206024169922, mean_rew: -5.379802716279548, variance: 54.582293731689454, mean_q: -79.57687377929688, std_q: 10.844117164611816
Running avgs for agent 2: q_loss: 2.473116636276245, p_loss: 80.95321655273438, mean_rew: -5.383449179393698, variance: 83.25338095092773, mean_q: -81.22692108154297, std_q: 10.805103302001953

steps: 499975, episodes: 20000, mean episode reward: -362.88237219575575, agent episode reward: [-120.96079073191859, -120.96079073191859, -120.96079073191859], time: 76.458
steps: 499975, episodes: 20000, mean episode variance: 55.31116262054444, agent episode variance: [20.214462837219237, 13.83420600128174, 21.262493782043457], time: 76.458
Running avgs for agent 0: q_loss: 2.4415123462677, p_loss: 79.8741683959961, mean_rew: -5.35588866959977, variance: 80.85785134887695, mean_q: -80.25959014892578, std_q: 10.852652549743652
Running avgs for agent 1: q_loss: 1.956544041633606, p_loss: 79.93238067626953, mean_rew: -5.361777274182134, variance: 55.33682400512696, mean_q: -80.31840515136719, std_q: 10.796539306640625
Running avgs for agent 2: q_loss: 2.4474527835845947, p_loss: 81.5778579711914, mean_rew: -5.360406792785952, variance: 85.04997512817383, mean_q: -81.84750366210938, std_q: 10.731552124023438

steps: 524975, episodes: 21000, mean episode reward: -360.9526715690207, agent episode reward: [-120.31755718967356, -120.31755718967356, -120.31755718967356], time: 78.854
steps: 524975, episodes: 21000, mean episode variance: 55.11866375160217, agent episode variance: [20.217703018188477, 14.098198877334594, 20.8027618560791], time: 78.855
Running avgs for agent 0: q_loss: 2.430128574371338, p_loss: 80.48060607910156, mean_rew: -5.3349802835236275, variance: 80.8708120727539, mean_q: -80.86209869384766, std_q: 10.608681678771973
Running avgs for agent 1: q_loss: 1.9219987392425537, p_loss: 80.50804901123047, mean_rew: -5.334992684055115, variance: 56.39279550933838, mean_q: -80.89087677001953, std_q: 10.57883071899414
Running avgs for agent 2: q_loss: 2.4202518463134766, p_loss: 82.04077911376953, mean_rew: -5.336130370736522, variance: 83.2110474243164, mean_q: -82.30975341796875, std_q: 10.69871711730957

steps: 549975, episodes: 22000, mean episode reward: -359.57948493447105, agent episode reward: [-119.85982831149038, -119.85982831149038, -119.85982831149038], time: 78.719
steps: 549975, episodes: 22000, mean episode variance: 54.593178760528566, agent episode variance: [20.41968126296997, 13.810291625976562, 20.363205871582032], time: 78.72
Running avgs for agent 0: q_loss: 2.384340524673462, p_loss: 80.99679565429688, mean_rew: -5.314264206281815, variance: 81.67872505187988, mean_q: -81.37179565429688, std_q: 10.630550384521484
Running avgs for agent 1: q_loss: 1.8551909923553467, p_loss: 80.98432159423828, mean_rew: -5.310088878220888, variance: 55.24116650390625, mean_q: -81.35372161865234, std_q: 10.346541404724121
Running avgs for agent 2: q_loss: 2.3889949321746826, p_loss: 82.33277893066406, mean_rew: -5.308965261657598, variance: 81.45282348632813, mean_q: -82.5967025756836, std_q: 10.45259952545166

steps: 574975, episodes: 23000, mean episode reward: -354.983206649224, agent episode reward: [-118.32773554974132, -118.32773554974132, -118.32773554974132], time: 78.969
steps: 574975, episodes: 23000, mean episode variance: 54.92611803245544, agent episode variance: [20.063878664016723, 14.289277614593505, 20.572961753845213], time: 78.97
Running avgs for agent 0: q_loss: 2.367551326751709, p_loss: 81.36328887939453, mean_rew: -5.286910996926101, variance: 80.2555146560669, mean_q: -81.746826171875, std_q: 10.443452835083008
Running avgs for agent 1: q_loss: 1.8621107339859009, p_loss: 81.3498764038086, mean_rew: -5.284591430122522, variance: 57.15711045837402, mean_q: -81.712890625, std_q: 10.262365341186523
Running avgs for agent 2: q_loss: 2.3323745727539062, p_loss: 82.54039001464844, mean_rew: -5.287676242828669, variance: 82.29184701538085, mean_q: -82.7982177734375, std_q: 10.385445594787598

steps: 599975, episodes: 24000, mean episode reward: -355.37137511345327, agent episode reward: [-118.45712503781776, -118.45712503781776, -118.45712503781776], time: 79.097
steps: 599975, episodes: 24000, mean episode variance: 53.370891441345215, agent episode variance: [19.81016554260254, 13.343904479980468, 20.21682141876221], time: 79.097
Running avgs for agent 0: q_loss: 2.3518967628479004, p_loss: 81.6700439453125, mean_rew: -5.26423972953794, variance: 79.24066217041016, mean_q: -82.04859161376953, std_q: 10.35659122467041
Running avgs for agent 1: q_loss: 1.807509183883667, p_loss: 81.71759033203125, mean_rew: -5.261725824887742, variance: 53.37561791992187, mean_q: -82.07925415039062, std_q: 10.270832061767578
Running avgs for agent 2: q_loss: 2.2992918491363525, p_loss: 82.6399154663086, mean_rew: -5.259820254115335, variance: 80.86728567504883, mean_q: -82.89521026611328, std_q: 10.118531227111816

steps: 624975, episodes: 25000, mean episode reward: -353.9381778074342, agent episode reward: [-117.9793926024781, -117.9793926024781, -117.9793926024781], time: 79.74
steps: 624975, episodes: 25000, mean episode variance: 54.100935108184814, agent episode variance: [20.327454490661623, 14.10401022720337, 19.669470390319823], time: 79.74
Running avgs for agent 0: q_loss: 2.3048458099365234, p_loss: 81.87373352050781, mean_rew: -5.239712717295799, variance: 81.30981796264649, mean_q: -82.24568939208984, std_q: 10.2664155960083
Running avgs for agent 1: q_loss: 1.814894199371338, p_loss: 81.9325180053711, mean_rew: -5.236930961333914, variance: 56.41604090881348, mean_q: -82.28340911865234, std_q: 9.977933883666992
Running avgs for agent 2: q_loss: 2.2522499561309814, p_loss: 82.76781463623047, mean_rew: -5.242455991888838, variance: 78.67788156127929, mean_q: -83.02100372314453, std_q: 9.94139289855957

steps: 649975, episodes: 26000, mean episode reward: -353.10399187451367, agent episode reward: [-117.70133062483791, -117.70133062483791, -117.70133062483791], time: 78.001
steps: 649975, episodes: 26000, mean episode variance: 52.69065032196045, agent episode variance: [19.250986671447755, 13.831597137451173, 19.608066513061523], time: 78.002
Running avgs for agent 0: q_loss: 2.272690773010254, p_loss: 82.0451431274414, mean_rew: -5.213395195341578, variance: 77.00394668579102, mean_q: -82.41224670410156, std_q: 10.099136352539062
Running avgs for agent 1: q_loss: 1.78520667552948, p_loss: 82.0887222290039, mean_rew: -5.215709634083061, variance: 55.32638854980469, mean_q: -82.43404388427734, std_q: 9.832473754882812
Running avgs for agent 2: q_loss: 2.1841676235198975, p_loss: 82.7608413696289, mean_rew: -5.219198710424272, variance: 78.43226605224609, mean_q: -83.00639343261719, std_q: 9.715209007263184

steps: 674975, episodes: 27000, mean episode reward: -350.15745479542704, agent episode reward: [-116.71915159847568, -116.71915159847568, -116.71915159847568], time: 78.04
steps: 674975, episodes: 27000, mean episode variance: 52.94723843193054, agent episode variance: [19.792863254547118, 13.751622766494751, 19.40275241088867], time: 78.041
Running avgs for agent 0: q_loss: 2.271252393722534, p_loss: 82.11204528808594, mean_rew: -5.204221670881502, variance: 79.17145301818847, mean_q: -82.48481750488281, std_q: 9.916080474853516
Running avgs for agent 1: q_loss: 1.8044931888580322, p_loss: 82.18940734863281, mean_rew: -5.20024114252075, variance: 55.006491065979006, mean_q: -82.52032470703125, std_q: 9.781094551086426
Running avgs for agent 2: q_loss: 2.182568311691284, p_loss: 82.764892578125, mean_rew: -5.19892033254703, variance: 77.61100964355468, mean_q: -83.0057373046875, std_q: 9.707014083862305

steps: 699975, episodes: 28000, mean episode reward: -349.323010500779, agent episode reward: [-116.44100350025964, -116.44100350025964, -116.44100350025964], time: 78.173
steps: 699975, episodes: 28000, mean episode variance: 52.021326017379764, agent episode variance: [19.571666687011717, 13.68198171043396, 18.767677619934084], time: 78.174
Running avgs for agent 0: q_loss: 2.2169606685638428, p_loss: 82.11177825927734, mean_rew: -5.178037349516246, variance: 78.28666674804687, mean_q: -82.47888946533203, std_q: 9.958699226379395
Running avgs for agent 1: q_loss: 1.8114924430847168, p_loss: 82.28617095947266, mean_rew: -5.183516901016379, variance: 54.72792684173584, mean_q: -82.60922241210938, std_q: 9.780047416687012
Running avgs for agent 2: q_loss: 2.104609489440918, p_loss: 82.69854736328125, mean_rew: -5.179579992315882, variance: 75.07071047973633, mean_q: -82.93140411376953, std_q: 9.596941947937012

steps: 724975, episodes: 29000, mean episode reward: -348.4966700379391, agent episode reward: [-116.16555667931304, -116.16555667931304, -116.16555667931304], time: 77.736
steps: 724975, episodes: 29000, mean episode variance: 52.01147648620606, agent episode variance: [19.259834442138672, 13.425570114135741, 19.32607192993164], time: 77.736
Running avgs for agent 0: q_loss: 2.1618573665618896, p_loss: 82.0627212524414, mean_rew: -5.159354739350085, variance: 77.03933776855469, mean_q: -82.42401123046875, std_q: 9.760348320007324
Running avgs for agent 1: q_loss: 1.7627146244049072, p_loss: 82.33513641357422, mean_rew: -5.164529285195549, variance: 53.702280456542965, mean_q: -82.65650177001953, std_q: 9.712777137756348
Running avgs for agent 2: q_loss: 2.1074917316436768, p_loss: 82.57952117919922, mean_rew: -5.164745227856531, variance: 77.30428771972656, mean_q: -82.80972290039062, std_q: 9.527461051940918

steps: 749975, episodes: 30000, mean episode reward: -347.94573217225036, agent episode reward: [-115.98191072408346, -115.98191072408346, -115.98191072408346], time: 78.098
steps: 749975, episodes: 30000, mean episode variance: 50.9949582862854, agent episode variance: [18.850073913574217, 13.52135478591919, 18.623529586791992], time: 78.099
Running avgs for agent 0: q_loss: 2.143958330154419, p_loss: 82.05013275146484, mean_rew: -5.1432779520807435, variance: 75.40029565429687, mean_q: -82.40509033203125, std_q: 9.58716869354248
Running avgs for agent 1: q_loss: 1.8083631992340088, p_loss: 82.26471710205078, mean_rew: -5.139898282628176, variance: 54.08541914367676, mean_q: -82.58131408691406, std_q: 9.397150039672852
Running avgs for agent 2: q_loss: 2.04170298576355, p_loss: 82.45382690429688, mean_rew: -5.143549564436342, variance: 74.49411834716797, mean_q: -82.67958068847656, std_q: 9.400982856750488

steps: 774975, episodes: 31000, mean episode reward: -344.8263112716184, agent episode reward: [-114.94210375720614, -114.94210375720614, -114.94210375720614], time: 78.238
steps: 774975, episodes: 31000, mean episode variance: 50.10184447097778, agent episode variance: [18.818635314941407, 13.015111545562744, 18.26809761047363], time: 78.238
Running avgs for agent 0: q_loss: 2.070193290710449, p_loss: 81.99034118652344, mean_rew: -5.125671239218404, variance: 75.27454125976563, mean_q: -82.34370422363281, std_q: 9.279763221740723
Running avgs for agent 1: q_loss: 1.7787445783615112, p_loss: 82.24974822998047, mean_rew: -5.130201273533622, variance: 52.06044618225098, mean_q: -82.56459045410156, std_q: 9.40427017211914
Running avgs for agent 2: q_loss: 2.0116584300994873, p_loss: 82.30028533935547, mean_rew: -5.124854356908927, variance: 73.07239044189453, mean_q: -82.5208740234375, std_q: 9.389876365661621

steps: 799975, episodes: 32000, mean episode reward: -345.0466262986527, agent episode reward: [-115.01554209955088, -115.01554209955088, -115.01554209955088], time: 78.793
steps: 799975, episodes: 32000, mean episode variance: 50.19243371963501, agent episode variance: [18.35539616394043, 13.452362060546875, 18.384675495147704], time: 78.794
Running avgs for agent 0: q_loss: 2.040426254272461, p_loss: 81.87676239013672, mean_rew: -5.10785799818864, variance: 73.42158465576172, mean_q: -82.23419952392578, std_q: 9.236433982849121
Running avgs for agent 1: q_loss: 1.7174015045166016, p_loss: 82.10257720947266, mean_rew: -5.109080989723145, variance: 53.8094482421875, mean_q: -82.41273498535156, std_q: 9.076878547668457
Running avgs for agent 2: q_loss: 1.9861540794372559, p_loss: 82.19384002685547, mean_rew: -5.111172967025422, variance: 73.53870198059082, mean_q: -82.40823364257812, std_q: 9.298787117004395

steps: 824975, episodes: 33000, mean episode reward: -342.859488017999, agent episode reward: [-114.28649600599967, -114.28649600599967, -114.28649600599967], time: 77.858
steps: 824975, episodes: 33000, mean episode variance: 49.202846489906314, agent episode variance: [18.472018238067626, 12.894596376419067, 17.836231875419617], time: 77.858
Running avgs for agent 0: q_loss: 1.9787424802780151, p_loss: 81.81000518798828, mean_rew: -5.094797827128772, variance: 73.8880729522705, mean_q: -82.1592025756836, std_q: 9.193524360656738
Running avgs for agent 1: q_loss: 1.7064507007598877, p_loss: 82.02576446533203, mean_rew: -5.090569721597004, variance: 51.578385505676266, mean_q: -82.33487701416016, std_q: 9.176485061645508
Running avgs for agent 2: q_loss: 1.9932928085327148, p_loss: 82.07620239257812, mean_rew: -5.095884506670612, variance: 71.34492750167847, mean_q: -82.28984069824219, std_q: 9.33246898651123

steps: 849975, episodes: 34000, mean episode reward: -342.35894676321084, agent episode reward: [-114.11964892107027, -114.11964892107027, -114.11964892107027], time: 78.528
steps: 849975, episodes: 34000, mean episode variance: 47.85073707199097, agent episode variance: [17.7130505027771, 12.821673049926758, 17.31601351928711], time: 78.529
Running avgs for agent 0: q_loss: 1.9675376415252686, p_loss: 81.69148254394531, mean_rew: -5.076972244194756, variance: 70.8522020111084, mean_q: -82.0386734008789, std_q: 9.00793170928955
Running avgs for agent 1: q_loss: 1.7533488273620605, p_loss: 81.98783874511719, mean_rew: -5.085263641983698, variance: 51.28669219970703, mean_q: -82.29374694824219, std_q: 9.111248016357422
Running avgs for agent 2: q_loss: 1.9161103963851929, p_loss: 81.98838806152344, mean_rew: -5.077771020645443, variance: 69.26405407714844, mean_q: -82.20381164550781, std_q: 9.194694519042969

steps: 874975, episodes: 35000, mean episode reward: -342.08475286931724, agent episode reward: [-114.02825095643908, -114.02825095643908, -114.02825095643908], time: 87.784
steps: 874975, episodes: 35000, mean episode variance: 48.405516807556154, agent episode variance: [17.573848720550536, 13.051908111572265, 17.77975997543335], time: 87.785
Running avgs for agent 0: q_loss: 1.9181804656982422, p_loss: 81.60086822509766, mean_rew: -5.062806314257316, variance: 70.29539488220215, mean_q: -81.93952941894531, std_q: 8.935751914978027
Running avgs for agent 1: q_loss: 1.6224768161773682, p_loss: 81.84326934814453, mean_rew: -5.062470710920592, variance: 52.20763244628906, mean_q: -82.14160919189453, std_q: 8.93260669708252
Running avgs for agent 2: q_loss: 1.9025394916534424, p_loss: 81.81117248535156, mean_rew: -5.064447218827165, variance: 71.1190399017334, mean_q: -82.02627563476562, std_q: 9.129703521728516

steps: 899975, episodes: 36000, mean episode reward: -338.0517195078081, agent episode reward: [-112.68390650260271, -112.68390650260271, -112.68390650260271], time: 83.772
steps: 899975, episodes: 36000, mean episode variance: 47.94697132873535, agent episode variance: [17.344696125030516, 13.039858226776124, 17.56241697692871], time: 83.772
Running avgs for agent 0: q_loss: 1.877784252166748, p_loss: 81.48619842529297, mean_rew: -5.047175973334981, variance: 69.37878450012207, mean_q: -81.82379913330078, std_q: 8.8435640335083
Running avgs for agent 1: q_loss: 1.6583242416381836, p_loss: 81.83590698242188, mean_rew: -5.051842740978458, variance: 52.159432907104495, mean_q: -82.13070678710938, std_q: 8.830277442932129
Running avgs for agent 2: q_loss: 1.8930917978286743, p_loss: 81.66487121582031, mean_rew: -5.047859616662138, variance: 70.24966790771484, mean_q: -81.8825454711914, std_q: 9.030519485473633

steps: 924975, episodes: 37000, mean episode reward: -341.73378624542863, agent episode reward: [-113.91126208180951, -113.91126208180951, -113.91126208180951], time: 81.523
steps: 924975, episodes: 37000, mean episode variance: 47.484411908626555, agent episode variance: [17.420438278198244, 12.915818707942963, 17.14815492248535], time: 81.524
Running avgs for agent 0: q_loss: 1.8958858251571655, p_loss: 81.44367218017578, mean_rew: -5.042132235773028, variance: 69.68175311279298, mean_q: -81.77872467041016, std_q: 8.905181884765625
Running avgs for agent 1: q_loss: 1.7851332426071167, p_loss: 81.75712585449219, mean_rew: -5.038794196330863, variance: 51.66327483177185, mean_q: -82.05039978027344, std_q: 8.741769790649414
Running avgs for agent 2: q_loss: 1.8697841167449951, p_loss: 81.5002670288086, mean_rew: -5.036135081816505, variance: 68.5926196899414, mean_q: -81.71803283691406, std_q: 9.072287559509277

steps: 949975, episodes: 38000, mean episode reward: -338.71530077785644, agent episode reward: [-112.90510025928548, -112.90510025928548, -112.90510025928548], time: 81.977
steps: 949975, episodes: 38000, mean episode variance: 57.014179481506346, agent episode variance: [16.81522317504883, 22.749524452209474, 17.449431854248047], time: 81.978
Running avgs for agent 0: q_loss: 1.8193432092666626, p_loss: 81.31521606445312, mean_rew: -5.0243935459747116, variance: 67.26089270019531, mean_q: -81.64064025878906, std_q: 8.610271453857422
Running avgs for agent 1: q_loss: 2.7160444259643555, p_loss: 81.71693420410156, mean_rew: -5.023595355863177, variance: 90.9980978088379, mean_q: -82.00052642822266, std_q: 8.622858047485352
Running avgs for agent 2: q_loss: 1.8536977767944336, p_loss: 81.29303741455078, mean_rew: -5.021258669131497, variance: 69.79772741699219, mean_q: -81.50863647460938, std_q: 9.078239440917969

steps: 974975, episodes: 39000, mean episode reward: -337.33789236139427, agent episode reward: [-112.44596412046474, -112.44596412046474, -112.44596412046474], time: 82.458
steps: 974975, episodes: 39000, mean episode variance: 55.89304872894287, agent episode variance: [17.0419189453125, 22.130962112426758, 16.720167671203612], time: 82.459
Running avgs for agent 0: q_loss: 1.8058000802993774, p_loss: 81.21460723876953, mean_rew: -5.010430225477743, variance: 68.16767578125, mean_q: -81.53748321533203, std_q: 8.531312942504883
Running avgs for agent 1: q_loss: 2.5064237117767334, p_loss: 81.62718963623047, mean_rew: -5.008478574158058, variance: 88.52384844970703, mean_q: -81.9052505493164, std_q: 8.364517211914062
Running avgs for agent 2: q_loss: 1.8360304832458496, p_loss: 81.15090942382812, mean_rew: -5.011685159624996, variance: 66.88067068481445, mean_q: -81.37055206298828, std_q: 8.954642295837402

steps: 999975, episodes: 40000, mean episode reward: -337.2185694361735, agent episode reward: [-112.40618981205783, -112.40618981205783, -112.40618981205783], time: 112.146
steps: 999975, episodes: 40000, mean episode variance: 55.225688678741456, agent episode variance: [16.633203910827635, 22.133439735412598, 16.45904503250122], time: 112.147
Running avgs for agent 0: q_loss: 1.7959692478179932, p_loss: 81.09331512451172, mean_rew: -4.996906101966041, variance: 66.53281564331054, mean_q: -81.41329956054688, std_q: 8.53017520904541
Running avgs for agent 1: q_loss: 2.466904401779175, p_loss: 81.50858306884766, mean_rew: -4.998459043672916, variance: 88.53375894165039, mean_q: -81.78536987304688, std_q: 8.439769744873047
Running avgs for agent 2: q_loss: 1.8188730478286743, p_loss: 80.92390441894531, mean_rew: -4.997748564477581, variance: 65.83618013000488, mean_q: -81.1463623046875, std_q: 8.825875282287598

steps: 1024975, episodes: 41000, mean episode reward: -335.67446878592636, agent episode reward: [-111.8914895953088, -111.8914895953088, -111.8914895953088], time: 133.789
steps: 1024975, episodes: 41000, mean episode variance: 52.31965686798096, agent episode variance: [16.206966464996338, 19.20830141067505, 16.90438899230957], time: 133.79
Running avgs for agent 0: q_loss: 1.7519617080688477, p_loss: 80.83534240722656, mean_rew: -4.959186949693526, variance: 64.82786585998535, mean_q: -81.14724731445312, std_q: 8.36082935333252
Running avgs for agent 1: q_loss: 2.265498161315918, p_loss: 81.18248748779297, mean_rew: -4.95640244729347, variance: 76.8332056427002, mean_q: -81.44569396972656, std_q: 8.241782188415527
Running avgs for agent 2: q_loss: 1.795902967453003, p_loss: 80.54397583007812, mean_rew: -4.956979365440591, variance: 67.61755596923828, mean_q: -80.76045227050781, std_q: 8.873662948608398

steps: 1049975, episodes: 42000, mean episode reward: -338.64021041958534, agent episode reward: [-112.8800701398618, -112.8800701398618, -112.8800701398618], time: 85.772
steps: 1049975, episodes: 42000, mean episode variance: 43.57006856918335, agent episode variance: [16.307587760925294, 11.100722652435303, 16.161758155822753], time: 85.773
Running avgs for agent 0: q_loss: 1.681632399559021, p_loss: 80.1858139038086, mean_rew: -4.880800375133917, variance: 65.23035104370118, mean_q: -80.46839904785156, std_q: 6.972568511962891
Running avgs for agent 1: q_loss: 1.4601527452468872, p_loss: 80.54576873779297, mean_rew: -4.882240229086618, variance: 44.40289060974121, mean_q: -80.77497100830078, std_q: 6.663436412811279
Running avgs for agent 2: q_loss: 1.7244524955749512, p_loss: 79.85557556152344, mean_rew: -4.883399643835137, variance: 64.64703262329101, mean_q: -80.04839324951172, std_q: 7.273667812347412

steps: 1074975, episodes: 43000, mean episode reward: -334.18758953459513, agent episode reward: [-111.3958631781984, -111.3958631781984, -111.3958631781984], time: 81.62
steps: 1074975, episodes: 43000, mean episode variance: 42.53292738723755, agent episode variance: [15.754533172607422, 11.161527851104736, 15.616866363525391], time: 81.62
Running avgs for agent 0: q_loss: 1.593178153038025, p_loss: 79.59927368164062, mean_rew: -4.816652233551289, variance: 63.01813269042969, mean_q: -79.86072540283203, std_q: 6.1468281745910645
Running avgs for agent 1: q_loss: 1.3292654752731323, p_loss: 80.10951232910156, mean_rew: -4.823374670448697, variance: 44.646111404418946, mean_q: -80.31828308105469, std_q: 5.929667949676514
Running avgs for agent 2: q_loss: 1.6528762578964233, p_loss: 79.25834655761719, mean_rew: -4.8200167488654735, variance: 62.467465454101564, mean_q: -79.43350219726562, std_q: 6.297130107879639

steps: 1099975, episodes: 44000, mean episode reward: -333.34586381793804, agent episode reward: [-111.1152879393127, -111.1152879393127, -111.1152879393127], time: 78.899
steps: 1099975, episodes: 44000, mean episode variance: 42.4169326133728, agent episode variance: [15.307135787963867, 11.26655061340332, 15.843246212005615], time: 78.9
Running avgs for agent 0: q_loss: 1.5389541387557983, p_loss: 79.24925994873047, mean_rew: -4.778369691527029, variance: 61.22854315185547, mean_q: -79.49694061279297, std_q: 6.011260032653809
Running avgs for agent 1: q_loss: 1.2886450290679932, p_loss: 79.82548522949219, mean_rew: -4.782115032481487, variance: 45.06620245361328, mean_q: -80.02832794189453, std_q: 5.849755764007568
Running avgs for agent 2: q_loss: 1.6088706254959106, p_loss: 78.86503601074219, mean_rew: -4.78428941670401, variance: 63.37298484802246, mean_q: -79.03578186035156, std_q: 6.204325199127197

steps: 1124975, episodes: 45000, mean episode reward: -333.0871543801935, agent episode reward: [-111.02905146006451, -111.02905146006451, -111.02905146006451], time: 78.238
steps: 1124975, episodes: 45000, mean episode variance: 42.10884734725952, agent episode variance: [15.305082530975342, 11.116677207946777, 15.687087608337402], time: 78.238
Running avgs for agent 0: q_loss: 1.4849841594696045, p_loss: 78.88400268554688, mean_rew: -4.7520878202525685, variance: 61.22033012390137, mean_q: -79.11981201171875, std_q: 5.9981465339660645
Running avgs for agent 1: q_loss: 1.2439547777175903, p_loss: 79.52223205566406, mean_rew: -4.759647505956121, variance: 44.46670883178711, mean_q: -79.72430419921875, std_q: 5.846796989440918
Running avgs for agent 2: q_loss: 1.5757635831832886, p_loss: 78.52648162841797, mean_rew: -4.756084036114606, variance: 62.74835043334961, mean_q: -78.6885757446289, std_q: 6.123778820037842

steps: 1149975, episodes: 46000, mean episode reward: -332.1539319940017, agent episode reward: [-110.71797733133394, -110.71797733133394, -110.71797733133394], time: 79.35
steps: 1149975, episodes: 46000, mean episode variance: 40.381723587036134, agent episode variance: [14.887840404510499, 10.69183390045166, 14.802049282073975], time: 79.351
Running avgs for agent 0: q_loss: 1.4674384593963623, p_loss: 78.55351257324219, mean_rew: -4.735076387135732, variance: 59.551361618041994, mean_q: -78.78134155273438, std_q: 5.959624767303467
Running avgs for agent 1: q_loss: 1.230729579925537, p_loss: 79.22142791748047, mean_rew: -4.730687565397817, variance: 42.76733560180664, mean_q: -79.4281997680664, std_q: 5.831693172454834
Running avgs for agent 2: q_loss: 1.5634006261825562, p_loss: 78.21038818359375, mean_rew: -4.732667432156302, variance: 59.2081971282959, mean_q: -78.36949920654297, std_q: 6.08411169052124

steps: 1174975, episodes: 47000, mean episode reward: -330.68714297763063, agent episode reward: [-110.22904765921021, -110.22904765921021, -110.22904765921021], time: 78.483
steps: 1174975, episodes: 47000, mean episode variance: 40.65993418502808, agent episode variance: [14.444558555603027, 10.831939201354981, 15.383436428070068], time: 78.483
Running avgs for agent 0: q_loss: 1.4389231204986572, p_loss: 78.23136138916016, mean_rew: -4.71492102061098, variance: 57.77823422241211, mean_q: -78.45429992675781, std_q: 5.894182205200195
Running avgs for agent 1: q_loss: 1.1987807750701904, p_loss: 78.88732147216797, mean_rew: -4.712849870810219, variance: 43.327756805419924, mean_q: -79.09848022460938, std_q: 5.855875015258789
Running avgs for agent 2: q_loss: 1.5470364093780518, p_loss: 77.86088562011719, mean_rew: -4.717958081162153, variance: 61.53374571228027, mean_q: -78.00757598876953, std_q: 6.09458589553833

steps: 1199975, episodes: 48000, mean episode reward: -330.04645762695395, agent episode reward: [-110.01548587565131, -110.01548587565131, -110.01548587565131], time: 78.942
steps: 1199975, episodes: 48000, mean episode variance: 40.092892963409426, agent episode variance: [14.295417114257813, 10.91303674697876, 14.884439102172852], time: 78.943
Running avgs for agent 0: q_loss: 1.415644884109497, p_loss: 77.8946304321289, mean_rew: -4.6945397316560245, variance: 57.18166845703125, mean_q: -78.11290740966797, std_q: 5.906166076660156
Running avgs for agent 1: q_loss: 1.1871205568313599, p_loss: 78.5720443725586, mean_rew: -4.7014010988169135, variance: 43.65214698791504, mean_q: -78.78675842285156, std_q: 5.873884677886963
Running avgs for agent 2: q_loss: 1.5216538906097412, p_loss: 77.59331512451172, mean_rew: -4.698755996727132, variance: 59.53775640869141, mean_q: -77.73628234863281, std_q: 6.05978536605835

steps: 1224975, episodes: 49000, mean episode reward: -332.11621911641805, agent episode reward: [-110.70540637213935, -110.70540637213935, -110.70540637213935], time: 86.619
steps: 1224975, episodes: 49000, mean episode variance: 39.7258998374939, agent episode variance: [14.461723861694336, 10.771400173187256, 14.492775802612305], time: 86.62
Running avgs for agent 0: q_loss: 1.4043904542922974, p_loss: 77.5760498046875, mean_rew: -4.680336138066756, variance: 57.846895446777346, mean_q: -77.7898178100586, std_q: 5.884605884552002
Running avgs for agent 1: q_loss: 1.1692299842834473, p_loss: 78.22493743896484, mean_rew: -4.679178079603418, variance: 43.085600692749026, mean_q: -78.43829345703125, std_q: 5.887946128845215
Running avgs for agent 2: q_loss: 1.5129814147949219, p_loss: 77.3230209350586, mean_rew: -4.681487933667765, variance: 57.97110321044922, mean_q: -77.46190643310547, std_q: 6.014754772186279

steps: 1249975, episodes: 50000, mean episode reward: -328.9923311996491, agent episode reward: [-109.66411039988304, -109.66411039988304, -109.66411039988304], time: 78.82
steps: 1249975, episodes: 50000, mean episode variance: 39.90386640548706, agent episode variance: [14.16850647354126, 11.109164867401123, 14.626195064544678], time: 78.82
Running avgs for agent 0: q_loss: 1.3815358877182007, p_loss: 77.24557495117188, mean_rew: -4.6658582800412285, variance: 56.67402589416504, mean_q: -77.4561996459961, std_q: 5.881973743438721
Running avgs for agent 1: q_loss: 1.150051474571228, p_loss: 77.85321044921875, mean_rew: -4.664763594063005, variance: 44.43665946960449, mean_q: -78.0663070678711, std_q: 5.899422645568848
Running avgs for agent 2: q_loss: 1.4918198585510254, p_loss: 77.0340576171875, mean_rew: -4.662769443905308, variance: 58.50478025817871, mean_q: -77.17412567138672, std_q: 5.978398323059082

steps: 1274975, episodes: 51000, mean episode reward: -329.76519882519517, agent episode reward: [-109.92173294173172, -109.92173294173172, -109.92173294173172], time: 78.625
steps: 1274975, episodes: 51000, mean episode variance: 36.85095123744011, agent episode variance: [13.976147613525391, 8.507575089216232, 14.367228534698487], time: 78.625
Running avgs for agent 0: q_loss: 1.3639349937438965, p_loss: 76.91204833984375, mean_rew: -4.646013532927343, variance: 55.904590454101566, mean_q: -77.11917114257812, std_q: 5.839432716369629
Running avgs for agent 1: q_loss: 1.1904807090759277, p_loss: 77.57782745361328, mean_rew: -4.647750290542917, variance: 34.03030035686493, mean_q: -77.79068756103516, std_q: 5.909542083740234
Running avgs for agent 2: q_loss: 1.4566887617111206, p_loss: 76.78028106689453, mean_rew: -4.645745082283997, variance: 57.46891413879395, mean_q: -76.91593933105469, std_q: 5.938772678375244

steps: 1299975, episodes: 52000, mean episode reward: -326.1727384650825, agent episode reward: [-108.7242461550275, -108.7242461550275, -108.7242461550275], time: 74.667
steps: 1299975, episodes: 52000, mean episode variance: 38.634718196868896, agent episode variance: [14.085324893951416, 10.58974584197998, 13.9596474609375], time: 74.667
Running avgs for agent 0: q_loss: 1.350390911102295, p_loss: 76.56904602050781, mean_rew: -4.633274797870356, variance: 56.341299575805664, mean_q: -76.77433013916016, std_q: 5.824377059936523
Running avgs for agent 1: q_loss: 1.1226969957351685, p_loss: 77.24490356445312, mean_rew: -4.632352556690068, variance: 42.35898336791992, mean_q: -77.45398712158203, std_q: 5.914428234100342
Running avgs for agent 2: q_loss: 1.4543871879577637, p_loss: 76.57376861572266, mean_rew: -4.634534126130662, variance: 55.83858984375, mean_q: -76.71392059326172, std_q: 5.940590858459473

steps: 1324975, episodes: 53000, mean episode reward: -326.513435536457, agent episode reward: [-108.83781184548567, -108.83781184548567, -108.83781184548567], time: 74.063
steps: 1324975, episodes: 53000, mean episode variance: 38.03073575973511, agent episode variance: [13.56695352935791, 10.146841571807862, 14.316940658569337], time: 74.063
Running avgs for agent 0: q_loss: 1.338093638420105, p_loss: 76.28669738769531, mean_rew: -4.616809022546391, variance: 54.26781411743164, mean_q: -76.4875259399414, std_q: 5.847063064575195
Running avgs for agent 1: q_loss: 1.1138453483581543, p_loss: 76.96976470947266, mean_rew: -4.618230641642485, variance: 40.58736628723145, mean_q: -77.17806243896484, std_q: 5.9184041023254395
Running avgs for agent 2: q_loss: 1.42744779586792, p_loss: 76.3302230834961, mean_rew: -4.619742102297391, variance: 57.26776263427735, mean_q: -76.46714782714844, std_q: 5.953049182891846

steps: 1349975, episodes: 54000, mean episode reward: -325.4714117327534, agent episode reward: [-108.49047057758446, -108.49047057758446, -108.49047057758446], time: 74.66
steps: 1349975, episodes: 54000, mean episode variance: 37.540709701538084, agent episode variance: [13.284407344818115, 10.050826564788819, 14.205475791931152], time: 74.66
Running avgs for agent 0: q_loss: 1.325639009475708, p_loss: 76.03907775878906, mean_rew: -4.599935291471055, variance: 53.13762937927246, mean_q: -76.23613739013672, std_q: 5.854852199554443
Running avgs for agent 1: q_loss: 1.1137932538986206, p_loss: 76.66254425048828, mean_rew: -4.601264761765547, variance: 40.203306259155276, mean_q: -76.8635482788086, std_q: 5.926196098327637
Running avgs for agent 2: q_loss: 1.4106364250183105, p_loss: 76.05259704589844, mean_rew: -4.603360941648043, variance: 56.82190316772461, mean_q: -76.19074249267578, std_q: 5.931216239929199

steps: 1374975, episodes: 55000, mean episode reward: -325.1062410665392, agent episode reward: [-108.36874702217973, -108.36874702217973, -108.36874702217973], time: 75.407
steps: 1374975, episodes: 55000, mean episode variance: 37.20185384750366, agent episode variance: [13.676795711517334, 9.924462665557861, 13.600595470428466], time: 75.408
Running avgs for agent 0: q_loss: 1.320831537246704, p_loss: 75.78535461425781, mean_rew: -4.589557102728521, variance: 54.70718284606934, mean_q: -75.97953796386719, std_q: 5.869207859039307
Running avgs for agent 1: q_loss: 1.1057814359664917, p_loss: 76.35730743408203, mean_rew: -4.585319625125084, variance: 39.697850662231446, mean_q: -76.55309295654297, std_q: 5.934622287750244
Running avgs for agent 2: q_loss: 1.3806555271148682, p_loss: 75.84172058105469, mean_rew: -4.587374166877312, variance: 54.402381881713865, mean_q: -75.97409057617188, std_q: 5.954600811004639

steps: 1399975, episodes: 56000, mean episode reward: -322.1897263856884, agent episode reward: [-107.39657546189613, -107.39657546189613, -107.39657546189613], time: 73.747
steps: 1399975, episodes: 56000, mean episode variance: 36.531790927886966, agent episode variance: [12.861500839233399, 10.23756358718872, 13.432726501464844], time: 73.747
Running avgs for agent 0: q_loss: 1.3069484233856201, p_loss: 75.48358154296875, mean_rew: -4.570810742163887, variance: 51.446003356933595, mean_q: -75.6814193725586, std_q: 5.9065117835998535
Running avgs for agent 1: q_loss: 1.0763251781463623, p_loss: 76.00096893310547, mean_rew: -4.5695815500830745, variance: 40.95025434875488, mean_q: -76.1910171508789, std_q: 5.952726364135742
Running avgs for agent 2: q_loss: 1.3592406511306763, p_loss: 75.61604309082031, mean_rew: -4.575465575305845, variance: 53.730906005859374, mean_q: -75.74600982666016, std_q: 6.014493942260742

steps: 1424975, episodes: 57000, mean episode reward: -320.92192513801405, agent episode reward: [-106.97397504600468, -106.97397504600468, -106.97397504600468], time: 74.73
steps: 1424975, episodes: 57000, mean episode variance: 36.32900965690613, agent episode variance: [12.955297416687012, 9.95179542350769, 13.421916816711425], time: 74.731
Running avgs for agent 0: q_loss: 1.2969822883605957, p_loss: 75.17969512939453, mean_rew: -4.555331456279062, variance: 51.82118966674805, mean_q: -75.37194061279297, std_q: 5.8731231689453125
Running avgs for agent 1: q_loss: 1.0557045936584473, p_loss: 75.67839050292969, mean_rew: -4.554382163627568, variance: 39.80718169403076, mean_q: -75.86576080322266, std_q: 5.963890075683594
Running avgs for agent 2: q_loss: 1.3298286199569702, p_loss: 75.30278015136719, mean_rew: -4.555734058917208, variance: 53.6876672668457, mean_q: -75.43692779541016, std_q: 5.9982476234436035

steps: 1449975, episodes: 58000, mean episode reward: -319.71075316704434, agent episode reward: [-106.57025105568147, -106.57025105568147, -106.57025105568147], time: 73.894
steps: 1449975, episodes: 58000, mean episode variance: 35.96109503555298, agent episode variance: [12.87224045944214, 9.946144733428955, 13.142709842681885], time: 73.894
Running avgs for agent 0: q_loss: 1.2838190793991089, p_loss: 74.8897933959961, mean_rew: -4.5398960059300855, variance: 51.48896183776856, mean_q: -75.0783462524414, std_q: 5.9101738929748535
Running avgs for agent 1: q_loss: 1.0577884912490845, p_loss: 75.32353210449219, mean_rew: -4.5434850778951175, variance: 39.78457893371582, mean_q: -75.50946044921875, std_q: 5.947207927703857
Running avgs for agent 2: q_loss: 1.3111732006072998, p_loss: 75.04554748535156, mean_rew: -4.543814849897392, variance: 52.57083937072754, mean_q: -75.17671966552734, std_q: 6.018164157867432

steps: 1474975, episodes: 59000, mean episode reward: -318.5482954588411, agent episode reward: [-106.18276515294701, -106.18276515294701, -106.18276515294701], time: 74.159
steps: 1474975, episodes: 59000, mean episode variance: 35.80323924446106, agent episode variance: [12.472742975234985, 9.823809913635253, 13.506686355590821], time: 74.16
Running avgs for agent 0: q_loss: 1.2647932767868042, p_loss: 74.63623809814453, mean_rew: -4.530605106167537, variance: 49.89097190093994, mean_q: -74.81900787353516, std_q: 5.954385757446289
Running avgs for agent 1: q_loss: 1.0448952913284302, p_loss: 74.95594024658203, mean_rew: -4.5294147189953184, variance: 39.295239654541014, mean_q: -75.13877868652344, std_q: 5.963484764099121
Running avgs for agent 2: q_loss: 1.2996975183486938, p_loss: 74.72315216064453, mean_rew: -4.5343093751639945, variance: 54.026745422363284, mean_q: -74.85184478759766, std_q: 6.069616317749023

steps: 1499975, episodes: 60000, mean episode reward: -319.203745738056, agent episode reward: [-106.40124857935201, -106.40124857935201, -106.40124857935201], time: 71.543
steps: 1499975, episodes: 60000, mean episode variance: 35.52781467819214, agent episode variance: [12.779769847869874, 9.710297016143798, 13.037747814178466], time: 71.544
Running avgs for agent 0: q_loss: 1.2466038465499878, p_loss: 74.29137420654297, mean_rew: -4.508376676510142, variance: 51.119079391479495, mean_q: -74.47293853759766, std_q: 5.936872959136963
Running avgs for agent 1: q_loss: 1.0242118835449219, p_loss: 74.59175872802734, mean_rew: -4.5112225407940985, variance: 38.84118806457519, mean_q: -74.76927947998047, std_q: 5.947225570678711
Running avgs for agent 2: q_loss: 1.279032826423645, p_loss: 74.39788055419922, mean_rew: -4.51656543651249, variance: 52.150991256713866, mean_q: -74.52684783935547, std_q: 6.092762470245361

...Finished total of 60001 episodes... Now freezing policy and running for 10000 more episodes to get cvar estimation
steps: 24975, episodes: 1000, mean episode reward: -318.61156153817126, agent episode reward: [-106.20385384605707, -106.20385384605707, -106.20385384605707], time: 54.054
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 54.054
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -318.4976875995691, agent episode reward: [-106.16589586652303, -106.16589586652303, -106.16589586652303], time: 66.324
steps: 49975, episodes: 2000, mean episode variance: 29.65044450378418, agent episode variance: [0.0, 12.39049185180664, 17.25995265197754], time: 66.325
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.253192079800847, variance: 0.0, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -73.64964294433594, std_q: 5.9620442390441895
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.257847560208658, variance: 50.780704498291016, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -73.40095520019531, std_q: 5.912193298339844
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.255134791650745, variance: 70.73751068115234, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -72.79106140136719, std_q: 6.269201278686523

steps: 74975, episodes: 3000, mean episode reward: -320.4717326810915, agent episode reward: [-106.82391089369717, -106.82391089369717, -106.82391089369717], time: 70.741
steps: 74975, episodes: 3000, mean episode variance: 30.06670285797119, agent episode variance: [0.0, 12.637099452972413, 17.42960340499878], time: 70.742
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.252609644835851, variance: 0.0, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -73.65238952636719, std_q: 5.941257953643799
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.257456692067222, variance: 50.548397064208984, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -73.39647674560547, std_q: 5.9136834144592285
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.250953635193429, variance: 69.71842193603516, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -72.75550079345703, std_q: 6.266299247741699

steps: 99975, episodes: 4000, mean episode reward: -319.5923570396649, agent episode reward: [-106.5307856798883, -106.5307856798883, -106.5307856798883], time: 70.673
steps: 99975, episodes: 4000, mean episode variance: 30.077640522003176, agent episode variance: [0.0, 12.664159107208253, 17.413481414794923], time: 70.674
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.259579604301281, variance: 0.0, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -73.66603088378906, std_q: 6.010150909423828
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.256594856805991, variance: 50.65663146972656, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -73.38765716552734, std_q: 5.924211502075195
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.266530330412568, variance: 69.65392303466797, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -72.82289123535156, std_q: 6.33620023727417

steps: 124975, episodes: 5000, mean episode reward: -316.88217013183885, agent episode reward: [-105.62739004394628, -105.62739004394628, -105.62739004394628], time: 70.148
steps: 124975, episodes: 5000, mean episode variance: 29.90833783340454, agent episode variance: [0.0, 12.598602043151855, 17.309735790252684], time: 70.148
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.260037251927017, variance: 0.0, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -73.6556396484375, std_q: 5.980183124542236
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.260461458758304, variance: 50.394405364990234, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -73.42121887207031, std_q: 5.930603504180908
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.253521243661357, variance: 69.23893737792969, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -72.76939392089844, std_q: 6.3031158447265625

steps: 149975, episodes: 6000, mean episode reward: -321.33078096323055, agent episode reward: [-107.11026032107684, -107.11026032107684, -107.11026032107684], time: 70.412
steps: 149975, episodes: 6000, mean episode variance: 30.0144895324707, agent episode variance: [0.0, 12.627388210296631, 17.387101322174072], time: 70.412
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.259684464887308, variance: 0.0, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -73.61675262451172, std_q: 5.975697040557861
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.259445223076712, variance: 50.509552001953125, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -73.3902359008789, std_q: 5.921738624572754
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.256636963581838, variance: 69.54840850830078, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -72.7829818725586, std_q: 6.296712875366211

steps: 174975, episodes: 7000, mean episode reward: -319.31690182966764, agent episode reward: [-106.43896727655587, -106.43896727655587, -106.43896727655587], time: 70.898
steps: 174975, episodes: 7000, mean episode variance: 30.147937568664553, agent episode variance: [0.0, 12.660834823608399, 17.487102745056152], time: 70.898
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.259380909258214, variance: 0.0, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -73.65145874023438, std_q: 5.987901210784912
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.256870306189441, variance: 50.643341064453125, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -73.38224029541016, std_q: 5.9020466804504395
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.2623069760261725, variance: 69.94841003417969, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -72.81581115722656, std_q: 6.33207893371582

steps: 199975, episodes: 8000, mean episode reward: -319.5677177785924, agent episode reward: [-106.52257259286412, -106.52257259286412, -106.52257259286412], time: 70.768
steps: 199975, episodes: 8000, mean episode variance: 29.939953441619874, agent episode variance: [0.0, 12.543869617462159, 17.396083824157714], time: 70.768
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.262244938172848, variance: 0.0, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -73.67059326171875, std_q: 5.987948894500732
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.259377992958278, variance: 50.17547607421875, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -73.39491271972656, std_q: 5.907172679901123
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.263125674519537, variance: 69.5843276977539, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -72.80281066894531, std_q: 6.32218074798584

steps: 224975, episodes: 9000, mean episode reward: -320.220065093761, agent episode reward: [-106.74002169792034, -106.74002169792034, -106.74002169792034], time: 76.283
steps: 224975, episodes: 9000, mean episode variance: 30.38948483276367, agent episode variance: [0.0, 12.753879093170166, 17.635605739593505], time: 76.284
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.261090990422916, variance: 0.0, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -73.65548706054688, std_q: 5.993411064147949
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.262351411585537, variance: 51.0155143737793, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -73.39314270019531, std_q: 5.899266719818115
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.263371593504584, variance: 70.54241943359375, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -72.78471374511719, std_q: 6.324496746063232

steps: 249975, episodes: 10000, mean episode reward: -318.06433873981274, agent episode reward: [-106.02144624660421, -106.02144624660421, -106.02144624660421], time: 70.121
steps: 249975, episodes: 10000, mean episode variance: 30.405984622955323, agent episode variance: [0.0, 12.87653509902954, 17.52944952392578], time: 70.122
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.262217216136411, variance: 0.0, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -73.6618423461914, std_q: 6.015273571014404
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.264287992940689, variance: 51.506141662597656, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -73.40132904052734, std_q: 5.917271137237549
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.2607265264395915, variance: 70.1177978515625, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -72.78192138671875, std_q: 6.302892684936523

steps: 274975, episodes: 11000, mean episode reward: -319.2596651983526, agent episode reward: [-106.41988839945087, -106.41988839945087, -106.41988839945087], time: 68.186
steps: 274975, episodes: 11000, mean episode variance: 30.343377628326415, agent episode variance: [0.0, 12.779196830749513, 17.564180797576903], time: 68.186
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.263963021097812, variance: 0.0, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -73.67633819580078, std_q: 5.99867582321167
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.265851287215966, variance: 51.11678695678711, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -73.42862701416016, std_q: 5.930794715881348
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.264672270306341, variance: 70.25672149658203, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -72.7984390258789, std_q: 6.345172882080078

steps: 299975, episodes: 12000, mean episode reward: -316.9824129113707, agent episode reward: [-105.66080430379024, -105.66080430379024, -105.66080430379024], time: 67.444
steps: 299975, episodes: 12000, mean episode variance: 30.054225440979003, agent episode variance: [0.0, 12.629714965820312, 17.42451047515869], time: 67.444
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.25915039865387, variance: 0.0, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -73.66105651855469, std_q: 6.009965419769287
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.256503850006096, variance: 50.51885986328125, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -73.38066864013672, std_q: 5.916023254394531
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.25854887019793, variance: 69.69804382324219, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -72.76995849609375, std_q: 6.310769557952881

steps: 324975, episodes: 13000, mean episode reward: -321.07837955243554, agent episode reward: [-107.02612651747849, -107.02612651747849, -107.02612651747849], time: 71.063
steps: 324975, episodes: 13000, mean episode variance: 30.104166374206542, agent episode variance: [0.0, 12.720994842529297, 17.383171531677245], time: 71.064
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.2570951931493015, variance: 0.0, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -73.65192413330078, std_q: 5.985559463500977
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.261492605723182, variance: 50.88397979736328, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -73.40848541259766, std_q: 5.929345607757568
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.257836396104396, variance: 69.53268432617188, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -72.7704086303711, std_q: 6.30106258392334

steps: 349975, episodes: 14000, mean episode reward: -318.0222344755558, agent episode reward: [-106.00741149185194, -106.00741149185194, -106.00741149185194], time: 75.167
steps: 349975, episodes: 14000, mean episode variance: 30.309489482879638, agent episode variance: [0.0, 12.84828296661377, 17.461206516265868], time: 75.167
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.259593077640661, variance: 0.0, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -73.65615844726562, std_q: 5.997055530548096
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.258796325094856, variance: 51.393131256103516, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -73.39739990234375, std_q: 5.908138751983643
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.261333722388556, variance: 69.8448257446289, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -72.7890625, std_q: 6.341200828552246

steps: 374975, episodes: 15000, mean episode reward: -317.3377630497224, agent episode reward: [-105.77925434990748, -105.77925434990748, -105.77925434990748], time: 71.666
steps: 374975, episodes: 15000, mean episode variance: 30.132059944152832, agent episode variance: [0.0, 12.827568367004394, 17.304491577148436], time: 71.667
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.256685238100923, variance: 0.0, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -73.65779876708984, std_q: 5.991377830505371
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.258698514707035, variance: 51.31027603149414, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -73.40287780761719, std_q: 5.920256614685059
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.253161833745884, variance: 69.21797180175781, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -72.76960754394531, std_q: 6.3083624839782715

steps: 399975, episodes: 16000, mean episode reward: -319.1562382878997, agent episode reward: [-106.38541276263324, -106.38541276263324, -106.38541276263324], time: 71.143
steps: 399975, episodes: 16000, mean episode variance: 30.059866729736328, agent episode variance: [0.0, 12.76521437072754, 17.29465235900879], time: 71.143
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.256175635712294, variance: 0.0, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -73.64915466308594, std_q: 5.9632368087768555
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.255626920069823, variance: 51.060855865478516, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -73.38287353515625, std_q: 5.895417213439941
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.26207087557197, variance: 69.1786117553711, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -72.8057632446289, std_q: 6.3326640129089355

steps: 424975, episodes: 17000, mean episode reward: -319.07775427241864, agent episode reward: [-106.35925142413954, -106.35925142413954, -106.35925142413954], time: 71.332
steps: 424975, episodes: 17000, mean episode variance: 30.19456368255615, agent episode variance: [0.0, 12.81525940322876, 17.379304279327393], time: 71.332
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.2593026108484455, variance: 0.0, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -73.67105102539062, std_q: 5.9659953117370605
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.263240357973229, variance: 51.26103973388672, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -73.42500305175781, std_q: 5.944659233093262
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.259376493479647, variance: 69.51721954345703, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -72.7793197631836, std_q: 6.284703254699707

steps: 449975, episodes: 18000, mean episode reward: -318.12385778310835, agent episode reward: [-106.04128592770277, -106.04128592770277, -106.04128592770277], time: 72.967
steps: 449975, episodes: 18000, mean episode variance: 30.14620528793335, agent episode variance: [0.0, 12.829880432128906, 17.316324855804442], time: 72.968
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.258039776860531, variance: 0.0, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -73.67840576171875, std_q: 5.984561443328857
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.258422516244018, variance: 51.319522857666016, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -73.39765930175781, std_q: 5.900477886199951
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.257959330475423, variance: 69.26529693603516, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -72.79639434814453, std_q: 6.321021556854248

steps: 474975, episodes: 19000, mean episode reward: -318.82934966533657, agent episode reward: [-106.2764498884455, -106.2764498884455, -106.2764498884455], time: 76.713
steps: 474975, episodes: 19000, mean episode variance: 30.203829357147217, agent episode variance: [0.0, 12.867149394989013, 17.336679962158204], time: 76.713
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.253049128755104, variance: 0.0, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -73.64822387695312, std_q: 5.965365409851074
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.254428792779878, variance: 51.46859359741211, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -73.37091827392578, std_q: 5.891327381134033
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.255016549649354, variance: 69.34671783447266, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -72.76787567138672, std_q: 6.271309852600098

Traceback (most recent call last):
  File "train.py", line 493, in <module>
    train(arglist)
  File "train.py", line 216, in train
    obs_n = env.reset()
  File "../../multiagent-particle-envs/multiagent/environment.py", line 108, in reset
    self.reset_callback(self.world)
  File "../../multiagent-particle-envs/multiagent/scenarios/simple_spread.py", line 66, in reset_world
    save([world.agents, world.landmarks], 'ss_state')
  File "../../multiagent-particle-envs/multiagent/scenarios/simple_spread.py", line 10, in save
    pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)
KeyboardInterrupt
