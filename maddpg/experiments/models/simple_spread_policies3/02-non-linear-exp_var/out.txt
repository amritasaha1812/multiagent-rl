# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 23.72 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies3/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies3/02-non-linear-exp_var/
Job <1090533> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc245>>
arglist.u_estimation True
2019-09-06 04:44:14.364139: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -524.368342625494, agent episode reward: [-174.78944754183135, -174.78944754183135, -174.78944754183135], time: 44.872
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 44.873
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -732.8731879248666, agent episode reward: [-244.2910626416222, -244.2910626416222, -244.2910626416222], time: 81.962
steps: 49975, episodes: 2000, mean episode variance: 7.147195772856474, agent episode variance: [3.0852863734662535, 2.852750070691109, 1.209159328699112], time: 81.962
Running avgs for agent 0: q_loss: 149.74655151367188, p_loss: -5.394909381866455, mean_rew: -7.748382430902895, variance: 12.64461612701416, lamda: 1.0095816850662231
Running avgs for agent 1: q_loss: 75.78093719482422, p_loss: -5.482938289642334, mean_rew: -7.7410103612851024, variance: 11.691598650373397, lamda: 1.0094791650772095
Running avgs for agent 2: q_loss: 765.05078125, p_loss: 12.537312507629395, mean_rew: -7.74217362488233, variance: 4.955571019258655, lamda: 1.010532259941101

steps: 74975, episodes: 3000, mean episode reward: -845.5054622027773, agent episode reward: [-281.8351540675924, -281.8351540675924, -281.8351540675924], time: 86.735
steps: 74975, episodes: 3000, mean episode variance: 8.649714048922062, agent episode variance: [2.18778297662735, 2.18634369969368, 4.2755873726010325], time: 86.736
Running avgs for agent 0: q_loss: 40.936798095703125, p_loss: -4.875482559204102, mean_rew: -9.011078163114238, variance: 8.75113296508789, lamda: 1.0331122875213623
Running avgs for agent 1: q_loss: 50.357635498046875, p_loss: -4.8058929443359375, mean_rew: -9.014659107661585, variance: 8.74537467956543, lamda: 1.0327852964401245
Running avgs for agent 2: q_loss: 11340.7548828125, p_loss: 32.4288215637207, mean_rew: -9.006923480315553, variance: 17.10234949040413, lamda: 1.0351879596710205

steps: 99975, episodes: 4000, mean episode reward: -671.175324142555, agent episode reward: [-223.72510804751835, -223.72510804751835, -223.72510804751835], time: 84.19
steps: 99975, episodes: 4000, mean episode variance: 28.17996365427971, agent episode variance: [2.225893246650696, 2.2348020403385163, 23.719268367290496], time: 84.191
Running avgs for agent 0: q_loss: 53.72909164428711, p_loss: -4.819972515106201, mean_rew: -9.356504155722389, variance: 8.903573036193848, lamda: 1.0564674139022827
Running avgs for agent 1: q_loss: 49.33784103393555, p_loss: -4.735745906829834, mean_rew: -9.35494807343879, variance: 8.939208030700684, lamda: 1.0578237771987915
Running avgs for agent 2: q_loss: 143686.109375, p_loss: 52.810665130615234, mean_rew: -9.362350965347407, variance: 94.87707346916199, lamda: 1.0601922273635864

steps: 124975, episodes: 5000, mean episode reward: -648.5799525471557, agent episode reward: [-216.19331751571855, -216.19331751571855, -216.19331751571855], time: 84.2
steps: 124975, episodes: 5000, mean episode variance: 25.992944343566894, agent episode variance: [2.165989292144775, 2.1431914777755736, 21.683763573646544], time: 84.2
Running avgs for agent 0: q_loss: 35.973960876464844, p_loss: -4.7926435470581055, mean_rew: -9.203073390969887, variance: 8.663956642150879, lamda: 1.079239010810852
Running avgs for agent 1: q_loss: 46.27653503417969, p_loss: -4.732954025268555, mean_rew: -9.193951701076712, variance: 8.572765350341797, lamda: 1.0822408199310303
Running avgs for agent 2: q_loss: 135976.109375, p_loss: 70.03600311279297, mean_rew: -9.190583368040294, variance: 86.73505429458618, lamda: 1.0851962566375732

steps: 149975, episodes: 6000, mean episode reward: -572.9089017947555, agent episode reward: [-190.96963393158518, -190.96963393158518, -190.96963393158518], time: 84.32
steps: 149975, episodes: 6000, mean episode variance: 41.41159780788421, agent episode variance: [2.0933191432952882, 2.067086946487427, 37.2511917181015], time: 84.32
Running avgs for agent 0: q_loss: 30.71126365661621, p_loss: -4.697714805603027, mean_rew: -9.01845242265168, variance: 8.37327766418457, lamda: 1.1025532484054565
Running avgs for agent 1: q_loss: 31.7045841217041, p_loss: -4.635169982910156, mean_rew: -9.015218505812836, variance: 8.26834774017334, lamda: 1.1037817001342773
Running avgs for agent 2: q_loss: 367169.09375, p_loss: 83.75824737548828, mean_rew: -9.000970798441095, variance: 149.004766872406, lamda: 1.1102005243301392

steps: 174975, episodes: 7000, mean episode reward: -573.2094784107874, agent episode reward: [-191.0698261369291, -191.0698261369291, -191.0698261369291], time: 84.22
steps: 174975, episodes: 7000, mean episode variance: 40.99818152523041, agent episode variance: [1.9797206568717955, 2.0059487261772158, 37.01251214218139], time: 84.221
Running avgs for agent 0: q_loss: 27.380647659301758, p_loss: -4.560462951660156, mean_rew: -8.789263241936705, variance: 7.918882846832275, lamda: 1.1258960962295532
Running avgs for agent 1: q_loss: 35.121891021728516, p_loss: -4.529233932495117, mean_rew: -8.793172249224765, variance: 8.023795127868652, lamda: 1.121377944946289
Running avgs for agent 2: q_loss: 470304.09375, p_loss: 95.73419952392578, mean_rew: -8.794440899213049, variance: 148.05004856872557, lamda: 1.1352046728134155

steps: 199975, episodes: 8000, mean episode reward: -567.885537884164, agent episode reward: [-189.29517929472138, -189.29517929472138, -189.29517929472138], time: 84.951
steps: 199975, episodes: 8000, mean episode variance: 40.85644927954674, agent episode variance: [1.9006681232452391, 1.9057788865566254, 37.05000226974487], time: 84.951
Running avgs for agent 0: q_loss: 32.53103256225586, p_loss: -4.479398727416992, mean_rew: -8.641377493601269, variance: 7.602672576904297, lamda: 1.148080587387085
Running avgs for agent 1: q_loss: 36.389739990234375, p_loss: -4.466609477996826, mean_rew: -8.650275578017615, variance: 7.623115062713623, lamda: 1.1396911144256592
Running avgs for agent 2: q_loss: 405276.34375, p_loss: 105.96084594726562, mean_rew: -8.640069828624442, variance: 148.2000090789795, lamda: 1.1602087020874023

steps: 224975, episodes: 9000, mean episode reward: -538.2462681813713, agent episode reward: [-179.41542272712377, -179.41542272712377, -179.41542272712377], time: 81.278
steps: 224975, episodes: 9000, mean episode variance: 41.16837256240845, agent episode variance: [1.8426905102729798, 1.8469554286003114, 37.47872662353516], time: 81.278
Running avgs for agent 0: q_loss: 24.513954162597656, p_loss: -4.359699249267578, mean_rew: -8.489731820293551, variance: 7.370762348175049, lamda: 1.1688591241836548
Running avgs for agent 1: q_loss: 24.691593170166016, p_loss: -4.365691184997559, mean_rew: -8.494192164646087, variance: 7.387821674346924, lamda: 1.1551837921142578
Running avgs for agent 2: q_loss: 413357.0625, p_loss: 114.76762390136719, mean_rew: -8.498359655412953, variance: 149.91490649414064, lamda: 1.1852128505706787

steps: 249975, episodes: 10000, mean episode reward: -513.6474373737689, agent episode reward: [-171.21581245792296, -171.21581245792296, -171.21581245792296], time: 82.666
steps: 249975, episodes: 10000, mean episode variance: 38.176592084884646, agent episode variance: [1.7981670522689819, 1.8047360677719115, 34.57368896484375], time: 82.666
Running avgs for agent 0: q_loss: 25.995361328125, p_loss: -4.2755818367004395, mean_rew: -8.345624135673058, variance: 7.1926679611206055, lamda: 1.1931499242782593
Running avgs for agent 1: q_loss: 20.272703170776367, p_loss: -4.2775750160217285, mean_rew: -8.338226325087064, variance: 7.218944549560547, lamda: 1.1708788871765137
Running avgs for agent 2: q_loss: 525551.0, p_loss: 121.73489379882812, mean_rew: -8.333657451214837, variance: 138.294755859375, lamda: 1.2102171182632446

steps: 274975, episodes: 11000, mean episode reward: -513.1146610805397, agent episode reward: [-171.03822036017988, -171.03822036017988, -171.03822036017988], time: 82.092
steps: 274975, episodes: 11000, mean episode variance: 43.6375953643322, agent episode variance: [1.729599238872528, 1.7431779415607453, 40.16481818389892], time: 82.092
Running avgs for agent 0: q_loss: 22.584869384765625, p_loss: -4.219083309173584, mean_rew: -8.201043959548599, variance: 6.918396472930908, lamda: 1.2171993255615234
Running avgs for agent 1: q_loss: 19.392242431640625, p_loss: -4.199613094329834, mean_rew: -8.190258915903637, variance: 6.97271203994751, lamda: 1.1905266046524048
Running avgs for agent 2: q_loss: 556317.5625, p_loss: 127.0849380493164, mean_rew: -8.193839050395962, variance: 160.6592727355957, lamda: 1.2352211475372314

steps: 299975, episodes: 12000, mean episode reward: -497.900961263386, agent episode reward: [-165.96698708779533, -165.96698708779533, -165.96698708779533], time: 82.698
steps: 299975, episodes: 12000, mean episode variance: 44.502360572338105, agent episode variance: [1.6551866049766542, 1.6893040990829469, 41.1578698682785], time: 82.698
Running avgs for agent 0: q_loss: 20.650259017944336, p_loss: -4.134955883026123, mean_rew: -8.063147500739674, variance: 6.62074613571167, lamda: 1.240532398223877
Running avgs for agent 1: q_loss: 18.479494094848633, p_loss: -4.164149284362793, mean_rew: -8.069769418647107, variance: 6.757216453552246, lamda: 1.212723731994629
Running avgs for agent 2: q_loss: 612870.5, p_loss: 131.24929809570312, mean_rew: -8.071163408706221, variance: 164.631479473114, lamda: 1.2602252960205078

steps: 324975, episodes: 13000, mean episode reward: -497.43386958727945, agent episode reward: [-165.8112898624265, -165.8112898624265, -165.8112898624265], time: 84.961
steps: 324975, episodes: 13000, mean episode variance: 80.18651584935188, agent episode variance: [1.6054728577136994, 1.6298394241333007, 76.95120356750488], time: 84.962
Running avgs for agent 0: q_loss: 21.269508361816406, p_loss: -4.067455768585205, mean_rew: -7.955669132765659, variance: 6.421891689300537, lamda: 1.2620351314544678
Running avgs for agent 1: q_loss: 20.944137573242188, p_loss: -4.089286804199219, mean_rew: -7.95709207643052, variance: 6.519357204437256, lamda: 1.2328636646270752
Running avgs for agent 2: q_loss: 1423037.125, p_loss: 133.8876495361328, mean_rew: -7.951675319219436, variance: 307.8048142700195, lamda: 1.2852294445037842

steps: 349975, episodes: 14000, mean episode reward: -496.855319559169, agent episode reward: [-165.61843985305634, -165.61843985305634, -165.61843985305634], time: 83.954
steps: 349975, episodes: 14000, mean episode variance: 81.28017281126976, agent episode variance: [1.559182363986969, 1.602557341814041, 78.11843310546875], time: 83.954
Running avgs for agent 0: q_loss: 22.757125854492188, p_loss: -4.031116485595703, mean_rew: -7.848185717773331, variance: 6.236729621887207, lamda: 1.270484209060669
Running avgs for agent 1: q_loss: 17.26811408996582, p_loss: -4.027313709259033, mean_rew: -7.851182026594723, variance: 6.410229682922363, lamda: 1.2553659677505493
Running avgs for agent 2: q_loss: 1463115.125, p_loss: 135.69232177734375, mean_rew: -7.856948531959801, variance: 312.473732421875, lamda: 1.3102335929870605

steps: 374975, episodes: 15000, mean episode reward: -498.11004375991973, agent episode reward: [-166.03668125330657, -166.03668125330657, -166.03668125330657], time: 84.522
steps: 374975, episodes: 15000, mean episode variance: 79.20745247793198, agent episode variance: [1.5459842925071716, 1.5463737945556641, 76.11509439086915], time: 84.522
Running avgs for agent 0: q_loss: 18.408336639404297, p_loss: -3.992743730545044, mean_rew: -7.7723468633629, variance: 6.1839375495910645, lamda: 1.2797220945358276
Running avgs for agent 1: q_loss: 16.383506774902344, p_loss: -3.9554364681243896, mean_rew: -7.7645848858831545, variance: 6.185494899749756, lamda: 1.2784414291381836
Running avgs for agent 2: q_loss: 1490699.375, p_loss: 136.3977508544922, mean_rew: -7.762079962198819, variance: 304.4603775634766, lamda: 1.335237741470337

steps: 399975, episodes: 16000, mean episode reward: -494.6596817361198, agent episode reward: [-164.88656057870662, -164.88656057870662, -164.88656057870662], time: 87.186
steps: 399975, episodes: 16000, mean episode variance: 74.34378381967545, agent episode variance: [1.5070404009819032, 1.5079771757125855, 71.32876624298096], time: 87.186
Running avgs for agent 0: q_loss: 16.218917846679688, p_loss: -3.959368944168091, mean_rew: -7.686755695159376, variance: 6.028162002563477, lamda: 1.300816297531128
Running avgs for agent 1: q_loss: 15.878151893615723, p_loss: -3.9520442485809326, mean_rew: -7.704004762506724, variance: 6.0319085121154785, lamda: 1.3019187450408936
Running avgs for agent 2: q_loss: 1546620.25, p_loss: 135.1392822265625, mean_rew: -7.693840933130287, variance: 285.31506497192385, lamda: 1.3602418899536133

steps: 424975, episodes: 17000, mean episode reward: -486.8256616907267, agent episode reward: [-162.27522056357557, -162.27522056357557, -162.27522056357557], time: 84.554
steps: 424975, episodes: 17000, mean episode variance: 70.11046999144554, agent episode variance: [1.4715915055274964, 1.4860559380054474, 67.15282254791259], time: 84.554
Running avgs for agent 0: q_loss: 14.882803916931152, p_loss: -3.915872573852539, mean_rew: -7.624814015648837, variance: 5.886366367340088, lamda: 1.3222697973251343
Running avgs for agent 1: q_loss: 18.24225425720215, p_loss: -3.9048922061920166, mean_rew: -7.631895931040932, variance: 5.944223403930664, lamda: 1.3193366527557373
Running avgs for agent 2: q_loss: 1524086.75, p_loss: 133.5684814453125, mean_rew: -7.6188176000460714, variance: 268.61129019165037, lamda: 1.3852460384368896

steps: 449975, episodes: 18000, mean episode reward: -480.3861569428331, agent episode reward: [-160.12871898094437, -160.12871898094437, -160.12871898094437], time: 108.571
steps: 449975, episodes: 18000, mean episode variance: 69.21230145716667, agent episode variance: [1.4296985015869141, 1.4414020736217499, 66.341200881958], time: 108.571
Running avgs for agent 0: q_loss: 17.090654373168945, p_loss: -3.86407470703125, mean_rew: -7.5524285996051255, variance: 5.718793869018555, lamda: 1.3436872959136963
Running avgs for agent 1: q_loss: 14.315649032592773, p_loss: -3.8788392543792725, mean_rew: -7.564974258598573, variance: 5.765608310699463, lamda: 1.3380589485168457
Running avgs for agent 2: q_loss: 1477335.25, p_loss: 132.44618225097656, mean_rew: -7.559946704248005, variance: 265.364803527832, lamda: 1.410250186920166

steps: 474975, episodes: 19000, mean episode reward: -485.1331113259214, agent episode reward: [-161.71103710864045, -161.71103710864045, -161.71103710864045], time: 119.092
steps: 474975, episodes: 19000, mean episode variance: 67.48548576784134, agent episode variance: [1.4060397617816924, 1.4093395912647246, 64.67010641479492], time: 119.093
Running avgs for agent 0: q_loss: 16.23963737487793, p_loss: -3.8569607734680176, mean_rew: -7.497418200287651, variance: 5.624159336090088, lamda: 1.360794186592102
Running avgs for agent 1: q_loss: 14.239806175231934, p_loss: -3.835683822631836, mean_rew: -7.495825967036186, variance: 5.63735818862915, lamda: 1.3579797744750977
Running avgs for agent 2: q_loss: 1442154.0, p_loss: 131.2097930908203, mean_rew: -7.50804313846279, variance: 258.68042565917966, lamda: 1.4352545738220215

steps: 499975, episodes: 20000, mean episode reward: -486.56820059649505, agent episode reward: [-162.1894001988317, -162.1894001988317, -162.1894001988317], time: 115.29
steps: 499975, episodes: 20000, mean episode variance: 67.13459445118905, agent episode variance: [1.3823270523548126, 1.375962474822998, 64.37630492401124], time: 115.291
Running avgs for agent 0: q_loss: 17.106233596801758, p_loss: -3.8335952758789062, mean_rew: -7.453299617713327, variance: 5.529308795928955, lamda: 1.3762749433517456
Running avgs for agent 1: q_loss: 13.048663139343262, p_loss: -3.8078644275665283, mean_rew: -7.441656598214846, variance: 5.503849983215332, lamda: 1.3763034343719482
Running avgs for agent 2: q_loss: 1441469.125, p_loss: 129.89749145507812, mean_rew: -7.4450158298719655, variance: 257.50521969604495, lamda: 1.4602586030960083

steps: 524975, episodes: 21000, mean episode reward: -483.91423635894745, agent episode reward: [-161.3047454529825, -161.3047454529825, -161.3047454529825], time: 114.199
steps: 524975, episodes: 21000, mean episode variance: 63.905014219284055, agent episode variance: [1.3468226120471953, 1.3520395991802217, 61.20615200805664], time: 114.199
Running avgs for agent 0: q_loss: 14.847131729125977, p_loss: -3.792111873626709, mean_rew: -7.3965026106264835, variance: 5.387290000915527, lamda: 1.395194172859192
Running avgs for agent 1: q_loss: 13.39587688446045, p_loss: -3.7898125648498535, mean_rew: -7.412559389360653, variance: 5.408158302307129, lamda: 1.39492666721344
Running avgs for agent 2: q_loss: 1460228.875, p_loss: 129.45570373535156, mean_rew: -7.3987743223944396, variance: 244.82460803222656, lamda: 1.4852627515792847

steps: 549975, episodes: 22000, mean episode reward: -494.2965812944343, agent episode reward: [-164.76552709814476, -164.76552709814476, -164.76552709814476], time: 114.006
steps: 549975, episodes: 22000, mean episode variance: 63.195895790100096, agent episode variance: [1.3234727683067322, 1.3412657952308655, 60.5311572265625], time: 114.007
Running avgs for agent 0: q_loss: 13.778331756591797, p_loss: -3.7630209922790527, mean_rew: -7.363598891053589, variance: 5.293890953063965, lamda: 1.4173225164413452
Running avgs for agent 1: q_loss: 15.154886245727539, p_loss: -3.7708022594451904, mean_rew: -7.358499582937522, variance: 5.365063667297363, lamda: 1.4095237255096436
Running avgs for agent 2: q_loss: 1427717.5, p_loss: 129.2513427734375, mean_rew: -7.361132036089993, variance: 242.12462890625, lamda: 1.510266900062561

steps: 574975, episodes: 23000, mean episode reward: -491.33807001290984, agent episode reward: [-163.77935667096995, -163.77935667096995, -163.77935667096995], time: 159.132
steps: 574975, episodes: 23000, mean episode variance: 62.24890628671646, agent episode variance: [1.3099355635643006, 1.311851460456848, 59.62711926269531], time: 159.132
Running avgs for agent 0: q_loss: 15.482258796691895, p_loss: -3.7377095222473145, mean_rew: -7.326425455777858, variance: 5.239742279052734, lamda: 1.439799189567566
Running avgs for agent 1: q_loss: 14.130361557006836, p_loss: -3.753540515899658, mean_rew: -7.32588555866087, variance: 5.247405529022217, lamda: 1.4222139120101929
Running avgs for agent 2: q_loss: 1429711.875, p_loss: 129.3201904296875, mean_rew: -7.318697183591251, variance: 238.50847705078124, lamda: 1.5352710485458374

steps: 599975, episodes: 24000, mean episode reward: -497.71034706904857, agent episode reward: [-165.9034490230162, -165.9034490230162, -165.9034490230162], time: 197.932
steps: 599975, episodes: 24000, mean episode variance: 63.84853339099884, agent episode variance: [1.2756459217071534, 1.299493472099304, 61.273393997192386], time: 197.933
Running avgs for agent 0: q_loss: 13.941296577453613, p_loss: -3.7194912433624268, mean_rew: -7.288277774445934, variance: 5.102582931518555, lamda: 1.4578242301940918
Running avgs for agent 1: q_loss: 10.53531265258789, p_loss: -3.732548475265503, mean_rew: -7.289947081227143, variance: 5.197973728179932, lamda: 1.4364535808563232
Running avgs for agent 2: q_loss: 1475653.375, p_loss: 129.3776397705078, mean_rew: -7.304278035464382, variance: 245.09357598876954, lamda: 1.5602751970291138

steps: 624975, episodes: 25000, mean episode reward: -496.64602641332135, agent episode reward: [-165.54867547110712, -165.54867547110712, -165.54867547110712], time: 192.702
steps: 624975, episodes: 25000, mean episode variance: 61.61931024193764, agent episode variance: [1.2646339128017425, 1.2775728516578675, 59.077103477478026], time: 192.702
Running avgs for agent 0: q_loss: 14.104819297790527, p_loss: -3.719792366027832, mean_rew: -7.271734420374709, variance: 5.058535099029541, lamda: 1.4793437719345093
Running avgs for agent 1: q_loss: 11.236422538757324, p_loss: -3.725126028060913, mean_rew: -7.274238506404069, variance: 5.110291004180908, lamda: 1.4566329717636108
Running avgs for agent 2: q_loss: 1443475.375, p_loss: 129.49766540527344, mean_rew: -7.2652456200596385, variance: 236.3084139099121, lamda: 1.5852793455123901

steps: 649975, episodes: 26000, mean episode reward: -497.36657809810697, agent episode reward: [-165.78885936603564, -165.78885936603564, -165.78885936603564], time: 191.729
steps: 649975, episodes: 26000, mean episode variance: 59.90519286799431, agent episode variance: [1.233688495874405, 1.261133614063263, 57.410370758056644], time: 191.729
Running avgs for agent 0: q_loss: 12.198022842407227, p_loss: -3.7150492668151855, mean_rew: -7.249331002860161, variance: 4.934753894805908, lamda: 1.4995511770248413
Running avgs for agent 1: q_loss: 10.088201522827148, p_loss: -3.70595121383667, mean_rew: -7.2505963566155645, variance: 5.044534683227539, lamda: 1.4741755723953247
Running avgs for agent 2: q_loss: 1463079.5, p_loss: 130.25408935546875, mean_rew: -7.241632203603137, variance: 229.64148303222657, lamda: 1.6102834939956665

steps: 674975, episodes: 27000, mean episode reward: -498.31408645944583, agent episode reward: [-166.10469548648194, -166.10469548648194, -166.10469548648194], time: 191.343
steps: 674975, episodes: 27000, mean episode variance: 57.18017981743812, agent episode variance: [1.2251478915214538, 1.236169772386551, 54.71886215353012], time: 191.344
Running avgs for agent 0: q_loss: 14.834442138671875, p_loss: -3.686908483505249, mean_rew: -7.211180084664685, variance: 4.900591850280762, lamda: 1.521429181098938
Running avgs for agent 1: q_loss: 11.344905853271484, p_loss: -3.694430351257324, mean_rew: -7.216341756347782, variance: 4.944678783416748, lamda: 1.4907125234603882
Running avgs for agent 2: q_loss: 1427235.625, p_loss: 130.97227478027344, mean_rew: -7.2233320393113205, variance: 218.87544861412047, lamda: 1.6352876424789429

steps: 699975, episodes: 28000, mean episode reward: -498.876225356095, agent episode reward: [-166.29207511869836, -166.29207511869836, -166.29207511869836], time: 187.365
steps: 699975, episodes: 28000, mean episode variance: 39.5942391409874, agent episode variance: [1.1916702494621276, 1.2255269603729249, 37.177041931152345], time: 187.365
Running avgs for agent 0: q_loss: 16.835132598876953, p_loss: -3.6868717670440674, mean_rew: -7.197809379461072, variance: 4.76668119430542, lamda: 1.5346975326538086
Running avgs for agent 1: q_loss: 11.152548789978027, p_loss: -3.6920809745788574, mean_rew: -7.200926578610582, variance: 4.9021077156066895, lamda: 1.5021116733551025
Running avgs for agent 2: q_loss: 1099585.25, p_loss: 131.75619506835938, mean_rew: -7.199587513764898, variance: 148.70816772460938, lamda: 1.6602917909622192

steps: 724975, episodes: 29000, mean episode reward: -496.7991454376234, agent episode reward: [-165.59971514587446, -165.59971514587446, -165.59971514587446], time: 192.674
steps: 724975, episodes: 29000, mean episode variance: 32.730568891048435, agent episode variance: [1.1768925623893738, 1.2099845333099366, 30.34369179534912], time: 192.674
Running avgs for agent 0: q_loss: 14.524507522583008, p_loss: -3.667818307876587, mean_rew: -7.178640579715672, variance: 4.7075700759887695, lamda: 1.5506477355957031
Running avgs for agent 1: q_loss: 10.328892707824707, p_loss: -3.685105562210083, mean_rew: -7.17472707429945, variance: 4.839938640594482, lamda: 1.518211007118225
Running avgs for agent 2: q_loss: 742318.3125, p_loss: 132.31752014160156, mean_rew: -7.176717282417178, variance: 121.37476718139648, lamda: 1.6852959394454956

steps: 749975, episodes: 30000, mean episode reward: -493.49148742289003, agent episode reward: [-164.4971624742967, -164.4971624742967, -164.4971624742967], time: 190.908
steps: 749975, episodes: 30000, mean episode variance: 49.190162160396575, agent episode variance: [1.1689327776432037, 1.199671954393387, 46.82155742835999], time: 190.909
Running avgs for agent 0: q_loss: 12.763710975646973, p_loss: -3.6641595363616943, mean_rew: -7.156156420095521, variance: 4.675731182098389, lamda: 1.5685416460037231
Running avgs for agent 1: q_loss: 9.871891021728516, p_loss: -3.674039125442505, mean_rew: -7.168027140223956, variance: 4.79868745803833, lamda: 1.5302766561508179
Running avgs for agent 2: q_loss: 1273754.625, p_loss: 132.82574462890625, mean_rew: -7.162714048638214, variance: 187.28622971343995, lamda: 1.710300087928772

steps: 774975, episodes: 31000, mean episode reward: -501.8026920869012, agent episode reward: [-167.26756402896706, -167.26756402896706, -167.26756402896706], time: 184.743
steps: 774975, episodes: 31000, mean episode variance: 59.47021934056282, agent episode variance: [1.1589003925323487, 1.1821416714191437, 57.12917727661133], time: 184.743
Running avgs for agent 0: q_loss: 13.993513107299805, p_loss: -3.6640419960021973, mean_rew: -7.141943129653553, variance: 4.63560152053833, lamda: 1.5870493650436401
Running avgs for agent 1: q_loss: 8.871511459350586, p_loss: -3.6643919944763184, mean_rew: -7.146562574130747, variance: 4.7285661697387695, lamda: 1.5459579229354858
Running avgs for agent 2: q_loss: 1475719.875, p_loss: 133.16189575195312, mean_rew: -7.143153051201614, variance: 228.51670910644532, lamda: 1.7353042364120483

steps: 799975, episodes: 32000, mean episode reward: -500.3093060863365, agent episode reward: [-166.7697686954455, -166.7697686954455, -166.7697686954455], time: 191.653
steps: 799975, episodes: 32000, mean episode variance: 54.272200129508974, agent episode variance: [1.1405701506137849, 1.1693712356090546, 51.962258743286135], time: 191.654
Running avgs for agent 0: q_loss: 11.674400329589844, p_loss: -3.650747060775757, mean_rew: -7.138710477265223, variance: 4.562280178070068, lamda: 1.6058698892593384
Running avgs for agent 1: q_loss: 9.029898643493652, p_loss: -3.6551973819732666, mean_rew: -7.12421298331112, variance: 4.67748498916626, lamda: 1.5642536878585815
Running avgs for agent 2: q_loss: 1386451.75, p_loss: 133.78282165527344, mean_rew: -7.133921068381087, variance: 207.84903497314454, lamda: 1.7603083848953247

steps: 824975, episodes: 33000, mean episode reward: -501.68338743613435, agent episode reward: [-167.22779581204477, -167.22779581204477, -167.22779581204477], time: 186.64
steps: 824975, episodes: 33000, mean episode variance: 59.30817516899109, agent episode variance: [1.1209327301979064, 1.160007956981659, 57.027234481811526], time: 186.641
Running avgs for agent 0: q_loss: 12.890027046203613, p_loss: -3.641780376434326, mean_rew: -7.119048976026737, variance: 4.483731269836426, lamda: 1.6257950067520142
Running avgs for agent 1: q_loss: 10.824775695800781, p_loss: -3.6504433155059814, mean_rew: -7.127252760899115, variance: 4.640031814575195, lamda: 1.5794485807418823
Running avgs for agent 2: q_loss: 1454962.875, p_loss: 134.05918884277344, mean_rew: -7.113110601695273, variance: 228.1089379272461, lamda: 1.785312533378601

steps: 849975, episodes: 34000, mean episode reward: -503.27031013230277, agent episode reward: [-167.75677004410088, -167.75677004410088, -167.75677004410088], time: 193.247
steps: 849975, episodes: 34000, mean episode variance: 58.62302979135513, agent episode variance: [1.1106433787345886, 1.1442200231552124, 56.36816638946533], time: 193.247
Running avgs for agent 0: q_loss: 10.733856201171875, p_loss: -3.6428143978118896, mean_rew: -7.103086283765531, variance: 4.442573547363281, lamda: 1.6375788450241089
Running avgs for agent 1: q_loss: 10.019091606140137, p_loss: -3.6435232162475586, mean_rew: -7.1003317678422295, variance: 4.576879978179932, lamda: 1.5906765460968018
Running avgs for agent 2: q_loss: 1458878.5, p_loss: 134.2869110107422, mean_rew: -7.100738898085918, variance: 225.47266555786132, lamda: 1.8103166818618774

steps: 874975, episodes: 35000, mean episode reward: -511.92448703810584, agent episode reward: [-170.6414956793686, -170.6414956793686, -170.6414956793686], time: 194.345
steps: 874975, episodes: 35000, mean episode variance: 58.06162146544457, agent episode variance: [1.1035669202804566, 1.1323881328105927, 55.82566641235351], time: 194.346
Running avgs for agent 0: q_loss: 10.140129089355469, p_loss: -3.627861261367798, mean_rew: -7.099039444707009, variance: 4.414267539978027, lamda: 1.6553822755813599
Running avgs for agent 1: q_loss: 9.437935829162598, p_loss: -3.640493154525757, mean_rew: -7.100492899250092, variance: 4.529552936553955, lamda: 1.6087063550949097
Running avgs for agent 2: q_loss: 1466949.375, p_loss: 134.1286163330078, mean_rew: -7.086640515824299, variance: 223.30266564941405, lamda: 1.8353208303451538

steps: 899975, episodes: 36000, mean episode reward: -511.57748229495047, agent episode reward: [-170.52582743165019, -170.52582743165019, -170.52582743165019], time: 195.694
steps: 899975, episodes: 36000, mean episode variance: 57.71766418409347, agent episode variance: [1.0953263623714447, 1.1193663556575775, 55.50297146606445], time: 195.695
Running avgs for agent 0: q_loss: 11.376962661743164, p_loss: -3.6199982166290283, mean_rew: -7.0898566665239215, variance: 4.381305694580078, lamda: 1.668915033340454
Running avgs for agent 1: q_loss: 9.052302360534668, p_loss: -3.643954038619995, mean_rew: -7.090323289649818, variance: 4.477465629577637, lamda: 1.626709222793579
Running avgs for agent 2: q_loss: 1458636.0, p_loss: 133.81472778320312, mean_rew: -7.084228139061588, variance: 222.0118858642578, lamda: 1.8603249788284302

steps: 924975, episodes: 37000, mean episode reward: -529.0789255608828, agent episode reward: [-176.35964185362758, -176.35964185362758, -176.35964185362758], time: 191.314
steps: 924975, episodes: 37000, mean episode variance: 56.3819561419487, agent episode variance: [1.075273246526718, 1.1061966435909272, 54.20048625183105], time: 191.314
Running avgs for agent 0: q_loss: 9.944664001464844, p_loss: -3.6363205909729004, mean_rew: -7.085358161771832, variance: 4.301093101501465, lamda: 1.6795532703399658
Running avgs for agent 1: q_loss: 10.536148071289062, p_loss: -3.6376311779022217, mean_rew: -7.094996209246498, variance: 4.424786567687988, lamda: 1.641210675239563
Running avgs for agent 2: q_loss: 1488269.0, p_loss: 133.91224670410156, mean_rew: -7.080214370940052, variance: 216.8019450073242, lamda: 1.8853291273117065

steps: 949975, episodes: 38000, mean episode reward: -527.2493576277083, agent episode reward: [-175.74978587590275, -175.74978587590275, -175.74978587590275], time: 185.263
steps: 949975, episodes: 38000, mean episode variance: 52.49752607703209, agent episode variance: [1.067754373073578, 1.0988372595310212, 50.33093444442749], time: 185.264
Running avgs for agent 0: q_loss: 11.47606086730957, p_loss: -3.629448175430298, mean_rew: -7.08532591338331, variance: 4.271017551422119, lamda: 1.693612813949585
Running avgs for agent 1: q_loss: 9.32070541381836, p_loss: -3.6366488933563232, mean_rew: -7.090653924390596, variance: 4.395349025726318, lamda: 1.6590147018432617
Running avgs for agent 2: q_loss: 1491917.0, p_loss: 134.30569458007812, mean_rew: -7.0822218198249995, variance: 201.32373777770997, lamda: 1.910333275794983

steps: 974975, episodes: 39000, mean episode reward: -525.3518428272137, agent episode reward: [-175.1172809424046, -175.1172809424046, -175.1172809424046], time: 186.731
steps: 974975, episodes: 39000, mean episode variance: 53.32157256937027, agent episode variance: [1.0662371466159821, 1.0820337908267974, 51.17330163192749], time: 186.732
Running avgs for agent 0: q_loss: 10.259886741638184, p_loss: -3.630876064300537, mean_rew: -7.08468990740633, variance: 4.26494836807251, lamda: 1.7004119157791138
Running avgs for agent 1: q_loss: 10.111681938171387, p_loss: -3.6364071369171143, mean_rew: -7.083152395726828, variance: 4.328134536743164, lamda: 1.6736394166946411
Running avgs for agent 2: q_loss: 1484568.375, p_loss: 134.3009033203125, mean_rew: -7.082445549181825, variance: 204.69320652770995, lamda: 1.9353374242782593

steps: 999975, episodes: 40000, mean episode reward: -529.6223165268302, agent episode reward: [-176.54077217561013, -176.54077217561013, -176.54077217561013], time: 185.648
steps: 999975, episodes: 40000, mean episode variance: 54.52170131897926, agent episode variance: [1.058621342420578, 1.0673813452720642, 52.39569863128662], time: 185.649
Running avgs for agent 0: q_loss: 11.11174201965332, p_loss: -3.6289374828338623, mean_rew: -7.078869942674198, variance: 4.234485149383545, lamda: 1.7156060934066772
Running avgs for agent 1: q_loss: 10.164582252502441, p_loss: -3.6368844509124756, mean_rew: -7.073758412874656, variance: 4.269525527954102, lamda: 1.6866869926452637
Running avgs for agent 2: q_loss: 1537107.25, p_loss: 134.32037353515625, mean_rew: -7.084081373387456, variance: 209.58279452514648, lamda: 1.9603415727615356

steps: 1024975, episodes: 41000, mean episode reward: -536.5805899224076, agent episode reward: [-178.86019664080254, -178.86019664080254, -178.86019664080254], time: 191.067
steps: 1024975, episodes: 41000, mean episode variance: 28.89617481660843, agent episode variance: [1.043478669166565, 1.068767940044403, 26.783928207397462], time: 191.067
Running avgs for agent 0: q_loss: 8.880316734313965, p_loss: -3.638272523880005, mean_rew: -7.08026084478357, variance: 4.173914909362793, lamda: 1.7307043075561523
Running avgs for agent 1: q_loss: 8.926445007324219, p_loss: -3.6239209175109863, mean_rew: -7.0820536765801405, variance: 4.275071620941162, lamda: 1.6983256340026855
Running avgs for agent 2: q_loss: 843504.625, p_loss: 134.62942504882812, mean_rew: -7.08157270018452, variance: 107.13571282958985, lamda: 1.985345721244812

steps: 1049975, episodes: 42000, mean episode reward: -532.5274680987802, agent episode reward: [-177.50915603292674, -177.50915603292674, -177.50915603292674], time: 192.579
steps: 1049975, episodes: 42000, mean episode variance: 29.80758712732792, agent episode variance: [1.0288225971460343, 1.049803957939148, 27.728960572242737], time: 192.579
Running avgs for agent 0: q_loss: 10.161578178405762, p_loss: -3.6188318729400635, mean_rew: -7.050122934584513, variance: 4.11529016494751, lamda: 1.7426809072494507
Running avgs for agent 1: q_loss: 8.929384231567383, p_loss: -3.6130142211914062, mean_rew: -7.049463768498094, variance: 4.199215888977051, lamda: 1.7141640186309814
Running avgs for agent 2: q_loss: 696104.1875, p_loss: 134.03834533691406, mean_rew: -7.043550808643956, variance: 110.91584228897095, lamda: 2.0103375911712646

steps: 1074975, episodes: 43000, mean episode reward: -545.9621235793022, agent episode reward: [-181.98737452643405, -181.98737452643405, -181.98737452643405], time: 191.256
steps: 1074975, episodes: 43000, mean episode variance: 31.147607349157333, agent episode variance: [1.0084354836940765, 1.0316550045013428, 29.107516860961915], time: 191.256
Running avgs for agent 0: q_loss: 8.190016746520996, p_loss: -3.5835824012756348, mean_rew: -6.962682352786284, variance: 4.0337419509887695, lamda: 1.7530068159103394
Running avgs for agent 1: q_loss: 6.862234115600586, p_loss: -3.5681047439575195, mean_rew: -6.965133363793754, variance: 4.126620292663574, lamda: 1.727796196937561
Running avgs for agent 2: q_loss: 768495.8125, p_loss: 132.6121826171875, mean_rew: -6.964201901563021, variance: 116.43006744384766, lamda: 2.0353119373321533

steps: 1099975, episodes: 44000, mean episode reward: -551.7979951575398, agent episode reward: [-183.9326650525133, -183.9326650525133, -183.9326650525133], time: 189.734
steps: 1099975, episodes: 44000, mean episode variance: 52.393711053848264, agent episode variance: [1.0023222403526306, 1.0149187016487122, 50.37647011184692], time: 189.734
Running avgs for agent 0: q_loss: 7.374151229858398, p_loss: -3.537327527999878, mean_rew: -6.886191104634067, variance: 4.009289264678955, lamda: 1.758663296699524
Running avgs for agent 1: q_loss: 5.8201446533203125, p_loss: -3.536536455154419, mean_rew: -6.887878866597925, variance: 4.0596747398376465, lamda: 1.740530252456665
Running avgs for agent 2: q_loss: 1231579.5, p_loss: 131.11712646484375, mean_rew: -6.891437984032371, variance: 201.50588044738768, lamda: 2.060286045074463

steps: 1124975, episodes: 45000, mean episode reward: -552.4732750324004, agent episode reward: [-184.15775834413344, -184.15775834413344, -184.15775834413344], time: 188.836
steps: 1124975, episodes: 45000, mean episode variance: 49.85896256518364, agent episode variance: [0.9872348275184631, 0.9985671999454498, 47.87316053771973], time: 188.837
Running avgs for agent 0: q_loss: 7.149867057800293, p_loss: -3.5321552753448486, mean_rew: -6.85829648952167, variance: 3.948939561843872, lamda: 1.7598979473114014
Running avgs for agent 1: q_loss: 5.648890018463135, p_loss: -3.5268564224243164, mean_rew: -6.857486738739221, variance: 3.9942688941955566, lamda: 1.7500594854354858
Running avgs for agent 2: q_loss: 1114809.0, p_loss: 130.65480041503906, mean_rew: -6.855859983923307, variance: 191.4926421508789, lamda: 2.0852606296539307

steps: 1149975, episodes: 46000, mean episode reward: -556.010847444941, agent episode reward: [-185.33694914831366, -185.33694914831366, -185.33694914831366], time: 191.507
steps: 1149975, episodes: 46000, mean episode variance: 47.668779603242875, agent episode variance: [0.9910649778842926, 0.9867054243087768, 45.6910092010498], time: 191.508
Running avgs for agent 0: q_loss: 6.343491077423096, p_loss: -3.510730028152466, mean_rew: -6.840777003759037, variance: 3.9642601013183594, lamda: 1.7598979473114014
Running avgs for agent 1: q_loss: 6.344118118286133, p_loss: -3.5125391483306885, mean_rew: -6.837963372495896, variance: 3.946821689605713, lamda: 1.7509877681732178
Running avgs for agent 2: q_loss: 1013427.1875, p_loss: 130.5222625732422, mean_rew: -6.837140512065495, variance: 182.7640368041992, lamda: 2.1102349758148193

steps: 1174975, episodes: 47000, mean episode reward: -563.476372844522, agent episode reward: [-187.82545761484067, -187.82545761484067, -187.82545761484067], time: 190.386
steps: 1174975, episodes: 47000, mean episode variance: 49.1954277677536, agent episode variance: [0.9853134610652924, 0.9886886322498322, 47.22142567443848], time: 190.386
Running avgs for agent 0: q_loss: 6.007053852081299, p_loss: -3.520096778869629, mean_rew: -6.837876843062736, variance: 3.941253662109375, lamda: 1.7601391077041626
Running avgs for agent 1: q_loss: 6.134282112121582, p_loss: -3.512070894241333, mean_rew: -6.833351249349355, variance: 3.954754590988159, lamda: 1.7509942054748535
Running avgs for agent 2: q_loss: 956792.4375, p_loss: 130.5070037841797, mean_rew: -6.8407286730308305, variance: 188.8857026977539, lamda: 2.135209321975708

steps: 1199975, episodes: 48000, mean episode reward: -565.6799330385173, agent episode reward: [-188.55997767950578, -188.55997767950578, -188.55997767950578], time: 189.452
steps: 1199975, episodes: 48000, mean episode variance: 46.3102795059681, agent episode variance: [0.9812967693805694, 0.9927386569976807, 44.33624407958985], time: 189.453
Running avgs for agent 0: q_loss: 5.81048583984375, p_loss: -3.514371633529663, mean_rew: -6.832019967935354, variance: 3.925187110900879, lamda: 1.7602460384368896
Running avgs for agent 1: q_loss: 5.971616268157959, p_loss: -3.50752592086792, mean_rew: -6.834299297730736, variance: 3.970954656600952, lamda: 1.7511652708053589
Running avgs for agent 2: q_loss: 925306.125, p_loss: 130.60826110839844, mean_rew: -6.832814703750811, variance: 177.3449763183594, lamda: 2.1601836681365967

steps: 1224975, episodes: 49000, mean episode reward: -566.654967493373, agent episode reward: [-188.88498916445766, -188.88498916445766, -188.88498916445766], time: 197.388
steps: 1224975, episodes: 49000, mean episode variance: 46.09001453518867, agent episode variance: [0.9850920605659484, 0.9870362899303436, 44.11788618469238], time: 197.389
Running avgs for agent 0: q_loss: 5.632962703704834, p_loss: -3.5157666206359863, mean_rew: -6.8346184854635625, variance: 3.940368413925171, lamda: 1.7602704763412476
Running avgs for agent 1: q_loss: 5.962265968322754, p_loss: -3.521681070327759, mean_rew: -6.839776247586896, variance: 3.9481449127197266, lamda: 1.7513957023620605
Running avgs for agent 2: q_loss: 897158.8125, p_loss: 130.7705078125, mean_rew: -6.838274370766414, variance: 176.47154473876952, lamda: 2.1851580142974854

steps: 1249975, episodes: 50000, mean episode reward: -570.7931533027472, agent episode reward: [-190.26438443424908, -190.26438443424908, -190.26438443424908], time: 187.094
steps: 1249975, episodes: 50000, mean episode variance: 45.40917019867897, agent episode variance: [0.988987973690033, 0.9945007827281952, 43.42568144226074], time: 187.095
Running avgs for agent 0: q_loss: 5.636595726013184, p_loss: -3.528419256210327, mean_rew: -6.85446724565936, variance: 3.9559521675109863, lamda: 1.7602958679199219
Running avgs for agent 1: q_loss: 5.950241565704346, p_loss: -3.5188841819763184, mean_rew: -6.854063163028335, variance: 3.9780032634735107, lamda: 1.7514523267745972
Running avgs for agent 2: q_loss: 894287.0625, p_loss: 131.14083862304688, mean_rew: -6.856895850657573, variance: 173.70272576904296, lamda: 2.210132360458374

steps: 1274975, episodes: 51000, mean episode reward: -568.8761091153594, agent episode reward: [-189.62536970511985, -189.62536970511985, -189.62536970511985], time: 190.819
steps: 1274975, episodes: 51000, mean episode variance: 44.650504408597946, agent episode variance: [0.9883234410285949, 0.99350341629982, 42.66867755126953], time: 190.819
Running avgs for agent 0: q_loss: 5.631612300872803, p_loss: -3.532310724258423, mean_rew: -6.87037952009688, variance: 3.953294038772583, lamda: 1.7602959871292114
Running avgs for agent 1: q_loss: 5.920135021209717, p_loss: -3.5275003910064697, mean_rew: -6.870843844865525, variance: 3.974013566970825, lamda: 1.7515265941619873
Running avgs for agent 2: q_loss: 889129.9375, p_loss: 131.88372802734375, mean_rew: -6.868139377452637, variance: 170.67471020507813, lamda: 2.2351067066192627

steps: 1299975, episodes: 52000, mean episode reward: -573.904731909283, agent episode reward: [-191.3015773030943, -191.3015773030943, -191.3015773030943], time: 193.302
steps: 1299975, episodes: 52000, mean episode variance: 43.83276633477211, agent episode variance: [0.9972230155467987, 0.9970562891960144, 41.8384870300293], time: 193.302
Running avgs for agent 0: q_loss: 5.625184535980225, p_loss: -3.5446531772613525, mean_rew: -6.887892816517092, variance: 3.988892078399658, lamda: 1.760452389717102
Running avgs for agent 1: q_loss: 4.856019496917725, p_loss: -3.5385985374450684, mean_rew: -6.892978516789788, variance: 3.988224744796753, lamda: 1.7529473304748535
Running avgs for agent 2: q_loss: 900228.0625, p_loss: 132.7837677001953, mean_rew: -6.893679532087187, variance: 167.3539481201172, lamda: 2.2600810527801514

steps: 1324975, episodes: 53000, mean episode reward: -577.7410320508986, agent episode reward: [-192.5803440169662, -192.5803440169662, -192.5803440169662], time: 192.8
steps: 1324975, episodes: 53000, mean episode variance: 44.70810091114044, agent episode variance: [1.0015825619697571, 0.9986759877204895, 42.7078423614502], time: 192.8
Running avgs for agent 0: q_loss: 4.52936315536499, p_loss: -3.5622150897979736, mean_rew: -6.929166975333021, variance: 4.006330490112305, lamda: 1.7617944478988647
Running avgs for agent 1: q_loss: 5.352334499359131, p_loss: -3.5526750087738037, mean_rew: -6.9248290428626555, variance: 3.994703769683838, lamda: 1.7578022480010986
Running avgs for agent 2: q_loss: 902003.8125, p_loss: 133.5928497314453, mean_rew: -6.923474237957735, variance: 170.8313694458008, lamda: 2.285055160522461

steps: 1349975, episodes: 54000, mean episode reward: -577.0660482160505, agent episode reward: [-192.3553494053501, -192.3553494053501, -192.3553494053501], time: 191.371
steps: 1349975, episodes: 54000, mean episode variance: 43.81438876080513, agent episode variance: [1.0032140328884125, 1.0035294489860536, 41.807645278930664], time: 191.372
Running avgs for agent 0: q_loss: 3.7621121406555176, p_loss: -3.5738868713378906, mean_rew: -6.944258089805293, variance: 4.0128560066223145, lamda: 1.7658263444900513
Running avgs for agent 1: q_loss: 6.160733699798584, p_loss: -3.5656285285949707, mean_rew: -6.946378471085507, variance: 4.01411771774292, lamda: 1.7588945627212524
Running avgs for agent 2: q_loss: 913311.4375, p_loss: 134.38755798339844, mean_rew: -6.94473686879583, variance: 167.23058111572266, lamda: 2.3100297451019287

steps: 1374975, episodes: 55000, mean episode reward: -569.7495295418853, agent episode reward: [-189.9165098472951, -189.9165098472951, -189.9165098472951], time: 189.533
steps: 1374975, episodes: 55000, mean episode variance: 43.155079399824146, agent episode variance: [1.001707799911499, 1.0052178676128387, 41.148153732299804], time: 189.534
Running avgs for agent 0: q_loss: 3.6108319759368896, p_loss: -3.5884175300598145, mean_rew: -6.973300516026181, variance: 4.006831169128418, lamda: 1.7685366868972778
Running avgs for agent 1: q_loss: 6.086734294891357, p_loss: -3.5797994136810303, mean_rew: -6.97116244241646, variance: 4.020871639251709, lamda: 1.7595144510269165
Running avgs for agent 2: q_loss: 926102.125, p_loss: 135.1483612060547, mean_rew: -6.9680369997482785, variance: 164.59261492919921, lamda: 2.3350038528442383

steps: 1399975, episodes: 56000, mean episode reward: -572.3742518939094, agent episode reward: [-190.79141729796982, -190.79141729796982, -190.79141729796982], time: 194.161
steps: 1399975, episodes: 56000, mean episode variance: 43.08189333605766, agent episode variance: [1.0071118004322053, 1.0059547839164733, 41.06882675170898], time: 194.161
Running avgs for agent 0: q_loss: 3.4795479774475098, p_loss: -3.599937677383423, mean_rew: -6.99674760267599, variance: 4.028447151184082, lamda: 1.7700598239898682
Running avgs for agent 1: q_loss: 5.979937553405762, p_loss: -3.5857787132263184, mean_rew: -6.992054691066893, variance: 4.0238189697265625, lamda: 1.7595795392990112
Running avgs for agent 2: q_loss: 925160.625, p_loss: 135.92575073242188, mean_rew: -7.000635147160274, variance: 164.27530700683593, lamda: 2.359978437423706

steps: 1424975, episodes: 57000, mean episode reward: -572.438296147085, agent episode reward: [-190.81276538236165, -190.81276538236165, -190.81276538236165], time: 192.909
steps: 1424975, episodes: 57000, mean episode variance: 42.799928792238234, agent episode variance: [1.0091293609142304, 1.0193378071784973, 40.77146162414551], time: 192.91
Running avgs for agent 0: q_loss: 3.4198923110961914, p_loss: -3.6123766899108887, mean_rew: -7.025725058615464, variance: 4.03651762008667, lamda: 1.770290493965149
Running avgs for agent 1: q_loss: 5.294175624847412, p_loss: -3.599761724472046, mean_rew: -7.023879396806178, variance: 4.077351093292236, lamda: 1.7603830099105835
Running avgs for agent 2: q_loss: 927675.0, p_loss: 136.58883666992188, mean_rew: -7.022690295976327, variance: 163.08584649658204, lamda: 2.3849525451660156

steps: 1449975, episodes: 58000, mean episode reward: -579.1813822736422, agent episode reward: [-193.0604607578807, -193.0604607578807, -193.0604607578807], time: 194.257
steps: 1449975, episodes: 58000, mean episode variance: 42.915784287452695, agent episode variance: [1.0165205047130585, 1.0197469065189362, 40.879516876220706], time: 194.257
Running avgs for agent 0: q_loss: 4.1395087242126465, p_loss: -3.626518487930298, mean_rew: -7.059921103388448, variance: 4.06608247756958, lamda: 1.7722878456115723
Running avgs for agent 1: q_loss: 4.016448497772217, p_loss: -3.6185264587402344, mean_rew: -7.054675186038063, variance: 4.078988075256348, lamda: 1.7657395601272583
Running avgs for agent 2: q_loss: 924807.25, p_loss: 137.13525390625, mean_rew: -7.055799805298246, variance: 163.51806750488282, lamda: 2.4099271297454834

steps: 1474975, episodes: 59000, mean episode reward: -577.1611654792505, agent episode reward: [-192.38705515975016, -192.38705515975016, -192.38705515975016], time: 190.044
steps: 1474975, episodes: 59000, mean episode variance: 43.18099419736862, agent episode variance: [1.0180196363925933, 1.0230226109027862, 41.13995195007324], time: 190.045
Running avgs for agent 0: q_loss: 3.4310715198516846, p_loss: -3.646561622619629, mean_rew: -7.091552840612119, variance: 4.072078704833984, lamda: 1.7743686437606812
Running avgs for agent 1: q_loss: 4.260096549987793, p_loss: -3.6377837657928467, mean_rew: -7.095858981228402, variance: 4.092090129852295, lamda: 1.7694087028503418
Running avgs for agent 2: q_loss: 937315.375, p_loss: 137.66331481933594, mean_rew: -7.086677783920883, variance: 164.55980780029296, lamda: 2.434901475906372

steps: 1499975, episodes: 60000, mean episode reward: -576.576797295542, agent episode reward: [-192.1922657651807, -192.1922657651807, -192.1922657651807], time: 193.815
steps: 1499975, episodes: 60000, mean episode variance: 42.599426702976224, agent episode variance: [1.0243789417743683, 1.0192172863483429, 40.555830474853515], time: 193.816
Running avgs for agent 0: q_loss: 3.3562889099121094, p_loss: -3.658763885498047, mean_rew: -7.113870006871545, variance: 4.09751558303833, lamda: 1.7744475603103638
Running avgs for agent 1: q_loss: 5.9938459396362305, p_loss: -3.651878833770752, mean_rew: -7.119663604668666, variance: 4.076869010925293, lamda: 1.7727397680282593
Running avgs for agent 2: q_loss: 942340.625, p_loss: 138.2662811279297, mean_rew: -7.114426149462262, variance: 162.22332189941406, lamda: 2.4598758220672607

steps: 1524975, episodes: 61000, mean episode reward: -582.9837121182488, agent episode reward: [-194.32790403941624, -194.32790403941624, -194.32790403941624], time: 194.119
steps: 1524975, episodes: 61000, mean episode variance: 42.26817411017418, agent episode variance: [1.025512968301773, 1.0248214812278749, 40.21783966064453], time: 194.119
Running avgs for agent 0: q_loss: 3.436185598373413, p_loss: -3.679598808288574, mean_rew: -7.150129080780294, variance: 4.102051734924316, lamda: 1.7744503021240234
Running avgs for agent 1: q_loss: 4.242313861846924, p_loss: -3.6616621017456055, mean_rew: -7.142761511570885, variance: 4.099286079406738, lamda: 1.7775973081588745
Running avgs for agent 2: q_loss: 959390.0625, p_loss: 138.72605895996094, mean_rew: -7.146599261074964, variance: 160.87135864257812, lamda: 2.4848501682281494

steps: 1549975, episodes: 62000, mean episode reward: -589.0599436243738, agent episode reward: [-196.35331454145796, -196.35331454145796, -196.35331454145796], time: 189.746
steps: 1549975, episodes: 62000, mean episode variance: 41.99974271988869, agent episode variance: [1.0315623426437377, 1.0259643585681915, 39.94221601867676], time: 189.746
Running avgs for agent 0: q_loss: 3.4353647232055664, p_loss: -3.6926534175872803, mean_rew: -7.1870737908436215, variance: 4.126248836517334, lamda: 1.774553656578064
Running avgs for agent 1: q_loss: 3.9377448558807373, p_loss: -3.6906535625457764, mean_rew: -7.184503879121227, variance: 4.103857517242432, lamda: 1.782472848892212
Running avgs for agent 2: q_loss: 955811.5625, p_loss: 139.02886962890625, mean_rew: -7.176633119807759, variance: 159.76886407470704, lamda: 2.509824514389038

steps: 1574975, episodes: 63000, mean episode reward: -598.0468555224419, agent episode reward: [-199.34895184081395, -199.34895184081395, -199.34895184081395], time: 192.163
steps: 1574975, episodes: 63000, mean episode variance: 41.08397844219208, agent episode variance: [1.0344462954998017, 1.033268718957901, 39.01626342773437], time: 192.163
Running avgs for agent 0: q_loss: 3.5602948665618896, p_loss: -3.707566738128662, mean_rew: -7.216630373474235, variance: 4.137784957885742, lamda: 1.7747225761413574
Running avgs for agent 1: q_loss: 4.225728511810303, p_loss: -3.7035372257232666, mean_rew: -7.222483722164108, variance: 4.13307523727417, lamda: 1.7846970558166504
Running avgs for agent 2: q_loss: 951930.75, p_loss: 139.18768310546875, mean_rew: -7.21251934075855, variance: 156.0650537109375, lamda: 2.5347988605499268

steps: 1599975, episodes: 64000, mean episode reward: -604.5916029316776, agent episode reward: [-201.5305343105592, -201.5305343105592, -201.5305343105592], time: 189.217
steps: 1599975, episodes: 64000, mean episode variance: 41.08697551321983, agent episode variance: [1.0417144341468811, 1.036347672700882, 39.00891340637207], time: 189.217
Running avgs for agent 0: q_loss: 3.9977753162384033, p_loss: -3.7166221141815186, mean_rew: -7.248913945705068, variance: 4.1668572425842285, lamda: 1.7770206928253174
Running avgs for agent 1: q_loss: 5.010226249694824, p_loss: -3.72088885307312, mean_rew: -7.245605724640033, variance: 4.145390510559082, lamda: 1.7865073680877686
Running avgs for agent 2: q_loss: 956277.6875, p_loss: 139.38131713867188, mean_rew: -7.246593675846027, variance: 156.0356536254883, lamda: 2.5597729682922363

steps: 1624975, episodes: 65000, mean episode reward: -604.6522652935671, agent episode reward: [-201.5507550978557, -201.5507550978557, -201.5507550978557], time: 188.309
steps: 1624975, episodes: 65000, mean episode variance: 41.808350919008255, agent episode variance: [1.046048776626587, 1.036511370897293, 39.72579077148438], time: 188.309
Running avgs for agent 0: q_loss: 3.6905956268310547, p_loss: -3.741476535797119, mean_rew: -7.288479098781894, variance: 4.184195518493652, lamda: 1.7789301872253418
Running avgs for agent 1: q_loss: 4.32571268081665, p_loss: -3.735933542251587, mean_rew: -7.283147250239496, variance: 4.146044731140137, lamda: 1.7918466329574585
Running avgs for agent 2: q_loss: 960939.5625, p_loss: 139.68312072753906, mean_rew: -7.284517432492982, variance: 158.9031630859375, lamda: 2.584747552871704

steps: 1649975, episodes: 66000, mean episode reward: -609.2899791214169, agent episode reward: [-203.09665970713903, -203.09665970713903, -203.09665970713903], time: 187.267
steps: 1649975, episodes: 66000, mean episode variance: 40.32865973210335, agent episode variance: [1.0481661965847016, 1.0408848624229432, 38.2396086730957], time: 187.267
Running avgs for agent 0: q_loss: 3.5981016159057617, p_loss: -3.758859872817993, mean_rew: -7.315476348088342, variance: 4.192664623260498, lamda: 1.7790508270263672
Running avgs for agent 1: q_loss: 5.558911323547363, p_loss: -3.765645980834961, mean_rew: -7.325667625086479, variance: 4.163539409637451, lamda: 1.7945905923843384
Running avgs for agent 2: q_loss: 972139.8125, p_loss: 140.09210205078125, mean_rew: -7.319179702797477, variance: 152.9584346923828, lamda: 2.6097216606140137

steps: 1674975, episodes: 67000, mean episode reward: -601.6366345643609, agent episode reward: [-200.54554485478695, -200.54554485478695, -200.54554485478695], time: 189.396
steps: 1674975, episodes: 67000, mean episode variance: 40.78921914768219, agent episode variance: [1.0514418749809264, 1.040589162349701, 38.69718811035156], time: 189.396
Running avgs for agent 0: q_loss: 3.748485803604126, p_loss: -3.7869760990142822, mean_rew: -7.3593055137310985, variance: 4.205767631530762, lamda: 1.7791317701339722
Running avgs for agent 1: q_loss: 4.384382247924805, p_loss: -3.7799179553985596, mean_rew: -7.359763671005279, variance: 4.162356853485107, lamda: 1.8001043796539307
Running avgs for agent 2: q_loss: 981475.4375, p_loss: 140.4851531982422, mean_rew: -7.354049599618595, variance: 154.78875244140625, lamda: 2.6346962451934814

steps: 1699975, episodes: 68000, mean episode reward: -606.8465055755717, agent episode reward: [-202.28216852519057, -202.28216852519057, -202.28216852519057], time: 186.25
steps: 1699975, episodes: 68000, mean episode variance: 41.967542455911634, agent episode variance: [1.060950670003891, 1.0450513806343078, 39.86154040527344], time: 186.25
Running avgs for agent 0: q_loss: 3.7867050170898438, p_loss: -3.7969064712524414, mean_rew: -7.3940687346515, variance: 4.243802547454834, lamda: 1.7793060541152954
Running avgs for agent 1: q_loss: 5.187364101409912, p_loss: -3.7989144325256348, mean_rew: -7.395737556996856, variance: 4.18020486831665, lamda: 1.8056918382644653
Running avgs for agent 2: q_loss: 1000800.0, p_loss: 140.7787628173828, mean_rew: -7.392163947288027, variance: 159.44616162109375, lamda: 2.659670352935791

steps: 1724975, episodes: 69000, mean episode reward: -603.7246775043325, agent episode reward: [-201.2415591681108, -201.2415591681108, -201.2415591681108], time: 191.477
steps: 1724975, episodes: 69000, mean episode variance: 41.0465065305233, agent episode variance: [1.055315085887909, 1.0412301714420318, 38.94996127319336], time: 191.477
Running avgs for agent 0: q_loss: 6.6144537925720215, p_loss: -3.819809675216675, mean_rew: -7.434421105837588, variance: 4.2212605476379395, lamda: 1.7797445058822632
Running avgs for agent 1: q_loss: 6.315446853637695, p_loss: -3.8200294971466064, mean_rew: -7.4284892083817615, variance: 4.164920806884766, lamda: 1.8089832067489624
Running avgs for agent 2: q_loss: 1009573.5, p_loss: 141.02426147460938, mean_rew: -7.42814220442793, variance: 155.79984509277344, lamda: 2.6846446990966797

steps: 1749975, episodes: 70000, mean episode reward: -605.6897328438498, agent episode reward: [-201.8965776146166, -201.8965776146166, -201.8965776146166], time: 193.481
steps: 1749975, episodes: 70000, mean episode variance: 41.2899684548378, agent episode variance: [1.0712878792285918, 1.0487614319324494, 39.16991914367676], time: 193.481
Running avgs for agent 0: q_loss: 6.7586565017700195, p_loss: -3.8387763500213623, mean_rew: -7.469381730834166, variance: 4.285151481628418, lamda: 1.7799499034881592
Running avgs for agent 1: q_loss: 4.601974010467529, p_loss: -3.8361713886260986, mean_rew: -7.464947940092823, variance: 4.1950459480285645, lamda: 1.8135634660720825
Running avgs for agent 2: q_loss: 1030670.0, p_loss: 141.40830993652344, mean_rew: -7.468674017548341, variance: 156.67967657470703, lamda: 2.7096190452575684

steps: 1774975, episodes: 71000, mean episode reward: -607.5438717708361, agent episode reward: [-202.514623923612, -202.514623923612, -202.514623923612], time: 188.545
steps: 1774975, episodes: 71000, mean episode variance: 40.7151534512043, agent episode variance: [1.0732368021011351, 1.0538982164859771, 38.58801843261719], time: 188.546
Running avgs for agent 0: q_loss: 6.822432518005371, p_loss: -3.859074354171753, mean_rew: -7.504946496989114, variance: 4.292947292327881, lamda: 1.779966115951538
Running avgs for agent 1: q_loss: 4.575207233428955, p_loss: -3.8535873889923096, mean_rew: -7.50451283187397, variance: 4.215592861175537, lamda: 1.8179852962493896
Running avgs for agent 2: q_loss: 1039356.5625, p_loss: 141.62841796875, mean_rew: -7.500764804076626, variance: 154.35207373046876, lamda: 2.734593391418457

steps: 1799975, episodes: 72000, mean episode reward: -613.99775378298, agent episode reward: [-204.66591792766, -204.66591792766, -204.66591792766], time: 188.527
steps: 1799975, episodes: 72000, mean episode variance: 40.950293016672134, agent episode variance: [1.075931218624115, 1.054121685743332, 38.82024011230469], time: 188.527
Running avgs for agent 0: q_loss: 6.926836967468262, p_loss: -3.8804898262023926, mean_rew: -7.543488517666232, variance: 4.303724765777588, lamda: 1.7799662351608276
Running avgs for agent 1: q_loss: 5.046284198760986, p_loss: -3.8764922618865967, mean_rew: -7.535627507514615, variance: 4.21648645401001, lamda: 1.8239353895187378
Running avgs for agent 2: q_loss: 1063689.875, p_loss: 141.98831176757812, mean_rew: -7.535241560240261, variance: 155.28096044921875, lamda: 2.759567975997925

steps: 1824975, episodes: 73000, mean episode reward: -606.7513901564963, agent episode reward: [-202.2504633854988, -202.2504633854988, -202.2504633854988], time: 190.189
steps: 1824975, episodes: 73000, mean episode variance: 40.06888017034531, agent episode variance: [1.0871786544322968, 1.0561943137645722, 37.92550720214844], time: 190.189
Running avgs for agent 0: q_loss: 7.042986869812012, p_loss: -3.8928189277648926, mean_rew: -7.579110758206137, variance: 4.348714828491211, lamda: 1.7800419330596924
Running avgs for agent 1: q_loss: 6.9981560707092285, p_loss: -3.892052173614502, mean_rew: -7.5777819215138384, variance: 4.2247772216796875, lamda: 1.826932668685913
Running avgs for agent 2: q_loss: 1077050.875, p_loss: 142.37306213378906, mean_rew: -7.577434805647883, variance: 151.70202880859375, lamda: 2.7845423221588135

steps: 1849975, episodes: 74000, mean episode reward: -599.7394442720569, agent episode reward: [-199.91314809068564, -199.91314809068564, -199.91314809068564], time: 189.09
steps: 1849975, episodes: 74000, mean episode variance: 41.669173156261444, agent episode variance: [1.09364989733696, 1.0601548659801483, 39.51536839294434], time: 189.091
Running avgs for agent 0: q_loss: 7.07101583480835, p_loss: -3.9111831188201904, mean_rew: -7.609146338889963, variance: 4.374599456787109, lamda: 1.7803040742874146
Running avgs for agent 1: q_loss: 6.433032035827637, p_loss: -3.9145212173461914, mean_rew: -7.610277329338925, variance: 4.24061918258667, lamda: 1.8279908895492554
Running avgs for agent 2: q_loss: 1100378.0, p_loss: 142.60292053222656, mean_rew: -7.612656214305881, variance: 158.06147357177736, lamda: 2.809516668319702

steps: 1874975, episodes: 75000, mean episode reward: -602.2657689428688, agent episode reward: [-200.75525631428957, -200.75525631428957, -200.75525631428957], time: 186.872
steps: 1874975, episodes: 75000, mean episode variance: 41.88454680228234, agent episode variance: [1.0921084325313568, 1.0625220642089843, 39.72991630554199], time: 186.872
Running avgs for agent 0: q_loss: 7.086780548095703, p_loss: -3.9263927936553955, mean_rew: -7.645565345800497, variance: 4.368433475494385, lamda: 1.7803068161010742
Running avgs for agent 1: q_loss: 6.855559349060059, p_loss: -3.9331533908843994, mean_rew: -7.640918636107441, variance: 4.250087738037109, lamda: 1.8319871425628662
Running avgs for agent 2: q_loss: 1123903.75, p_loss: 142.83773803710938, mean_rew: -7.630005042911027, variance: 158.91966522216796, lamda: 2.8344907760620117

steps: 1899975, episodes: 76000, mean episode reward: -598.9783811333104, agent episode reward: [-199.65946037777013, -199.65946037777013, -199.65946037777013], time: 189.216
steps: 1899975, episodes: 76000, mean episode variance: 40.99562636041641, agent episode variance: [1.1005641994476318, 1.0634689297676085, 38.831593231201175], time: 189.216
Running avgs for agent 0: q_loss: 7.133143901824951, p_loss: -3.9433934688568115, mean_rew: -7.676821127108323, variance: 4.402256965637207, lamda: 1.7803068161010742
Running avgs for agent 1: q_loss: 5.5675578117370605, p_loss: -3.9472289085388184, mean_rew: -7.673730363201512, variance: 4.253876209259033, lamda: 1.8354548215866089
Running avgs for agent 2: q_loss: 1155519.25, p_loss: 143.24366760253906, mean_rew: -7.6672547509901445, variance: 155.3263729248047, lamda: 2.8594653606414795

steps: 1924975, episodes: 77000, mean episode reward: -609.9949335190987, agent episode reward: [-203.33164450636625, -203.33164450636625, -203.33164450636625], time: 187.624
steps: 1924975, episodes: 77000, mean episode variance: 41.808341447353364, agent episode variance: [1.1039302060604095, 1.0687220628261567, 39.6356891784668], time: 187.624
Running avgs for agent 0: q_loss: 7.167896270751953, p_loss: -3.9568848609924316, mean_rew: -7.7038192770998855, variance: 4.4157209396362305, lamda: 1.7803066968917847
Running avgs for agent 1: q_loss: 4.598455429077148, p_loss: -3.9635674953460693, mean_rew: -7.700570030311555, variance: 4.274888515472412, lamda: 1.839107632637024
Running avgs for agent 2: q_loss: 1162016.5, p_loss: 143.48031616210938, mean_rew: -7.699872975231498, variance: 158.5427567138672, lamda: 2.884439706802368

steps: 1949975, episodes: 78000, mean episode reward: -604.054651626944, agent episode reward: [-201.35155054231464, -201.35155054231464, -201.35155054231464], time: 184.435
steps: 1949975, episodes: 78000, mean episode variance: 41.52279415750503, agent episode variance: [1.104528924703598, 1.0710895736217498, 39.34717565917969], time: 184.435
Running avgs for agent 0: q_loss: 7.190844535827637, p_loss: -3.968459367752075, mean_rew: -7.720226643176037, variance: 4.418115139007568, lamda: 1.7803068161010742
Running avgs for agent 1: q_loss: 4.858989238739014, p_loss: -3.970996618270874, mean_rew: -7.718573396729337, variance: 4.284358501434326, lamda: 1.8447359800338745
Running avgs for agent 2: q_loss: 1202822.375, p_loss: 143.77871704101562, mean_rew: -7.718221536743131, variance: 157.38870263671876, lamda: 2.909414052963257

steps: 1974975, episodes: 79000, mean episode reward: -603.5578340302692, agent episode reward: [-201.1859446767564, -201.1859446767564, -201.1859446767564], time: 183.997
steps: 1974975, episodes: 79000, mean episode variance: 41.97899899625778, agent episode variance: [1.1126929655075073, 1.0698683171272279, 39.79643771362305], time: 183.998
Running avgs for agent 0: q_loss: 7.20194149017334, p_loss: -3.97866153717041, mean_rew: -7.752220015612698, variance: 4.450771331787109, lamda: 1.7803068161010742
Running avgs for agent 1: q_loss: 6.302298069000244, p_loss: -3.9851982593536377, mean_rew: -7.752657356944691, variance: 4.279473781585693, lamda: 1.8508864641189575
Running avgs for agent 2: q_loss: 1215841.25, p_loss: 144.1596221923828, mean_rew: -7.75078651931132, variance: 159.1857508544922, lamda: 2.9343883991241455

steps: 1999975, episodes: 80000, mean episode reward: -606.7924091062154, agent episode reward: [-202.26413636873843, -202.26413636873843, -202.26413636873843], time: 188.45
steps: 1999975, episodes: 80000, mean episode variance: 41.89621904969216, agent episode variance: [1.1197001345157622, 1.0716477146148682, 39.70487120056152], time: 188.451
Running avgs for agent 0: q_loss: 7.3116302490234375, p_loss: -3.993988037109375, mean_rew: -7.779449335412099, variance: 4.478800296783447, lamda: 1.7803066968917847
Running avgs for agent 1: q_loss: 7.3325371742248535, p_loss: -3.9992973804473877, mean_rew: -7.775979243828944, variance: 4.286591053009033, lamda: 1.852360486984253
Running avgs for agent 2: q_loss: 1248318.75, p_loss: 144.49880981445312, mean_rew: -7.780382581369233, variance: 158.81948480224608, lamda: 2.959362745285034

steps: 2024975, episodes: 81000, mean episode reward: -609.356106530559, agent episode reward: [-203.11870217685302, -203.11870217685302, -203.11870217685302], time: 185.631
steps: 2024975, episodes: 81000, mean episode variance: 42.50703184199333, agent episode variance: [1.1190086123943328, 1.0725783596038818, 40.31544486999512], time: 185.632
Running avgs for agent 0: q_loss: 7.370441436767578, p_loss: -4.006235122680664, mean_rew: -7.802849958630245, variance: 4.476034164428711, lamda: 1.7803068161010742
Running avgs for agent 1: q_loss: 7.396556854248047, p_loss: -4.0177154541015625, mean_rew: -7.798000924361806, variance: 4.290313243865967, lamda: 1.8525511026382446
Running avgs for agent 2: q_loss: 1257412.125, p_loss: 144.6492462158203, mean_rew: -7.797663707596128, variance: 161.26177947998048, lamda: 2.984337091445923

steps: 2049975, episodes: 82000, mean episode reward: -608.3693385285455, agent episode reward: [-202.7897795095152, -202.7897795095152, -202.7897795095152], time: 186.924
steps: 2049975, episodes: 82000, mean episode variance: 41.510687215805056, agent episode variance: [1.1231168828010558, 1.077723165035248, 39.30984716796875], time: 186.925
Running avgs for agent 0: q_loss: 7.3992462158203125, p_loss: -4.016293525695801, mean_rew: -7.824251996309644, variance: 4.492467880249023, lamda: 1.7803068161010742
Running avgs for agent 1: q_loss: 7.428429126739502, p_loss: -4.029864311218262, mean_rew: -7.826437006156548, variance: 4.310892581939697, lamda: 1.8526091575622559
Running avgs for agent 2: q_loss: 1286957.0, p_loss: 144.87562561035156, mean_rew: -7.8321921518449775, variance: 157.239388671875, lamda: 3.0093114376068115

steps: 2074975, episodes: 83000, mean episode reward: -608.6935903771144, agent episode reward: [-202.89786345903815, -202.89786345903815, -202.89786345903815], time: 189.672
steps: 2074975, episodes: 83000, mean episode variance: 43.4575777580738, agent episode variance: [1.127915992975235, 1.0819883184432983, 41.24767344665527], time: 189.673
Running avgs for agent 0: q_loss: 7.506237983703613, p_loss: -4.026551723480225, mean_rew: -7.849418721617271, variance: 4.511663913726807, lamda: 1.7803066968917847
Running avgs for agent 1: q_loss: 7.256339073181152, p_loss: -4.042552947998047, mean_rew: -7.855106256527551, variance: 4.327953338623047, lamda: 1.8526701927185059
Running avgs for agent 2: q_loss: 1325235.5, p_loss: 145.11080932617188, mean_rew: -7.859283347983504, variance: 164.99069378662108, lamda: 3.034285545349121

steps: 2099975, episodes: 84000, mean episode reward: -599.5687246493427, agent episode reward: [-199.85624154978095, -199.85624154978095, -199.85624154978095], time: 190.538
steps: 2099975, episodes: 84000, mean episode variance: 43.19763059449196, agent episode variance: [1.1287626118659972, 1.087437043905258, 40.981430938720706], time: 190.538
Running avgs for agent 0: q_loss: 7.620331287384033, p_loss: -4.0370402336120605, mean_rew: -7.861848270735139, variance: 4.515050411224365, lamda: 1.7803068161010742
Running avgs for agent 1: q_loss: 5.367407321929932, p_loss: -4.040174961090088, mean_rew: -7.86449697868697, variance: 4.349748134613037, lamda: 1.856918215751648
Running avgs for agent 2: q_loss: 1342897.875, p_loss: 145.3023681640625, mean_rew: -7.864752681587577, variance: 163.92572375488282, lamda: 3.059260368347168

steps: 2124975, episodes: 85000, mean episode reward: -601.9957371795965, agent episode reward: [-200.6652457265322, -200.6652457265322, -200.6652457265322], time: 192.644
steps: 2124975, episodes: 85000, mean episode variance: 42.043010041236876, agent episode variance: [1.1329935326576233, 1.0806121201515198, 39.82940438842773], time: 192.645
Running avgs for agent 0: q_loss: 7.485909938812256, p_loss: -4.041440010070801, mean_rew: -7.883589171865462, variance: 4.531974792480469, lamda: 1.7803068161010742
Running avgs for agent 1: q_loss: 4.9688191413879395, p_loss: -4.056400775909424, mean_rew: -7.881529457679131, variance: 4.322448253631592, lamda: 1.8625620603561401
Running avgs for agent 2: q_loss: 1361893.625, p_loss: 145.58303833007812, mean_rew: -7.88125665250819, variance: 159.31761755371093, lamda: 3.0842344760894775

steps: 2149975, episodes: 86000, mean episode reward: -603.9386019876415, agent episode reward: [-201.31286732921384, -201.31286732921384, -201.31286732921384], time: 189.897
steps: 2149975, episodes: 86000, mean episode variance: 43.23150088477135, agent episode variance: [1.1366211454868316, 1.0850264678001404, 41.009853271484374], time: 189.898
Running avgs for agent 0: q_loss: 7.521078586578369, p_loss: -4.061171054840088, mean_rew: -7.903151109810878, variance: 4.546484470367432, lamda: 1.7803066968917847
Running avgs for agent 1: q_loss: 5.088534355163574, p_loss: -4.06355094909668, mean_rew: -7.902096814635017, variance: 4.340106010437012, lamda: 1.8662070035934448
Running avgs for agent 2: q_loss: 1387962.5, p_loss: 145.7008819580078, mean_rew: -7.894776583785937, variance: 164.0394130859375, lamda: 3.1092090606689453

steps: 2174975, episodes: 87000, mean episode reward: -604.2611344854822, agent episode reward: [-201.42037816182744, -201.42037816182744, -201.42037816182744], time: 192.848
steps: 2174975, episodes: 87000, mean episode variance: 43.08583972001076, agent episode variance: [1.132817579984665, 1.0864877314567567, 40.866534408569336], time: 192.848
Running avgs for agent 0: q_loss: 7.545846939086914, p_loss: -4.06388521194458, mean_rew: -7.9147091418360525, variance: 4.5312700271606445, lamda: 1.7803568840026855
Running avgs for agent 1: q_loss: 6.933861255645752, p_loss: -4.08019495010376, mean_rew: -7.922152437734041, variance: 4.345951080322266, lamda: 1.8697350025177002
Running avgs for agent 2: q_loss: 1385838.375, p_loss: 145.9145050048828, mean_rew: -7.918559553496893, variance: 163.46613763427735, lamda: 3.134183168411255

steps: 2199975, episodes: 88000, mean episode reward: -602.9463660742206, agent episode reward: [-200.98212202474022, -200.98212202474022, -200.98212202474022], time: 194.031
steps: 2199975, episodes: 88000, mean episode variance: 42.67855410528183, agent episode variance: [1.141336826324463, 1.086129678249359, 40.45108760070801], time: 194.031
Running avgs for agent 0: q_loss: 7.569881916046143, p_loss: -4.06692361831665, mean_rew: -7.925998784251804, variance: 4.565347671508789, lamda: 1.780388355255127
Running avgs for agent 1: q_loss: 7.517801761627197, p_loss: -4.077425479888916, mean_rew: -7.918367718752907, variance: 4.344518661499023, lamda: 1.870389699935913
Running avgs for agent 2: q_loss: 1417442.125, p_loss: 146.04925537109375, mean_rew: -7.932153781338339, variance: 161.80435040283203, lamda: 3.1591575145721436

steps: 2224975, episodes: 89000, mean episode reward: -599.897627722656, agent episode reward: [-199.96587590755203, -199.96587590755203, -199.96587590755203], time: 192.42
steps: 2224975, episodes: 89000, mean episode variance: 43.92221928143501, agent episode variance: [1.1407660360336305, 1.0917649672031402, 41.68968827819824], time: 192.421
Running avgs for agent 0: q_loss: 7.543562412261963, p_loss: -4.080113887786865, mean_rew: -7.94863341153066, variance: 4.563064098358154, lamda: 1.7803882360458374
Running avgs for agent 1: q_loss: 7.590099811553955, p_loss: -4.086953163146973, mean_rew: -7.942403369703007, variance: 4.367059707641602, lamda: 1.8703898191452026
Running avgs for agent 2: q_loss: 1435901.125, p_loss: 146.18881225585938, mean_rew: -7.940650590668436, variance: 166.75875311279296, lamda: 3.1841318607330322

steps: 2249975, episodes: 90000, mean episode reward: -598.3537560407735, agent episode reward: [-199.4512520135912, -199.4512520135912, -199.4512520135912], time: 194.488
steps: 2249975, episodes: 90000, mean episode variance: 43.12965872740745, agent episode variance: [1.1409948959350587, 1.08872709441185, 40.89993673706055], time: 194.489
Running avgs for agent 0: q_loss: 7.6020050048828125, p_loss: -4.078718662261963, mean_rew: -7.951577458037788, variance: 4.563979625701904, lamda: 1.7803884744644165
Running avgs for agent 1: q_loss: 7.63253116607666, p_loss: -4.092096328735352, mean_rew: -7.946341548892029, variance: 4.354907989501953, lamda: 1.8704158067703247
Running avgs for agent 2: q_loss: 1447869.625, p_loss: 146.3144989013672, mean_rew: -7.949200901843365, variance: 163.5997469482422, lamda: 3.209106206893921

steps: 2274975, episodes: 91000, mean episode reward: -604.4526005517733, agent episode reward: [-201.4842001839244, -201.4842001839244, -201.4842001839244], time: 195.988
steps: 2274975, episodes: 91000, mean episode variance: 42.249119348287586, agent episode variance: [1.144159701347351, 1.0888985507488251, 40.01606109619141], time: 195.988
Running avgs for agent 0: q_loss: 7.616801738739014, p_loss: -4.085944652557373, mean_rew: -7.963870489932613, variance: 4.576638698577881, lamda: 1.780388355255127
Running avgs for agent 1: q_loss: 7.525062561035156, p_loss: -4.094401836395264, mean_rew: -7.960546288718063, variance: 4.355594158172607, lamda: 1.8704899549484253
Running avgs for agent 2: q_loss: 1470997.75, p_loss: 146.48863220214844, mean_rew: -7.958712613749452, variance: 160.06424438476563, lamda: 3.2340805530548096

steps: 2299975, episodes: 92000, mean episode reward: -601.9437319942535, agent episode reward: [-200.64791066475112, -200.64791066475112, -200.64791066475112], time: 195.856
steps: 2299975, episodes: 92000, mean episode variance: 43.75598484277725, agent episode variance: [1.1475734443664551, 1.086258551120758, 41.52215284729004], time: 195.857
Running avgs for agent 0: q_loss: 7.719405174255371, p_loss: -4.088905334472656, mean_rew: -7.970702683500188, variance: 4.590293884277344, lamda: 1.7805039882659912
Running avgs for agent 1: q_loss: 7.531876564025879, p_loss: -4.107542991638184, mean_rew: -7.973968126407936, variance: 4.345034122467041, lamda: 1.8704899549484253
Running avgs for agent 2: q_loss: 1483591.375, p_loss: 146.73562622070312, mean_rew: -7.971424311291454, variance: 166.08861138916015, lamda: 3.259054660797119

steps: 2324975, episodes: 93000, mean episode reward: -605.5410163056869, agent episode reward: [-201.847005435229, -201.847005435229, -201.847005435229], time: 204.182
steps: 2324975, episodes: 93000, mean episode variance: 42.21070877528191, agent episode variance: [1.1437581703662871, 1.0891167578697205, 39.9778338470459], time: 204.182
Running avgs for agent 0: q_loss: 7.607724189758301, p_loss: -4.094754219055176, mean_rew: -7.97866592376323, variance: 4.575032711029053, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.692296981811523, p_loss: -4.1124725341796875, mean_rew: -7.979112647566045, variance: 4.356466770172119, lamda: 1.8708416223526
Running avgs for agent 2: q_loss: 1504352.25, p_loss: 146.96710205078125, mean_rew: -7.980060727249209, variance: 159.9113353881836, lamda: 3.284029245376587

steps: 2349975, episodes: 94000, mean episode reward: -605.1085454440491, agent episode reward: [-201.70284848134972, -201.70284848134972, -201.70284848134972], time: 185.945
steps: 2349975, episodes: 94000, mean episode variance: 42.71634928607941, agent episode variance: [1.1556818249225616, 1.0920687367916107, 40.468598724365236], time: 185.945
Running avgs for agent 0: q_loss: 7.581784725189209, p_loss: -4.094527721405029, mean_rew: -7.9904199030618726, variance: 4.622727394104004, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.673141002655029, p_loss: -4.111837863922119, mean_rew: -7.983453975940807, variance: 4.368274688720703, lamda: 1.8714035749435425
Running avgs for agent 2: q_loss: 1511769.125, p_loss: 147.18006896972656, mean_rew: -7.9915630365672525, variance: 161.87439489746095, lamda: 3.3090033531188965

steps: 2374975, episodes: 95000, mean episode reward: -603.6565368708392, agent episode reward: [-201.21884562361305, -201.21884562361305, -201.21884562361305], time: 185.887
steps: 2374975, episodes: 95000, mean episode variance: 44.04610465669632, agent episode variance: [1.151088707447052, 1.0943198890686034, 41.80069606018066], time: 185.887
Running avgs for agent 0: q_loss: 7.654080390930176, p_loss: -4.115738868713379, mean_rew: -8.002733391992923, variance: 4.6043548583984375, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.569701671600342, p_loss: -4.111868381500244, mean_rew: -7.997618249202747, variance: 4.377279281616211, lamda: 1.8715256452560425
Running avgs for agent 2: q_loss: 1536779.5, p_loss: 147.36087036132812, mean_rew: -8.001086003947051, variance: 167.20278424072265, lamda: 3.3339779376983643

steps: 2399975, episodes: 96000, mean episode reward: -600.7368048818424, agent episode reward: [-200.24560162728082, -200.24560162728082, -200.24560162728082], time: 185.54
steps: 2399975, episodes: 96000, mean episode variance: 42.36849928998947, agent episode variance: [1.1548454220294952, 1.0876832258701326, 40.12597064208985], time: 185.54
Running avgs for agent 0: q_loss: 7.668188095092773, p_loss: -4.107146263122559, mean_rew: -8.013194031369805, variance: 4.619381904602051, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.718964099884033, p_loss: -4.122645378112793, mean_rew: -8.007998300030561, variance: 4.350733280181885, lamda: 1.8715258836746216
Running avgs for agent 2: q_loss: 1548985.375, p_loss: 147.55325317382812, mean_rew: -8.012783029463902, variance: 160.5038825683594, lamda: 3.358952045440674

steps: 2424975, episodes: 97000, mean episode reward: -601.6715712935868, agent episode reward: [-200.55719043119558, -200.55719043119558, -200.55719043119558], time: 189.309
steps: 2424975, episodes: 97000, mean episode variance: 42.16867127633095, agent episode variance: [1.1614148676395417, 1.0997658081054686, 39.90749060058594], time: 189.31
Running avgs for agent 0: q_loss: 7.681910037994385, p_loss: -4.116142272949219, mean_rew: -8.024136698843929, variance: 4.64565896987915, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.6323113441467285, p_loss: -4.125279426574707, mean_rew: -8.021546167940434, variance: 4.399063587188721, lamda: 1.8715256452560425
Running avgs for agent 2: q_loss: 1567372.125, p_loss: 147.81077575683594, mean_rew: -8.02528833223342, variance: 159.62996240234375, lamda: 3.3839268684387207

steps: 2449975, episodes: 98000, mean episode reward: -609.0228691033793, agent episode reward: [-203.00762303445978, -203.00762303445978, -203.00762303445978], time: 186.032
steps: 2449975, episodes: 98000, mean episode variance: 42.37851731181145, agent episode variance: [1.1542436909675597, 1.097920730829239, 40.12635289001465], time: 186.033
Running avgs for agent 0: q_loss: 7.631214618682861, p_loss: -4.117908000946045, mean_rew: -8.029491609130428, variance: 4.616974830627441, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.789176940917969, p_loss: -4.132752895355225, mean_rew: -8.031721875212554, variance: 4.391683101654053, lamda: 1.8715256452560425
Running avgs for agent 2: q_loss: 1569445.875, p_loss: 147.9398956298828, mean_rew: -8.025261976544932, variance: 160.5054115600586, lamda: 3.4089012145996094

steps: 2474975, episodes: 99000, mean episode reward: -602.0544947525502, agent episode reward: [-200.68483158418343, -200.68483158418343, -200.68483158418343], time: 191.55
steps: 2474975, episodes: 99000, mean episode variance: 43.120438144207, agent episode variance: [1.1496591320037841, 1.0960516715049744, 40.87472734069824], time: 191.551
Running avgs for agent 0: q_loss: 7.6786394119262695, p_loss: -4.128805637359619, mean_rew: -8.042718801711652, variance: 4.598636627197266, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.690464973449707, p_loss: -4.133049011230469, mean_rew: -8.032837753195675, variance: 4.384206771850586, lamda: 1.8715258836746216
Running avgs for agent 2: q_loss: 1581021.875, p_loss: 148.15472412109375, mean_rew: -8.034748995347517, variance: 163.49890936279297, lamda: 3.433875322341919

steps: 2499975, episodes: 100000, mean episode reward: -603.5992788105302, agent episode reward: [-201.19975960351007, -201.19975960351007, -201.19975960351007], time: 189.146
steps: 2499975, episodes: 100000, mean episode variance: 41.85416819190979, agent episode variance: [1.1607761459350585, 1.1007443103790284, 39.592647735595705], time: 189.146
Running avgs for agent 0: q_loss: 7.659840106964111, p_loss: -4.121013641357422, mean_rew: -8.042349718219912, variance: 4.643104553222656, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.725464820861816, p_loss: -4.140660285949707, mean_rew: -8.051069247531487, variance: 4.402977466583252, lamda: 1.8715256452560425
Running avgs for agent 2: q_loss: 1612985.625, p_loss: 148.4851837158203, mean_rew: -8.046146062143247, variance: 158.37059094238282, lamda: 3.4588496685028076

steps: 2524975, episodes: 101000, mean episode reward: -610.906909823519, agent episode reward: [-203.63563660783967, -203.63563660783967, -203.63563660783967], time: 188.081
steps: 2524975, episodes: 101000, mean episode variance: 43.00657474732399, agent episode variance: [1.1551824662685395, 1.1008368306159972, 40.75055545043945], time: 188.082
Running avgs for agent 0: q_loss: 7.616829872131348, p_loss: -4.133040904998779, mean_rew: -8.057080560322202, variance: 4.620729923248291, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.834778785705566, p_loss: -4.145237445831299, mean_rew: -8.060672609623923, variance: 4.403347492218018, lamda: 1.8715256452560425
Running avgs for agent 2: q_loss: 1620817.875, p_loss: 148.67205810546875, mean_rew: -8.053310325426018, variance: 163.0022218017578, lamda: 3.4838240146636963

steps: 2549975, episodes: 102000, mean episode reward: -601.6026046706249, agent episode reward: [-200.53420155687496, -200.53420155687496, -200.53420155687496], time: 187.341
steps: 2549975, episodes: 102000, mean episode variance: 43.65899887156486, agent episode variance: [1.1552888658046723, 1.1025691366195678, 41.401140869140626], time: 187.341
Running avgs for agent 0: q_loss: 7.553803443908691, p_loss: -4.131777286529541, mean_rew: -8.065874276969161, variance: 4.621155261993408, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.715083122253418, p_loss: -4.147071838378906, mean_rew: -8.063458590456339, variance: 4.410276412963867, lamda: 1.8715258836746216
Running avgs for agent 2: q_loss: 1646627.625, p_loss: 148.88656616210938, mean_rew: -8.064893601858326, variance: 165.6045634765625, lamda: 3.508798360824585

steps: 2574975, episodes: 103000, mean episode reward: -602.0021898472365, agent episode reward: [-200.66739661574553, -200.66739661574553, -200.66739661574553], time: 185.555
steps: 2574975, episodes: 103000, mean episode variance: 42.61598800778389, agent episode variance: [1.1649341344833375, 1.1002063086032867, 40.35084756469727], time: 185.555
Running avgs for agent 0: q_loss: 7.467201232910156, p_loss: -4.130307674407959, mean_rew: -8.06277499995532, variance: 4.659736156463623, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.69465446472168, p_loss: -4.148604869842529, mean_rew: -8.068043791769366, variance: 4.400825500488281, lamda: 1.8718173503875732
Running avgs for agent 2: q_loss: 1674656.5, p_loss: 149.29425048828125, mean_rew: -8.073868805105342, variance: 161.40339025878907, lamda: 3.5337724685668945

steps: 2599975, episodes: 104000, mean episode reward: -603.8257243288788, agent episode reward: [-201.27524144295958, -201.27524144295958, -201.27524144295958], time: 185.025
steps: 2599975, episodes: 104000, mean episode variance: 43.025017373800274, agent episode variance: [1.1608265914916993, 1.101163057088852, 40.76302772521973], time: 185.026
Running avgs for agent 0: q_loss: 7.531436443328857, p_loss: -4.139976501464844, mean_rew: -8.071688760462171, variance: 4.643306732177734, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.075850009918213, p_loss: -4.142731189727783, mean_rew: -8.065512357105197, variance: 4.404652118682861, lamda: 1.873598337173462
Running avgs for agent 2: q_loss: 1701898.5, p_loss: 149.54940795898438, mean_rew: -8.068388041503413, variance: 163.05211090087892, lamda: 3.5587470531463623

steps: 2624975, episodes: 105000, mean episode reward: -605.3199254003464, agent episode reward: [-201.77330846678208, -201.77330846678208, -201.77330846678208], time: 190.379
steps: 2624975, episodes: 105000, mean episode variance: 41.957424610853195, agent episode variance: [1.1529546422958374, 1.1032824532985688, 39.70118751525879], time: 190.38
Running avgs for agent 0: q_loss: 7.40765905380249, p_loss: -4.131902694702148, mean_rew: -8.06351690652736, variance: 4.611818313598633, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.432605266571045, p_loss: -4.143746852874756, mean_rew: -8.062213311802378, variance: 4.413129806518555, lamda: 1.876432180404663
Running avgs for agent 2: q_loss: 1731468.625, p_loss: 149.78866577148438, mean_rew: -8.068714252809363, variance: 158.80475006103515, lamda: 3.583721160888672

steps: 2649975, episodes: 106000, mean episode reward: -607.8951886182083, agent episode reward: [-202.63172953940278, -202.63172953940278, -202.63172953940278], time: 163.516
steps: 2649975, episodes: 106000, mean episode variance: 41.73428299164772, agent episode variance: [1.1624462993144988, 1.101426948070526, 39.470409744262696], time: 163.517
Running avgs for agent 0: q_loss: 7.418185234069824, p_loss: -4.130129814147949, mean_rew: -8.067325845063388, variance: 4.649785041809082, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.684130668640137, p_loss: -4.1472883224487305, mean_rew: -8.074898791372231, variance: 4.405707359313965, lamda: 1.8764326572418213
Running avgs for agent 2: q_loss: 1753586.5, p_loss: 150.1452178955078, mean_rew: -8.065861585243194, variance: 157.88163897705078, lamda: 3.6086957454681396

steps: 2674975, episodes: 107000, mean episode reward: -602.6952645774138, agent episode reward: [-200.89842152580462, -200.89842152580462, -200.89842152580462], time: 112.701
steps: 2674975, episodes: 107000, mean episode variance: 43.31457996797562, agent episode variance: [1.1575342364311219, 1.090593186378479, 41.066452545166015], time: 112.702
Running avgs for agent 0: q_loss: 7.4722514152526855, p_loss: -4.1403889656066895, mean_rew: -8.067032031962405, variance: 4.630136489868164, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.593883037567139, p_loss: -4.146501064300537, mean_rew: -8.063174962657927, variance: 4.362372398376465, lamda: 1.8764328956604004
Running avgs for agent 2: q_loss: 1770388.0, p_loss: 150.27952575683594, mean_rew: -8.07076591105508, variance: 164.26581018066406, lamda: 3.6336700916290283

steps: 2699975, episodes: 108000, mean episode reward: -598.9052312408932, agent episode reward: [-199.6350770802977, -199.6350770802977, -199.6350770802977], time: 107.232
steps: 2699975, episodes: 108000, mean episode variance: 41.84524792480469, agent episode variance: [1.15837286567688, 1.1008954601287841, 39.585979598999025], time: 107.232
Running avgs for agent 0: q_loss: 7.542116165161133, p_loss: -4.135192394256592, mean_rew: -8.062832106868196, variance: 4.633491039276123, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.609602451324463, p_loss: -4.1415696144104, mean_rew: -8.064926965625059, variance: 4.4035820960998535, lamda: 1.8764327764511108
Running avgs for agent 2: q_loss: 1796450.875, p_loss: 150.50836181640625, mean_rew: -8.068423818386687, variance: 158.3439183959961, lamda: 3.658644199371338

steps: 2724975, episodes: 109000, mean episode reward: -603.6894909665913, agent episode reward: [-201.2298303221971, -201.2298303221971, -201.2298303221971], time: 114.463
steps: 2724975, episodes: 109000, mean episode variance: 42.08983675765991, agent episode variance: [1.1579420132637024, 1.0995490412712097, 39.832345703125], time: 114.463
Running avgs for agent 0: q_loss: 7.405768871307373, p_loss: -4.135878086090088, mean_rew: -8.06241834897773, variance: 4.631768226623535, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.539490222930908, p_loss: -4.1438469886779785, mean_rew: -8.069456200991576, variance: 4.398196220397949, lamda: 1.8764326572418213
Running avgs for agent 2: q_loss: 1831783.375, p_loss: 150.60446166992188, mean_rew: -8.057646624633326, variance: 159.3293828125, lamda: 3.6836187839508057

steps: 2749975, episodes: 110000, mean episode reward: -599.2995510126132, agent episode reward: [-199.76651700420433, -199.76651700420433, -199.76651700420433], time: 111.212
steps: 2749975, episodes: 110000, mean episode variance: 42.57696255540848, agent episode variance: [1.1545170595645904, 1.0964801180362702, 40.32596537780762], time: 111.213
Running avgs for agent 0: q_loss: 7.305274486541748, p_loss: -4.133270740509033, mean_rew: -8.064621947380681, variance: 4.618068218231201, lamda: 1.7805898189544678
Running avgs for agent 1: q_loss: 7.749857425689697, p_loss: -4.149068355560303, mean_rew: -8.075205053530155, variance: 4.385920524597168, lamda: 1.8764328956604004
Running avgs for agent 2: q_loss: 1840373.0, p_loss: 150.64219665527344, mean_rew: -8.05877018131922, variance: 161.30386151123048, lamda: 3.7085931301116943

steps: 2774975, episodes: 111000, mean episode reward: -606.0794563582159, agent episode reward: [-202.02648545273863, -202.02648545273863, -202.02648545273863], time: 100.311
steps: 2774975, episodes: 111000, mean episode variance: 42.23192948198319, agent episode variance: [1.1559833512306212, 1.1023180484771729, 39.97362808227539], time: 100.312
Running avgs for agent 0: q_loss: 7.316291332244873, p_loss: -4.127448081970215, mean_rew: -8.056684100019332, variance: 4.623933792114258, lamda: 1.7805899381637573
Running avgs for agent 1: q_loss: 7.632330894470215, p_loss: -4.137959957122803, mean_rew: -8.061692992949274, variance: 4.40927267074585, lamda: 1.8765367269515991
Running avgs for agent 2: q_loss: 1866895.625, p_loss: 150.84938049316406, mean_rew: -8.063435355024856, variance: 159.89451232910156, lamda: 3.733567476272583

steps: 2799975, episodes: 112000, mean episode reward: -605.7130797464071, agent episode reward: [-201.90435991546903, -201.90435991546903, -201.90435991546903], time: 101.62
steps: 2799975, episodes: 112000, mean episode variance: 43.072247017383575, agent episode variance: [1.1571309452056884, 1.0943972611427306, 40.82071881103516], time: 101.621
Running avgs for agent 0: q_loss: 4.7162556648254395, p_loss: -4.131839752197266, mean_rew: -8.066462795951265, variance: 4.628523826599121, lamda: 1.7810251712799072
Running avgs for agent 1: q_loss: 7.740790843963623, p_loss: -4.144042015075684, mean_rew: -8.062474144367217, variance: 4.377588748931885, lamda: 1.8770053386688232
Running avgs for agent 2: q_loss: 1894042.5, p_loss: 150.8779754638672, mean_rew: -8.05919029687327, variance: 163.28287524414063, lamda: 3.758542060852051

steps: 2824975, episodes: 113000, mean episode reward: -595.6193183090866, agent episode reward: [-198.53977276969556, -198.53977276969556, -198.53977276969556], time: 107.949
steps: 2824975, episodes: 113000, mean episode variance: 42.11897586941719, agent episode variance: [1.1622781434059144, 1.0960495936870576, 39.86064813232422], time: 107.95
Running avgs for agent 0: q_loss: 7.311990261077881, p_loss: -4.129850387573242, mean_rew: -8.061585789086038, variance: 4.649112701416016, lamda: 1.782759666442871
Running avgs for agent 1: q_loss: 7.546267986297607, p_loss: -4.135959625244141, mean_rew: -8.051566897718725, variance: 4.384198188781738, lamda: 1.8770670890808105
Running avgs for agent 2: q_loss: 1896485.625, p_loss: 150.95989990234375, mean_rew: -8.050629515371691, variance: 159.4425925292969, lamda: 3.7835161685943604

steps: 2849975, episodes: 114000, mean episode reward: -602.7717548877073, agent episode reward: [-200.92391829590244, -200.92391829590244, -200.92391829590244], time: 100.149
steps: 2849975, episodes: 114000, mean episode variance: 41.22763109374046, agent episode variance: [1.155610436439514, 1.0970132567882538, 38.975007400512695], time: 100.15
Running avgs for agent 0: q_loss: 7.25270938873291, p_loss: -4.125018119812012, mean_rew: -8.058853678845649, variance: 4.622441291809082, lamda: 1.7828025817871094
Running avgs for agent 1: q_loss: 7.55030632019043, p_loss: -4.1282548904418945, mean_rew: -8.053075270593267, variance: 4.3880534172058105, lamda: 1.8770906925201416
Running avgs for agent 2: q_loss: 1921048.625, p_loss: 151.00575256347656, mean_rew: -8.060510318343296, variance: 155.90002960205078, lamda: 3.808490514755249

steps: 2874975, episodes: 115000, mean episode reward: -598.0875249724337, agent episode reward: [-199.36250832414456, -199.36250832414456, -199.36250832414456], time: 94.352
steps: 2874975, episodes: 115000, mean episode variance: 41.539736847162246, agent episode variance: [1.15762233376503, 1.097001232147217, 39.28511328125], time: 94.353
Running avgs for agent 0: q_loss: 7.233753681182861, p_loss: -4.127883434295654, mean_rew: -8.05037044610469, variance: 4.630489349365234, lamda: 1.7828024625778198
Running avgs for agent 1: q_loss: 7.579158306121826, p_loss: -4.132641315460205, mean_rew: -8.048002390600368, variance: 4.388004779815674, lamda: 1.8772015571594238
Running avgs for agent 2: q_loss: 1955762.375, p_loss: 151.12628173828125, mean_rew: -8.055721173665955, variance: 157.140453125, lamda: 3.8334648609161377

steps: 2899975, episodes: 116000, mean episode reward: -602.9412828749334, agent episode reward: [-200.98042762497784, -200.98042762497784, -200.98042762497784], time: 90.561
steps: 2899975, episodes: 116000, mean episode variance: 43.09185021042824, agent episode variance: [1.156965411901474, 1.0967876458168029, 40.83809715270996], time: 90.562
Running avgs for agent 0: q_loss: 7.185354232788086, p_loss: -4.126707077026367, mean_rew: -8.054727950058517, variance: 4.627861976623535, lamda: 1.7828024625778198
Running avgs for agent 1: q_loss: 7.540308475494385, p_loss: -4.135440826416016, mean_rew: -8.057721766503365, variance: 4.387150287628174, lamda: 1.8772016763687134
Running avgs for agent 2: q_loss: 1961380.75, p_loss: 151.16354370117188, mean_rew: -8.05583542353602, variance: 163.35238861083985, lamda: 3.8584389686584473

steps: 2924975, episodes: 117000, mean episode reward: -590.4306469395895, agent episode reward: [-196.8102156465298, -196.8102156465298, -196.8102156465298], time: 90.149
steps: 2924975, episodes: 117000, mean episode variance: 42.41226218676567, agent episode variance: [1.1499525172710419, 1.0976963272094726, 40.16461334228516], time: 90.149
Running avgs for agent 0: q_loss: 7.2385406494140625, p_loss: -4.128020763397217, mean_rew: -8.055334211391274, variance: 4.5998101234436035, lamda: 1.7828025817871094
Running avgs for agent 1: q_loss: 7.4599080085754395, p_loss: -4.1335577964782715, mean_rew: -8.048566983562816, variance: 4.390785217285156, lamda: 1.8772016763687134
Running avgs for agent 2: q_loss: 1986137.875, p_loss: 151.28759765625, mean_rew: -8.058386502985986, variance: 160.65845336914063, lamda: 3.883413553237915

steps: 2949975, episodes: 118000, mean episode reward: -595.4812121482108, agent episode reward: [-198.49373738273695, -198.49373738273695, -198.49373738273695], time: 90.442
steps: 2949975, episodes: 118000, mean episode variance: 41.16830763673782, agent episode variance: [1.156522159099579, 1.0940057077407837, 38.917779769897464], time: 90.443
Running avgs for agent 0: q_loss: 7.191812992095947, p_loss: -4.123620986938477, mean_rew: -8.047475711129334, variance: 4.6260881423950195, lamda: 1.7828024625778198
Running avgs for agent 1: q_loss: 7.463635444641113, p_loss: -4.131083965301514, mean_rew: -8.04656919125589, variance: 4.376022815704346, lamda: 1.8772015571594238
Running avgs for agent 2: q_loss: 2014098.75, p_loss: 151.33314514160156, mean_rew: -8.047635581670473, variance: 155.67111907958986, lamda: 3.9083878993988037

steps: 2974975, episodes: 119000, mean episode reward: -596.8427803212197, agent episode reward: [-198.94759344040654, -198.94759344040654, -198.94759344040654], time: 91.421
steps: 2974975, episodes: 119000, mean episode variance: 42.49973306465149, agent episode variance: [1.1495251982212067, 1.0930584218502044, 40.25714944458008], time: 91.421
Running avgs for agent 0: q_loss: 7.223823547363281, p_loss: -4.126694202423096, mean_rew: -8.05452614911417, variance: 4.598100662231445, lamda: 1.7828024625778198
Running avgs for agent 1: q_loss: 7.518972873687744, p_loss: -4.130083084106445, mean_rew: -8.043367565988916, variance: 4.3722333908081055, lamda: 1.8772016763687134
Running avgs for agent 2: q_loss: 2034143.0, p_loss: 151.5095672607422, mean_rew: -8.05575162886165, variance: 161.02859777832032, lamda: 3.9333622455596924

steps: 2999975, episodes: 120000, mean episode reward: -593.0857477785019, agent episode reward: [-197.69524925950063, -197.69524925950063, -197.69524925950063], time: 92.941
steps: 2999975, episodes: 120000, mean episode variance: 42.26236451268196, agent episode variance: [1.1549722862243652, 1.0962978508472443, 40.01109437561035], time: 92.942
Running avgs for agent 0: q_loss: 7.167212963104248, p_loss: -4.1181159019470215, mean_rew: -8.041093440880468, variance: 4.619889259338379, lamda: 1.7828025817871094
Running avgs for agent 1: q_loss: 7.36622953414917, p_loss: -4.123452663421631, mean_rew: -8.044250072902317, variance: 4.385191440582275, lamda: 1.8772016763687134
Running avgs for agent 2: q_loss: 2034645.75, p_loss: 151.34014892578125, mean_rew: -8.038133571578742, variance: 160.0443775024414, lamda: 3.958336353302002

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -591.689871454209, agent episode reward: [-197.229957151403, -197.229957151403, -197.229957151403], time: 72.066
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 72.066
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -593.5532623143005, agent episode reward: [-197.8510874381002, -197.8510874381002, -197.8510874381002], time: 78.542
steps: 49975, episodes: 2000, mean episode variance: 50.916197549581526, agent episode variance: [0.9422128837108612, 0.8173523721694946, 49.15663229370117], time: 78.543
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.89842505178035, variance: 3.861528158187866, lamda: 1.7827931642532349
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.8992933119978, variance: 3.3498048782348633, lamda: 1.8772085905075073
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.890159379875403, variance: 201.46160888671875, lamda: 3.970874309539795

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567759439.1090533: line 9: --exp_var_alpha: command not found
