# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 23.72 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies7/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies7/02-non-linear-exp_var/
Job <1090609> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc265>>
arglist.u_estimation True
2019-09-06 04:45:45.986308: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -526.9148003296627, agent episode reward: [-175.63826677655427, -175.63826677655427, -175.63826677655427], time: 52.478
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 52.479
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -641.5919592182117, agent episode reward: [-213.86398640607058, -213.86398640607058, -213.86398640607058], time: 76.937
steps: 49975, episodes: 2000, mean episode variance: 8.297902528584004, agent episode variance: [2.790763451695442, 2.738223695799708, 2.7689153810888527], time: 76.937
Running avgs for agent 0: q_loss: 88.50537109375, p_loss: -5.243789196014404, mean_rew: -7.4607816850365785, variance: 11.437555129899353, lamda: 1.0092685222625732
Running avgs for agent 1: q_loss: 83.73597717285156, p_loss: -4.8857622146606445, mean_rew: -7.463322195989368, variance: 11.22222900390625, lamda: 1.0083391666412354
Running avgs for agent 2: q_loss: 110.51556396484375, p_loss: -5.272236347198486, mean_rew: -7.454852608471113, variance: 11.348013856921527, lamda: 1.0101438760757446

steps: 74975, episodes: 3000, mean episode reward: -861.7314839673031, agent episode reward: [-287.243827989101, -287.243827989101, -287.243827989101], time: 73.84
steps: 74975, episodes: 3000, mean episode variance: 6.165078333616257, agent episode variance: [2.0443611187934874, 2.057032888889313, 2.0636843259334565], time: 73.841
Running avgs for agent 0: q_loss: 40.75517272949219, p_loss: -4.561760425567627, mean_rew: -8.324806824480769, variance: 8.177444458007812, lamda: 1.0312772989273071
Running avgs for agent 1: q_loss: 37.24369812011719, p_loss: -4.480770111083984, mean_rew: -8.337378983275881, variance: 8.228132247924805, lamda: 1.027532696723938
Running avgs for agent 2: q_loss: 40.58158874511719, p_loss: -4.554722785949707, mean_rew: -8.327623209690058, variance: 8.254737854003906, lamda: 1.032477855682373

steps: 99975, episodes: 4000, mean episode reward: -769.5473646716558, agent episode reward: [-256.5157882238853, -256.5157882238853, -256.5157882238853], time: 74.674
steps: 99975, episodes: 4000, mean episode variance: 6.538391134738922, agent episode variance: [2.164385184764862, 2.2076849961280822, 2.166320953845978], time: 74.674
Running avgs for agent 0: q_loss: 42.60146713256836, p_loss: -4.67728328704834, mean_rew: -9.207483810425758, variance: 8.657541275024414, lamda: 1.0563148260116577
Running avgs for agent 1: q_loss: 55.39909362792969, p_loss: -4.790835380554199, mean_rew: -9.219569912943125, variance: 8.830739974975586, lamda: 1.052309274673462
Running avgs for agent 2: q_loss: 44.854461669921875, p_loss: -4.725525379180908, mean_rew: -9.201843516542791, variance: 8.665283203125, lamda: 1.0574336051940918

steps: 124975, episodes: 5000, mean episode reward: -768.7058862636311, agent episode reward: [-256.23529542121037, -256.23529542121037, -256.23529542121037], time: 73.461
steps: 124975, episodes: 5000, mean episode variance: 6.5656363372802735, agent episode variance: [2.207670286178589, 2.1833050260543825, 2.1746610250473024], time: 73.461
Running avgs for agent 0: q_loss: 35.78940200805664, p_loss: -4.777158737182617, mean_rew: -9.476464530886192, variance: 8.830681800842285, lamda: 1.0808155536651611
Running avgs for agent 1: q_loss: 55.6753044128418, p_loss: -4.851231575012207, mean_rew: -9.478557925241137, variance: 8.733221054077148, lamda: 1.0773324966430664
Running avgs for agent 2: q_loss: 46.94449996948242, p_loss: -4.855019569396973, mean_rew: -9.473155173287703, variance: 8.698644638061523, lamda: 1.0825055837631226

steps: 149975, episodes: 6000, mean episode reward: -715.2995310130901, agent episode reward: [-238.4331770043634, -238.4331770043634, -238.4331770043634], time: 77.191
steps: 149975, episodes: 6000, mean episode variance: 6.465001750469208, agent episode variance: [2.166724452495575, 2.165380401611328, 2.1328968963623045], time: 77.191
Running avgs for agent 0: q_loss: 30.052106857299805, p_loss: -4.845168590545654, mean_rew: -9.528729524524655, variance: 8.66689682006836, lamda: 1.1031599044799805
Running avgs for agent 1: q_loss: 46.59111785888672, p_loss: -4.867844104766846, mean_rew: -9.53932875434945, variance: 8.66152286529541, lamda: 1.1023370027542114
Running avgs for agent 2: q_loss: 52.227630615234375, p_loss: -4.875344753265381, mean_rew: -9.535242778722404, variance: 8.531587600708008, lamda: 1.1075493097305298

steps: 174975, episodes: 7000, mean episode reward: -694.4961737599839, agent episode reward: [-231.49872458666124, -231.49872458666124, -231.49872458666124], time: 79.561
steps: 174975, episodes: 7000, mean episode variance: 6.324227200269699, agent episode variance: [2.1245854058265685, 2.106778180360794, 2.0928636140823365], time: 79.561
Running avgs for agent 0: q_loss: 29.88077735900879, p_loss: -4.820343971252441, mean_rew: -9.501764543487239, variance: 8.49834156036377, lamda: 1.1240382194519043
Running avgs for agent 1: q_loss: 48.39747619628906, p_loss: -4.855696201324463, mean_rew: -9.499209054733507, variance: 8.427112579345703, lamda: 1.1272919178009033
Running avgs for agent 2: q_loss: 47.90864944458008, p_loss: -4.81022834777832, mean_rew: -9.51237091130487, variance: 8.371454238891602, lamda: 1.1325832605361938

steps: 199975, episodes: 8000, mean episode reward: -705.5915073142826, agent episode reward: [-235.1971691047609, -235.1971691047609, -235.1971691047609], time: 80.28
steps: 199975, episodes: 8000, mean episode variance: 6.24081610751152, agent episode variance: [2.124827913045883, 2.06407790851593, 2.051910285949707], time: 80.28
Running avgs for agent 0: q_loss: 42.77398681640625, p_loss: -4.81394100189209, mean_rew: -9.488966904300469, variance: 8.499311447143555, lamda: 1.141988754272461
Running avgs for agent 1: q_loss: 44.48490905761719, p_loss: -4.840752601623535, mean_rew: -9.465828248771333, variance: 8.256312370300293, lamda: 1.151483416557312
Running avgs for agent 2: q_loss: 41.89203643798828, p_loss: -4.801018714904785, mean_rew: -9.472030654846835, variance: 8.207640647888184, lamda: 1.1576132774353027

steps: 224975, episodes: 9000, mean episode reward: -785.4176040004045, agent episode reward: [-261.8058680001348, -261.8058680001348, -261.8058680001348], time: 80.032
steps: 224975, episodes: 9000, mean episode variance: 6.167635634183884, agent episode variance: [2.0994471187591555, 2.043097048282623, 2.025091467142105], time: 80.033
Running avgs for agent 0: q_loss: 42.23484420776367, p_loss: -4.8478498458862305, mean_rew: -9.53396622859139, variance: 8.397788047790527, lamda: 1.1497926712036133
Running avgs for agent 1: q_loss: 57.6209602355957, p_loss: -4.872463703155518, mean_rew: -9.526055805130593, variance: 8.172388076782227, lamda: 1.175592064857483
Running avgs for agent 2: q_loss: 41.44782638549805, p_loss: -4.840678691864014, mean_rew: -9.53378888283576, variance: 8.10036563873291, lamda: 1.1826199293136597

steps: 249975, episodes: 10000, mean episode reward: -804.6956409809126, agent episode reward: [-268.2318803269709, -268.2318803269709, -268.2318803269709], time: 80.038
steps: 249975, episodes: 10000, mean episode variance: 6.156923665165901, agent episode variance: [2.1166989994049072, 2.025061991810799, 2.0151626739501953], time: 80.038
Running avgs for agent 0: q_loss: 41.24186706542969, p_loss: -4.878964424133301, mean_rew: -9.64480148171367, variance: 8.466794967651367, lamda: 1.154213309288025
Running avgs for agent 1: q_loss: 43.102970123291016, p_loss: -4.9341044425964355, mean_rew: -9.649428570744485, variance: 8.100247383117676, lamda: 1.1982208490371704
Running avgs for agent 2: q_loss: 45.568824768066406, p_loss: -4.873602390289307, mean_rew: -9.642681818197445, variance: 8.060650825500488, lamda: 1.2069520950317383

steps: 274975, episodes: 11000, mean episode reward: -850.5396394200458, agent episode reward: [-283.5132131400153, -283.5132131400153, -283.5132131400153], time: 81.06
steps: 274975, episodes: 11000, mean episode variance: 6.148847511291504, agent episode variance: [2.144671396255493, 2.0117778668403625, 1.9923982481956481], time: 81.061
Running avgs for agent 0: q_loss: 40.80433654785156, p_loss: -4.953280448913574, mean_rew: -9.774798698975173, variance: 8.578685760498047, lamda: 1.1578577756881714
Running avgs for agent 1: q_loss: 42.600677490234375, p_loss: -4.993298053741455, mean_rew: -9.788160493974344, variance: 8.047111511230469, lamda: 1.221841812133789
Running avgs for agent 2: q_loss: 40.75802993774414, p_loss: -4.932755947113037, mean_rew: -9.771405992496607, variance: 7.969592571258545, lamda: 1.2311787605285645

steps: 299975, episodes: 12000, mean episode reward: -824.9302170050536, agent episode reward: [-274.9767390016845, -274.9767390016845, -274.9767390016845], time: 83.396
steps: 299975, episodes: 12000, mean episode variance: 6.170065968990326, agent episode variance: [2.149609005689621, 2.01804204082489, 2.0024149224758148], time: 83.397
Running avgs for agent 0: q_loss: 42.645320892333984, p_loss: -5.018517017364502, mean_rew: -9.906011624136678, variance: 8.598435401916504, lamda: 1.1630014181137085
Running avgs for agent 1: q_loss: 47.97041320800781, p_loss: -5.0434746742248535, mean_rew: -9.917223924849582, variance: 8.072168350219727, lamda: 1.2456669807434082
Running avgs for agent 2: q_loss: 46.485069274902344, p_loss: -5.021090984344482, mean_rew: -9.916512337225523, variance: 8.009658813476562, lamda: 1.2523573637008667

steps: 324975, episodes: 13000, mean episode reward: -832.8424628578925, agent episode reward: [-277.6141542859642, -277.6141542859642, -277.6141542859642], time: 87.917
steps: 324975, episodes: 13000, mean episode variance: 6.11056626868248, agent episode variance: [2.139825785636902, 1.9948307116031647, 1.9759097714424134], time: 87.918
Running avgs for agent 0: q_loss: 42.788597106933594, p_loss: -5.059398174285889, mean_rew: -9.985968140049176, variance: 8.55930233001709, lamda: 1.169335126876831
Running avgs for agent 1: q_loss: 50.643516540527344, p_loss: -5.095174789428711, mean_rew: -10.00819839975879, variance: 7.979323387145996, lamda: 1.2684074640274048
Running avgs for agent 2: q_loss: 33.34895706176758, p_loss: -5.059948444366455, mean_rew: -9.998025399720353, variance: 7.90363883972168, lamda: 1.2706395387649536

steps: 349975, episodes: 14000, mean episode reward: -834.2067382872185, agent episode reward: [-278.06891276240617, -278.06891276240617, -278.06891276240617], time: 85.404
steps: 349975, episodes: 14000, mean episode variance: 6.110813651323318, agent episode variance: [2.176062295913696, 1.9691214847564698, 1.9656298706531525], time: 85.405
Running avgs for agent 0: q_loss: 43.41204071044922, p_loss: -5.105893135070801, mean_rew: -10.106181606232976, variance: 8.70425033569336, lamda: 1.1744121313095093
Running avgs for agent 1: q_loss: 42.45075607299805, p_loss: -5.135314464569092, mean_rew: -10.079238286584987, variance: 7.876486301422119, lamda: 1.289520025253296
Running avgs for agent 2: q_loss: 36.56675720214844, p_loss: -5.109639644622803, mean_rew: -10.090304407836825, variance: 7.86251974105835, lamda: 1.2857489585876465

steps: 374975, episodes: 15000, mean episode reward: -817.8613449215833, agent episode reward: [-272.6204483071944, -272.6204483071944, -272.6204483071944], time: 88.114
steps: 374975, episodes: 15000, mean episode variance: 6.087252938508987, agent episode variance: [2.1854275043010714, 1.9450065717697143, 1.956818862438202], time: 88.115
Running avgs for agent 0: q_loss: 42.475502014160156, p_loss: -5.12673807144165, mean_rew: -10.146933576375774, variance: 8.74170970916748, lamda: 1.180026650428772
Running avgs for agent 1: q_loss: 45.08980941772461, p_loss: -5.152038097381592, mean_rew: -10.154995609577076, variance: 7.780026435852051, lamda: 1.3109911680221558
Running avgs for agent 2: q_loss: 45.084373474121094, p_loss: -5.152743339538574, mean_rew: -10.148116261119336, variance: 7.827275276184082, lamda: 1.3012462854385376

steps: 399975, episodes: 16000, mean episode reward: -858.4908168702489, agent episode reward: [-286.16360562341623, -286.16360562341623, -286.16360562341623], time: 84.493
steps: 399975, episodes: 16000, mean episode variance: 6.041963145017624, agent episode variance: [2.1711017088890077, 1.9212393531799317, 1.9496220829486848], time: 84.493
Running avgs for agent 0: q_loss: 35.536556243896484, p_loss: -5.169122219085693, mean_rew: -10.217577454603838, variance: 8.684406280517578, lamda: 1.190556526184082
Running avgs for agent 1: q_loss: 48.530967712402344, p_loss: -5.189177513122559, mean_rew: -10.209461038805966, variance: 7.684957504272461, lamda: 1.3346672058105469
Running avgs for agent 2: q_loss: 51.972625732421875, p_loss: -5.174610614776611, mean_rew: -10.212223928827243, variance: 7.798488140106201, lamda: 1.3141567707061768

steps: 424975, episodes: 17000, mean episode reward: -864.0064331157234, agent episode reward: [-288.00214437190783, -288.00214437190783, -288.00214437190783], time: 88.709
steps: 424975, episodes: 17000, mean episode variance: 5.994015209913254, agent episode variance: [2.1439476675987246, 1.9023424050807953, 1.947725137233734], time: 88.71
Running avgs for agent 0: q_loss: 40.56858825683594, p_loss: -5.196478366851807, mean_rew: -10.295691064862593, variance: 8.575791358947754, lamda: 1.2080780267715454
Running avgs for agent 1: q_loss: 58.52892303466797, p_loss: -5.2162675857543945, mean_rew: -10.285327813954696, variance: 7.60936975479126, lamda: 1.3574981689453125
Running avgs for agent 2: q_loss: 52.072628021240234, p_loss: -5.193716049194336, mean_rew: -10.276957889889628, variance: 7.790900230407715, lamda: 1.3218894004821777

steps: 449975, episodes: 18000, mean episode reward: -901.2629503481681, agent episode reward: [-300.4209834493893, -300.4209834493893, -300.4209834493893], time: 119.291
steps: 449975, episodes: 18000, mean episode variance: 6.030301677703857, agent episode variance: [2.165816589355469, 1.8800229921340943, 1.9844620962142945], time: 119.291
Running avgs for agent 0: q_loss: 44.76849365234375, p_loss: -5.224545478820801, mean_rew: -10.3705546835382, variance: 8.663265228271484, lamda: 1.215118408203125
Running avgs for agent 1: q_loss: 63.200645446777344, p_loss: -5.271989822387695, mean_rew: -10.368440772769842, variance: 7.520092487335205, lamda: 1.3732465505599976
Running avgs for agent 2: q_loss: 52.13642501831055, p_loss: -5.242993354797363, mean_rew: -10.38835649045862, variance: 7.937848091125488, lamda: 1.328741431236267

steps: 474975, episodes: 19000, mean episode reward: -872.3565186358127, agent episode reward: [-290.7855062119375, -290.7855062119375, -290.7855062119375], time: 114.362
steps: 474975, episodes: 19000, mean episode variance: 6.028545236110687, agent episode variance: [2.1405888886451723, 1.9173850717544556, 1.9705712757110596], time: 114.362
Running avgs for agent 0: q_loss: 45.33381271362305, p_loss: -5.262781143188477, mean_rew: -10.432426557394944, variance: 8.562355041503906, lamda: 1.2210646867752075
Running avgs for agent 1: q_loss: 63.872467041015625, p_loss: -5.316366195678711, mean_rew: -10.462990238952113, variance: 7.669539928436279, lamda: 1.3835183382034302
Running avgs for agent 2: q_loss: 52.90789031982422, p_loss: -5.28178596496582, mean_rew: -10.45590940519185, variance: 7.882285118103027, lamda: 1.334604024887085

steps: 499975, episodes: 20000, mean episode reward: -872.3840484428631, agent episode reward: [-290.7946828142877, -290.7946828142877, -290.7946828142877], time: 115.745
steps: 499975, episodes: 20000, mean episode variance: 6.085502323627472, agent episode variance: [2.1887193865776062, 1.9076358819007873, 1.9891470551490784], time: 115.745
Running avgs for agent 0: q_loss: 42.58743667602539, p_loss: -5.302165985107422, mean_rew: -10.523888781106493, variance: 8.754878044128418, lamda: 1.2274264097213745
Running avgs for agent 1: q_loss: 63.44757080078125, p_loss: -5.3477959632873535, mean_rew: -10.519976352804571, variance: 7.6305437088012695, lamda: 1.393130898475647
Running avgs for agent 2: q_loss: 50.65958023071289, p_loss: -5.336047172546387, mean_rew: -10.540608807577833, variance: 7.956587791442871, lamda: 1.339234471321106

steps: 524975, episodes: 21000, mean episode reward: -848.2906202837509, agent episode reward: [-282.7635400945836, -282.7635400945836, -282.7635400945836], time: 115.756
steps: 524975, episodes: 21000, mean episode variance: 5.954386080503464, agent episode variance: [2.127535729885101, 1.8610349516868592, 1.9658153989315033], time: 115.756
Running avgs for agent 0: q_loss: 32.42966842651367, p_loss: -5.345469951629639, mean_rew: -10.557877081076674, variance: 8.51014232635498, lamda: 1.2454501390457153
Running avgs for agent 1: q_loss: 60.66786575317383, p_loss: -5.372262001037598, mean_rew: -10.566058381686585, variance: 7.44413948059082, lamda: 1.4040080308914185
Running avgs for agent 2: q_loss: 40.267974853515625, p_loss: -5.340415000915527, mean_rew: -10.564826963498612, variance: 7.863261699676514, lamda: 1.3540223836898804

steps: 549975, episodes: 22000, mean episode reward: -840.9063169902281, agent episode reward: [-280.30210566340935, -280.30210566340935, -280.30210566340935], time: 118.127
steps: 549975, episodes: 22000, mean episode variance: 5.916860251903534, agent episode variance: [2.08300368309021, 1.8740203020572663, 1.9598362667560578], time: 118.127
Running avgs for agent 0: q_loss: 33.12641143798828, p_loss: -5.3484578132629395, mean_rew: -10.579804703486458, variance: 8.332015037536621, lamda: 1.2659704685211182
Running avgs for agent 1: q_loss: 48.673614501953125, p_loss: -5.371591567993164, mean_rew: -10.58872052030177, variance: 7.4960808753967285, lamda: 1.4204844236373901
Running avgs for agent 2: q_loss: 41.82484817504883, p_loss: -5.338037967681885, mean_rew: -10.603509115421978, variance: 7.839344501495361, lamda: 1.3652257919311523

steps: 574975, episodes: 23000, mean episode reward: -845.5928675377464, agent episode reward: [-281.86428917924883, -281.86428917924883, -281.86428917924883], time: 168.55
steps: 574975, episodes: 23000, mean episode variance: 5.880867372274399, agent episode variance: [2.066014892101288, 1.8750459144115448, 1.9398065657615662], time: 168.551
Running avgs for agent 0: q_loss: 32.13312530517578, p_loss: -5.358788013458252, mean_rew: -10.621570717156542, variance: 8.264060020446777, lamda: 1.2854316234588623
Running avgs for agent 1: q_loss: 52.51633834838867, p_loss: -5.407078742980957, mean_rew: -10.642889046095329, variance: 7.500183582305908, lamda: 1.43499755859375
Running avgs for agent 2: q_loss: 42.008548736572266, p_loss: -5.355976581573486, mean_rew: -10.630344232436816, variance: 7.7592267990112305, lamda: 1.382261037826538

steps: 599975, episodes: 24000, mean episode reward: -841.2242083297036, agent episode reward: [-280.4080694432346, -280.4080694432346, -280.4080694432346], time: 169.557
steps: 599975, episodes: 24000, mean episode variance: 5.850616980075836, agent episode variance: [2.0432758369445803, 1.8579782104492188, 1.9493629326820374], time: 169.557
Running avgs for agent 0: q_loss: 38.47040939331055, p_loss: -5.3766679763793945, mean_rew: -10.655555402383358, variance: 8.173103332519531, lamda: 1.3038839101791382
Running avgs for agent 1: q_loss: 44.495399475097656, p_loss: -5.393521308898926, mean_rew: -10.64660648032168, variance: 7.431912899017334, lamda: 1.4550087451934814
Running avgs for agent 2: q_loss: 45.82255554199219, p_loss: -5.348074436187744, mean_rew: -10.651068257113897, variance: 7.797451496124268, lamda: 1.4004124402999878

steps: 624975, episodes: 25000, mean episode reward: -847.7429942994446, agent episode reward: [-282.58099809981485, -282.58099809981485, -282.58099809981485], time: 166.348
steps: 624975, episodes: 25000, mean episode variance: 5.77019964003563, agent episode variance: [2.050597361087799, 1.8160678873062133, 1.9035343916416168], time: 166.348
Running avgs for agent 0: q_loss: 46.92303466796875, p_loss: -5.382465839385986, mean_rew: -10.698602461991399, variance: 8.20238971710205, lamda: 1.3111106157302856
Running avgs for agent 1: q_loss: 41.16555404663086, p_loss: -5.424762725830078, mean_rew: -10.695034424967032, variance: 7.264271259307861, lamda: 1.4767441749572754
Running avgs for agent 2: q_loss: 52.2904052734375, p_loss: -5.371252059936523, mean_rew: -10.668543254802316, variance: 7.614137649536133, lamda: 1.407248854637146

steps: 649975, episodes: 26000, mean episode reward: -835.5111893495442, agent episode reward: [-278.50372978318137, -278.50372978318137, -278.50372978318137], time: 170.582
steps: 649975, episodes: 26000, mean episode variance: 5.750797538995743, agent episode variance: [2.0283820705413818, 1.7979144163131713, 1.9245010521411896], time: 170.583
Running avgs for agent 0: q_loss: 46.97572708129883, p_loss: -5.391749858856201, mean_rew: -10.698929262296861, variance: 8.11352825164795, lamda: 1.3152811527252197
Running avgs for agent 1: q_loss: 41.240055084228516, p_loss: -5.420551300048828, mean_rew: -10.710982024029159, variance: 7.191657543182373, lamda: 1.4946534633636475
Running avgs for agent 2: q_loss: 52.15685272216797, p_loss: -5.398422718048096, mean_rew: -10.697829507438668, variance: 7.698004245758057, lamda: 1.4086101055145264

steps: 674975, episodes: 27000, mean episode reward: -840.4076455984628, agent episode reward: [-280.13588186615425, -280.13588186615425, -280.13588186615425], time: 168.761
steps: 674975, episodes: 27000, mean episode variance: 5.739218479156494, agent episode variance: [2.0414042215347292, 1.7856957850456239, 1.9121184725761413], time: 168.761
Running avgs for agent 0: q_loss: 46.299739837646484, p_loss: -5.383009433746338, mean_rew: -10.706163317928622, variance: 8.165616989135742, lamda: 1.3197391033172607
Running avgs for agent 1: q_loss: 40.60707092285156, p_loss: -5.423299312591553, mean_rew: -10.732335680484029, variance: 7.142783164978027, lamda: 1.5147178173065186
Running avgs for agent 2: q_loss: 52.17056655883789, p_loss: -5.413151264190674, mean_rew: -10.741259709908409, variance: 7.648474216461182, lamda: 1.4118794202804565

steps: 699975, episodes: 28000, mean episode reward: -862.1439781742409, agent episode reward: [-287.3813260580803, -287.3813260580803, -287.3813260580803], time: 165.857
steps: 699975, episodes: 28000, mean episode variance: 5.694843172550201, agent episode variance: [2.0292465043067933, 1.7557997627258302, 1.909796905517578], time: 165.857
Running avgs for agent 0: q_loss: 34.892303466796875, p_loss: -5.405498504638672, mean_rew: -10.730746343016094, variance: 8.116986274719238, lamda: 1.3321640491485596
Running avgs for agent 1: q_loss: 40.9496955871582, p_loss: -5.421756744384766, mean_rew: -10.728575044338049, variance: 7.023199081420898, lamda: 1.5325613021850586
Running avgs for agent 2: q_loss: 51.649810791015625, p_loss: -5.409678936004639, mean_rew: -10.734509919055029, variance: 7.639187812805176, lamda: 1.414850115776062

steps: 724975, episodes: 29000, mean episode reward: -892.5946595130296, agent episode reward: [-297.5315531710099, -297.5315531710099, -297.5315531710099], time: 158.509
steps: 724975, episodes: 29000, mean episode variance: 5.664473432540894, agent episode variance: [1.9904963483810425, 1.7680127799510956, 1.9059643042087555], time: 158.509
Running avgs for agent 0: q_loss: 35.47556686401367, p_loss: -5.423172950744629, mean_rew: -10.750645199989565, variance: 7.9619855880737305, lamda: 1.350010871887207
Running avgs for agent 1: q_loss: 60.374725341796875, p_loss: -5.444436073303223, mean_rew: -10.768037259356776, variance: 7.07205057144165, lamda: 1.543735146522522
Running avgs for agent 2: q_loss: 50.93818664550781, p_loss: -5.426401138305664, mean_rew: -10.78459661348728, variance: 7.623857498168945, lamda: 1.4200830459594727

steps: 749975, episodes: 30000, mean episode reward: -910.8720215436144, agent episode reward: [-303.62400718120483, -303.62400718120483, -303.62400718120483], time: 161.743
steps: 749975, episodes: 30000, mean episode variance: 5.629163508653641, agent episode variance: [1.9975378682613374, 1.7356979806423187, 1.8959276597499848], time: 161.743
Running avgs for agent 0: q_loss: 48.418426513671875, p_loss: -5.435393810272217, mean_rew: -10.809239805551822, variance: 7.990151405334473, lamda: 1.3629199266433716
Running avgs for agent 1: q_loss: 58.632904052734375, p_loss: -5.484206199645996, mean_rew: -10.831966448503572, variance: 6.942791938781738, lamda: 1.5486308336257935
Running avgs for agent 2: q_loss: 39.082664489746094, p_loss: -5.451578140258789, mean_rew: -10.824520229433295, variance: 7.58371114730835, lamda: 1.426560401916504

steps: 774975, episodes: 31000, mean episode reward: -907.6582700784151, agent episode reward: [-302.552756692805, -302.552756692805, -302.552756692805], time: 161.257
steps: 774975, episodes: 31000, mean episode variance: 5.61727980542183, agent episode variance: [1.9928333973884582, 1.7287034583091736, 1.8957429497241973], time: 161.257
Running avgs for agent 0: q_loss: 47.89048385620117, p_loss: -5.458754539489746, mean_rew: -10.862501629348298, variance: 7.971333980560303, lamda: 1.3684436082839966
Running avgs for agent 1: q_loss: 57.16885757446289, p_loss: -5.488021373748779, mean_rew: -10.857084912988894, variance: 6.91481351852417, lamda: 1.553226113319397
Running avgs for agent 2: q_loss: 48.344078063964844, p_loss: -5.4679741859436035, mean_rew: -10.86350718333984, variance: 7.582971572875977, lamda: 1.4365441799163818

steps: 799975, episodes: 32000, mean episode reward: -910.9066239057017, agent episode reward: [-303.63554130190056, -303.63554130190056, -303.63554130190056], time: 161.142
steps: 799975, episodes: 32000, mean episode variance: 5.631047634124756, agent episode variance: [1.9980928823947905, 1.7453684222698211, 1.887586329460144], time: 161.143
Running avgs for agent 0: q_loss: 48.89430618286133, p_loss: -5.468835353851318, mean_rew: -10.895619046129287, variance: 7.992371082305908, lamda: 1.3730889558792114
Running avgs for agent 1: q_loss: 56.80010986328125, p_loss: -5.515262603759766, mean_rew: -10.90380028895124, variance: 6.981473445892334, lamda: 1.556957483291626
Running avgs for agent 2: q_loss: 48.013858795166016, p_loss: -5.4847636222839355, mean_rew: -10.89334383279062, variance: 7.550345420837402, lamda: 1.443127155303955

steps: 824975, episodes: 33000, mean episode reward: -933.7538858144162, agent episode reward: [-311.2512952714721, -311.2512952714721, -311.2512952714721], time: 159.698
steps: 824975, episodes: 33000, mean episode variance: 5.676784719705582, agent episode variance: [1.9956512372493744, 1.7567359943389893, 1.9243974881172181], time: 159.699
Running avgs for agent 0: q_loss: 48.386905670166016, p_loss: -5.485713958740234, mean_rew: -10.936661942877771, variance: 7.98260498046875, lamda: 1.3787978887557983
Running avgs for agent 1: q_loss: 55.725868225097656, p_loss: -5.529422760009766, mean_rew: -10.939314211011478, variance: 7.026944160461426, lamda: 1.5614315271377563
Running avgs for agent 2: q_loss: 49.23916244506836, p_loss: -5.497949123382568, mean_rew: -10.945456356643335, variance: 7.697589874267578, lamda: 1.4466729164123535

steps: 849975, episodes: 34000, mean episode reward: -911.6505871958814, agent episode reward: [-303.8835290652937, -303.8835290652937, -303.8835290652937], time: 157.418
steps: 849975, episodes: 34000, mean episode variance: 5.6702405180931095, agent episode variance: [2.0032664914131164, 1.7680823409557342, 1.8988916857242584], time: 157.419
Running avgs for agent 0: q_loss: 39.289100646972656, p_loss: -5.51590633392334, mean_rew: -10.983929840331262, variance: 8.013066291809082, lamda: 1.3867313861846924
Running avgs for agent 1: q_loss: 54.30852127075195, p_loss: -5.536890506744385, mean_rew: -10.96883517400064, variance: 7.072329521179199, lamda: 1.5645575523376465
Running avgs for agent 2: q_loss: 49.390724182128906, p_loss: -5.525261402130127, mean_rew: -10.976021393832823, variance: 7.595566272735596, lamda: 1.4490761756896973

steps: 874975, episodes: 35000, mean episode reward: -933.542190226211, agent episode reward: [-311.18073007540363, -311.18073007540363, -311.18073007540363], time: 157.412
steps: 874975, episodes: 35000, mean episode variance: 5.696633204460144, agent episode variance: [2.0012203278541567, 1.779109936952591, 1.9163029396533966], time: 157.413
Running avgs for agent 0: q_loss: 49.775875091552734, p_loss: -5.520707130432129, mean_rew: -11.016257981301097, variance: 8.004880905151367, lamda: 1.4007477760314941
Running avgs for agent 1: q_loss: 54.488494873046875, p_loss: -5.57640266418457, mean_rew: -11.034941221675318, variance: 7.116439342498779, lamda: 1.567039966583252
Running avgs for agent 2: q_loss: 48.77102279663086, p_loss: -5.545962333679199, mean_rew: -11.028619977160345, variance: 7.6652116775512695, lamda: 1.4510276317596436

steps: 899975, episodes: 36000, mean episode reward: -935.5180916154311, agent episode reward: [-311.83936387181035, -311.83936387181035, -311.83936387181035], time: 154.421
steps: 899975, episodes: 36000, mean episode variance: 5.6699802482128145, agent episode variance: [1.970845751285553, 1.777476479291916, 1.9216580176353455], time: 154.421
Running avgs for agent 0: q_loss: 49.00705337524414, p_loss: -5.551383972167969, mean_rew: -11.050167714140084, variance: 7.883382797241211, lamda: 1.404332160949707
Running avgs for agent 1: q_loss: 45.156349182128906, p_loss: -5.581398963928223, mean_rew: -11.070336067841469, variance: 7.109906196594238, lamda: 1.5732030868530273
Running avgs for agent 2: q_loss: 42.759952545166016, p_loss: -5.566040515899658, mean_rew: -11.074341742628809, variance: 7.68663215637207, lamda: 1.454452633857727

steps: 924975, episodes: 37000, mean episode reward: -948.360392985049, agent episode reward: [-316.12013099501627, -316.12013099501627, -316.12013099501627], time: 157.919
steps: 924975, episodes: 37000, mean episode variance: 5.645842439174652, agent episode variance: [1.9756585464477538, 1.7593284738063812, 1.910855418920517], time: 157.92
Running avgs for agent 0: q_loss: 48.17192840576172, p_loss: -5.58035945892334, mean_rew: -11.125385975783976, variance: 7.902634143829346, lamda: 1.4125354290008545
Running avgs for agent 1: q_loss: 52.97428894042969, p_loss: -5.605626106262207, mean_rew: -11.107950606265357, variance: 7.037313938140869, lamda: 1.587245225906372
Running avgs for agent 2: q_loss: 42.58607864379883, p_loss: -5.583200454711914, mean_rew: -11.099090466616335, variance: 7.643421649932861, lamda: 1.4646327495574951

steps: 949975, episodes: 38000, mean episode reward: -941.9973791460814, agent episode reward: [-313.99912638202716, -313.99912638202716, -313.99912638202716], time: 158.511
steps: 949975, episodes: 38000, mean episode variance: 5.654339244365692, agent episode variance: [1.974625331401825, 1.7530803761482239, 1.9266335368156433], time: 158.511
Running avgs for agent 0: q_loss: 38.57405090332031, p_loss: -5.58505392074585, mean_rew: -11.147210267854287, variance: 7.898500919342041, lamda: 1.421492099761963
Running avgs for agent 1: q_loss: 51.26080322265625, p_loss: -5.639826774597168, mean_rew: -11.153518749311893, variance: 7.012321472167969, lamda: 1.591321587562561
Running avgs for agent 2: q_loss: 49.29265594482422, p_loss: -5.606131076812744, mean_rew: -11.149874763993587, variance: 7.706534385681152, lamda: 1.4692541360855103

steps: 974975, episodes: 39000, mean episode reward: -944.6035434532992, agent episode reward: [-314.8678478177664, -314.8678478177664, -314.8678478177664], time: 158.711
steps: 974975, episodes: 39000, mean episode variance: 5.6141618564128875, agent episode variance: [1.9529638805389404, 1.7354023110866548, 1.9257956647872925], time: 158.712
Running avgs for agent 0: q_loss: 35.896575927734375, p_loss: -5.606817245483398, mean_rew: -11.179607917025482, variance: 7.811855316162109, lamda: 1.4399123191833496
Running avgs for agent 1: q_loss: 49.169551849365234, p_loss: -5.6542887687683105, mean_rew: -11.18888175489355, variance: 6.9416093826293945, lamda: 1.5978869199752808
Running avgs for agent 2: q_loss: 48.83924102783203, p_loss: -5.619636535644531, mean_rew: -11.177406590270188, variance: 7.703182697296143, lamda: 1.470961570739746

steps: 999975, episodes: 40000, mean episode reward: -917.4403555109219, agent episode reward: [-305.813451836974, -305.813451836974, -305.813451836974], time: 157.942
steps: 999975, episodes: 40000, mean episode variance: 5.621404067277909, agent episode variance: [1.9443953981399535, 1.7519648673534394, 1.9250438017845153], time: 157.943
Running avgs for agent 0: q_loss: 34.371299743652344, p_loss: -5.641994953155518, mean_rew: -11.233722155269065, variance: 7.777581691741943, lamda: 1.4556933641433716
Running avgs for agent 1: q_loss: 49.625911712646484, p_loss: -5.672941207885742, mean_rew: -11.226148478270794, variance: 7.007859230041504, lamda: 1.59993314743042
Running avgs for agent 2: q_loss: 48.15678405761719, p_loss: -5.645554542541504, mean_rew: -11.22312412552181, variance: 7.7001752853393555, lamda: 1.4725414514541626

steps: 1024975, episodes: 41000, mean episode reward: -940.1279185911028, agent episode reward: [-313.37597286370095, -313.37597286370095, -313.37597286370095], time: 154.293
steps: 1024975, episodes: 41000, mean episode variance: 5.641045292377472, agent episode variance: [1.9245694818496704, 1.781044178724289, 1.9354316318035125], time: 154.293
Running avgs for agent 0: q_loss: 34.51210021972656, p_loss: -5.6742939949035645, mean_rew: -11.29851016589933, variance: 7.698278427124023, lamda: 1.4701846837997437
Running avgs for agent 1: q_loss: 50.72427749633789, p_loss: -5.692554473876953, mean_rew: -11.29143740438613, variance: 7.124176979064941, lamda: 1.6019984483718872
Running avgs for agent 2: q_loss: 49.385047912597656, p_loss: -5.6854658126831055, mean_rew: -11.292321018034231, variance: 7.741726398468018, lamda: 1.4753003120422363

steps: 1049975, episodes: 42000, mean episode reward: -949.0455582250082, agent episode reward: [-316.3485194083361, -316.3485194083361, -316.3485194083361], time: 154.145
steps: 1049975, episodes: 42000, mean episode variance: 5.687560680627823, agent episode variance: [1.9318098397254944, 1.7957982096672058, 1.9599526312351228], time: 154.146
Running avgs for agent 0: q_loss: 37.13116455078125, p_loss: -5.743142604827881, mean_rew: -11.419695952088285, variance: 7.72723913192749, lamda: 1.4871209859848022
Running avgs for agent 1: q_loss: 51.45954895019531, p_loss: -5.760397911071777, mean_rew: -11.415879293610532, variance: 7.183192253112793, lamda: 1.6037814617156982
Running avgs for agent 2: q_loss: 49.69062423706055, p_loss: -5.735228061676025, mean_rew: -11.408036979877835, variance: 7.839810848236084, lamda: 1.4784427881240845

steps: 1074975, episodes: 43000, mean episode reward: -927.6169993714556, agent episode reward: [-309.2056664571519, -309.2056664571519, -309.2056664571519], time: 152.155
steps: 1074975, episodes: 43000, mean episode variance: 5.654347183227539, agent episode variance: [1.912431359767914, 1.7959008967876435, 1.9460149266719817], time: 152.155
Running avgs for agent 0: q_loss: 34.268150329589844, p_loss: -5.775241374969482, mean_rew: -11.483115779629818, variance: 7.649725437164307, lamda: 1.5033818483352661
Running avgs for agent 1: q_loss: 50.06721115112305, p_loss: -5.784822940826416, mean_rew: -11.471938356013338, variance: 7.183603286743164, lamda: 1.605108618736267
Running avgs for agent 2: q_loss: 49.651241302490234, p_loss: -5.797491073608398, mean_rew: -11.486801726610256, variance: 7.784060001373291, lamda: 1.4849236011505127

steps: 1099975, episodes: 44000, mean episode reward: -939.6265557656837, agent episode reward: [-313.20885192189456, -313.20885192189456, -313.20885192189456], time: 156.998
steps: 1099975, episodes: 44000, mean episode variance: 5.654040634393692, agent episode variance: [1.9075620379447937, 1.7893413002490997, 1.9571372961997986], time: 156.998
Running avgs for agent 0: q_loss: 32.538658142089844, p_loss: -5.7826151847839355, mean_rew: -11.507370692250285, variance: 7.630248069763184, lamda: 1.5159908533096313
Running avgs for agent 1: q_loss: 49.55088806152344, p_loss: -5.814663887023926, mean_rew: -11.517271218434878, variance: 7.157365322113037, lamda: 1.6062748432159424
Running avgs for agent 2: q_loss: 49.29845428466797, p_loss: -5.793974876403809, mean_rew: -11.52465395521481, variance: 7.828548908233643, lamda: 1.4873645305633545

steps: 1124975, episodes: 45000, mean episode reward: -923.0117106632887, agent episode reward: [-307.6705702210962, -307.6705702210962, -307.6705702210962], time: 153.753
steps: 1124975, episodes: 45000, mean episode variance: 5.669735101938247, agent episode variance: [1.9040354051589965, 1.816137281894684, 1.9495624148845672], time: 153.753
Running avgs for agent 0: q_loss: 32.328651428222656, p_loss: -5.804479598999023, mean_rew: -11.572771480613483, variance: 7.616141319274902, lamda: 1.5293024778366089
Running avgs for agent 1: q_loss: 42.70138931274414, p_loss: -5.83992862701416, mean_rew: -11.582157330069899, variance: 7.2645487785339355, lamda: 1.608721137046814
Running avgs for agent 2: q_loss: 45.88768005371094, p_loss: -5.8138508796691895, mean_rew: -11.549570594163045, variance: 7.798249244689941, lamda: 1.489501953125

steps: 1149975, episodes: 46000, mean episode reward: -914.5245647505792, agent episode reward: [-304.8415215835264, -304.8415215835264, -304.8415215835264], time: 162.064
steps: 1149975, episodes: 46000, mean episode variance: 5.672677996397018, agent episode variance: [1.914515793800354, 1.79291343832016, 1.9652487642765044], time: 162.065
Running avgs for agent 0: q_loss: 33.244712829589844, p_loss: -5.84076452255249, mean_rew: -11.649436137949502, variance: 7.658062934875488, lamda: 1.5375429391860962
Running avgs for agent 1: q_loss: 35.102203369140625, p_loss: -5.860464096069336, mean_rew: -11.635044729147547, variance: 7.1716532707214355, lamda: 1.6207040548324585
Running avgs for agent 2: q_loss: 46.23928451538086, p_loss: -5.847330093383789, mean_rew: -11.630739261080235, variance: 7.860995292663574, lamda: 1.4965225458145142

steps: 1174975, episodes: 47000, mean episode reward: -899.3721469563269, agent episode reward: [-299.790715652109, -299.790715652109, -299.790715652109], time: 162.467
steps: 1174975, episodes: 47000, mean episode variance: 5.671395296573639, agent episode variance: [1.9068223123550414, 1.7981805291175843, 1.9663924551010132], time: 162.467
Running avgs for agent 0: q_loss: 33.10334014892578, p_loss: -5.8841776847839355, mean_rew: -11.704082798656527, variance: 7.627289295196533, lamda: 1.5452892780303955
Running avgs for agent 1: q_loss: 29.62493324279785, p_loss: -5.893115043640137, mean_rew: -11.690755957386264, variance: 7.192721843719482, lamda: 1.6336917877197266
Running avgs for agent 2: q_loss: 48.44740676879883, p_loss: -5.873950004577637, mean_rew: -11.686835195627886, variance: 7.865569591522217, lamda: 1.498428463935852

steps: 1199975, episodes: 48000, mean episode reward: -890.3835185764976, agent episode reward: [-296.7945061921659, -296.7945061921659, -296.7945061921659], time: 160.806
steps: 1199975, episodes: 48000, mean episode variance: 5.654336745500564, agent episode variance: [1.8939048447608948, 1.78970188331604, 1.9707300174236297], time: 160.806
Running avgs for agent 0: q_loss: 34.48149871826172, p_loss: -5.917949199676514, mean_rew: -11.775947789359853, variance: 7.575619220733643, lamda: 1.557199478149414
Running avgs for agent 1: q_loss: 30.424793243408203, p_loss: -5.934310436248779, mean_rew: -11.77301235455826, variance: 7.158807754516602, lamda: 1.6421475410461426
Running avgs for agent 2: q_loss: 48.16816329956055, p_loss: -5.91395902633667, mean_rew: -11.767576238017599, variance: 7.882919788360596, lamda: 1.4998314380645752

steps: 1224975, episodes: 49000, mean episode reward: -872.9737541409685, agent episode reward: [-290.9912513803228, -290.9912513803228, -290.9912513803228], time: 155.199
steps: 1224975, episodes: 49000, mean episode variance: 5.706018152475357, agent episode variance: [1.8987179317474365, 1.8084855744838715, 1.998814646244049], time: 155.2
Running avgs for agent 0: q_loss: 31.765037536621094, p_loss: -5.915375709533691, mean_rew: -11.785383757342535, variance: 7.594871997833252, lamda: 1.564672827720642
Running avgs for agent 1: q_loss: 35.69192123413086, p_loss: -5.950122833251953, mean_rew: -11.803161428248917, variance: 7.23394250869751, lamda: 1.6510257720947266
Running avgs for agent 2: q_loss: 48.76583480834961, p_loss: -5.935288906097412, mean_rew: -11.814678964816988, variance: 7.995258331298828, lamda: 1.5014832019805908

steps: 1249975, episodes: 50000, mean episode reward: -874.2854408226781, agent episode reward: [-291.428480274226, -291.428480274226, -291.428480274226], time: 153.063
steps: 1249975, episodes: 50000, mean episode variance: 5.667689412593842, agent episode variance: [1.8864657521247863, 1.7987168896198273, 1.9825067708492279], time: 153.063
Running avgs for agent 0: q_loss: 33.350162506103516, p_loss: -5.951530456542969, mean_rew: -11.832407283679315, variance: 7.545862674713135, lamda: 1.5733999013900757
Running avgs for agent 1: q_loss: 50.759742736816406, p_loss: -5.9658894538879395, mean_rew: -11.846344547152185, variance: 7.194867134094238, lamda: 1.6573725938796997
Running avgs for agent 2: q_loss: 48.23945999145508, p_loss: -5.965153694152832, mean_rew: -11.849017977722832, variance: 7.930027008056641, lamda: 1.5030667781829834

steps: 1274975, episodes: 51000, mean episode reward: -889.5811995155533, agent episode reward: [-296.52706650518445, -296.52706650518445, -296.52706650518445], time: 154.227
steps: 1274975, episodes: 51000, mean episode variance: 5.684266225814819, agent episode variance: [1.882364474773407, 1.810097407579422, 1.9918043434619903], time: 154.228
Running avgs for agent 0: q_loss: 32.98875427246094, p_loss: -5.965126037597656, mean_rew: -11.86879481564596, variance: 7.529458045959473, lamda: 1.5794053077697754
Running avgs for agent 1: q_loss: 48.74465560913086, p_loss: -5.973623275756836, mean_rew: -11.843427799185534, variance: 7.240389823913574, lamda: 1.6592612266540527
Running avgs for agent 2: q_loss: 49.022308349609375, p_loss: -5.956089496612549, mean_rew: -11.853846038149737, variance: 7.967217445373535, lamda: 1.5040643215179443

steps: 1299975, episodes: 52000, mean episode reward: -896.8773439910065, agent episode reward: [-298.95911466366886, -298.95911466366886, -298.95911466366886], time: 152.466
steps: 1299975, episodes: 52000, mean episode variance: 5.6462519094944, agent episode variance: [1.8825318088531495, 1.7806490824222565, 1.9830710182189941], time: 152.466
Running avgs for agent 0: q_loss: 32.573883056640625, p_loss: -5.9656805992126465, mean_rew: -11.869276451862133, variance: 7.530127048492432, lamda: 1.5885803699493408
Running avgs for agent 1: q_loss: 49.2330322265625, p_loss: -5.976928234100342, mean_rew: -11.856519014159328, variance: 7.122596263885498, lamda: 1.661050796508789
Running avgs for agent 2: q_loss: 41.08576965332031, p_loss: -5.987024307250977, mean_rew: -11.85255084930742, variance: 7.932284355163574, lamda: 1.5107941627502441

steps: 1324975, episodes: 53000, mean episode reward: -894.176057819245, agent episode reward: [-298.05868593974833, -298.05868593974833, -298.05868593974833], time: 159.085
steps: 1324975, episodes: 53000, mean episode variance: 5.626709733486176, agent episode variance: [1.87272873544693, 1.7930984721183776, 1.960882525920868], time: 159.085
Running avgs for agent 0: q_loss: 31.443307876586914, p_loss: -5.98126745223999, mean_rew: -11.896862525365233, variance: 7.490915298461914, lamda: 1.5939339399337769
Running avgs for agent 1: q_loss: 47.994991302490234, p_loss: -5.988729476928711, mean_rew: -11.885252209742324, variance: 7.172394275665283, lamda: 1.6628859043121338
Running avgs for agent 2: q_loss: 35.56764221191406, p_loss: -6.011275291442871, mean_rew: -11.908980582325905, variance: 7.84352970123291, lamda: 1.5310031175613403

steps: 1349975, episodes: 54000, mean episode reward: -891.7630547663667, agent episode reward: [-297.2543515887889, -297.2543515887889, -297.2543515887889], time: 165.235
steps: 1349975, episodes: 54000, mean episode variance: 5.644262223958969, agent episode variance: [1.8724391775131226, 1.820257812023163, 1.9515652344226837], time: 165.235
Running avgs for agent 0: q_loss: 30.421031951904297, p_loss: -5.996328353881836, mean_rew: -11.926155746743417, variance: 7.489757061004639, lamda: 1.597391963005066
Running avgs for agent 1: q_loss: 37.217010498046875, p_loss: -6.021212577819824, mean_rew: -11.932412100212938, variance: 7.281031131744385, lamda: 1.6664241552352905
Running avgs for agent 2: q_loss: 43.859676361083984, p_loss: -6.0169219970703125, mean_rew: -11.930160104033577, variance: 7.806260585784912, lamda: 1.547616720199585

steps: 1374975, episodes: 55000, mean episode reward: -890.1760852224431, agent episode reward: [-296.7253617408144, -296.7253617408144, -296.7253617408144], time: 158.06
steps: 1374975, episodes: 55000, mean episode variance: 5.600472995758056, agent episode variance: [1.879259736776352, 1.7915531697273255, 1.9296600892543794], time: 158.061
Running avgs for agent 0: q_loss: 49.6530876159668, p_loss: -5.993640422821045, mean_rew: -11.918373791603493, variance: 7.5170392990112305, lamda: 1.6016494035720825
Running avgs for agent 1: q_loss: 33.73470687866211, p_loss: -6.033443927764893, mean_rew: -11.933405281000137, variance: 7.166213035583496, lamda: 1.6801538467407227
Running avgs for agent 2: q_loss: 50.65362548828125, p_loss: -6.025416374206543, mean_rew: -11.95231236609725, variance: 7.718640327453613, lamda: 1.5520124435424805

steps: 1399975, episodes: 56000, mean episode reward: -879.1694377795395, agent episode reward: [-293.0564792598465, -293.0564792598465, -293.0564792598465], time: 158.33
steps: 1399975, episodes: 56000, mean episode variance: 5.620914856910706, agent episode variance: [1.8807221355438233, 1.786866696357727, 1.9533260250091553], time: 158.33
Running avgs for agent 0: q_loss: 52.791969299316406, p_loss: -6.015446186065674, mean_rew: -11.969564160257855, variance: 7.522888660430908, lamda: 1.602828860282898
Running avgs for agent 1: q_loss: 29.59398078918457, p_loss: -6.028382778167725, mean_rew: -11.95403667763728, variance: 7.147466659545898, lamda: 1.6900529861450195
Running avgs for agent 2: q_loss: 51.040287017822266, p_loss: -6.006563186645508, mean_rew: -11.940586556904627, variance: 7.8133039474487305, lamda: 1.5546852350234985

steps: 1424975, episodes: 57000, mean episode reward: -892.1720044273534, agent episode reward: [-297.39066814245115, -297.39066814245115, -297.39066814245115], time: 155.736
steps: 1424975, episodes: 57000, mean episode variance: 5.576883192062378, agent episode variance: [1.8834798593521118, 1.7742924885749818, 1.9191108441352844], time: 155.737
Running avgs for agent 0: q_loss: 52.993587493896484, p_loss: -6.030979156494141, mean_rew: -11.981964452046457, variance: 7.533919334411621, lamda: 1.6039398908615112
Running avgs for agent 1: q_loss: 33.02186584472656, p_loss: -6.032698154449463, mean_rew: -11.953655029748537, variance: 7.097169876098633, lamda: 1.700100064277649
Running avgs for agent 2: q_loss: 46.93599319458008, p_loss: -6.025730609893799, mean_rew: -11.960422875201276, variance: 7.676443576812744, lamda: 1.55746328830719

steps: 1449975, episodes: 58000, mean episode reward: -897.0753147411656, agent episode reward: [-299.0251049137218, -299.0251049137218, -299.0251049137218], time: 155.769
steps: 1449975, episodes: 58000, mean episode variance: 5.516981109619141, agent episode variance: [1.8489661798477173, 1.763165123939514, 1.9048498058319092], time: 155.77
Running avgs for agent 0: q_loss: 52.89205551147461, p_loss: -6.016267776489258, mean_rew: -11.962248956111257, variance: 7.395864963531494, lamda: 1.605633020401001
Running avgs for agent 1: q_loss: 30.447017669677734, p_loss: -6.042810916900635, mean_rew: -11.961892176071554, variance: 7.0526604652404785, lamda: 1.7114018201828003
Running avgs for agent 2: q_loss: 50.924678802490234, p_loss: -6.011713981628418, mean_rew: -11.94380773568883, variance: 7.619399547576904, lamda: 1.5633684396743774

steps: 1474975, episodes: 59000, mean episode reward: -903.0181953249543, agent episode reward: [-301.00606510831807, -301.00606510831807, -301.00606510831807], time: 153.213
steps: 1474975, episodes: 59000, mean episode variance: 5.559252318620682, agent episode variance: [1.8748132743835448, 1.7498647875785827, 1.934574256658554], time: 153.214
Running avgs for agent 0: q_loss: 51.870662689208984, p_loss: -6.024963855743408, mean_rew: -11.98219846117635, variance: 7.499252796173096, lamda: 1.6070668697357178
Running avgs for agent 1: q_loss: 31.37607765197754, p_loss: -6.035234451293945, mean_rew: -11.960314060152337, variance: 6.9994587898254395, lamda: 1.7215389013290405
Running avgs for agent 2: q_loss: 42.33024597167969, p_loss: -6.033934593200684, mean_rew: -11.982072540338242, variance: 7.738297462463379, lamda: 1.5682016611099243

steps: 1499975, episodes: 60000, mean episode reward: -906.3334949890218, agent episode reward: [-302.1111649963406, -302.1111649963406, -302.1111649963406], time: 153.284
steps: 1499975, episodes: 60000, mean episode variance: 5.514992926836014, agent episode variance: [1.874764719247818, 1.7480239276885987, 1.8922042798995973], time: 153.285
Running avgs for agent 0: q_loss: 51.53324890136719, p_loss: -6.03060245513916, mean_rew: -11.992125133529168, variance: 7.499058723449707, lamda: 1.6076076030731201
Running avgs for agent 1: q_loss: 31.641870498657227, p_loss: -6.047970771789551, mean_rew: -11.990655611663092, variance: 6.992095470428467, lamda: 1.7323068380355835
Running avgs for agent 2: q_loss: 34.08649444580078, p_loss: -6.029720783233643, mean_rew: -11.962620455079785, variance: 7.568817615509033, lamda: 1.5843783617019653

steps: 1524975, episodes: 61000, mean episode reward: -922.1375846955108, agent episode reward: [-307.3791948985036, -307.3791948985036, -307.3791948985036], time: 157.667
steps: 1524975, episodes: 61000, mean episode variance: 5.483354386568069, agent episode variance: [1.8630052273273467, 1.725464975833893, 1.8948841834068297], time: 157.667
Running avgs for agent 0: q_loss: 50.987945556640625, p_loss: -6.037827968597412, mean_rew: -11.985671244593238, variance: 7.45202112197876, lamda: 1.6082561016082764
Running avgs for agent 1: q_loss: 30.872793197631836, p_loss: -6.054172992706299, mean_rew: -11.987594972499796, variance: 6.901860237121582, lamda: 1.743100643157959
Running avgs for agent 2: q_loss: 35.196659088134766, p_loss: -6.038758277893066, mean_rew: -11.98993275546257, variance: 7.579536437988281, lamda: 1.5971829891204834

steps: 1549975, episodes: 62000, mean episode reward: -907.1672568614072, agent episode reward: [-302.3890856204691, -302.3890856204691, -302.3890856204691], time: 159.204
steps: 1549975, episodes: 62000, mean episode variance: 5.476325827360153, agent episode variance: [1.8694106996059419, 1.7271502499580382, 1.879764877796173], time: 159.205
Running avgs for agent 0: q_loss: 51.908660888671875, p_loss: -6.048851490020752, mean_rew: -12.026133678280093, variance: 7.477643013000488, lamda: 1.6102840900421143
Running avgs for agent 1: q_loss: 30.119468688964844, p_loss: -6.06210470199585, mean_rew: -12.012469117975478, variance: 6.908601760864258, lamda: 1.7517255544662476
Running avgs for agent 2: q_loss: 35.41436004638672, p_loss: -6.068989276885986, mean_rew: -12.037201272113299, variance: 7.519059658050537, lamda: 1.6120904684066772

steps: 1574975, episodes: 63000, mean episode reward: -906.9323671026549, agent episode reward: [-302.3107890342183, -302.3107890342183, -302.3107890342183], time: 159.878
steps: 1574975, episodes: 63000, mean episode variance: 5.460890740156174, agent episode variance: [1.8894117555618286, 1.719550404071808, 1.8519285805225372], time: 159.878
Running avgs for agent 0: q_loss: 50.814876556396484, p_loss: -6.055742263793945, mean_rew: -12.041915189159083, variance: 7.557647228240967, lamda: 1.612244725227356
Running avgs for agent 1: q_loss: 30.021352767944336, p_loss: -6.076186180114746, mean_rew: -12.035962714626981, variance: 6.878201484680176, lamda: 1.7600693702697754
Running avgs for agent 2: q_loss: 52.128517150878906, p_loss: -6.078499794006348, mean_rew: -12.04664439883177, variance: 7.407714366912842, lamda: 1.627061367034912

steps: 1599975, episodes: 64000, mean episode reward: -920.6961196498822, agent episode reward: [-306.8987065499607, -306.8987065499607, -306.8987065499607], time: 156.138
steps: 1599975, episodes: 64000, mean episode variance: 5.4411687862873075, agent episode variance: [1.8737728114128114, 1.710916209936142, 1.8564797649383544], time: 156.138
Running avgs for agent 0: q_loss: 51.33403015136719, p_loss: -6.071684837341309, mean_rew: -12.05761686936118, variance: 7.495091438293457, lamda: 1.6143854856491089
Running avgs for agent 1: q_loss: 34.98186111450195, p_loss: -6.094821453094482, mean_rew: -12.068425299517086, variance: 6.84366512298584, lamda: 1.769411563873291
Running avgs for agent 2: q_loss: 53.674781799316406, p_loss: -6.078052520751953, mean_rew: -12.057240712858883, variance: 7.425919055938721, lamda: 1.6302225589752197

steps: 1624975, episodes: 65000, mean episode reward: -926.1932802486748, agent episode reward: [-308.73109341622495, -308.73109341622495, -308.73109341622495], time: 155.962
steps: 1624975, episodes: 65000, mean episode variance: 5.41866807961464, agent episode variance: [1.8663984229564667, 1.7129628834724426, 1.83930677318573], time: 155.962
Running avgs for agent 0: q_loss: 36.99174880981445, p_loss: -6.097360134124756, mean_rew: -12.096637707937944, variance: 7.4655938148498535, lamda: 1.619537353515625
Running avgs for agent 1: q_loss: 48.581565856933594, p_loss: -6.104588031768799, mean_rew: -12.09338694295214, variance: 6.851851463317871, lamda: 1.7737133502960205
Running avgs for agent 2: q_loss: 54.25918960571289, p_loss: -6.098727226257324, mean_rew: -12.098189402826382, variance: 7.357227325439453, lamda: 1.632979154586792

steps: 1649975, episodes: 66000, mean episode reward: -914.3325003031237, agent episode reward: [-304.7775001010412, -304.7775001010412, -304.7775001010412], time: 156.004
steps: 1649975, episodes: 66000, mean episode variance: 5.433220429182053, agent episode variance: [1.8574070472717286, 1.7072283554077148, 1.8685850265026092], time: 156.005
Running avgs for agent 0: q_loss: 32.33017349243164, p_loss: -6.106898307800293, mean_rew: -12.117922439188616, variance: 7.429627895355225, lamda: 1.626638650894165
Running avgs for agent 1: q_loss: 47.68733215332031, p_loss: -6.126337051391602, mean_rew: -12.126350900260833, variance: 6.828913688659668, lamda: 1.774733066558838
Running avgs for agent 2: q_loss: 54.404808044433594, p_loss: -6.101365089416504, mean_rew: -12.119903651553402, variance: 7.474339962005615, lamda: 1.6355284452438354

steps: 1674975, episodes: 67000, mean episode reward: -933.9746810617596, agent episode reward: [-311.3248936872532, -311.3248936872532, -311.3248936872532], time: 159.052
steps: 1674975, episodes: 67000, mean episode variance: 5.44291419672966, agent episode variance: [1.851148465156555, 1.7096471045017243, 1.8821186270713806], time: 159.052
Running avgs for agent 0: q_loss: 32.97762680053711, p_loss: -6.128952980041504, mean_rew: -12.151275915179143, variance: 7.4045939445495605, lamda: 1.6416438817977905
Running avgs for agent 1: q_loss: 48.403709411621094, p_loss: -6.126107692718506, mean_rew: -12.154556654788193, variance: 6.838588714599609, lamda: 1.775417685508728
Running avgs for agent 2: q_loss: 55.174312591552734, p_loss: -6.1188249588012695, mean_rew: -12.151990228301738, variance: 7.528474807739258, lamda: 1.6380587816238403

steps: 1699975, episodes: 68000, mean episode reward: -916.9797682339466, agent episode reward: [-305.6599227446489, -305.6599227446489, -305.6599227446489], time: 160.071
steps: 1699975, episodes: 68000, mean episode variance: 5.4248197333812715, agent episode variance: [1.8462933611869812, 1.7150773506164552, 1.863449021577835], time: 160.071
Running avgs for agent 0: q_loss: 31.10662269592285, p_loss: -6.141347885131836, mean_rew: -12.171960498819283, variance: 7.385173320770264, lamda: 1.652185082435608
Running avgs for agent 1: q_loss: 48.44340515136719, p_loss: -6.141089916229248, mean_rew: -12.175283579742052, variance: 6.860309600830078, lamda: 1.7788845300674438
Running avgs for agent 2: q_loss: 54.48267364501953, p_loss: -6.140385150909424, mean_rew: -12.194354762323727, variance: 7.45379638671875, lamda: 1.6406306028366089

steps: 1724975, episodes: 69000, mean episode reward: -933.2780114165456, agent episode reward: [-311.09267047218185, -311.09267047218185, -311.09267047218185], time: 156.387
steps: 1724975, episodes: 69000, mean episode variance: 5.423972359418869, agent episode variance: [1.8485206294059753, 1.7212667431831359, 1.8541849868297577], time: 156.388
Running avgs for agent 0: q_loss: 31.484901428222656, p_loss: -6.149205207824707, mean_rew: -12.192078422370358, variance: 7.394082546234131, lamda: 1.6592180728912354
Running avgs for agent 1: q_loss: 48.60663604736328, p_loss: -6.141728401184082, mean_rew: -12.184329393031367, variance: 6.885066986083984, lamda: 1.7814688682556152
Running avgs for agent 2: q_loss: 55.196659088134766, p_loss: -6.130854606628418, mean_rew: -12.179830133970857, variance: 7.4167399406433105, lamda: 1.644094467163086

steps: 1749975, episodes: 70000, mean episode reward: -911.0043576280054, agent episode reward: [-303.6681192093352, -303.6681192093352, -303.6681192093352], time: 156.104
steps: 1749975, episodes: 70000, mean episode variance: 5.3757853467464445, agent episode variance: [1.8136626636981965, 1.7168489339351654, 1.8452737491130828], time: 156.104
Running avgs for agent 0: q_loss: 47.58281326293945, p_loss: -6.151469707489014, mean_rew: -12.196195545586837, variance: 7.254650592803955, lamda: 1.6641007661819458
Running avgs for agent 1: q_loss: 44.13236618041992, p_loss: -6.148050308227539, mean_rew: -12.201085601169394, variance: 6.867395877838135, lamda: 1.7858374118804932
Running avgs for agent 2: q_loss: 45.91261672973633, p_loss: -6.133487701416016, mean_rew: -12.200008496106754, variance: 7.3810954093933105, lamda: 1.653841257095337

steps: 1774975, episodes: 71000, mean episode reward: -940.5287613698863, agent episode reward: [-313.50958712329543, -313.50958712329543, -313.50958712329543], time: 166.362
steps: 1774975, episodes: 71000, mean episode variance: 5.3629982357025145, agent episode variance: [1.8364687962532042, 1.6777881405353545, 1.8487412989139558], time: 166.362
Running avgs for agent 0: q_loss: 51.643409729003906, p_loss: -6.157402515411377, mean_rew: -12.203434565276858, variance: 7.345874786376953, lamda: 1.6650667190551758
Running avgs for agent 1: q_loss: 42.381492614746094, p_loss: -6.145087718963623, mean_rew: -12.195477051437384, variance: 6.71115255355835, lamda: 1.7994052171707153
Running avgs for agent 2: q_loss: 56.174285888671875, p_loss: -6.125270843505859, mean_rew: -12.205800331934027, variance: 7.394965171813965, lamda: 1.65972900390625

steps: 1799975, episodes: 72000, mean episode reward: -942.1594379021806, agent episode reward: [-314.05314596739356, -314.05314596739356, -314.05314596739356], time: 160.542
steps: 1799975, episodes: 72000, mean episode variance: 5.336722667694092, agent episode variance: [1.8194873671531677, 1.6922796182632447, 1.8249556822776793], time: 160.543
Running avgs for agent 0: q_loss: 53.537437438964844, p_loss: -6.165121078491211, mean_rew: -12.210276391195752, variance: 7.277949333190918, lamda: 1.6670476198196411
Running avgs for agent 1: q_loss: 49.52655029296875, p_loss: -6.151236534118652, mean_rew: -12.213952895087578, variance: 6.769118309020996, lamda: 1.8034216165542603
Running avgs for agent 2: q_loss: 56.712100982666016, p_loss: -6.140790939331055, mean_rew: -12.215037842781994, variance: 7.299822807312012, lamda: 1.6641643047332764

steps: 1824975, episodes: 73000, mean episode reward: -940.0303472398629, agent episode reward: [-313.3434490799543, -313.3434490799543, -313.3434490799543], time: 160.804
steps: 1824975, episodes: 73000, mean episode variance: 5.38447336101532, agent episode variance: [1.8469483516216278, 1.695043621301651, 1.842481388092041], time: 160.805
Running avgs for agent 0: q_loss: 52.34577941894531, p_loss: -6.1534600257873535, mean_rew: -12.214724630465236, variance: 7.387794017791748, lamda: 1.6697471141815186
Running avgs for agent 1: q_loss: 47.57722473144531, p_loss: -6.143924713134766, mean_rew: -12.204719176897047, variance: 6.780174732208252, lamda: 1.8062504529953003
Running avgs for agent 2: q_loss: 56.776702880859375, p_loss: -6.1393280029296875, mean_rew: -12.213541325340577, variance: 7.3699259757995605, lamda: 1.6693288087844849

steps: 1849975, episodes: 74000, mean episode reward: -931.0499774221798, agent episode reward: [-310.34999247406, -310.34999247406, -310.34999247406], time: 157.178
steps: 1849975, episodes: 74000, mean episode variance: 5.356314781188964, agent episode variance: [1.830587881565094, 1.6900624880790711, 1.8356644115447998], time: 157.179
Running avgs for agent 0: q_loss: 51.7159538269043, p_loss: -6.1496806144714355, mean_rew: -12.199622596683355, variance: 7.322351455688477, lamda: 1.6717126369476318
Running avgs for agent 1: q_loss: 32.27317428588867, p_loss: -6.1357879638671875, mean_rew: -12.207219136053414, variance: 6.760250091552734, lamda: 1.8128256797790527
Running avgs for agent 2: q_loss: 55.63138198852539, p_loss: -6.143817901611328, mean_rew: -12.214262966868848, variance: 7.342657566070557, lamda: 1.6743496656417847

steps: 1874975, episodes: 75000, mean episode reward: -954.216195147429, agent episode reward: [-318.07206504914296, -318.07206504914296, -318.07206504914296], time: 156.028
steps: 1874975, episodes: 75000, mean episode variance: 5.318882878780365, agent episode variance: [1.8253080608844756, 1.676926171541214, 1.8166486463546754], time: 156.029
Running avgs for agent 0: q_loss: 52.69717025756836, p_loss: -6.168400764465332, mean_rew: -12.23365298424192, variance: 7.30123233795166, lamda: 1.6728558540344238
Running avgs for agent 1: q_loss: 39.53163528442383, p_loss: -6.146681785583496, mean_rew: -12.240537984657207, variance: 6.707704544067383, lamda: 1.8288630247116089
Running avgs for agent 2: q_loss: 56.23895263671875, p_loss: -6.156673431396484, mean_rew: -12.23874921112054, variance: 7.266594886779785, lamda: 1.67836332321167

steps: 1899975, episodes: 76000, mean episode reward: -953.2095891543642, agent episode reward: [-317.7365297181214, -317.7365297181214, -317.7365297181214], time: 152.551
steps: 1899975, episodes: 76000, mean episode variance: 5.326635559797287, agent episode variance: [1.8338544940948487, 1.672564466714859, 1.8202165989875794], time: 152.552
Running avgs for agent 0: q_loss: 53.2132453918457, p_loss: -6.1678338050842285, mean_rew: -12.219505605248253, variance: 7.335417747497559, lamda: 1.675371527671814
Running avgs for agent 1: q_loss: 50.769927978515625, p_loss: -6.147912979125977, mean_rew: -12.228993691082444, variance: 6.690258026123047, lamda: 1.8357914686203003
Running avgs for agent 2: q_loss: 57.34848403930664, p_loss: -6.157540798187256, mean_rew: -12.23532380150573, variance: 7.2808661460876465, lamda: 1.683197259902954

steps: 1924975, episodes: 77000, mean episode reward: -973.8574658552184, agent episode reward: [-324.61915528507274, -324.61915528507274, -324.61915528507274], time: 160.154
steps: 1924975, episodes: 77000, mean episode variance: 5.298731025934219, agent episode variance: [1.825217342853546, 1.657114678621292, 1.8163990044593812], time: 160.155
Running avgs for agent 0: q_loss: 52.87678146362305, p_loss: -6.174273490905762, mean_rew: -12.248913329714854, variance: 7.300869464874268, lamda: 1.6791532039642334
Running avgs for agent 1: q_loss: 51.40599822998047, p_loss: -6.156923770904541, mean_rew: -12.237622477250913, variance: 6.6284589767456055, lamda: 1.8383582830429077
Running avgs for agent 2: q_loss: 55.86395263671875, p_loss: -6.154622554779053, mean_rew: -12.228849924767372, variance: 7.265596389770508, lamda: 1.6881771087646484

steps: 1949975, episodes: 78000, mean episode reward: -955.0891708148688, agent episode reward: [-318.3630569382896, -318.3630569382896, -318.3630569382896], time: 166.96
steps: 1949975, episodes: 78000, mean episode variance: 5.283713578701019, agent episode variance: [1.8204803330898285, 1.6584984045028686, 1.8047348411083222], time: 166.961
Running avgs for agent 0: q_loss: 52.400997161865234, p_loss: -6.169261932373047, mean_rew: -12.239248391363207, variance: 7.28192138671875, lamda: 1.6817673444747925
Running avgs for agent 1: q_loss: 51.047645568847656, p_loss: -6.156406402587891, mean_rew: -12.24492786199547, variance: 6.633993625640869, lamda: 1.841512680053711
Running avgs for agent 2: q_loss: 56.445152282714844, p_loss: -6.1606292724609375, mean_rew: -12.233511987756648, variance: 7.218939304351807, lamda: 1.6938097476959229

steps: 1974975, episodes: 79000, mean episode reward: -951.2050736306454, agent episode reward: [-317.06835787688186, -317.06835787688186, -317.06835787688186], time: 164.313
steps: 1974975, episodes: 79000, mean episode variance: 5.282872993469239, agent episode variance: [1.8195067465305328, 1.6600676414966584, 1.803298605442047], time: 164.314
Running avgs for agent 0: q_loss: 52.103580474853516, p_loss: -6.177269458770752, mean_rew: -12.258440237773405, variance: 7.278026580810547, lamda: 1.6853322982788086
Running avgs for agent 1: q_loss: 51.43755340576172, p_loss: -6.1566667556762695, mean_rew: -12.251703363054725, variance: 6.640270709991455, lamda: 1.8433743715286255
Running avgs for agent 2: q_loss: 55.68600082397461, p_loss: -6.159210681915283, mean_rew: -12.2468780792348, variance: 7.213194370269775, lamda: 1.698943018913269

steps: 1999975, episodes: 80000, mean episode reward: -947.3357450188811, agent episode reward: [-315.77858167296034, -315.77858167296034, -315.77858167296034], time: 160.312
steps: 1999975, episodes: 80000, mean episode variance: 5.284625875473022, agent episode variance: [1.82064670586586, 1.6576447732448578, 1.8063343963623046], time: 160.313
Running avgs for agent 0: q_loss: 52.53956985473633, p_loss: -6.174173831939697, mean_rew: -12.265637890729963, variance: 7.282586574554443, lamda: 1.688858985900879
Running avgs for agent 1: q_loss: 51.915157318115234, p_loss: -6.1725850105285645, mean_rew: -12.26434711351894, variance: 6.630578994750977, lamda: 1.8464982509613037
Running avgs for agent 2: q_loss: 55.34914779663086, p_loss: -6.179014682769775, mean_rew: -12.279368723966236, variance: 7.225337505340576, lamda: 1.7043193578720093

steps: 2024975, episodes: 81000, mean episode reward: -949.838534874409, agent episode reward: [-316.61284495813635, -316.61284495813635, -316.61284495813635], time: 163.034
steps: 2024975, episodes: 81000, mean episode variance: 5.234128630876541, agent episode variance: [1.7981122369766236, 1.658221286535263, 1.7777951073646545], time: 163.035
Running avgs for agent 0: q_loss: 38.68946838378906, p_loss: -6.178691864013672, mean_rew: -12.26848889107384, variance: 7.192448616027832, lamda: 1.6978381872177124
Running avgs for agent 1: q_loss: 51.53622817993164, p_loss: -6.168887615203857, mean_rew: -12.268462963977598, variance: 6.632885456085205, lamda: 1.8495436906814575
Running avgs for agent 2: q_loss: 42.625423431396484, p_loss: -6.16737699508667, mean_rew: -12.261074170816675, variance: 7.111180782318115, lamda: 1.7182039022445679

steps: 2049975, episodes: 82000, mean episode reward: -943.9632215588495, agent episode reward: [-314.65440718628315, -314.65440718628315, -314.65440718628315], time: 161.516
steps: 2049975, episodes: 82000, mean episode variance: 5.206341702222824, agent episode variance: [1.7958510928153992, 1.6469589531421662, 1.7635316562652588], time: 161.517
Running avgs for agent 0: q_loss: 33.354373931884766, p_loss: -6.183279514312744, mean_rew: -12.27822741997378, variance: 7.183404445648193, lamda: 1.710924506187439
Running avgs for agent 1: q_loss: 52.1487922668457, p_loss: -6.163638591766357, mean_rew: -12.248522068351594, variance: 6.587835788726807, lamda: 1.8520437479019165
Running avgs for agent 2: q_loss: 39.16391372680664, p_loss: -6.169573783874512, mean_rew: -12.269423500011762, variance: 7.054125785827637, lamda: 1.7379194498062134

steps: 2074975, episodes: 83000, mean episode reward: -944.5721273090002, agent episode reward: [-314.8573757696667, -314.8573757696667, -314.8573757696667], time: 161.616
steps: 2074975, episodes: 83000, mean episode variance: 5.168297739744187, agent episode variance: [1.7788834836483, 1.6355875916481017, 1.7538266644477845], time: 161.617
Running avgs for agent 0: q_loss: 41.73847579956055, p_loss: -6.1720757484436035, mean_rew: -12.260304107995056, variance: 7.11553430557251, lamda: 1.7226088047027588
Running avgs for agent 1: q_loss: 41.1435661315918, p_loss: -6.166253566741943, mean_rew: -12.26400725619437, variance: 6.542350769042969, lamda: 1.8596711158752441
Running avgs for agent 2: q_loss: 38.82670974731445, p_loss: -6.171536922454834, mean_rew: -12.272081041487436, variance: 7.01530647277832, lamda: 1.756894826889038

steps: 2099975, episodes: 84000, mean episode reward: -943.2024080595488, agent episode reward: [-314.4008026865163, -314.4008026865163, -314.4008026865163], time: 161.35
steps: 2099975, episodes: 84000, mean episode variance: 5.128533441781998, agent episode variance: [1.7642362534999847, 1.6328682050704957, 1.7314289832115173], time: 161.351
Running avgs for agent 0: q_loss: 52.7780876159668, p_loss: -6.179278373718262, mean_rew: -12.272801328910287, variance: 7.056944847106934, lamda: 1.7290775775909424
Running avgs for agent 1: q_loss: 35.2771110534668, p_loss: -6.172121047973633, mean_rew: -12.266321239239023, variance: 6.531472682952881, lamda: 1.8739604949951172
Running avgs for agent 2: q_loss: 39.51960754394531, p_loss: -6.171122074127197, mean_rew: -12.269663574701083, variance: 6.925715923309326, lamda: 1.7763181924819946

steps: 2124975, episodes: 85000, mean episode reward: -937.2381224264186, agent episode reward: [-312.41270747547287, -312.41270747547287, -312.41270747547287], time: 161.369
steps: 2124975, episodes: 85000, mean episode variance: 5.113402927398682, agent episode variance: [1.7660165281295777, 1.6389584293365478, 1.7084279699325562], time: 161.369
Running avgs for agent 0: q_loss: 52.55280685424805, p_loss: -6.17838191986084, mean_rew: -12.273859504378263, variance: 7.064066410064697, lamda: 1.7314821481704712
Running avgs for agent 1: q_loss: 36.08629608154297, p_loss: -6.175393581390381, mean_rew: -12.274280232192357, variance: 6.555832862854004, lamda: 1.8884902000427246
Running avgs for agent 2: q_loss: 40.08625030517578, p_loss: -6.174627304077148, mean_rew: -12.27310698125042, variance: 6.833712100982666, lamda: 1.7958199977874756

steps: 2149975, episodes: 86000, mean episode reward: -945.5388081062489, agent episode reward: [-315.179602702083, -315.179602702083, -315.179602702083], time: 163.675
steps: 2149975, episodes: 86000, mean episode variance: 5.062128813266754, agent episode variance: [1.7577112777233124, 1.6189792869091033, 1.6854382486343384], time: 163.676
Running avgs for agent 0: q_loss: 49.15952682495117, p_loss: -6.17949914932251, mean_rew: -12.275753434805045, variance: 7.030844688415527, lamda: 1.7349811792373657
Running avgs for agent 1: q_loss: 45.58336639404297, p_loss: -6.185431480407715, mean_rew: -12.292897616001765, variance: 6.475916862487793, lamda: 1.9031357765197754
Running avgs for agent 2: q_loss: 41.091102600097656, p_loss: -6.172688007354736, mean_rew: -12.258448292170565, variance: 6.741753101348877, lamda: 1.8136755228042603

steps: 2174975, episodes: 87000, mean episode reward: -942.7088410622987, agent episode reward: [-314.23628035409956, -314.23628035409956, -314.23628035409956], time: 159.852
steps: 2174975, episodes: 87000, mean episode variance: 5.022802431821823, agent episode variance: [1.7543533220291139, 1.6194308631420136, 1.6490182466506957], time: 159.853
Running avgs for agent 0: q_loss: 34.68117141723633, p_loss: -6.196727275848389, mean_rew: -12.310734604299796, variance: 7.017413139343262, lamda: 1.7463979721069336
Running avgs for agent 1: q_loss: 53.9527473449707, p_loss: -6.202817440032959, mean_rew: -12.300120593150387, variance: 6.477723598480225, lamda: 1.908186912536621
Running avgs for agent 2: q_loss: 58.49551773071289, p_loss: -6.188295841217041, mean_rew: -12.280611888131627, variance: 6.596073150634766, lamda: 1.8291574716567993

steps: 2199975, episodes: 88000, mean episode reward: -945.273566975754, agent episode reward: [-315.091188991918, -315.091188991918, -315.091188991918], time: 160.974
steps: 2199975, episodes: 88000, mean episode variance: 5.0161197955608365, agent episode variance: [1.745490758419037, 1.5951875820159913, 1.6754414551258088], time: 160.974
Running avgs for agent 0: q_loss: 33.46404266357422, p_loss: -6.185502529144287, mean_rew: -12.310647843478145, variance: 6.98196268081665, lamda: 1.758842945098877
Running avgs for agent 1: q_loss: 44.32585525512695, p_loss: -6.211755752563477, mean_rew: -12.31270501200159, variance: 6.38075065612793, lamda: 1.9133179187774658
Running avgs for agent 2: q_loss: 60.04486083984375, p_loss: -6.201284885406494, mean_rew: -12.313791835979872, variance: 6.701765537261963, lamda: 1.834007740020752

steps: 2224975, episodes: 89000, mean episode reward: -929.1379799789465, agent episode reward: [-309.71265999298214, -309.71265999298214, -309.71265999298214], time: 158.378
steps: 2224975, episodes: 89000, mean episode variance: 5.003431914806366, agent episode variance: [1.7295176918506623, 1.5967391376495361, 1.6771750853061675], time: 158.378
Running avgs for agent 0: q_loss: 44.425350189208984, p_loss: -6.19036340713501, mean_rew: -12.313152580114402, variance: 6.9180707931518555, lamda: 1.7694188356399536
Running avgs for agent 1: q_loss: 37.00803756713867, p_loss: -6.209296703338623, mean_rew: -12.322937685608341, variance: 6.386956691741943, lamda: 1.9265618324279785
Running avgs for agent 2: q_loss: 59.37120819091797, p_loss: -6.207426071166992, mean_rew: -12.31884343727626, variance: 6.708700180053711, lamda: 1.8398646116256714

steps: 2249975, episodes: 90000, mean episode reward: -946.3027120057023, agent episode reward: [-315.4342373352341, -315.4342373352341, -315.4342373352341], time: 160.731
steps: 2249975, episodes: 90000, mean episode variance: 4.971357721805573, agent episode variance: [1.7218322730064393, 1.590079634666443, 1.6594458141326904], time: 160.732
Running avgs for agent 0: q_loss: 53.61813735961914, p_loss: -6.202448844909668, mean_rew: -12.330589696872762, variance: 6.8873291015625, lamda: 1.7738229036331177
Running avgs for agent 1: q_loss: 33.816017150878906, p_loss: -6.232804298400879, mean_rew: -12.352376578142758, variance: 6.360318183898926, lamda: 1.9377819299697876
Running avgs for agent 2: q_loss: 60.66310119628906, p_loss: -6.238234519958496, mean_rew: -12.36538794255433, variance: 6.637783050537109, lamda: 1.8447235822677612

steps: 2274975, episodes: 91000, mean episode reward: -957.8627943204096, agent episode reward: [-319.2875981068033, -319.2875981068033, -319.2875981068033], time: 160.188
steps: 2274975, episodes: 91000, mean episode variance: 4.976598606348038, agent episode variance: [1.7402776172161103, 1.58229301404953, 1.6540279750823974], time: 160.189
Running avgs for agent 0: q_loss: 54.153289794921875, p_loss: -6.228922367095947, mean_rew: -12.376335103275624, variance: 6.9611101150512695, lamda: 1.7751511335372925
Running avgs for agent 1: q_loss: 34.71249008178711, p_loss: -6.253425598144531, mean_rew: -12.383490532512745, variance: 6.329172134399414, lamda: 1.9484134912490845
Running avgs for agent 2: q_loss: 42.41720199584961, p_loss: -6.238955020904541, mean_rew: -12.377407889212272, variance: 6.616111755371094, lamda: 1.856673240661621

steps: 2299975, episodes: 92000, mean episode reward: -940.6580199836932, agent episode reward: [-313.55267332789765, -313.55267332789765, -313.55267332789765], time: 161.21
steps: 2299975, episodes: 92000, mean episode variance: 4.950031136989593, agent episode variance: [1.7354072716236115, 1.5730283834934236, 1.6415954818725587], time: 161.21
Running avgs for agent 0: q_loss: 54.90645980834961, p_loss: -6.245376110076904, mean_rew: -12.41369426540318, variance: 6.941629409790039, lamda: 1.7770147323608398
Running avgs for agent 1: q_loss: 56.81364059448242, p_loss: -6.24768590927124, mean_rew: -12.379699577280215, variance: 6.292113304138184, lamda: 1.9569406509399414
Running avgs for agent 2: q_loss: 41.882171630859375, p_loss: -6.251834392547607, mean_rew: -12.392342857342358, variance: 6.566381931304932, lamda: 1.875433325767517

steps: 2324975, episodes: 93000, mean episode reward: -936.4457657080103, agent episode reward: [-312.1485885693368, -312.1485885693368, -312.1485885693368], time: 158.456
steps: 2324975, episodes: 93000, mean episode variance: 4.93065563750267, agent episode variance: [1.7237110154628754, 1.5759151949882508, 1.6310294270515442], time: 158.456
Running avgs for agent 0: q_loss: 55.10337448120117, p_loss: -6.245483875274658, mean_rew: -12.399123575685204, variance: 6.894844055175781, lamda: 1.7795495986938477
Running avgs for agent 1: q_loss: 41.31019592285156, p_loss: -6.251905918121338, mean_rew: -12.388736407547057, variance: 6.303661346435547, lamda: 1.9654711484909058
Running avgs for agent 2: q_loss: 44.625030517578125, p_loss: -6.258721828460693, mean_rew: -12.408269167346752, variance: 6.524117946624756, lamda: 1.892843246459961

steps: 2349975, episodes: 94000, mean episode reward: -930.1128374346646, agent episode reward: [-310.03761247822155, -310.03761247822155, -310.03761247822155], time: 157.09
steps: 2349975, episodes: 94000, mean episode variance: 4.930684697628021, agent episode variance: [1.738298083782196, 1.5687654371261597, 1.6236211767196655], time: 157.09
Running avgs for agent 0: q_loss: 56.61399841308594, p_loss: -6.255947113037109, mean_rew: -12.43721373504393, variance: 6.953192234039307, lamda: 1.784883737564087
Running avgs for agent 1: q_loss: 38.29707717895508, p_loss: -6.264519691467285, mean_rew: -12.420006309267308, variance: 6.27506160736084, lamda: 1.9831806421279907
Running avgs for agent 2: q_loss: 63.67851638793945, p_loss: -6.26418924331665, mean_rew: -12.42383153473575, variance: 6.4944844245910645, lamda: 1.9069348573684692

steps: 2374975, episodes: 95000, mean episode reward: -940.6368598447579, agent episode reward: [-313.54561994825264, -313.54561994825264, -313.54561994825264], time: 158.995
steps: 2374975, episodes: 95000, mean episode variance: 4.9173226776123045, agent episode variance: [1.7348122034072877, 1.5571401193141938, 1.6253703548908234], time: 158.995
Running avgs for agent 0: q_loss: 54.52621841430664, p_loss: -6.249594688415527, mean_rew: -12.431482434265774, variance: 6.939249038696289, lamda: 1.787925124168396
Running avgs for agent 1: q_loss: 54.465572357177734, p_loss: -6.2798051834106445, mean_rew: -12.422349170255268, variance: 6.228560447692871, lamda: 1.9959450960159302
Running avgs for agent 2: q_loss: 62.26793670654297, p_loss: -6.281170845031738, mean_rew: -12.44813051726028, variance: 6.501481533050537, lamda: 1.9130377769470215

steps: 2399975, episodes: 96000, mean episode reward: -949.5035357102876, agent episode reward: [-316.5011785700958, -316.5011785700958, -316.5011785700958], time: 162.536
steps: 2399975, episodes: 96000, mean episode variance: 4.870242900133133, agent episode variance: [1.7194303145408631, 1.5375179402828216, 1.6132946453094483], time: 162.537
Running avgs for agent 0: q_loss: 55.587406158447266, p_loss: -6.266232967376709, mean_rew: -12.455276504588085, variance: 6.877721786499023, lamda: 1.7911105155944824
Running avgs for agent 1: q_loss: 58.3982048034668, p_loss: -6.287580966949463, mean_rew: -12.435374928592193, variance: 6.15007209777832, lamda: 1.9987963438034058
Running avgs for agent 2: q_loss: 62.504459381103516, p_loss: -6.285350799560547, mean_rew: -12.462847513143355, variance: 6.453178882598877, lamda: 1.918285608291626

steps: 2424975, episodes: 97000, mean episode reward: -936.8395800867403, agent episode reward: [-312.27986002891345, -312.27986002891345, -312.27986002891345], time: 160.964
steps: 2424975, episodes: 97000, mean episode variance: 4.8906877481937405, agent episode variance: [1.731394224882126, 1.5565640020370484, 1.6027295212745667], time: 160.964
Running avgs for agent 0: q_loss: 56.01571273803711, p_loss: -6.269346237182617, mean_rew: -12.46970505771932, variance: 6.925577163696289, lamda: 1.7948477268218994
Running avgs for agent 1: q_loss: 58.97178268432617, p_loss: -6.3070783615112305, mean_rew: -12.46160493126697, variance: 6.226255893707275, lamda: 2.002096652984619
Running avgs for agent 2: q_loss: 62.70569610595703, p_loss: -6.291450023651123, mean_rew: -12.479001805095356, variance: 6.41091775894165, lamda: 1.9243173599243164

steps: 2449975, episodes: 98000, mean episode reward: -931.6509252199423, agent episode reward: [-310.5503084066474, -310.5503084066474, -310.5503084066474], time: 158.731
steps: 2449975, episodes: 98000, mean episode variance: 4.863830784797669, agent episode variance: [1.723691469192505, 1.5387445764541625, 1.601394739151001], time: 158.732
Running avgs for agent 0: q_loss: 55.238685607910156, p_loss: -6.26487922668457, mean_rew: -12.469005351794701, variance: 6.894765853881836, lamda: 1.7984044551849365
Running avgs for agent 1: q_loss: 59.005287170410156, p_loss: -6.322048664093018, mean_rew: -12.497199569136923, variance: 6.1549787521362305, lamda: 2.0051095485687256
Running avgs for agent 2: q_loss: 48.67747497558594, p_loss: -6.288896560668945, mean_rew: -12.487328727679264, variance: 6.40557861328125, lamda: 1.9379851818084717

steps: 2474975, episodes: 99000, mean episode reward: -948.6336333867706, agent episode reward: [-316.2112111289236, -316.2112111289236, -316.2112111289236], time: 157.594
steps: 2474975, episodes: 99000, mean episode variance: 4.844281609296798, agent episode variance: [1.711330879688263, 1.546503167629242, 1.586447561979294], time: 157.595
Running avgs for agent 0: q_loss: 55.66721725463867, p_loss: -6.293574333190918, mean_rew: -12.516993262432107, variance: 6.845323085784912, lamda: 1.801452398300171
Running avgs for agent 1: q_loss: 59.18336868286133, p_loss: -6.330363750457764, mean_rew: -12.50086805343255, variance: 6.1860127449035645, lamda: 2.008042812347412
Running avgs for agent 2: q_loss: 45.38621520996094, p_loss: -6.296903133392334, mean_rew: -12.497624710304287, variance: 6.345789909362793, lamda: 1.9589993953704834

steps: 2499975, episodes: 100000, mean episode reward: -941.4525830927103, agent episode reward: [-313.8175276975701, -313.8175276975701, -313.8175276975701], time: 159.821
steps: 2499975, episodes: 100000, mean episode variance: 4.847533935546875, agent episode variance: [1.7274675016403198, 1.547904363632202, 1.572162070274353], time: 159.822
Running avgs for agent 0: q_loss: 56.02290344238281, p_loss: -6.291515827178955, mean_rew: -12.509040829557883, variance: 6.909870147705078, lamda: 1.803497076034546
Running avgs for agent 1: q_loss: 59.68267822265625, p_loss: -6.352545261383057, mean_rew: -12.532506268010971, variance: 6.191617488861084, lamda: 2.010385036468506
Running avgs for agent 2: q_loss: 62.73137283325195, p_loss: -6.312244892120361, mean_rew: -12.51047174970772, variance: 6.28864860534668, lamda: 1.9734793901443481

steps: 2524975, episodes: 101000, mean episode reward: -946.3469917125572, agent episode reward: [-315.4489972375191, -315.4489972375191, -315.4489972375191], time: 155.442
steps: 2524975, episodes: 101000, mean episode variance: 4.855991429805756, agent episode variance: [1.72000989484787, 1.56195326256752, 1.5740282723903656], time: 155.443
Running avgs for agent 0: q_loss: 55.70452117919922, p_loss: -6.293177604675293, mean_rew: -12.521508711598248, variance: 6.880039691925049, lamda: 1.8078268766403198
Running avgs for agent 1: q_loss: 60.39891815185547, p_loss: -6.34118127822876, mean_rew: -12.534733892252149, variance: 6.247813701629639, lamda: 2.013975143432617
Running avgs for agent 2: q_loss: 61.50597381591797, p_loss: -6.313724517822266, mean_rew: -12.52820656975604, variance: 6.29611349105835, lamda: 1.9791117906570435

steps: 2549975, episodes: 102000, mean episode reward: -933.7338304822191, agent episode reward: [-311.2446101607397, -311.2446101607397, -311.2446101607397], time: 162.287
steps: 2549975, episodes: 102000, mean episode variance: 4.81960673904419, agent episode variance: [1.701006860256195, 1.5551243317127228, 1.5634755470752717], time: 162.287
Running avgs for agent 0: q_loss: 43.845123291015625, p_loss: -6.314288139343262, mean_rew: -12.534954117269303, variance: 6.804027557373047, lamda: 1.8151196241378784
Running avgs for agent 1: q_loss: 58.34371566772461, p_loss: -6.3520660400390625, mean_rew: -12.547084682886815, variance: 6.220497131347656, lamda: 2.0207223892211914
Running avgs for agent 2: q_loss: 61.6507682800293, p_loss: -6.33897066116333, mean_rew: -12.550974786871208, variance: 6.253902435302734, lamda: 1.9835294485092163

steps: 2574975, episodes: 103000, mean episode reward: -932.5542981355624, agent episode reward: [-310.8514327118541, -310.8514327118541, -310.8514327118541], time: 163.065
steps: 2574975, episodes: 103000, mean episode variance: 4.795674001455307, agent episode variance: [1.7077144618034363, 1.521864090681076, 1.5660954489707948], time: 163.065
Running avgs for agent 0: q_loss: 38.81564712524414, p_loss: -6.307299613952637, mean_rew: -12.539416213706946, variance: 6.83085823059082, lamda: 1.832358956336975
Running avgs for agent 1: q_loss: 58.3627815246582, p_loss: -6.356450080871582, mean_rew: -12.551848088143323, variance: 6.087456226348877, lamda: 2.0268189907073975
Running avgs for agent 2: q_loss: 62.940521240234375, p_loss: -6.332876205444336, mean_rew: -12.534071765723219, variance: 6.2643818855285645, lamda: 1.9879810810089111

steps: 2599975, episodes: 104000, mean episode reward: -918.5134851611881, agent episode reward: [-306.171161720396, -306.171161720396, -306.171161720396], time: 171.776
steps: 2599975, episodes: 104000, mean episode variance: 4.798963786840439, agent episode variance: [1.6865795288085939, 1.546680605649948, 1.565703652381897], time: 171.776
Running avgs for agent 0: q_loss: 37.966800689697266, p_loss: -6.30836296081543, mean_rew: -12.532558700879775, variance: 6.746318340301514, lamda: 1.8483506441116333
Running avgs for agent 1: q_loss: 54.71669387817383, p_loss: -6.338261604309082, mean_rew: -12.553720797510215, variance: 6.186722278594971, lamda: 2.0397512912750244
Running avgs for agent 2: q_loss: 60.74053192138672, p_loss: -6.331586837768555, mean_rew: -12.532857132139489, variance: 6.262814521789551, lamda: 1.9938369989395142

steps: 2624975, episodes: 105000, mean episode reward: -925.2332739527368, agent episode reward: [-308.4110913175789, -308.4110913175789, -308.4110913175789], time: 161.493
steps: 2624975, episodes: 105000, mean episode variance: 4.755049599885941, agent episode variance: [1.6773760750293731, 1.519886137008667, 1.5577873878479005], time: 161.493
Running avgs for agent 0: q_loss: 57.066226959228516, p_loss: -6.32476282119751, mean_rew: -12.544008082368109, variance: 6.709504127502441, lamda: 1.8575187921524048
Running avgs for agent 1: q_loss: 39.72804260253906, p_loss: -6.335643768310547, mean_rew: -12.53483351650021, variance: 6.079544544219971, lamda: 2.0550975799560547
Running avgs for agent 2: q_loss: 41.93589782714844, p_loss: -6.343823432922363, mean_rew: -12.55568035346939, variance: 6.231149196624756, lamda: 2.007321834564209

steps: 2649975, episodes: 106000, mean episode reward: -924.5757362020441, agent episode reward: [-308.191912067348, -308.191912067348, -308.191912067348], time: 173.722
steps: 2649975, episodes: 106000, mean episode variance: 4.722762265682221, agent episode variance: [1.669752461194992, 1.5225217590332032, 1.5304880454540253], time: 173.722
Running avgs for agent 0: q_loss: 49.32624816894531, p_loss: -6.327804088592529, mean_rew: -12.547467380667019, variance: 6.679009914398193, lamda: 1.8608444929122925
Running avgs for agent 1: q_loss: 40.810279846191406, p_loss: -6.347972869873047, mean_rew: -12.563628708800794, variance: 6.090086936950684, lamda: 2.069040060043335
Running avgs for agent 2: q_loss: 58.59663772583008, p_loss: -6.328088760375977, mean_rew: -12.537295003385166, variance: 6.121952056884766, lamda: 2.023289680480957

steps: 2674975, episodes: 107000, mean episode reward: -911.1708991000207, agent episode reward: [-303.7236330333402, -303.7236330333402, -303.7236330333402], time: 155.011
steps: 2674975, episodes: 107000, mean episode variance: 4.7173739361763, agent episode variance: [1.6738461308479309, 1.5032204151153565, 1.5403073902130127], time: 155.011
Running avgs for agent 0: q_loss: 37.405235290527344, p_loss: -6.332366466522217, mean_rew: -12.546179919109688, variance: 6.695384979248047, lamda: 1.8719854354858398
Running avgs for agent 1: q_loss: 59.06131362915039, p_loss: -6.343864917755127, mean_rew: -12.538832071702952, variance: 6.012881278991699, lamda: 2.0820834636688232
Running avgs for agent 2: q_loss: 61.98350524902344, p_loss: -6.3513360023498535, mean_rew: -12.548678797540287, variance: 6.161229610443115, lamda: 2.030925750732422

steps: 2699975, episodes: 108000, mean episode reward: -918.0504729700215, agent episode reward: [-306.0168243233404, -306.0168243233404, -306.0168243233404], time: 153.668
steps: 2699975, episodes: 108000, mean episode variance: 4.704917829513549, agent episode variance: [1.6579601926803589, 1.510556941986084, 1.536400694847107], time: 153.668
Running avgs for agent 0: q_loss: 50.51742935180664, p_loss: -6.3227081298828125, mean_rew: -12.532302442086094, variance: 6.631840705871582, lamda: 1.8840993642807007
Running avgs for agent 1: q_loss: 59.91422653198242, p_loss: -6.343486309051514, mean_rew: -12.534788068056603, variance: 6.042227745056152, lamda: 2.084460496902466
Running avgs for agent 2: q_loss: 57.75127410888672, p_loss: -6.355561256408691, mean_rew: -12.547531371265093, variance: 6.145603179931641, lamda: 2.0362610816955566

steps: 2724975, episodes: 109000, mean episode reward: -925.8986684199981, agent episode reward: [-308.6328894733327, -308.6328894733327, -308.6328894733327], time: 154.692
steps: 2724975, episodes: 109000, mean episode variance: 4.6713441612720485, agent episode variance: [1.650399598836899, 1.4950971426963806, 1.5258474197387695], time: 154.693
Running avgs for agent 0: q_loss: 56.0706901550293, p_loss: -6.335701942443848, mean_rew: -12.54079481561928, variance: 6.601598262786865, lamda: 1.8877146244049072
Running avgs for agent 1: q_loss: 60.5369873046875, p_loss: -6.354936122894287, mean_rew: -12.546878124400687, variance: 5.980388641357422, lamda: 2.0874991416931152
Running avgs for agent 2: q_loss: 50.125946044921875, p_loss: -6.346795082092285, mean_rew: -12.529603656995306, variance: 6.103389739990234, lamda: 2.048269510269165

steps: 2749975, episodes: 110000, mean episode reward: -909.9434313312096, agent episode reward: [-303.3144771104032, -303.3144771104032, -303.3144771104032], time: 151.98
steps: 2749975, episodes: 110000, mean episode variance: 4.639253954887391, agent episode variance: [1.6475848891735076, 1.4855175862312318, 1.5061514794826507], time: 151.981
Running avgs for agent 0: q_loss: 56.10475540161133, p_loss: -6.335297584533691, mean_rew: -12.54484254213241, variance: 6.590339183807373, lamda: 1.8903262615203857
Running avgs for agent 1: q_loss: 58.79746627807617, p_loss: -6.351120948791504, mean_rew: -12.530433472860619, variance: 5.942070484161377, lamda: 2.090181589126587
Running avgs for agent 2: q_loss: 43.75415802001953, p_loss: -6.3299126625061035, mean_rew: -12.510557315129875, variance: 6.024605751037598, lamda: 2.0671019554138184

steps: 2774975, episodes: 111000, mean episode reward: -903.5431075204215, agent episode reward: [-301.18103584014057, -301.18103584014057, -301.18103584014057], time: 156.781
steps: 2774975, episodes: 111000, mean episode variance: 4.644973608016968, agent episode variance: [1.6564912481307983, 1.489429615497589, 1.4990527443885804], time: 156.781
Running avgs for agent 0: q_loss: 55.991737365722656, p_loss: -6.324375629425049, mean_rew: -12.536327377887648, variance: 6.625964641571045, lamda: 1.8931025266647339
Running avgs for agent 1: q_loss: 59.6527099609375, p_loss: -6.358685493469238, mean_rew: -12.542504849174843, variance: 5.957718372344971, lamda: 2.0928773880004883
Running avgs for agent 2: q_loss: 55.533504486083984, p_loss: -6.353271484375, mean_rew: -12.54381297175956, variance: 5.996211051940918, lamda: 2.0821919441223145

steps: 2799975, episodes: 112000, mean episode reward: -932.4252126529141, agent episode reward: [-310.8084042176381, -310.8084042176381, -310.8084042176381], time: 157.598
steps: 2799975, episodes: 112000, mean episode variance: 4.635688457489014, agent episode variance: [1.6507775633335113, 1.4940276081562043, 1.4908832859992982], time: 157.598
Running avgs for agent 0: q_loss: 56.05500411987305, p_loss: -6.32674503326416, mean_rew: -12.520189135066262, variance: 6.603110313415527, lamda: 1.8957659006118774
Running avgs for agent 1: q_loss: 59.31379699707031, p_loss: -6.346130847930908, mean_rew: -12.52683903828737, variance: 5.976110458374023, lamda: 2.0957577228546143
Running avgs for agent 2: q_loss: 59.031978607177734, p_loss: -6.335625171661377, mean_rew: -12.522297621754358, variance: 5.963533401489258, lamda: 2.0936405658721924

steps: 2824975, episodes: 113000, mean episode reward: -918.2322102570429, agent episode reward: [-306.0774034190143, -306.0774034190143, -306.0774034190143], time: 159.607
steps: 2824975, episodes: 113000, mean episode variance: 4.622439123630524, agent episode variance: [1.641473447561264, 1.4774349851608277, 1.503530690908432], time: 159.608
Running avgs for agent 0: q_loss: 52.96366500854492, p_loss: -6.334869861602783, mean_rew: -12.530200132274443, variance: 6.565893650054932, lamda: 1.9007987976074219
Running avgs for agent 1: q_loss: 58.6739387512207, p_loss: -6.333929061889648, mean_rew: -12.510124592714186, variance: 5.909740447998047, lamda: 2.098768949508667
Running avgs for agent 2: q_loss: 59.26073455810547, p_loss: -6.343635559082031, mean_rew: -12.52767652972677, variance: 6.01412296295166, lamda: 2.098571300506592

steps: 2849975, episodes: 114000, mean episode reward: -928.5619722446025, agent episode reward: [-309.5206574148675, -309.5206574148675, -309.5206574148675], time: 156.34
steps: 2849975, episodes: 114000, mean episode variance: 4.627432954072952, agent episode variance: [1.6339109189510346, 1.499597564458847, 1.4939244706630708], time: 156.341
Running avgs for agent 0: q_loss: 56.15440368652344, p_loss: -6.326846599578857, mean_rew: -12.508539726800537, variance: 6.535643577575684, lamda: 1.9047375917434692
Running avgs for agent 1: q_loss: 59.373043060302734, p_loss: -6.356032371520996, mean_rew: -12.53156911413277, variance: 5.9983906745910645, lamda: 2.1022121906280518
Running avgs for agent 2: q_loss: 47.25868606567383, p_loss: -6.360249042510986, mean_rew: -12.532180551719286, variance: 5.9756975173950195, lamda: 2.1083569526672363

steps: 2874975, episodes: 115000, mean episode reward: -944.4967206803174, agent episode reward: [-314.8322402267725, -314.8322402267725, -314.8322402267725], time: 157.011
steps: 2874975, episodes: 115000, mean episode variance: 4.608713173389435, agent episode variance: [1.6339354343414307, 1.4877498898506165, 1.4870278491973876], time: 157.011
Running avgs for agent 0: q_loss: 54.224727630615234, p_loss: -6.342221736907959, mean_rew: -12.531021141589651, variance: 6.535742282867432, lamda: 1.907999038696289
Running avgs for agent 1: q_loss: 58.30165481567383, p_loss: -6.3482513427734375, mean_rew: -12.520269834174188, variance: 5.9509992599487305, lamda: 2.1054506301879883
Running avgs for agent 2: q_loss: 57.413883209228516, p_loss: -6.339880466461182, mean_rew: -12.513436986920356, variance: 5.948111534118652, lamda: 2.1219582557678223

steps: 2899975, episodes: 116000, mean episode reward: -926.4310518824481, agent episode reward: [-308.81035062748265, -308.81035062748265, -308.81035062748265], time: 156.42
steps: 2899975, episodes: 116000, mean episode variance: 4.562789354085922, agent episode variance: [1.6230878965854645, 1.4740052237510681, 1.4656962337493897], time: 156.42
Running avgs for agent 0: q_loss: 51.345252990722656, p_loss: -6.320621013641357, mean_rew: -12.50742900612527, variance: 6.492351531982422, lamda: 1.916583776473999
Running avgs for agent 1: q_loss: 58.372406005859375, p_loss: -6.335806846618652, mean_rew: -12.494987960321497, variance: 5.896020889282227, lamda: 2.1084952354431152
Running avgs for agent 2: q_loss: 60.81466293334961, p_loss: -6.342040061950684, mean_rew: -12.495701177756876, variance: 5.862785339355469, lamda: 2.128610849380493

steps: 2924975, episodes: 117000, mean episode reward: -912.4958808786113, agent episode reward: [-304.1652936262038, -304.1652936262038, -304.1652936262038], time: 158.731
steps: 2924975, episodes: 117000, mean episode variance: 4.594121804952621, agent episode variance: [1.6323084216117858, 1.487593870639801, 1.4742195127010345], time: 158.732
Running avgs for agent 0: q_loss: 56.113868713378906, p_loss: -6.3339152336120605, mean_rew: -12.50567463378849, variance: 6.529233932495117, lamda: 1.9212576150894165
Running avgs for agent 1: q_loss: 58.57075500488281, p_loss: -6.3361310958862305, mean_rew: -12.500440509874949, variance: 5.950375556945801, lamda: 2.1112821102142334
Running avgs for agent 2: q_loss: 59.85890579223633, p_loss: -6.350773811340332, mean_rew: -12.504564897900138, variance: 5.896877765655518, lamda: 2.1332616806030273

steps: 2949975, episodes: 118000, mean episode reward: -928.3740376068608, agent episode reward: [-309.4580125356203, -309.4580125356203, -309.4580125356203], time: 160.949
steps: 2949975, episodes: 118000, mean episode variance: 4.56366595196724, agent episode variance: [1.6305347697734833, 1.4769647252559661, 1.45616645693779], time: 160.949
Running avgs for agent 0: q_loss: 54.941070556640625, p_loss: -6.321835994720459, mean_rew: -12.481542057372977, variance: 6.522138595581055, lamda: 1.9249660968780518
Running avgs for agent 1: q_loss: 58.11863708496094, p_loss: -6.340269565582275, mean_rew: -12.488227300140915, variance: 5.9078593254089355, lamda: 2.1139943599700928
Running avgs for agent 2: q_loss: 60.133731842041016, p_loss: -6.3380279541015625, mean_rew: -12.48831914922665, variance: 5.8246660232543945, lamda: 2.1388535499572754

steps: 2974975, episodes: 119000, mean episode reward: -915.7776915949448, agent episode reward: [-305.25923053164826, -305.25923053164826, -305.25923053164826], time: 160.637
steps: 2974975, episodes: 119000, mean episode variance: 4.531522566080094, agent episode variance: [1.612461100578308, 1.4657554502487182, 1.453306015253067], time: 160.637
Running avgs for agent 0: q_loss: 54.403236389160156, p_loss: -6.322442531585693, mean_rew: -12.46925232041498, variance: 6.449844837188721, lamda: 1.9279825687408447
Running avgs for agent 1: q_loss: 50.8945198059082, p_loss: -6.323620796203613, mean_rew: -12.466129934208206, variance: 5.863021373748779, lamda: 2.119765043258667
Running avgs for agent 2: q_loss: 57.298274993896484, p_loss: -6.332118988037109, mean_rew: -12.468712063602467, variance: 5.8132243156433105, lamda: 2.147094964981079

steps: 2999975, episodes: 120000, mean episode reward: -906.5987054536694, agent episode reward: [-302.19956848455644, -302.19956848455644, -302.19956848455644], time: 163.504
steps: 2999975, episodes: 120000, mean episode variance: 4.513261265039444, agent episode variance: [1.6132914540767669, 1.4498389616012572, 1.4501308493614198], time: 163.504
Running avgs for agent 0: q_loss: 54.72186279296875, p_loss: -6.318150520324707, mean_rew: -12.454643676371532, variance: 6.4531660079956055, lamda: 1.9319038391113281
Running avgs for agent 1: q_loss: 56.67186737060547, p_loss: -6.3209919929504395, mean_rew: -12.440749967693154, variance: 5.799355506896973, lamda: 2.1285836696624756
Running avgs for agent 2: q_loss: 59.209075927734375, p_loss: -6.32438850402832, mean_rew: -12.448623691822412, variance: 5.800523281097412, lamda: 2.154977560043335

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -902.0402621967104, agent episode reward: [-300.68008739890337, -300.68008739890337, -300.68008739890337], time: 140.306
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 140.307
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -905.4386187103297, agent episode reward: [-301.8128729034433, -301.8128729034433, -301.8128729034433], time: 121.588
steps: 49975, episodes: 2000, mean episode variance: 3.7530056726932526, agent episode variance: [1.1455747528076172, 1.697918088912964, 0.9095128309726715], time: 121.588
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.047291812129712, variance: 4.694977760314941, lamda: 1.9347758293151855
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.053329223022804, variance: 6.958680152893066, lamda: 2.129944324493408
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -12.048228059549572, variance: 3.7275116443634033, lamda: 2.1572821140289307

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567759534.1090609: line 9: --exp_var_alpha: command not found
