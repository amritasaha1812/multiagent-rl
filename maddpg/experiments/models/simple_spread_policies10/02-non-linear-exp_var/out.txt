# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 40.0 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies10/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies10/02-non-linear-exp_var/
Job <1091627> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc133>>
arglist.u_estimation True
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -528.8085318022347, agent episode reward: [-176.26951060074492, -176.26951060074492, -176.26951060074492], time: 95.072
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 95.072
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -679.7354200046813, agent episode reward: [-226.57847333489374, -226.57847333489374, -226.57847333489374], time: 118.733
steps: 49975, episodes: 2000, mean episode variance: 6.914077833048999, agent episode variance: [0.9458754745647311, 3.0439626519680023, 2.924239706516266], time: 118.733
Running avgs for agent 0: q_loss: 620.042236328125, p_loss: 11.956384658813477, mean_rew: -7.841056600960743, variance: 3.8765388301833243, lamda: 1.0112251043319702
Running avgs for agent 1: q_loss: 173.1630401611328, p_loss: -5.676074504852295, mean_rew: -7.8504001788961055, variance: 12.475256770360666, lamda: 1.0110570192337036
Running avgs for agent 2: q_loss: 123.96146392822266, p_loss: -5.460346221923828, mean_rew: -7.849827277801428, variance: 11.984588961132237, lamda: 1.011449933052063

steps: 74975, episodes: 3000, mean episode reward: -634.6344216874394, agent episode reward: [-211.54480722914647, -211.54480722914647, -211.54480722914647], time: 115.506
steps: 74975, episodes: 3000, mean episode variance: 6.333303841769696, agent episode variance: [2.3144799367785454, 1.998181625843048, 2.020642279148102], time: 115.506
Running avgs for agent 0: q_loss: 4737.0517578125, p_loss: 28.40686798095703, mean_rew: -8.163308287777209, variance: 9.257919747114181, lamda: 1.0359067916870117
Running avgs for agent 1: q_loss: 32.04010009765625, p_loss: -4.3627238273620605, mean_rew: -8.14942506734084, variance: 7.9927263259887695, lamda: 1.0355890989303589
Running avgs for agent 2: q_loss: 33.02574157714844, p_loss: -4.497557640075684, mean_rew: -8.157225783547103, variance: 8.082569122314453, lamda: 1.0361295938491821

steps: 99975, episodes: 4000, mean episode reward: -575.0718711804312, agent episode reward: [-191.69062372681043, -191.69062372681043, -191.69062372681043], time: 115.957
steps: 99975, episodes: 4000, mean episode variance: 9.851091557383537, agent episode variance: [5.999223864912986, 1.9364214205741883, 1.9154462718963623], time: 115.958
Running avgs for agent 0: q_loss: 19046.748046875, p_loss: 45.143733978271484, mean_rew: -8.13423132100491, variance: 23.996895459651945, lamda: 1.060910940170288
Running avgs for agent 1: q_loss: 33.70237731933594, p_loss: -4.2706146240234375, mean_rew: -8.137860385774248, variance: 7.745685577392578, lamda: 1.0605932474136353
Running avgs for agent 2: q_loss: 29.854867935180664, p_loss: -4.14716100692749, mean_rew: -8.138561769350398, variance: 7.661785125732422, lamda: 1.0611337423324585

steps: 124975, episodes: 5000, mean episode reward: -518.1784433312723, agent episode reward: [-172.72614777709077, -172.72614777709077, -172.72614777709077], time: 119.81
steps: 124975, episodes: 5000, mean episode variance: 13.786960215568543, agent episode variance: [10.112074527740479, 1.838705400943756, 1.8361802868843078], time: 119.81
Running avgs for agent 0: q_loss: 45837.578125, p_loss: 58.752742767333984, mean_rew: -7.959125642468276, variance: 40.448298110961915, lamda: 1.0859150886535645
Running avgs for agent 1: q_loss: 24.482824325561523, p_loss: -4.064131259918213, mean_rew: -7.966345975916327, variance: 7.354821681976318, lamda: 1.0855973958969116
Running avgs for agent 2: q_loss: 21.898534774780273, p_loss: -4.045395851135254, mean_rew: -7.963055366266251, variance: 7.34472131729126, lamda: 1.0861378908157349

steps: 149975, episodes: 6000, mean episode reward: -489.2285219822128, agent episode reward: [-163.07617399407093, -163.07617399407093, -163.07617399407093], time: 168.962
steps: 149975, episodes: 6000, mean episode variance: 13.409557143926621, agent episode variance: [9.904610713005066, 1.754122347831726, 1.7508240830898285], time: 168.963
Running avgs for agent 0: q_loss: 59232.18359375, p_loss: 68.73178100585938, mean_rew: -7.724275826448879, variance: 39.618442852020266, lamda: 1.1109192371368408
Running avgs for agent 1: q_loss: 20.54140281677246, p_loss: -3.9275848865509033, mean_rew: -7.731018299476584, variance: 7.016489028930664, lamda: 1.1104753017425537
Running avgs for agent 2: q_loss: 20.00827980041504, p_loss: -3.954448699951172, mean_rew: -7.727997403954898, variance: 7.0032958984375, lamda: 1.1111375093460083

steps: 174975, episodes: 7000, mean episode reward: -482.71987552006067, agent episode reward: [-160.90662517335358, -160.90662517335358, -160.90662517335358], time: 183.561
steps: 174975, episodes: 7000, mean episode variance: 19.547788244485854, agent episode variance: [16.188475875854493, 1.6919633650779724, 1.6673490035533904], time: 183.561
Running avgs for agent 0: q_loss: 117923.8046875, p_loss: 76.65267944335938, mean_rew: -7.533448139200601, variance: 64.75390350341797, lamda: 1.1359233856201172
Running avgs for agent 1: q_loss: 16.800994873046875, p_loss: -3.8384506702423096, mean_rew: -7.5247387782955535, variance: 6.767853260040283, lamda: 1.1345897912979126
Running avgs for agent 2: q_loss: 17.531982421875, p_loss: -3.861175298690796, mean_rew: -7.530237813376377, variance: 6.669395923614502, lamda: 1.1352802515029907

steps: 199975, episodes: 8000, mean episode reward: -481.88048663047505, agent episode reward: [-160.62682887682502, -160.62682887682502, -160.62682887682502], time: 180.733
steps: 199975, episodes: 8000, mean episode variance: 16.774617918014528, agent episode variance: [13.55543832540512, 1.6146864876747131, 1.6044931049346924], time: 180.734
Running avgs for agent 0: q_loss: 113903.921875, p_loss: 83.43678283691406, mean_rew: -7.391150570812489, variance: 54.22175330162048, lamda: 1.1609275341033936
Running avgs for agent 1: q_loss: 16.168113708496094, p_loss: -3.7765052318573, mean_rew: -7.386994677158222, variance: 6.458745956420898, lamda: 1.1580584049224854
Running avgs for agent 2: q_loss: 16.88681983947754, p_loss: -3.7612955570220947, mean_rew: -7.38372976189193, variance: 6.417972564697266, lamda: 1.1573563814163208

steps: 224975, episodes: 9000, mean episode reward: -485.0818817949234, agent episode reward: [-161.69396059830777, -161.69396059830777, -161.69396059830777], time: 185.772
steps: 224975, episodes: 9000, mean episode variance: 23.29818697810173, agent episode variance: [20.16008597445488, 1.5674075298309327, 1.570693473815918], time: 185.773
Running avgs for agent 0: q_loss: 179450.109375, p_loss: 89.1598892211914, mean_rew: -7.272076822352019, variance: 80.64034389781952, lamda: 1.18593168258667
Running avgs for agent 1: q_loss: 14.560193061828613, p_loss: -3.7200210094451904, mean_rew: -7.272814015769891, variance: 6.269630432128906, lamda: 1.1805883646011353
Running avgs for agent 2: q_loss: 12.489805221557617, p_loss: -3.694401741027832, mean_rew: -7.2714687078677605, variance: 6.282773971557617, lamda: 1.1775562763214111

steps: 249975, episodes: 10000, mean episode reward: -484.0408037163451, agent episode reward: [-161.346934572115, -161.346934572115, -161.346934572115], time: 178.738
steps: 249975, episodes: 10000, mean episode variance: 18.964441455602646, agent episode variance: [15.914412736654281, 1.5227486877441405, 1.5272800312042236], time: 178.739
Running avgs for agent 0: q_loss: 187401.125, p_loss: 93.77127838134766, mean_rew: -7.186538714456866, variance: 63.657650946617125, lamda: 1.2109358310699463
Running avgs for agent 1: q_loss: 13.781196594238281, p_loss: -3.616513252258301, mean_rew: -7.19143504519146, variance: 6.0909953117370605, lamda: 1.2021526098251343
Running avgs for agent 2: q_loss: 11.01395320892334, p_loss: -3.6758720874786377, mean_rew: -7.190909849941545, variance: 6.109119892120361, lamda: 1.197934865951538

steps: 274975, episodes: 11000, mean episode reward: -484.48317353678505, agent episode reward: [-161.49439117892837, -161.49439117892837, -161.49439117892837], time: 174.033
steps: 274975, episodes: 11000, mean episode variance: 29.06644795703888, agent episode variance: [26.090297392845155, 1.4768955254554748, 1.4992550387382508], time: 174.034
Running avgs for agent 0: q_loss: 285398.59375, p_loss: 97.5032958984375, mean_rew: -7.119690959749944, variance: 104.36118957138062, lamda: 1.235939860343933
Running avgs for agent 1: q_loss: 13.86666202545166, p_loss: -3.6084213256835938, mean_rew: -7.118325477656202, variance: 5.9075822830200195, lamda: 1.2237358093261719
Running avgs for agent 2: q_loss: 9.821369171142578, p_loss: -3.6175918579101562, mean_rew: -7.110602998696735, variance: 5.997020244598389, lamda: 1.2165939807891846

steps: 299975, episodes: 12000, mean episode reward: -482.8369851443912, agent episode reward: [-160.94566171479707, -160.94566171479707, -160.94566171479707], time: 178.541
steps: 299975, episodes: 12000, mean episode variance: 19.328030469894408, agent episode variance: [16.418227111816407, 1.4559560525417328, 1.4538473055362702], time: 178.541
Running avgs for agent 0: q_loss: 218489.25, p_loss: 101.00751495361328, mean_rew: -7.053997662962467, variance: 65.67290844726563, lamda: 1.260944128036499
Running avgs for agent 1: q_loss: 16.05592155456543, p_loss: -3.613565444946289, mean_rew: -7.056880266222373, variance: 5.823824405670166, lamda: 1.2386997938156128
Running avgs for agent 2: q_loss: 11.1062650680542, p_loss: -3.598618745803833, mean_rew: -7.065629496722604, variance: 5.815389633178711, lamda: 1.231469750404358

steps: 324975, episodes: 13000, mean episode reward: -486.83774377055, agent episode reward: [-162.27924792351664, -162.27924792351664, -162.27924792351664], time: 179.892
steps: 324975, episodes: 13000, mean episode variance: 32.19257892465591, agent episode variance: [29.306231956481934, 1.4328538272380829, 1.4534931409358978], time: 179.892
Running avgs for agent 0: q_loss: 371281.15625, p_loss: 104.2282943725586, mean_rew: -7.018899659212805, variance: 117.22492782592774, lamda: 1.2859482765197754
Running avgs for agent 1: q_loss: 11.07785701751709, p_loss: -3.581496477127075, mean_rew: -7.005707304051382, variance: 5.731415271759033, lamda: 1.2552028894424438
Running avgs for agent 2: q_loss: 9.986257553100586, p_loss: -3.5870630741119385, mean_rew: -7.016743485364703, variance: 5.813972473144531, lamda: 1.2462365627288818

steps: 349975, episodes: 14000, mean episode reward: -483.4900880697543, agent episode reward: [-161.1633626899181, -161.1633626899181, -161.1633626899181], time: 179.27
steps: 349975, episodes: 14000, mean episode variance: 24.00364094376564, agent episode variance: [21.18227731847763, 1.4110724115371704, 1.4102912137508392], time: 179.271
Running avgs for agent 0: q_loss: 304781.75, p_loss: 107.2120132446289, mean_rew: -6.97466697392574, variance: 84.72910927391052, lamda: 1.3109523057937622
Running avgs for agent 1: q_loss: 11.081527709960938, p_loss: -3.5651040077209473, mean_rew: -6.974416692419218, variance: 5.644289493560791, lamda: 1.273640751838684
Running avgs for agent 2: q_loss: 12.021482467651367, p_loss: -3.5709497928619385, mean_rew: -6.980039250080147, variance: 5.641165256500244, lamda: 1.26171875

steps: 374975, episodes: 15000, mean episode reward: -490.6719621158985, agent episode reward: [-163.5573207052995, -163.5573207052995, -163.5573207052995], time: 174.103
steps: 374975, episodes: 15000, mean episode variance: 20.63843470978737, agent episode variance: [17.858503677368166, 1.3796177804470062, 1.4003132519721986], time: 174.104
Running avgs for agent 0: q_loss: 285805.0625, p_loss: 109.32782745361328, mean_rew: -6.94373674364836, variance: 71.43401470947266, lamda: 1.3359565734863281
Running avgs for agent 1: q_loss: 12.331642150878906, p_loss: -3.5548272132873535, mean_rew: -6.937538715789373, variance: 5.5184712409973145, lamda: 1.2932851314544678
Running avgs for agent 2: q_loss: 11.7609281539917, p_loss: -3.5411081314086914, mean_rew: -6.946535327676604, variance: 5.601253032684326, lamda: 1.2747570276260376

steps: 399975, episodes: 16000, mean episode reward: -491.9380040651468, agent episode reward: [-163.9793346883822, -163.9793346883822, -163.9793346883822], time: 160.792
steps: 399975, episodes: 16000, mean episode variance: 20.937125504016876, agent episode variance: [18.208363429069518, 1.343541986465454, 1.385220088481903], time: 160.793
Running avgs for agent 0: q_loss: 299059.78125, p_loss: 111.28925323486328, mean_rew: -6.914306764819695, variance: 72.83345371627807, lamda: 1.3609607219696045
Running avgs for agent 1: q_loss: 13.458564758300781, p_loss: -3.539311408996582, mean_rew: -6.917950745808432, variance: 5.3741679191589355, lamda: 1.3123425245285034
Running avgs for agent 2: q_loss: 13.196578025817871, p_loss: -3.536259651184082, mean_rew: -6.918262425976487, variance: 5.54088020324707, lamda: 1.2802385091781616

steps: 424975, episodes: 17000, mean episode reward: -493.8658312255866, agent episode reward: [-164.6219437418622, -164.6219437418622, -164.6219437418622], time: 163.776
steps: 424975, episodes: 17000, mean episode variance: 34.07900281882286, agent episode variance: [31.379535995483398, 1.322254930973053, 1.3772118923664094], time: 163.776
Running avgs for agent 0: q_loss: 428988.3125, p_loss: 112.9483871459961, mean_rew: -6.897697733624944, variance: 125.51814398193359, lamda: 1.3859647512435913
Running avgs for agent 1: q_loss: 13.930902481079102, p_loss: -3.509986162185669, mean_rew: -6.891125118455308, variance: 5.28902006149292, lamda: 1.3271757364273071
Running avgs for agent 2: q_loss: 11.63587760925293, p_loss: -3.5373823642730713, mean_rew: -6.903991040733232, variance: 5.508847713470459, lamda: 1.2840352058410645

steps: 449975, episodes: 18000, mean episode reward: -495.0881215937324, agent episode reward: [-165.02937386457745, -165.02937386457745, -165.02937386457745], time: 158.397
steps: 449975, episodes: 18000, mean episode variance: 44.06376489925385, agent episode variance: [41.407255374908445, 1.3097600836753844, 1.3467494406700133], time: 158.398
Running avgs for agent 0: q_loss: 555627.25, p_loss: 114.45101928710938, mean_rew: -6.878198152203178, variance: 165.62902149963378, lamda: 1.4109690189361572
Running avgs for agent 1: q_loss: 12.411981582641602, p_loss: -3.5202364921569824, mean_rew: -6.876495496623595, variance: 5.239039897918701, lamda: 1.3391332626342773
Running avgs for agent 2: q_loss: 11.50972843170166, p_loss: -3.5135176181793213, mean_rew: -6.867700803954058, variance: 5.386998176574707, lamda: 1.2946171760559082

steps: 474975, episodes: 19000, mean episode reward: -500.77700001765254, agent episode reward: [-166.92566667255085, -166.92566667255085, -166.92566667255085], time: 167.964
steps: 474975, episodes: 19000, mean episode variance: 39.1579124174118, agent episode variance: [36.50580063438416, 1.3024513139724732, 1.3496604690551757], time: 167.964
Running avgs for agent 0: q_loss: 520553.28125, p_loss: 115.63709259033203, mean_rew: -6.869813265196992, variance: 146.02320253753663, lamda: 1.4359731674194336
Running avgs for agent 1: q_loss: 12.58349609375, p_loss: -3.5310444831848145, mean_rew: -6.866935275611368, variance: 5.209805011749268, lamda: 1.355959177017212
Running avgs for agent 2: q_loss: 11.224431991577148, p_loss: -3.5116260051727295, mean_rew: -6.863972107398879, variance: 5.398642063140869, lamda: 1.2984856367111206

steps: 499975, episodes: 20000, mean episode reward: -497.4956301714917, agent episode reward: [-165.8318767238306, -165.8318767238306, -165.8318767238306], time: 171.262
steps: 499975, episodes: 20000, mean episode variance: 23.871495236873628, agent episode variance: [21.25874401140213, 1.2781808395385743, 1.3345703859329223], time: 171.262
Running avgs for agent 0: q_loss: 336762.0625, p_loss: 117.00968933105469, mean_rew: -6.8543368318789675, variance: 85.03497604560852, lamda: 1.4609771966934204
Running avgs for agent 1: q_loss: 10.468008041381836, p_loss: -3.5127346515655518, mean_rew: -6.8571580877977985, variance: 5.1127238273620605, lamda: 1.3736205101013184
Running avgs for agent 2: q_loss: 9.464670181274414, p_loss: -3.5087287425994873, mean_rew: -6.856208487692374, variance: 5.338281631469727, lamda: 1.307428002357483

steps: 524975, episodes: 21000, mean episode reward: -505.7827765010797, agent episode reward: [-168.59425883369323, -168.59425883369323, -168.59425883369323], time: 172.948
steps: 524975, episodes: 21000, mean episode variance: 38.341125468969345, agent episode variance: [35.754008827209475, 1.2626632330417633, 1.3244534087181092], time: 172.949
Running avgs for agent 0: q_loss: 542786.3125, p_loss: 118.29830169677734, mean_rew: -6.853912207026653, variance: 143.0160353088379, lamda: 1.4859814643859863
Running avgs for agent 1: q_loss: 11.492568969726562, p_loss: -3.5105698108673096, mean_rew: -6.838853293644992, variance: 5.050652980804443, lamda: 1.394950032234192
Running avgs for agent 2: q_loss: 9.496464729309082, p_loss: -3.493143081665039, mean_rew: -6.848382444135226, variance: 5.297813415527344, lamda: 1.3241136074066162

steps: 549975, episodes: 22000, mean episode reward: -509.8933175351747, agent episode reward: [-169.96443917839156, -169.96443917839156, -169.96443917839156], time: 183.949
steps: 549975, episodes: 22000, mean episode variance: 24.959948838949202, agent episode variance: [22.410477519989012, 1.2465939154624939, 1.302877403497696], time: 183.949
Running avgs for agent 0: q_loss: 337697.46875, p_loss: 119.39940643310547, mean_rew: -6.846776537930029, variance: 89.64191007995605, lamda: 1.5109856128692627
Running avgs for agent 1: q_loss: 11.49357795715332, p_loss: -3.5118277072906494, mean_rew: -6.849608839260094, variance: 4.98637580871582, lamda: 1.4068864583969116
Running avgs for agent 2: q_loss: 9.045930862426758, p_loss: -3.4926388263702393, mean_rew: -6.83808895324569, variance: 5.211509704589844, lamda: 1.3362685441970825

steps: 574975, episodes: 23000, mean episode reward: -516.4685098817298, agent episode reward: [-172.15616996057662, -172.15616996057662, -172.15616996057662], time: 183.988
steps: 574975, episodes: 23000, mean episode variance: 25.044656573534013, agent episode variance: [22.505920844078062, 1.2421559555530548, 1.296579773902893], time: 183.989
Running avgs for agent 0: q_loss: 352808.0, p_loss: 120.56793975830078, mean_rew: -6.842031878976742, variance: 90.02368337631225, lamda: 1.5359896421432495
Running avgs for agent 1: q_loss: 10.826740264892578, p_loss: -3.489114761352539, mean_rew: -6.84378989027385, variance: 4.968623638153076, lamda: 1.4215657711029053
Running avgs for agent 2: q_loss: 8.410223007202148, p_loss: -3.50229549407959, mean_rew: -6.842287034093497, variance: 5.186319351196289, lamda: 1.3504250049591064

steps: 599975, episodes: 24000, mean episode reward: -519.9197179157358, agent episode reward: [-173.3065726385786, -173.3065726385786, -173.3065726385786], time: 182.958
steps: 599975, episodes: 24000, mean episode variance: 29.240638439893722, agent episode variance: [26.728264877319337, 1.2301051342487335, 1.2822684283256531], time: 182.958
Running avgs for agent 0: q_loss: 380290.8125, p_loss: 121.48770141601562, mean_rew: -6.845043190308788, variance: 106.91305950927735, lamda: 1.5609939098358154
Running avgs for agent 1: q_loss: 11.865984916687012, p_loss: -3.493590831756592, mean_rew: -6.846668655786657, variance: 4.920421123504639, lamda: 1.4322118759155273
Running avgs for agent 2: q_loss: 9.191777229309082, p_loss: -3.505664110183716, mean_rew: -6.843611391434051, variance: 5.129073619842529, lamda: 1.3654440641403198

steps: 624975, episodes: 25000, mean episode reward: -527.933626321407, agent episode reward: [-175.977875440469, -175.977875440469, -175.977875440469], time: 182.672
steps: 624975, episodes: 25000, mean episode variance: 47.13114851140976, agent episode variance: [44.6470082244873, 1.2090263051986694, 1.2751139817237853], time: 182.672
Running avgs for agent 0: q_loss: 653206.75, p_loss: 122.37915802001953, mean_rew: -6.85150273071388, variance: 178.5880328979492, lamda: 1.5859980583190918
Running avgs for agent 1: q_loss: 12.347803115844727, p_loss: -3.5196030139923096, mean_rew: -6.851495273198235, variance: 4.8361053466796875, lamda: 1.44150710105896
Running avgs for agent 2: q_loss: 9.338287353515625, p_loss: -3.4990861415863037, mean_rew: -6.853776804017598, variance: 5.100456237792969, lamda: 1.377505898475647

steps: 649975, episodes: 26000, mean episode reward: -533.2393574484453, agent episode reward: [-177.7464524828151, -177.7464524828151, -177.7464524828151], time: 172.332
steps: 649975, episodes: 26000, mean episode variance: 46.34286724972725, agent episode variance: [43.86930433654785, 1.2134466454982757, 1.2601162676811217], time: 172.333
Running avgs for agent 0: q_loss: 685042.0625, p_loss: 123.00129699707031, mean_rew: -6.867390788234079, variance: 175.4772173461914, lamda: 1.6110020875930786
Running avgs for agent 1: q_loss: 11.787065505981445, p_loss: -3.5287795066833496, mean_rew: -6.865594244945776, variance: 4.853786945343018, lamda: 1.4484922885894775
Running avgs for agent 2: q_loss: 8.285299301147461, p_loss: -3.511446475982666, mean_rew: -6.860603522214589, variance: 5.040464878082275, lamda: 1.3958336114883423

steps: 674975, episodes: 27000, mean episode reward: -534.8916856704203, agent episode reward: [-178.29722855680677, -178.29722855680677, -178.29722855680677], time: 171.722
steps: 674975, episodes: 27000, mean episode variance: 45.9885992000103, agent episode variance: [43.53464009094238, 1.2061450493335724, 1.2478140597343446], time: 171.723
Running avgs for agent 0: q_loss: 677459.0625, p_loss: 123.49250793457031, mean_rew: -6.872377500056388, variance: 174.13856036376953, lamda: 1.6360063552856445
Running avgs for agent 1: q_loss: 10.490315437316895, p_loss: -3.535282611846924, mean_rew: -6.867388982502947, variance: 4.824580192565918, lamda: 1.4608592987060547
Running avgs for agent 2: q_loss: 9.616935729980469, p_loss: -3.5184266567230225, mean_rew: -6.87040363614046, variance: 4.991255760192871, lamda: 1.4051073789596558

steps: 699975, episodes: 28000, mean episode reward: -534.554281359232, agent episode reward: [-178.18476045307733, -178.18476045307733, -178.18476045307733], time: 172.035
steps: 699975, episodes: 28000, mean episode variance: 46.55463787722588, agent episode variance: [44.12602830505371, 1.1857533745765687, 1.2428561975955963], time: 172.035
Running avgs for agent 0: q_loss: 701600.25, p_loss: 123.85707092285156, mean_rew: -6.881536494125364, variance: 176.50411322021483, lamda: 1.661010503768921
Running avgs for agent 1: q_loss: 10.57773494720459, p_loss: -3.537811279296875, mean_rew: -6.886510202406189, variance: 4.743013858795166, lamda: 1.477426290512085
Running avgs for agent 2: q_loss: 9.554976463317871, p_loss: -3.5181918144226074, mean_rew: -6.882976889389377, variance: 4.971424579620361, lamda: 1.4119497537612915

steps: 724975, episodes: 29000, mean episode reward: -534.6378339549359, agent episode reward: [-178.21261131831196, -178.21261131831196, -178.21261131831196], time: 169.248
steps: 724975, episodes: 29000, mean episode variance: 46.14026257896423, agent episode variance: [43.730114784240726, 1.1805436460971832, 1.2296041486263276], time: 169.248
Running avgs for agent 0: q_loss: 711109.5, p_loss: 124.37374114990234, mean_rew: -6.8944712154061705, variance: 174.9204591369629, lamda: 1.6860145330429077
Running avgs for agent 1: q_loss: 10.14465045928955, p_loss: -3.5336105823516846, mean_rew: -6.882309276651444, variance: 4.722174167633057, lamda: 1.4823449850082397
Running avgs for agent 2: q_loss: 9.375481605529785, p_loss: -3.532155752182007, mean_rew: -6.887545944917903, variance: 4.918416500091553, lamda: 1.4190024137496948

steps: 749975, episodes: 30000, mean episode reward: -540.1662965483271, agent episode reward: [-180.0554321827757, -180.0554321827757, -180.0554321827757], time: 172.138
steps: 749975, episodes: 30000, mean episode variance: 45.769081553459166, agent episode variance: [43.33437463378906, 1.1926826183795929, 1.2420243012905121], time: 172.139
Running avgs for agent 0: q_loss: 701587.0625, p_loss: 124.75188446044922, mean_rew: -6.894589927509278, variance: 173.33749853515624, lamda: 1.7110188007354736
Running avgs for agent 1: q_loss: 9.433274269104004, p_loss: -3.53802752494812, mean_rew: -6.901374320042765, variance: 4.770730495452881, lamda: 1.489452838897705
Running avgs for agent 2: q_loss: 8.52589225769043, p_loss: -3.5356056690216064, mean_rew: -6.903128342586392, variance: 4.968096733093262, lamda: 1.425710678100586

steps: 774975, episodes: 31000, mean episode reward: -537.6023281292481, agent episode reward: [-179.2007760430827, -179.2007760430827, -179.2007760430827], time: 170.421
steps: 774975, episodes: 31000, mean episode variance: 45.075918806552885, agent episode variance: [42.670346336364744, 1.1766004939079284, 1.2289719762802125], time: 170.422
Running avgs for agent 0: q_loss: 734388.875, p_loss: 125.40943145751953, mean_rew: -6.91273285014397, variance: 170.68138534545898, lamda: 1.73602294921875
Running avgs for agent 1: q_loss: 9.054615020751953, p_loss: -3.5439741611480713, mean_rew: -6.911767594801124, variance: 4.706401824951172, lamda: 1.5084500312805176
Running avgs for agent 2: q_loss: 7.376820087432861, p_loss: -3.533703565597534, mean_rew: -6.907541873092226, variance: 4.915887832641602, lamda: 1.4408979415893555

steps: 799975, episodes: 32000, mean episode reward: -545.3165599788911, agent episode reward: [-181.77218665963036, -181.77218665963036, -181.77218665963036], time: 169.888
steps: 799975, episodes: 32000, mean episode variance: 45.246911909103396, agent episode variance: [42.85938195800781, 1.1579288945198059, 1.2296010565757751], time: 169.888
Running avgs for agent 0: q_loss: 764754.625, p_loss: 126.0979232788086, mean_rew: -6.922156612012823, variance: 171.43752783203124, lamda: 1.7610268592834473
Running avgs for agent 1: q_loss: 8.76506233215332, p_loss: -3.5559329986572266, mean_rew: -6.919574807186578, variance: 4.631715297698975, lamda: 1.5196470022201538
Running avgs for agent 2: q_loss: 8.921431541442871, p_loss: -3.540196418762207, mean_rew: -6.927681019569632, variance: 4.9184041023254395, lamda: 1.452968955039978

steps: 824975, episodes: 33000, mean episode reward: -550.8815335286259, agent episode reward: [-183.6271778428753, -183.6271778428753, -183.6271778428753], time: 170.774
steps: 824975, episodes: 33000, mean episode variance: 43.80381831336022, agent episode variance: [41.43253712463379, 1.1581009447574615, 1.2131802439689636], time: 170.775
Running avgs for agent 0: q_loss: 744214.8125, p_loss: 126.60054779052734, mean_rew: -6.936401512234204, variance: 165.73014849853516, lamda: 1.7860311269760132
Running avgs for agent 1: q_loss: 7.988528251647949, p_loss: -3.542024850845337, mean_rew: -6.937426410420281, variance: 4.632403373718262, lamda: 1.5342663526535034
Running avgs for agent 2: q_loss: 7.837038040161133, p_loss: -3.5509583950042725, mean_rew: -6.937578858486659, variance: 4.852720737457275, lamda: 1.4578163623809814

steps: 849975, episodes: 34000, mean episode reward: -560.8603099790441, agent episode reward: [-186.9534366596814, -186.9534366596814, -186.9534366596814], time: 179.066
steps: 849975, episodes: 34000, mean episode variance: 44.5611090233326, agent episode variance: [42.206852493286135, 1.148457236766815, 1.2057992932796477], time: 179.067
Running avgs for agent 0: q_loss: 782793.25, p_loss: 127.1043930053711, mean_rew: -6.947120766421346, variance: 168.82740997314454, lamda: 1.811035394668579
Running avgs for agent 1: q_loss: 9.001564979553223, p_loss: -3.5572705268859863, mean_rew: -6.951278919607082, variance: 4.593829154968262, lamda: 1.547613501548767
Running avgs for agent 2: q_loss: 7.834737300872803, p_loss: -3.562204599380493, mean_rew: -6.957020489759835, variance: 4.823196887969971, lamda: 1.4702399969100952

steps: 874975, episodes: 35000, mean episode reward: -566.4257898113469, agent episode reward: [-188.80859660378232, -188.80859660378232, -188.80859660378232], time: 173.202
steps: 874975, episodes: 35000, mean episode variance: 45.960673196792605, agent episode variance: [43.62212252807617, 1.1394959933757782, 1.1990546753406526], time: 173.203
Running avgs for agent 0: q_loss: 768581.25, p_loss: 127.49613952636719, mean_rew: -6.960777651565806, variance: 174.48849011230467, lamda: 1.836039423942566
Running avgs for agent 1: q_loss: 8.774185180664062, p_loss: -3.564340591430664, mean_rew: -6.95813516144964, variance: 4.557983875274658, lamda: 1.5524100065231323
Running avgs for agent 2: q_loss: 8.075531959533691, p_loss: -3.5682082176208496, mean_rew: -6.968927856211554, variance: 4.7962188720703125, lamda: 1.4753481149673462

steps: 899975, episodes: 36000, mean episode reward: -569.084786200059, agent episode reward: [-189.69492873335304, -189.69492873335304, -189.69492873335304], time: 178.714
steps: 899975, episodes: 36000, mean episode variance: 45.2812816464901, agent episode variance: [42.93030812072754, 1.1481814715862275, 1.2027920541763306], time: 178.715
Running avgs for agent 0: q_loss: 788804.375, p_loss: 128.27239990234375, mean_rew: -6.983557991785556, variance: 171.72123248291015, lamda: 1.8610435724258423
Running avgs for agent 1: q_loss: 8.224907875061035, p_loss: -3.5683138370513916, mean_rew: -6.9759424040884275, variance: 4.592726230621338, lamda: 1.5582091808319092
Running avgs for agent 2: q_loss: 7.9402947425842285, p_loss: -3.5766944885253906, mean_rew: -6.983720473994306, variance: 4.811168670654297, lamda: 1.4767500162124634

steps: 924975, episodes: 37000, mean episode reward: -569.7554494799494, agent episode reward: [-189.91848315998317, -189.91848315998317, -189.91848315998317], time: 182.865
steps: 924975, episodes: 37000, mean episode variance: 45.96172083258629, agent episode variance: [43.60738725280762, 1.1501969709396362, 1.204136608839035], time: 182.866
Running avgs for agent 0: q_loss: 801444.875, p_loss: 128.99949645996094, mean_rew: -6.996484356974762, variance: 174.42954901123048, lamda: 1.8860478401184082
Running avgs for agent 1: q_loss: 8.203573226928711, p_loss: -3.5734267234802246, mean_rew: -6.997730062417492, variance: 4.600788116455078, lamda: 1.5619378089904785
Running avgs for agent 2: q_loss: 8.156750679016113, p_loss: -3.5955610275268555, mean_rew: -7.001830483145469, variance: 4.81654691696167, lamda: 1.4807345867156982

steps: 949975, episodes: 38000, mean episode reward: -566.6040934110515, agent episode reward: [-188.8680311370172, -188.8680311370172, -188.8680311370172], time: 184.313
steps: 949975, episodes: 38000, mean episode variance: 45.429098090410235, agent episode variance: [43.07029579162597, 1.1528733470439911, 1.205928951740265], time: 184.314
Running avgs for agent 0: q_loss: 832978.25, p_loss: 129.77703857421875, mean_rew: -7.011119397563513, variance: 172.2811831665039, lamda: 1.911051869392395
Running avgs for agent 1: q_loss: 8.179450035095215, p_loss: -3.5834639072418213, mean_rew: -7.016913861266603, variance: 4.6114935874938965, lamda: 1.5637154579162598
Running avgs for agent 2: q_loss: 7.971063613891602, p_loss: -3.5922722816467285, mean_rew: -7.01867790148661, variance: 4.823716163635254, lamda: 1.483888864517212

steps: 974975, episodes: 39000, mean episode reward: -567.0648369065053, agent episode reward: [-189.02161230216845, -189.02161230216845, -189.02161230216845], time: 181.286
steps: 974975, episodes: 39000, mean episode variance: 45.10053885054588, agent episode variance: [42.76057426452637, 1.1385016558170318, 1.2014629302024842], time: 181.287
Running avgs for agent 0: q_loss: 841416.9375, p_loss: 130.68643188476562, mean_rew: -7.026730017956357, variance: 171.04229705810548, lamda: 1.9360560178756714
Running avgs for agent 1: q_loss: 8.133061408996582, p_loss: -3.5951716899871826, mean_rew: -7.028592575067616, variance: 4.554007053375244, lamda: 1.564826250076294
Running avgs for agent 2: q_loss: 7.220515727996826, p_loss: -3.603447675704956, mean_rew: -7.031146893499439, variance: 4.805851459503174, lamda: 1.4923584461212158

steps: 999975, episodes: 40000, mean episode reward: -575.2015324662385, agent episode reward: [-191.73384415541284, -191.73384415541284, -191.73384415541284], time: 174.551
steps: 999975, episodes: 40000, mean episode variance: 45.455219435453415, agent episode variance: [43.11888394165039, 1.1434032618999481, 1.1929322319030762], time: 174.552
Running avgs for agent 0: q_loss: 836061.375, p_loss: 131.58563232421875, mean_rew: -7.042909333680531, variance: 172.47553576660155, lamda: 1.9610601663589478
Running avgs for agent 1: q_loss: 7.131943702697754, p_loss: -3.5966455936431885, mean_rew: -7.0455201808659025, variance: 4.573612689971924, lamda: 1.5669020414352417
Running avgs for agent 2: q_loss: 8.184076309204102, p_loss: -3.6163315773010254, mean_rew: -7.045167987178854, variance: 4.771728992462158, lamda: 1.4970329999923706

steps: 1024975, episodes: 41000, mean episode reward: -580.3675662840459, agent episode reward: [-193.45585542801533, -193.45585542801533, -193.45585542801533], time: 177.186
steps: 1024975, episodes: 41000, mean episode variance: 46.82118546247482, agent episode variance: [44.482986465454104, 1.1377215549945832, 1.2004774420261384], time: 177.187
Running avgs for agent 0: q_loss: 854781.3125, p_loss: 132.36965942382812, mean_rew: -7.063672956224402, variance: 177.93194586181642, lamda: 1.9860644340515137
Running avgs for agent 1: q_loss: 7.874664783477783, p_loss: -3.6060423851013184, mean_rew: -7.050735471870412, variance: 4.550886631011963, lamda: 1.5749884843826294
Running avgs for agent 2: q_loss: 7.69944429397583, p_loss: -3.608543872833252, mean_rew: -7.058418468340989, variance: 4.801909923553467, lamda: 1.4978328943252563

steps: 1049975, episodes: 42000, mean episode reward: -575.4569033191908, agent episode reward: [-191.8189677730636, -191.8189677730636, -191.8189677730636], time: 177.109
steps: 1049975, episodes: 42000, mean episode variance: 45.39180863857269, agent episode variance: [43.05959466552734, 1.1415636048316955, 1.1906503682136536], time: 177.109
Running avgs for agent 0: q_loss: 833521.4375, p_loss: 132.87332153320312, mean_rew: -7.042946079187938, variance: 172.23837866210937, lamda: 2.0110552310943604
Running avgs for agent 1: q_loss: 6.831176280975342, p_loss: -3.590592622756958, mean_rew: -7.041326838450155, variance: 4.566254615783691, lamda: 1.578513503074646
Running avgs for agent 2: q_loss: 5.285739898681641, p_loss: -3.606964349746704, mean_rew: -7.044384538826083, variance: 4.762601375579834, lamda: 1.5043017864227295

steps: 1074975, episodes: 43000, mean episode reward: -589.9002288344342, agent episode reward: [-196.63340961147804, -196.63340961147804, -196.63340961147804], time: 179.844
steps: 1074975, episodes: 43000, mean episode variance: 45.56319566679001, agent episode variance: [43.2342958984375, 1.1358188996315002, 1.1930808687210084], time: 179.845
Running avgs for agent 0: q_loss: 795955.3125, p_loss: 133.07139587402344, mean_rew: -7.025568547630232, variance: 172.93718359375, lamda: 2.036029577255249
Running avgs for agent 1: q_loss: 5.835476398468018, p_loss: -3.592433452606201, mean_rew: -7.02615099258544, variance: 4.543275356292725, lamda: 1.5790959596633911
Running avgs for agent 2: q_loss: 3.7841646671295166, p_loss: -3.6028876304626465, mean_rew: -7.030348209543319, variance: 4.7723236083984375, lamda: 1.5068217515945435

steps: 1099975, episodes: 44000, mean episode reward: -581.4305110449593, agent episode reward: [-193.8101703483198, -193.8101703483198, -193.8101703483198], time: 178.289
steps: 1099975, episodes: 44000, mean episode variance: 45.44932185983658, agent episode variance: [43.12639949035645, 1.1355439312458038, 1.1873784382343293], time: 178.289
Running avgs for agent 0: q_loss: 788859.5, p_loss: 133.06015014648438, mean_rew: -7.018528492460873, variance: 172.5055979614258, lamda: 2.0610039234161377
Running avgs for agent 1: q_loss: 5.569047451019287, p_loss: -3.5953917503356934, mean_rew: -7.022956049408131, variance: 4.542175769805908, lamda: 1.579105019569397
Running avgs for agent 2: q_loss: 3.4254038333892822, p_loss: -3.588603973388672, mean_rew: -7.017657436122471, variance: 4.749513626098633, lamda: 1.5077131986618042

steps: 1124975, episodes: 45000, mean episode reward: -578.1743888846211, agent episode reward: [-192.7247962948737, -192.7247962948737, -192.7247962948737], time: 178.386
steps: 1124975, episodes: 45000, mean episode variance: 44.88231346654892, agent episode variance: [42.557882919311524, 1.1356903500556945, 1.1887401971817018], time: 178.386
Running avgs for agent 0: q_loss: 799036.8125, p_loss: 133.2120361328125, mean_rew: -7.0372039482895286, variance: 170.2315316772461, lamda: 2.0859782695770264
Running avgs for agent 1: q_loss: 5.556617259979248, p_loss: -3.603572130203247, mean_rew: -7.03119293738483, variance: 4.54276180267334, lamda: 1.579135537147522
Running avgs for agent 2: q_loss: 3.3807578086853027, p_loss: -3.593518018722534, mean_rew: -7.030951730750634, variance: 4.754961013793945, lamda: 1.508080005645752

steps: 1149975, episodes: 46000, mean episode reward: -581.2877981825292, agent episode reward: [-193.7625993941764, -193.7625993941764, -193.7625993941764], time: 180.319
steps: 1149975, episodes: 46000, mean episode variance: 45.30854977893829, agent episode variance: [42.973897903442385, 1.1425976123809813, 1.1920542631149291], time: 180.319
Running avgs for agent 0: q_loss: 814462.1875, p_loss: 133.29000854492188, mean_rew: -7.057442695923635, variance: 171.89559161376954, lamda: 2.110952615737915
Running avgs for agent 1: q_loss: 5.4896345138549805, p_loss: -3.617122173309326, mean_rew: -7.057098682230644, variance: 4.570390701293945, lamda: 1.5791515111923218
Running avgs for agent 2: q_loss: 3.3065786361694336, p_loss: -3.6099181175231934, mean_rew: -7.054504124235735, variance: 4.768216609954834, lamda: 1.508409857749939

steps: 1174975, episodes: 47000, mean episode reward: -582.482708549451, agent episode reward: [-194.160902849817, -194.160902849817, -194.160902849817], time: 176.715
steps: 1174975, episodes: 47000, mean episode variance: 45.68753406238556, agent episode variance: [43.34196221923828, 1.1465586824417113, 1.1990131607055665], time: 176.715
Running avgs for agent 0: q_loss: 833195.5, p_loss: 133.60150146484375, mean_rew: -7.086930083763628, variance: 173.36784887695313, lamda: 2.1359269618988037
Running avgs for agent 1: q_loss: 5.670190334320068, p_loss: -3.6291897296905518, mean_rew: -7.0865110429206775, variance: 4.5862345695495605, lamda: 1.579206943511963
Running avgs for agent 2: q_loss: 3.822138786315918, p_loss: -3.628096580505371, mean_rew: -7.089502859146085, variance: 4.796052932739258, lamda: 1.5119255781173706

steps: 1199975, episodes: 48000, mean episode reward: -578.7413842567051, agent episode reward: [-192.91379475223508, -192.91379475223508, -192.91379475223508], time: 176.715
steps: 1199975, episodes: 48000, mean episode variance: 44.89409712481499, agent episode variance: [42.54022465515137, 1.1529654214382172, 1.2009070482254027], time: 176.715
Running avgs for agent 0: q_loss: 847768.6875, p_loss: 134.07846069335938, mean_rew: -7.127441272390821, variance: 170.16089862060548, lamda: 2.1609013080596924
Running avgs for agent 1: q_loss: 5.623071193695068, p_loss: -3.6491899490356445, mean_rew: -7.120955344479591, variance: 4.611861228942871, lamda: 1.5793083906173706
Running avgs for agent 2: q_loss: 5.199130058288574, p_loss: -3.6404974460601807, mean_rew: -7.1240983395068715, variance: 4.803627967834473, lamda: 1.5146559476852417

steps: 1224975, episodes: 49000, mean episode reward: -576.8005737800659, agent episode reward: [-192.26685792668863, -192.26685792668863, -192.26685792668863], time: 172.411
steps: 1224975, episodes: 49000, mean episode variance: 45.0748104326725, agent episode variance: [42.713080780029294, 1.1541758754253388, 1.207553777217865], time: 172.412
Running avgs for agent 0: q_loss: 867112.1875, p_loss: 134.4279327392578, mean_rew: -7.155665751827784, variance: 170.85232312011718, lamda: 2.185875654220581
Running avgs for agent 1: q_loss: 5.645725727081299, p_loss: -3.66825532913208, mean_rew: -7.156339830933274, variance: 4.616703033447266, lamda: 1.579308271408081
Running avgs for agent 2: q_loss: 5.8344035148620605, p_loss: -3.6576952934265137, mean_rew: -7.149647523141533, variance: 4.830214977264404, lamda: 1.5151176452636719

steps: 1249975, episodes: 50000, mean episode reward: -584.3904630621066, agent episode reward: [-194.7968210207022, -194.7968210207022, -194.7968210207022], time: 160.467
steps: 1249975, episodes: 50000, mean episode variance: 45.427976746320724, agent episode variance: [43.05856939697266, 1.1586258056163787, 1.2107815437316896], time: 160.467
Running avgs for agent 0: q_loss: 892355.8125, p_loss: 134.91458129882812, mean_rew: -7.184289584062127, variance: 172.23427758789063, lamda: 2.2108500003814697
Running avgs for agent 1: q_loss: 5.767378807067871, p_loss: -3.680973529815674, mean_rew: -7.184500391422314, variance: 4.63450288772583, lamda: 1.5793085098266602
Running avgs for agent 2: q_loss: 5.860143661499023, p_loss: -3.674699544906616, mean_rew: -7.17899843389278, variance: 4.843125820159912, lamda: 1.5151176452636719

steps: 1274975, episodes: 51000, mean episode reward: -577.9553578495959, agent episode reward: [-192.65178594986526, -192.65178594986526, -192.65178594986526], time: 157.356
steps: 1274975, episodes: 51000, mean episode variance: 45.266210645675656, agent episode variance: [42.874666549682615, 1.1683296995162964, 1.2232143964767457], time: 157.357
Running avgs for agent 0: q_loss: 910272.3125, p_loss: 135.4600067138672, mean_rew: -7.220679012380145, variance: 171.49866619873046, lamda: 2.2358243465423584
Running avgs for agent 1: q_loss: 5.702042102813721, p_loss: -3.6936137676239014, mean_rew: -7.217494886022228, variance: 4.673318862915039, lamda: 1.5793085098266602
Running avgs for agent 2: q_loss: 5.972560882568359, p_loss: -3.6940572261810303, mean_rew: -7.227526120383129, variance: 4.892857551574707, lamda: 1.5151176452636719

steps: 1299975, episodes: 52000, mean episode reward: -573.8439077424623, agent episode reward: [-191.28130258082078, -191.28130258082078, -191.28130258082078], time: 159.27
steps: 1299975, episodes: 52000, mean episode variance: 45.404937264442445, agent episode variance: [43.01155999755859, 1.174360993385315, 1.2190162734985353], time: 159.27
Running avgs for agent 0: q_loss: 932331.8125, p_loss: 135.85150146484375, mean_rew: -7.2480091815512475, variance: 172.04623999023437, lamda: 2.260798931121826
Running avgs for agent 1: q_loss: 5.8326497077941895, p_loss: -3.7133333683013916, mean_rew: -7.249188230008053, variance: 4.697443962097168, lamda: 1.5793083906173706
Running avgs for agent 2: q_loss: 5.945493221282959, p_loss: -3.7111024856567383, mean_rew: -7.2515451098759485, variance: 4.876065254211426, lamda: 1.5151176452636719

steps: 1324975, episodes: 53000, mean episode reward: -575.603113757481, agent episode reward: [-191.86770458582703, -191.86770458582703, -191.86770458582703], time: 159.047
steps: 1324975, episodes: 53000, mean episode variance: 45.51167345476151, agent episode variance: [43.111214767456055, 1.1756572976112365, 1.2248013896942138], time: 159.048
Running avgs for agent 0: q_loss: 948080.0, p_loss: 136.39303588867188, mean_rew: -7.274595534401365, variance: 172.44485906982422, lamda: 2.2857730388641357
Running avgs for agent 1: q_loss: 5.787233352661133, p_loss: -3.723149299621582, mean_rew: -7.274628505271672, variance: 4.702629089355469, lamda: 1.5793086290359497
Running avgs for agent 2: q_loss: 5.9819841384887695, p_loss: -3.7163801193237305, mean_rew: -7.277542736342708, variance: 4.899206161499023, lamda: 1.515142560005188

steps: 1349975, episodes: 54000, mean episode reward: -580.0165714956373, agent episode reward: [-193.3388571652124, -193.3388571652124, -193.3388571652124], time: 158.004
steps: 1349975, episodes: 54000, mean episode variance: 44.62925916624069, agent episode variance: [42.20909693908691, 1.1870830302238464, 1.2330791969299317], time: 158.005
Running avgs for agent 0: q_loss: 964438.625, p_loss: 137.06369018554688, mean_rew: -7.3152689582386685, variance: 168.83638775634765, lamda: 2.3107476234436035
Running avgs for agent 1: q_loss: 5.859204292297363, p_loss: -3.737422466278076, mean_rew: -7.307398133882894, variance: 4.7483320236206055, lamda: 1.5793085098266602
Running avgs for agent 2: q_loss: 4.843042373657227, p_loss: -3.739635705947876, mean_rew: -7.311914226430679, variance: 4.93231725692749, lamda: 1.5151935815811157

steps: 1374975, episodes: 55000, mean episode reward: -578.4712662273489, agent episode reward: [-192.82375540911633, -192.82375540911633, -192.82375540911633], time: 174.081
steps: 1374975, episodes: 55000, mean episode variance: 46.01392599987984, agent episode variance: [43.588203872680666, 1.1914275619983674, 1.2342945652008057], time: 174.082
Running avgs for agent 0: q_loss: 975828.4375, p_loss: 137.5171661376953, mean_rew: -7.3407329763673586, variance: 174.35281549072266, lamda: 2.335721731185913
Running avgs for agent 1: q_loss: 5.87107515335083, p_loss: -3.7593815326690674, mean_rew: -7.337794832923211, variance: 4.765710830688477, lamda: 1.5793083906173706
Running avgs for agent 2: q_loss: 3.895209789276123, p_loss: -3.753384828567505, mean_rew: -7.342727957154694, variance: 4.937178611755371, lamda: 1.515656590461731

steps: 1399975, episodes: 56000, mean episode reward: -584.6164681207584, agent episode reward: [-194.87215604025278, -194.87215604025278, -194.87215604025278], time: 181.242
steps: 1399975, episodes: 56000, mean episode variance: 45.592358044624326, agent episode variance: [43.16283479309082, 1.188343647956848, 1.2411796035766602], time: 181.243
Running avgs for agent 0: q_loss: 1010132.1875, p_loss: 138.24070739746094, mean_rew: -7.366503170254427, variance: 172.6513391723633, lamda: 2.3606960773468018
Running avgs for agent 1: q_loss: 5.960367202758789, p_loss: -3.7709615230560303, mean_rew: -7.3696649867071145, variance: 4.7533745765686035, lamda: 1.5793086290359497
Running avgs for agent 2: q_loss: 6.170213222503662, p_loss: -3.7678568363189697, mean_rew: -7.370584987689833, variance: 4.964718818664551, lamda: 1.5162546634674072

steps: 1424975, episodes: 57000, mean episode reward: -577.822125295517, agent episode reward: [-192.60737509850568, -192.60737509850568, -192.60737509850568], time: 178.374
steps: 1424975, episodes: 57000, mean episode variance: 45.349522186279295, agent episode variance: [42.897264282226566, 1.1975153665542602, 1.2547425374984742], time: 178.375
Running avgs for agent 0: q_loss: 1030753.9375, p_loss: 138.8372344970703, mean_rew: -7.392119650668001, variance: 171.58905712890626, lamda: 2.3856704235076904
Running avgs for agent 1: q_loss: 5.855984210968018, p_loss: -3.7815141677856445, mean_rew: -7.397338464008273, variance: 4.7900614738464355, lamda: 1.5793085098266602
Running avgs for agent 2: q_loss: 6.159704208374023, p_loss: -3.7780370712280273, mean_rew: -7.398002175103139, variance: 5.018969535827637, lamda: 1.516262412071228

steps: 1449975, episodes: 58000, mean episode reward: -585.6276669052704, agent episode reward: [-195.20922230175685, -195.20922230175685, -195.20922230175685], time: 176.365
steps: 1449975, episodes: 58000, mean episode variance: 46.536883893966674, agent episode variance: [44.084512634277345, 1.2077029955387115, 1.2446682641506195], time: 176.366
Running avgs for agent 0: q_loss: 1059923.125, p_loss: 139.4281005859375, mean_rew: -7.425397980286537, variance: 176.33805053710938, lamda: 2.410645008087158
Running avgs for agent 1: q_loss: 5.914412021636963, p_loss: -3.802133083343506, mean_rew: -7.430317452634548, variance: 4.830811977386475, lamda: 1.5793083906173706
Running avgs for agent 2: q_loss: 6.133911609649658, p_loss: -3.7963969707489014, mean_rew: -7.427095763368622, variance: 4.978672981262207, lamda: 1.5162625312805176

steps: 1474975, episodes: 59000, mean episode reward: -588.9991211390949, agent episode reward: [-196.3330403796983, -196.3330403796983, -196.3330403796983], time: 177.414
steps: 1474975, episodes: 59000, mean episode variance: 47.13120905780792, agent episode variance: [44.6658429107666, 1.2085361983776093, 1.2568299486637116], time: 177.414
Running avgs for agent 0: q_loss: 1082207.5, p_loss: 139.97787475585938, mean_rew: -7.4544557230333135, variance: 178.6633716430664, lamda: 2.4356191158294678
Running avgs for agent 1: q_loss: 5.9252238273620605, p_loss: -3.818223237991333, mean_rew: -7.46131394189824, variance: 4.834144592285156, lamda: 1.5793086290359497
Running avgs for agent 2: q_loss: 6.199732303619385, p_loss: -3.81708025932312, mean_rew: -7.457281074260166, variance: 5.02731990814209, lamda: 1.5162626504898071

steps: 1499975, episodes: 60000, mean episode reward: -580.2173995307083, agent episode reward: [-193.40579984356947, -193.40579984356947, -193.40579984356947], time: 176.952
steps: 1499975, episodes: 60000, mean episode variance: 48.12942521929741, agent episode variance: [45.65280508422852, 1.2094417126178743, 1.2671784224510192], time: 176.952
Running avgs for agent 0: q_loss: 1094409.125, p_loss: 140.21763610839844, mean_rew: -7.487370990644555, variance: 182.61122033691407, lamda: 2.4605937004089355
Running avgs for agent 1: q_loss: 6.053391933441162, p_loss: -3.8397066593170166, mean_rew: -7.488700757166746, variance: 4.837767124176025, lamda: 1.5793085098266602
Running avgs for agent 2: q_loss: 6.280803680419922, p_loss: -3.8234310150146484, mean_rew: -7.489017483148955, variance: 5.068713665008545, lamda: 1.516262412071228

steps: 1524975, episodes: 61000, mean episode reward: -578.0884344045738, agent episode reward: [-192.69614480152464, -192.69614480152464, -192.69614480152464], time: 177.7
steps: 1524975, episodes: 61000, mean episode variance: 46.742789068222045, agent episode variance: [44.25823974609375, 1.2197309386730195, 1.2648183834552764], time: 177.701
Running avgs for agent 0: q_loss: 1119568.25, p_loss: 140.5525360107422, mean_rew: -7.512643993048756, variance: 177.032958984375, lamda: 2.485567808151245
Running avgs for agent 1: q_loss: 5.871757984161377, p_loss: -3.842005729675293, mean_rew: -7.507265635968483, variance: 4.8789238929748535, lamda: 1.5793083906173706
Running avgs for agent 2: q_loss: 6.260774612426758, p_loss: -3.8393876552581787, mean_rew: -7.51618021043467, variance: 5.0592732429504395, lamda: 1.5162625312805176

steps: 1549975, episodes: 62000, mean episode reward: -575.0236078134628, agent episode reward: [-191.67453593782096, -191.67453593782096, -191.67453593782096], time: 172.601
steps: 1549975, episodes: 62000, mean episode variance: 48.10349351072311, agent episode variance: [45.61259950256348, 1.2197469041347504, 1.2711471040248872], time: 172.601
Running avgs for agent 0: q_loss: 1149640.25, p_loss: 140.8929443359375, mean_rew: -7.540432752066299, variance: 182.4503980102539, lamda: 2.5105419158935547
Running avgs for agent 1: q_loss: 5.993454456329346, p_loss: -3.865098714828491, mean_rew: -7.539382467138034, variance: 4.878987789154053, lamda: 1.5793086290359497
Running avgs for agent 2: q_loss: 6.358745098114014, p_loss: -3.8580522537231445, mean_rew: -7.542565509827757, variance: 5.084588527679443, lamda: 1.5162626504898071

steps: 1574975, episodes: 63000, mean episode reward: -582.9852776270159, agent episode reward: [-194.32842587567197, -194.32842587567197, -194.32842587567197], time: 172.744
steps: 1574975, episodes: 63000, mean episode variance: 46.27771667766571, agent episode variance: [43.77303939819336, 1.2244571740627288, 1.2802201054096223], time: 172.745
Running avgs for agent 0: q_loss: 1161998.625, p_loss: 141.324951171875, mean_rew: -7.559330648507295, variance: 175.09215759277345, lamda: 2.5355165004730225
Running avgs for agent 1: q_loss: 5.990008354187012, p_loss: -3.8707001209259033, mean_rew: -7.557684288638891, variance: 4.897829055786133, lamda: 1.5793085098266602
Running avgs for agent 2: q_loss: 6.319292068481445, p_loss: -3.856038808822632, mean_rew: -7.556185073593113, variance: 5.120881080627441, lamda: 1.516262412071228

steps: 1599975, episodes: 64000, mean episode reward: -578.9380128195887, agent episode reward: [-192.97933760652958, -192.97933760652958, -192.97933760652958], time: 176.277
steps: 1599975, episodes: 64000, mean episode variance: 47.166653974533084, agent episode variance: [44.67032821655273, 1.2275065939426422, 1.2688191640377045], time: 176.278
Running avgs for agent 0: q_loss: 1180702.375, p_loss: 141.59060668945312, mean_rew: -7.57713706060441, variance: 178.68131286621093, lamda: 2.5604910850524902
Running avgs for agent 1: q_loss: 6.077808380126953, p_loss: -3.878114938735962, mean_rew: -7.580200203374365, variance: 4.910026550292969, lamda: 1.5793083906173706
Running avgs for agent 2: q_loss: 6.387448310852051, p_loss: -3.8788833618164062, mean_rew: -7.579430609989211, variance: 5.075276851654053, lamda: 1.5162625312805176

steps: 1624975, episodes: 65000, mean episode reward: -575.3777546873993, agent episode reward: [-191.79258489579976, -191.79258489579976, -191.79258489579976], time: 177.041
steps: 1624975, episodes: 65000, mean episode variance: 48.053328645944596, agent episode variance: [45.53781770324707, 1.2305748541355133, 1.2849360885620118], time: 177.041
Running avgs for agent 0: q_loss: 1199805.875, p_loss: 141.97412109375, mean_rew: -7.593402064515328, variance: 182.1512708129883, lamda: 2.585465431213379
Running avgs for agent 1: q_loss: 6.120180606842041, p_loss: -3.8919291496276855, mean_rew: -7.597539553190006, variance: 4.922299861907959, lamda: 1.5793086290359497
Running avgs for agent 2: q_loss: 6.328723430633545, p_loss: -3.882317543029785, mean_rew: -7.597065445022966, variance: 5.139744281768799, lamda: 1.5162626504898071

steps: 1649975, episodes: 66000, mean episode reward: -573.8794678081457, agent episode reward: [-191.2931559360486, -191.2931559360486, -191.2931559360486], time: 174.607
steps: 1649975, episodes: 66000, mean episode variance: 46.983460130691526, agent episode variance: [44.46511456298828, 1.2375549743175507, 1.2807905933856965], time: 174.607
Running avgs for agent 0: q_loss: 1213142.0, p_loss: 142.4510040283203, mean_rew: -7.611661172477458, variance: 177.86045825195313, lamda: 2.6104397773742676
Running avgs for agent 1: q_loss: 6.069737911224365, p_loss: -3.898683786392212, mean_rew: -7.6108921578181485, variance: 4.950219631195068, lamda: 1.5793085098266602
Running avgs for agent 2: q_loss: 6.364120006561279, p_loss: -3.8896634578704834, mean_rew: -7.606833843713316, variance: 5.123162746429443, lamda: 1.516262412071228

steps: 1674975, episodes: 67000, mean episode reward: -579.6720952576625, agent episode reward: [-193.22403175255417, -193.22403175255417, -193.22403175255417], time: 175.879
steps: 1674975, episodes: 67000, mean episode variance: 47.56389782357216, agent episode variance: [45.05773570251465, 1.2331898264884948, 1.2729722945690154], time: 175.88
Running avgs for agent 0: q_loss: 1226311.25, p_loss: 142.71437072753906, mean_rew: -7.623408070225554, variance: 180.2309428100586, lamda: 2.635413885116577
Running avgs for agent 1: q_loss: 5.996557712554932, p_loss: -3.905839443206787, mean_rew: -7.626142402685406, variance: 4.932758808135986, lamda: 1.5793083906173706
Running avgs for agent 2: q_loss: 6.373050689697266, p_loss: -3.898895502090454, mean_rew: -7.626618520135806, variance: 5.09188985824585, lamda: 1.516262412071228

steps: 1699975, episodes: 68000, mean episode reward: -579.0971747614788, agent episode reward: [-193.0323915871596, -193.0323915871596, -193.0323915871596], time: 173.83
steps: 1699975, episodes: 68000, mean episode variance: 46.97986873435974, agent episode variance: [44.45388075256348, 1.2390867989063263, 1.2869011828899384], time: 173.83
Running avgs for agent 0: q_loss: 1253270.625, p_loss: 143.2526092529297, mean_rew: -7.6462234011126675, variance: 177.8155230102539, lamda: 2.660388231277466
Running avgs for agent 1: q_loss: 6.147762775421143, p_loss: -3.916020393371582, mean_rew: -7.643063577705321, variance: 4.95634651184082, lamda: 1.5793086290359497
Running avgs for agent 2: q_loss: 6.442404270172119, p_loss: -3.8991541862487793, mean_rew: -7.635363537230225, variance: 5.147604465484619, lamda: 1.5162625312805176

steps: 1724975, episodes: 69000, mean episode reward: -583.8844885870109, agent episode reward: [-194.62816286233695, -194.62816286233695, -194.62816286233695], time: 175.619
steps: 1724975, episodes: 69000, mean episode variance: 46.627073526382446, agent episode variance: [44.095418304443356, 1.2434317271709443, 1.2882234947681428], time: 175.619
Running avgs for agent 0: q_loss: 1263713.125, p_loss: 143.52188110351562, mean_rew: -7.655564919348861, variance: 176.38167321777343, lamda: 2.6853628158569336
Running avgs for agent 1: q_loss: 6.047018051147461, p_loss: -3.922053337097168, mean_rew: -7.65039422460499, variance: 4.973726749420166, lamda: 1.5793085098266602
Running avgs for agent 2: q_loss: 6.514655113220215, p_loss: -3.9123828411102295, mean_rew: -7.651699999784214, variance: 5.152893543243408, lamda: 1.516262412071228

steps: 1749975, episodes: 70000, mean episode reward: -592.6726290087084, agent episode reward: [-197.55754300290283, -197.55754300290283, -197.55754300290283], time: 173.94
steps: 1749975, episodes: 70000, mean episode variance: 46.26163864159584, agent episode variance: [43.73053987121582, 1.2436682434082031, 1.287430526971817], time: 173.941
Running avgs for agent 0: q_loss: 1275414.375, p_loss: 143.88914489746094, mean_rew: -7.665016967854251, variance: 174.92215948486327, lamda: 2.710336923599243
Running avgs for agent 1: q_loss: 6.0636796951293945, p_loss: -3.9274837970733643, mean_rew: -7.6699876093186035, variance: 4.974672794342041, lamda: 1.5793083906173706
Running avgs for agent 2: q_loss: 6.550413131713867, p_loss: -3.922712802886963, mean_rew: -7.674978463620431, variance: 5.149722576141357, lamda: 1.516262412071228

steps: 1774975, episodes: 71000, mean episode reward: -589.8743647411197, agent episode reward: [-196.6247882470399, -196.6247882470399, -196.6247882470399], time: 176.058
steps: 1774975, episodes: 71000, mean episode variance: 46.06994433426857, agent episode variance: [43.536659530639646, 1.2445544989109039, 1.2887303047180176], time: 176.058
Running avgs for agent 0: q_loss: 1316728.875, p_loss: 144.3733367919922, mean_rew: -7.6863414958386, variance: 174.14663812255858, lamda: 2.735311269760132
Running avgs for agent 1: q_loss: 6.052550792694092, p_loss: -3.938109874725342, mean_rew: -7.689107104571545, variance: 4.978218078613281, lamda: 1.5793086290359497
Running avgs for agent 2: q_loss: 6.6714277267456055, p_loss: -3.9373323917388916, mean_rew: -7.688174076396232, variance: 5.154921531677246, lamda: 1.5162625312805176

steps: 1799975, episodes: 72000, mean episode reward: -580.2406972903667, agent episode reward: [-193.41356576345558, -193.41356576345558, -193.41356576345558], time: 176.807
steps: 1799975, episodes: 72000, mean episode variance: 46.52430815911293, agent episode variance: [43.97289619445801, 1.2553851010799408, 1.2960268635749816], time: 176.807
Running avgs for agent 0: q_loss: 1322543.5, p_loss: 144.8385009765625, mean_rew: -7.707770231931352, variance: 175.89158477783204, lamda: 2.7602856159210205
Running avgs for agent 1: q_loss: 5.985133171081543, p_loss: -3.946199417114258, mean_rew: -7.703985354802558, variance: 5.021541118621826, lamda: 1.5793085098266602
Running avgs for agent 2: q_loss: 6.658903121948242, p_loss: -3.9411354064941406, mean_rew: -7.700980822087013, variance: 5.184107303619385, lamda: 1.516262412071228

steps: 1824975, episodes: 73000, mean episode reward: -591.5326613680642, agent episode reward: [-197.1775537893547, -197.1775537893547, -197.1775537893547], time: 177.988
steps: 1824975, episodes: 73000, mean episode variance: 47.16442751121521, agent episode variance: [44.62122200012207, 1.2551756508350371, 1.2880298602581024], time: 177.989
Running avgs for agent 0: q_loss: 1346720.5, p_loss: 145.26959228515625, mean_rew: -7.718662781366322, variance: 178.48488800048827, lamda: 2.785259962081909
Running avgs for agent 1: q_loss: 6.147284984588623, p_loss: -3.95851993560791, mean_rew: -7.723299869714501, variance: 5.020702838897705, lamda: 1.5793083906173706
Running avgs for agent 2: q_loss: 6.626722812652588, p_loss: -3.9457998275756836, mean_rew: -7.719418867501842, variance: 5.1521196365356445, lamda: 1.516262412071228

steps: 1849975, episodes: 74000, mean episode reward: -584.56594759362, agent episode reward: [-194.85531586454, -194.85531586454, -194.85531586454], time: 176.555
steps: 1849975, episodes: 74000, mean episode variance: 46.01132704639435, agent episode variance: [43.4593211517334, 1.2482737679481506, 1.303732126712799], time: 176.556
Running avgs for agent 0: q_loss: 1380833.25, p_loss: 145.70030212402344, mean_rew: -7.725891477928495, variance: 173.8372846069336, lamda: 2.810234308242798
Running avgs for agent 1: q_loss: 6.147447109222412, p_loss: -3.958648204803467, mean_rew: -7.722405475423706, variance: 4.9930949211120605, lamda: 1.5793086290359497
Running avgs for agent 2: q_loss: 6.701007843017578, p_loss: -3.9473345279693604, mean_rew: -7.726453936681368, variance: 5.21492862701416, lamda: 1.5162625312805176

steps: 1874975, episodes: 75000, mean episode reward: -585.7486174273804, agent episode reward: [-195.24953914246015, -195.24953914246015, -195.24953914246015], time: 174.385
steps: 1874975, episodes: 75000, mean episode variance: 46.71411472654343, agent episode variance: [44.14632308959961, 1.2617485189437867, 1.3060431180000305], time: 174.386
Running avgs for agent 0: q_loss: 1383490.875, p_loss: 145.97647094726562, mean_rew: -7.737633998959041, variance: 176.58529235839845, lamda: 2.8352086544036865
Running avgs for agent 1: q_loss: 6.098154067993164, p_loss: -3.965848207473755, mean_rew: -7.740554985638179, variance: 5.046994209289551, lamda: 1.5793085098266602
Running avgs for agent 2: q_loss: 6.827322006225586, p_loss: -3.957122325897217, mean_rew: -7.7348918487290215, variance: 5.224172592163086, lamda: 1.516262412071228

steps: 1899975, episodes: 76000, mean episode reward: -588.1125450079084, agent episode reward: [-196.03751500263613, -196.03751500263613, -196.03751500263613], time: 165.689
steps: 1899975, episodes: 76000, mean episode variance: 47.29336103272438, agent episode variance: [44.73013793945312, 1.2598058438301087, 1.303417249441147], time: 165.69
Running avgs for agent 0: q_loss: 1422954.75, p_loss: 146.326904296875, mean_rew: -7.738754310587239, variance: 178.9205517578125, lamda: 2.860183000564575
Running avgs for agent 1: q_loss: 6.282542705535889, p_loss: -3.97076678276062, mean_rew: -7.750881171463009, variance: 5.039223670959473, lamda: 1.5793324708938599
Running avgs for agent 2: q_loss: 6.86780309677124, p_loss: -3.957071542739868, mean_rew: -7.740261560655178, variance: 5.2136688232421875, lamda: 1.5163676738739014

steps: 1924975, episodes: 77000, mean episode reward: -580.0915625079514, agent episode reward: [-193.36385416931714, -193.36385416931714, -193.36385416931714], time: 164.797
steps: 1924975, episodes: 77000, mean episode variance: 47.26506776428223, agent episode variance: [44.71275895690918, 1.2489422569274902, 1.3033665504455567], time: 164.797
Running avgs for agent 0: q_loss: 1454620.375, p_loss: 146.637939453125, mean_rew: -7.746715675463815, variance: 178.8510358276367, lamda: 2.885157585144043
Running avgs for agent 1: q_loss: 6.030872821807861, p_loss: -3.9721713066101074, mean_rew: -7.744778621802364, variance: 4.995769023895264, lamda: 1.5795092582702637
Running avgs for agent 2: q_loss: 6.798720836639404, p_loss: -3.963728427886963, mean_rew: -7.746650735400681, variance: 5.213466167449951, lamda: 1.5164636373519897

steps: 1949975, episodes: 78000, mean episode reward: -581.4308097663846, agent episode reward: [-193.81026992212824, -193.81026992212824, -193.81026992212824], time: 161.425
steps: 1949975, episodes: 78000, mean episode variance: 46.52257680654526, agent episode variance: [43.957196075439455, 1.2569187138080598, 1.3084620172977448], time: 161.425
Running avgs for agent 0: q_loss: 1462814.625, p_loss: 146.8712615966797, mean_rew: -7.7512662247383854, variance: 175.82878430175782, lamda: 2.9101319313049316
Running avgs for agent 1: q_loss: 6.019879341125488, p_loss: -3.965773582458496, mean_rew: -7.74861890490404, variance: 5.027674674987793, lamda: 1.5795092582702637
Running avgs for agent 2: q_loss: 6.796328544616699, p_loss: -3.962303400039673, mean_rew: -7.748815257497268, variance: 5.2338480949401855, lamda: 1.5164633989334106

steps: 1974975, episodes: 79000, mean episode reward: -578.109852101903, agent episode reward: [-192.7032840339677, -192.7032840339677, -192.7032840339677], time: 158.804
steps: 1974975, episodes: 79000, mean episode variance: 47.02038208723068, agent episode variance: [44.45501435852051, 1.258846489906311, 1.3065212388038636], time: 158.804
Running avgs for agent 0: q_loss: 1472503.75, p_loss: 146.9340362548828, mean_rew: -7.749849817864933, variance: 177.82005743408203, lamda: 2.935106039047241
Running avgs for agent 1: q_loss: 6.093258380889893, p_loss: -3.971140146255493, mean_rew: -7.753676512554787, variance: 5.035385608673096, lamda: 1.5795092582702637
Running avgs for agent 2: q_loss: 6.7722039222717285, p_loss: -3.9684765338897705, mean_rew: -7.75088334878048, variance: 5.226085186004639, lamda: 1.5164633989334106

steps: 1999975, episodes: 80000, mean episode reward: -583.744524844254, agent episode reward: [-194.581508281418, -194.581508281418, -194.581508281418], time: 165.978
steps: 1999975, episodes: 80000, mean episode variance: 47.3785158803463, agent episode variance: [44.82713432312012, 1.2532005689144134, 1.2981809883117676], time: 165.979
Running avgs for agent 0: q_loss: 1512606.25, p_loss: 147.21926879882812, mean_rew: -7.758581381542594, variance: 179.30853729248048, lamda: 2.960080623626709
Running avgs for agent 1: q_loss: 6.166545867919922, p_loss: -3.9702095985412598, mean_rew: -7.755041765857993, variance: 5.0128021240234375, lamda: 1.5795092582702637
Running avgs for agent 2: q_loss: 6.808916091918945, p_loss: -3.9690799713134766, mean_rew: -7.749706048532058, variance: 5.192723751068115, lamda: 1.5164636373519897

steps: 2024975, episodes: 81000, mean episode reward: -575.1124550545762, agent episode reward: [-191.70415168485877, -191.70415168485877, -191.70415168485877], time: 169.523
steps: 2024975, episodes: 81000, mean episode variance: 47.61679809927941, agent episode variance: [45.040228103637695, 1.2593870227336883, 1.31718297290802], time: 169.524
Running avgs for agent 0: q_loss: 1539579.0, p_loss: 147.38157653808594, mean_rew: -7.760929809922183, variance: 180.16091241455078, lamda: 2.9850547313690186
Running avgs for agent 1: q_loss: 6.14262580871582, p_loss: -3.969656229019165, mean_rew: -7.762327983572133, variance: 5.037548065185547, lamda: 1.5795092582702637
Running avgs for agent 2: q_loss: 6.7729811668396, p_loss: -3.9674768447875977, mean_rew: -7.759139585404741, variance: 5.268732070922852, lamda: 1.5164633989334106

steps: 2049975, episodes: 82000, mean episode reward: -579.2832607499964, agent episode reward: [-193.0944202499988, -193.0944202499988, -193.0944202499988], time: 175.923
steps: 2049975, episodes: 82000, mean episode variance: 47.389209115982055, agent episode variance: [44.818277557373044, 1.2599800889492034, 1.3109514696598052], time: 175.924
Running avgs for agent 0: q_loss: 1551732.5, p_loss: 147.47354125976562, mean_rew: -7.752990481878863, variance: 179.27311022949218, lamda: 3.0100293159484863
Running avgs for agent 1: q_loss: 6.157423496246338, p_loss: -3.971686601638794, mean_rew: -7.764390883032221, variance: 5.039919853210449, lamda: 1.5795092582702637
Running avgs for agent 2: q_loss: 6.729395389556885, p_loss: -3.9713494777679443, mean_rew: -7.758150493323488, variance: 5.243805885314941, lamda: 1.5164633989334106

steps: 2074975, episodes: 83000, mean episode reward: -587.3582042899258, agent episode reward: [-195.78606809664188, -195.78606809664188, -195.78606809664188], time: 167.709
steps: 2074975, episodes: 83000, mean episode variance: 47.37163061118126, agent episode variance: [44.80561318969727, 1.2574597630500794, 1.3085576584339143], time: 167.71
Running avgs for agent 0: q_loss: 1593107.0, p_loss: 147.7404327392578, mean_rew: -7.757872751842708, variance: 179.22245275878907, lamda: 3.035003423690796
Running avgs for agent 1: q_loss: 6.245363712310791, p_loss: -3.973924160003662, mean_rew: -7.760036434049012, variance: 5.029839038848877, lamda: 1.5796282291412354
Running avgs for agent 2: q_loss: 6.819870471954346, p_loss: -3.9704322814941406, mean_rew: -7.7548817563482695, variance: 5.2342305183410645, lamda: 1.5164636373519897

steps: 2099975, episodes: 84000, mean episode reward: -583.1776871904417, agent episode reward: [-194.39256239681387, -194.39256239681387, -194.39256239681387], time: 161.378
steps: 2099975, episodes: 84000, mean episode variance: 45.57040449929237, agent episode variance: [42.997476852416995, 1.259252824306488, 1.3136748225688935], time: 161.378
Running avgs for agent 0: q_loss: 1599400.625, p_loss: 147.9601593017578, mean_rew: -7.763805629129449, variance: 171.98990740966798, lamda: 3.0599777698516846
Running avgs for agent 1: q_loss: 5.964776515960693, p_loss: -3.9700522422790527, mean_rew: -7.757045791593515, variance: 5.03701114654541, lamda: 1.5797102451324463
Running avgs for agent 2: q_loss: 6.800205707550049, p_loss: -3.9722506999969482, mean_rew: -7.761088604681175, variance: 5.254699230194092, lamda: 1.5164633989334106

steps: 2124975, episodes: 85000, mean episode reward: -587.7085311435059, agent episode reward: [-195.90284371450196, -195.90284371450196, -195.90284371450196], time: 159.144
steps: 2124975, episodes: 85000, mean episode variance: 47.01601731038094, agent episode variance: [44.45734156799316, 1.2579883034229278, 1.3006874389648437], time: 159.145
Running avgs for agent 0: q_loss: 1613950.375, p_loss: 148.07215881347656, mean_rew: -7.763999925453961, variance: 177.82936627197265, lamda: 3.0849523544311523
Running avgs for agent 1: q_loss: 6.062712669372559, p_loss: -3.9713120460510254, mean_rew: -7.762349613357579, variance: 5.03195333480835, lamda: 1.5797102451324463
Running avgs for agent 2: q_loss: 6.711622714996338, p_loss: -3.97287917137146, mean_rew: -7.7540259271865635, variance: 5.202750205993652, lamda: 1.5164633989334106

steps: 2149975, episodes: 86000, mean episode reward: -585.4761282551792, agent episode reward: [-195.1587094183931, -195.1587094183931, -195.1587094183931], time: 169.929
steps: 2149975, episodes: 86000, mean episode variance: 47.999513192415236, agent episode variance: [45.44207331848145, 1.254113344669342, 1.30332652926445], time: 169.93
Running avgs for agent 0: q_loss: 1654920.125, p_loss: 148.1957244873047, mean_rew: -7.760544730632915, variance: 181.7682932739258, lamda: 3.109926462173462
Running avgs for agent 1: q_loss: 6.097793102264404, p_loss: -3.972242832183838, mean_rew: -7.757684870345098, variance: 5.016453266143799, lamda: 1.5797103643417358
Running avgs for agent 2: q_loss: 6.873571872711182, p_loss: -3.974928855895996, mean_rew: -7.765942044016239, variance: 5.213305950164795, lamda: 1.5164636373519897

steps: 2174975, episodes: 87000, mean episode reward: -589.7900361243272, agent episode reward: [-196.59667870810907, -196.59667870810907, -196.59667870810907], time: 181.506
steps: 2174975, episodes: 87000, mean episode variance: 45.80974982404709, agent episode variance: [43.24105738830566, 1.2572797026634217, 1.311412733078003], time: 181.507
Running avgs for agent 0: q_loss: 1639313.625, p_loss: 148.2576904296875, mean_rew: -7.76318977878912, variance: 172.96422955322265, lamda: 3.1349008083343506
Running avgs for agent 1: q_loss: 6.1103973388671875, p_loss: -3.9714136123657227, mean_rew: -7.762173939611331, variance: 5.02911901473999, lamda: 1.5797102451324463
Running avgs for agent 2: q_loss: 6.859247207641602, p_loss: -3.9692306518554688, mean_rew: -7.763312243936204, variance: 5.245650768280029, lamda: 1.5164633989334106

steps: 2199975, episodes: 88000, mean episode reward: -587.7797643517745, agent episode reward: [-195.92658811725815, -195.92658811725815, -195.92658811725815], time: 178.311
steps: 2199975, episodes: 88000, mean episode variance: 45.74557889676094, agent episode variance: [43.18324195861816, 1.2596576936244965, 1.30267924451828], time: 178.311
Running avgs for agent 0: q_loss: 1685124.5, p_loss: 148.43174743652344, mean_rew: -7.760075829793154, variance: 172.73296783447265, lamda: 3.1598751544952393
Running avgs for agent 1: q_loss: 6.253040790557861, p_loss: -3.971040964126587, mean_rew: -7.762970994038185, variance: 5.038630962371826, lamda: 1.5797102451324463
Running avgs for agent 2: q_loss: 6.906722545623779, p_loss: -3.9713926315307617, mean_rew: -7.761403732182769, variance: 5.210716724395752, lamda: 1.5164633989334106

steps: 2224975, episodes: 89000, mean episode reward: -591.4811789627436, agent episode reward: [-197.16039298758122, -197.16039298758122, -197.16039298758122], time: 181.803
steps: 2224975, episodes: 89000, mean episode variance: 46.16054824137688, agent episode variance: [43.59682716369629, 1.256390370607376, 1.3073307070732116], time: 181.803
Running avgs for agent 0: q_loss: 1707850.875, p_loss: 148.65115356445312, mean_rew: -7.769688825455008, variance: 174.38730865478516, lamda: 3.184849500656128
Running avgs for agent 1: q_loss: 6.153945446014404, p_loss: -3.976743698120117, mean_rew: -7.768274245882621, variance: 5.025561332702637, lamda: 1.5807783603668213
Running avgs for agent 2: q_loss: 6.9025044441223145, p_loss: -3.979806423187256, mean_rew: -7.772908643447167, variance: 5.229323387145996, lamda: 1.5164636373519897

steps: 2249975, episodes: 90000, mean episode reward: -594.0406710820549, agent episode reward: [-198.01355702735168, -198.01355702735168, -198.01355702735168], time: 178.282
steps: 2249975, episodes: 90000, mean episode variance: 45.7466276307106, agent episode variance: [43.17410008239746, 1.2607249734401702, 1.3118025748729705], time: 178.283
Running avgs for agent 0: q_loss: 1697773.0, p_loss: 148.8736572265625, mean_rew: -7.777254975923198, variance: 172.69640032958984, lamda: 3.2098240852355957
Running avgs for agent 1: q_loss: 6.360782623291016, p_loss: -3.9797725677490234, mean_rew: -7.779550887889599, variance: 5.042900085449219, lamda: 1.5871078968048096
Running avgs for agent 2: q_loss: 6.883975982666016, p_loss: -3.9815351963043213, mean_rew: -7.777134506862608, variance: 5.2472100257873535, lamda: 1.5164633989334106

steps: 2274975, episodes: 91000, mean episode reward: -597.3787646892985, agent episode reward: [-199.1262548964328, -199.1262548964328, -199.1262548964328], time: 163.901
steps: 2274975, episodes: 91000, mean episode variance: 45.565355354309084, agent episode variance: [42.994731048583986, 1.2612422840595245, 1.3093820216655732], time: 163.902
Running avgs for agent 0: q_loss: 1756133.375, p_loss: 149.11509704589844, mean_rew: -7.783926468936017, variance: 171.97892419433595, lamda: 3.2347984313964844
Running avgs for agent 1: q_loss: 11.224184036254883, p_loss: -3.9848833084106445, mean_rew: -7.78273744204399, variance: 5.044969081878662, lamda: 1.5887755155563354
Running avgs for agent 2: q_loss: 6.94008207321167, p_loss: -3.9808857440948486, mean_rew: -7.780311855800257, variance: 5.237528324127197, lamda: 1.5164633989334106

steps: 2299975, episodes: 92000, mean episode reward: -596.4964286563031, agent episode reward: [-198.8321428854344, -198.8321428854344, -198.8321428854344], time: 165.97
steps: 2299975, episodes: 92000, mean episode variance: 45.47795674347878, agent episode variance: [42.91447161865234, 1.2516953017711638, 1.3117898230552674], time: 165.971
Running avgs for agent 0: q_loss: 1780918.125, p_loss: 149.4365692138672, mean_rew: -7.793227936739173, variance: 171.65788647460937, lamda: 3.259772539138794
Running avgs for agent 1: q_loss: 7.076645851135254, p_loss: -3.986565113067627, mean_rew: -7.78829958972349, variance: 5.006781101226807, lamda: 1.589598298072815
Running avgs for agent 2: q_loss: 6.897825241088867, p_loss: -3.9804604053497314, mean_rew: -7.78480265371898, variance: 5.247159004211426, lamda: 1.5164636373519897

steps: 2324975, episodes: 93000, mean episode reward: -606.7599842067087, agent episode reward: [-202.2533280689029, -202.2533280689029, -202.2533280689029], time: 159.011
steps: 2324975, episodes: 93000, mean episode variance: 44.48017168855667, agent episode variance: [41.916171920776364, 1.2528826539516449, 1.311117113828659], time: 159.011
Running avgs for agent 0: q_loss: 1772188.625, p_loss: 149.71287536621094, mean_rew: -7.80152695236315, variance: 167.66468768310546, lamda: 3.2847471237182617
Running avgs for agent 1: q_loss: 6.188356876373291, p_loss: -3.9915530681610107, mean_rew: -7.798128898797093, variance: 5.011530876159668, lamda: 1.5913625955581665
Running avgs for agent 2: q_loss: 6.878542900085449, p_loss: -3.987928867340088, mean_rew: -7.80040314861959, variance: 5.244468688964844, lamda: 1.5164633989334106

steps: 2349975, episodes: 94000, mean episode reward: -600.9603228114645, agent episode reward: [-200.3201076038215, -200.3201076038215, -200.3201076038215], time: 158.188
steps: 2349975, episodes: 94000, mean episode variance: 44.89246454620361, agent episode variance: [42.32214129638672, 1.2563759670257568, 1.3139472827911376], time: 158.188
Running avgs for agent 0: q_loss: 1811029.75, p_loss: 150.12872314453125, mean_rew: -7.800293026761809, variance: 169.28856518554687, lamda: 3.3097212314605713
Running avgs for agent 1: q_loss: 6.2527546882629395, p_loss: -3.991281270980835, mean_rew: -7.806828290739641, variance: 5.025504112243652, lamda: 1.5913845300674438
Running avgs for agent 2: q_loss: 7.0090436935424805, p_loss: -3.9878697395324707, mean_rew: -7.800019632269241, variance: 5.255789279937744, lamda: 1.5164633989334106

steps: 2374975, episodes: 95000, mean episode reward: -601.0359844537284, agent episode reward: [-200.34532815124277, -200.34532815124277, -200.34532815124277], time: 159.198
steps: 2374975, episodes: 95000, mean episode variance: 45.38585477685928, agent episode variance: [42.81189533996582, 1.2576235878467559, 1.3163358490467072], time: 159.199
Running avgs for agent 0: q_loss: 1850751.625, p_loss: 150.53765869140625, mean_rew: -7.811169026410522, variance: 171.2475813598633, lamda: 3.33469557762146
Running avgs for agent 1: q_loss: 6.308305740356445, p_loss: -3.9960615634918213, mean_rew: -7.8096823138720435, variance: 5.030494689941406, lamda: 1.5914376974105835
Running avgs for agent 2: q_loss: 6.97109842300415, p_loss: -3.990079879760742, mean_rew: -7.807894504924516, variance: 5.26534366607666, lamda: 1.5166127681732178

steps: 2399975, episodes: 96000, mean episode reward: -604.3390355197326, agent episode reward: [-201.4463451732442, -201.4463451732442, -201.4463451732442], time: 165.024
steps: 2399975, episodes: 96000, mean episode variance: 44.38745526742935, agent episode variance: [41.81562236022949, 1.2526385192871095, 1.3191943879127503], time: 165.024
Running avgs for agent 0: q_loss: 1883460.375, p_loss: 150.7567901611328, mean_rew: -7.8179747302161, variance: 167.26248944091796, lamda: 3.3596699237823486
Running avgs for agent 1: q_loss: 6.276571273803711, p_loss: -3.998014211654663, mean_rew: -7.81809008831616, variance: 5.010554313659668, lamda: 1.591437816619873
Running avgs for agent 2: q_loss: 7.172489643096924, p_loss: -3.9991464614868164, mean_rew: -7.824806832062108, variance: 5.276777267456055, lamda: 1.5167375802993774

steps: 2424975, episodes: 97000, mean episode reward: -599.7209478664813, agent episode reward: [-199.9069826221604, -199.9069826221604, -199.9069826221604], time: 142.874
steps: 2424975, episodes: 97000, mean episode variance: 45.63563507175446, agent episode variance: [43.06750442504883, 1.2545317587852478, 1.3135988879203797], time: 142.875
Running avgs for agent 0: q_loss: 1931102.5, p_loss: 151.11471557617188, mean_rew: -7.822398394289043, variance: 172.27001770019533, lamda: 3.3846442699432373
Running avgs for agent 1: q_loss: 6.479650020599365, p_loss: -4.007027626037598, mean_rew: -7.827271159612326, variance: 5.018126964569092, lamda: 1.5914949178695679
Running avgs for agent 2: q_loss: 5.297337055206299, p_loss: -4.002255439758301, mean_rew: -7.824627286175322, variance: 5.254395484924316, lamda: 1.5177922248840332

steps: 2449975, episodes: 98000, mean episode reward: -595.584962302507, agent episode reward: [-198.52832076750235, -198.52832076750235, -198.52832076750235], time: 110.757
steps: 2449975, episodes: 98000, mean episode variance: 46.29867068362236, agent episode variance: [43.72413935852051, 1.2624321763515471, 1.312099148750305], time: 110.757
Running avgs for agent 0: q_loss: 1949302.0, p_loss: 151.43272399902344, mean_rew: -7.829924794528873, variance: 174.89655743408204, lamda: 3.409618616104126
Running avgs for agent 1: q_loss: 5.947617530822754, p_loss: -4.002598762512207, mean_rew: -7.829723498544036, variance: 5.0497283935546875, lamda: 1.5927294492721558
Running avgs for agent 2: q_loss: 4.969335079193115, p_loss: -4.006714344024658, mean_rew: -7.835308890334812, variance: 5.248396873474121, lamda: 1.522868037223816

steps: 2474975, episodes: 99000, mean episode reward: -601.4631370952109, agent episode reward: [-200.48771236507025, -200.48771236507025, -200.48771236507025], time: 109.878
steps: 2474975, episodes: 99000, mean episode variance: 45.06724119210243, agent episode variance: [42.49451635742187, 1.260864560842514, 1.3118602738380432], time: 109.878
Running avgs for agent 0: q_loss: 2013745.75, p_loss: 151.90673828125, mean_rew: -7.832839158632088, variance: 169.9780654296875, lamda: 3.4345929622650146
Running avgs for agent 1: q_loss: 6.3978447914123535, p_loss: -4.007003307342529, mean_rew: -7.834901034592958, variance: 5.043457984924316, lamda: 1.594187617301941
Running avgs for agent 2: q_loss: 4.695403099060059, p_loss: -4.0022125244140625, mean_rew: -7.827283499571935, variance: 5.247440814971924, lamda: 1.528335452079773

steps: 2499975, episodes: 100000, mean episode reward: -600.4177595194361, agent episode reward: [-200.13925317314542, -200.13925317314542, -200.13925317314542], time: 112.46
steps: 2499975, episodes: 100000, mean episode variance: 45.49354821395874, agent episode variance: [42.92957356262207, 1.2614827737808227, 1.3024918775558472], time: 112.46
Running avgs for agent 0: q_loss: 2111384.25, p_loss: 152.402587890625, mean_rew: -7.8444787664952536, variance: 171.71829425048827, lamda: 3.4595673084259033
Running avgs for agent 1: q_loss: 6.50794792175293, p_loss: -4.013627529144287, mean_rew: -7.840383667194706, variance: 5.045931339263916, lamda: 1.5943142175674438
Running avgs for agent 2: q_loss: 4.407838821411133, p_loss: -4.011239528656006, mean_rew: -7.837307120046774, variance: 5.209967613220215, lamda: 1.5331127643585205

steps: 2524975, episodes: 101000, mean episode reward: -600.9788627270565, agent episode reward: [-200.32628757568548, -200.32628757568548, -200.32628757568548], time: 110.207
steps: 2524975, episodes: 101000, mean episode variance: 47.20367754244804, agent episode variance: [44.63106243896485, 1.2638273503780364, 1.3087877531051635], time: 110.207
Running avgs for agent 0: q_loss: 2157400.0, p_loss: 152.83990478515625, mean_rew: -7.846096702384962, variance: 178.5242497558594, lamda: 3.484541654586792
Running avgs for agent 1: q_loss: 6.449841022491455, p_loss: -4.017579078674316, mean_rew: -7.850021450491509, variance: 5.055309772491455, lamda: 1.594606637954712
Running avgs for agent 2: q_loss: 5.669811725616455, p_loss: -4.005009651184082, mean_rew: -7.848429214455303, variance: 5.2351508140563965, lamda: 1.5364603996276855

steps: 2549975, episodes: 102000, mean episode reward: -598.9798726376146, agent episode reward: [-199.65995754587155, -199.65995754587155, -199.65995754587155], time: 101.889
steps: 2549975, episodes: 102000, mean episode variance: 46.268826628685, agent episode variance: [43.71580763244629, 1.2564337747097016, 1.296585221529007], time: 101.89
Running avgs for agent 0: q_loss: 2225861.0, p_loss: 153.2048797607422, mean_rew: -7.847539670344108, variance: 174.86323052978517, lamda: 3.5095160007476807
Running avgs for agent 1: q_loss: 6.551395893096924, p_loss: -4.025503635406494, mean_rew: -7.855244169132105, variance: 5.025735378265381, lamda: 1.5947004556655884
Running avgs for agent 2: q_loss: 7.085410118103027, p_loss: -4.011953830718994, mean_rew: -7.850700105737449, variance: 5.186340808868408, lamda: 1.5389338731765747

steps: 2574975, episodes: 103000, mean episode reward: -608.4729461513077, agent episode reward: [-202.82431538376923, -202.82431538376923, -202.82431538376923], time: 100.749
steps: 2574975, episodes: 103000, mean episode variance: 46.76625634694099, agent episode variance: [44.19334507751465, 1.2658744332790375, 1.3070368361473084], time: 100.75
Running avgs for agent 0: q_loss: 2348854.25, p_loss: 153.50843811035156, mean_rew: -7.868495753033227, variance: 176.7733803100586, lamda: 3.5344903469085693
Running avgs for agent 1: q_loss: 6.719616413116455, p_loss: -4.028839111328125, mean_rew: -7.8652693032228616, variance: 5.063497543334961, lamda: 1.5947004556655884
Running avgs for agent 2: q_loss: 7.030325412750244, p_loss: -4.014383316040039, mean_rew: -7.858583139860863, variance: 5.228147506713867, lamda: 1.5389339923858643

steps: 2599975, episodes: 104000, mean episode reward: -605.0830204477958, agent episode reward: [-201.69434014926526, -201.69434014926526, -201.69434014926526], time: 104.192
steps: 2599975, episodes: 104000, mean episode variance: 49.23456556177139, agent episode variance: [46.67131066894531, 1.2617033421993256, 1.3015515506267548], time: 104.192
Running avgs for agent 0: q_loss: 2355330.0, p_loss: 153.7195587158203, mean_rew: -7.874529901686967, variance: 186.68524267578124, lamda: 3.559464931488037
Running avgs for agent 1: q_loss: 6.566163539886475, p_loss: -4.02689790725708, mean_rew: -7.8696577100057, variance: 5.046813488006592, lamda: 1.594700574874878
Running avgs for agent 2: q_loss: 7.0087480545043945, p_loss: -4.020941257476807, mean_rew: -7.871135021523416, variance: 5.20620584487915, lamda: 1.5389337539672852

steps: 2624975, episodes: 105000, mean episode reward: -606.3824467291194, agent episode reward: [-202.1274822430398, -202.1274822430398, -202.1274822430398], time: 99.422
steps: 2624975, episodes: 105000, mean episode variance: 47.33634382891655, agent episode variance: [44.770808807373044, 1.2637646930217743, 1.3017703285217286], time: 99.422
Running avgs for agent 0: q_loss: 2374838.0, p_loss: 154.04037475585938, mean_rew: -7.872080398980606, variance: 179.08323522949217, lamda: 3.584439277648926
Running avgs for agent 1: q_loss: 5.331814289093018, p_loss: -4.042623043060303, mean_rew: -7.883975385299183, variance: 5.055058479309082, lamda: 1.5960056781768799
Running avgs for agent 2: q_loss: 7.091458797454834, p_loss: -4.017037391662598, mean_rew: -7.867729288281715, variance: 5.207080841064453, lamda: 1.5389338731765747

steps: 2649975, episodes: 106000, mean episode reward: -600.2623541227475, agent episode reward: [-200.08745137424918, -200.08745137424918, -200.08745137424918], time: 91.515
steps: 2649975, episodes: 106000, mean episode variance: 48.81162979459763, agent episode variance: [46.24327308654785, 1.259007550239563, 1.3093491578102112], time: 91.516
Running avgs for agent 0: q_loss: 2450018.25, p_loss: 154.3006134033203, mean_rew: -7.890357280100762, variance: 184.9730923461914, lamda: 3.6094136238098145
Running avgs for agent 1: q_loss: 5.955106258392334, p_loss: -4.0351948738098145, mean_rew: -7.887212573185537, variance: 5.036030292510986, lamda: 1.6035256385803223
Running avgs for agent 2: q_loss: 7.255418300628662, p_loss: -4.030693054199219, mean_rew: -7.894795730413373, variance: 5.237396717071533, lamda: 1.5389339923858643

steps: 2674975, episodes: 107000, mean episode reward: -600.0922850722877, agent episode reward: [-200.0307616907626, -200.0307616907626, -200.0307616907626], time: 91.056
steps: 2674975, episodes: 107000, mean episode variance: 48.9919041492939, agent episode variance: [46.41947087097168, 1.2617103736400603, 1.3107229046821594], time: 91.056
Running avgs for agent 0: q_loss: 2505795.0, p_loss: 154.57632446289062, mean_rew: -7.906810119148568, variance: 185.67788348388672, lamda: 3.634387731552124
Running avgs for agent 1: q_loss: 6.757163047790527, p_loss: -4.034426212310791, mean_rew: -7.89289626246102, variance: 5.046841621398926, lamda: 1.6052290201187134
Running avgs for agent 2: q_loss: 7.276254653930664, p_loss: -4.031032085418701, mean_rew: -7.894352120732894, variance: 5.242891788482666, lamda: 1.5389337539672852

steps: 2699975, episodes: 108000, mean episode reward: -605.7250695507734, agent episode reward: [-201.90835651692444, -201.90835651692444, -201.90835651692444], time: 92.491
steps: 2699975, episodes: 108000, mean episode variance: 47.81723602294922, agent episode variance: [45.25006663513184, 1.2645513563156128, 1.30261803150177], time: 92.492
Running avgs for agent 0: q_loss: 2563742.5, p_loss: 155.04786682128906, mean_rew: -7.903704015961288, variance: 181.00026654052735, lamda: 3.659362316131592
Running avgs for agent 1: q_loss: 6.878653526306152, p_loss: -4.051215648651123, mean_rew: -7.912271512702878, variance: 5.058205604553223, lamda: 1.6052289009094238
Running avgs for agent 2: q_loss: 7.428540229797363, p_loss: -4.034042835235596, mean_rew: -7.901926508597087, variance: 5.210472106933594, lamda: 1.53895103931427

steps: 2724975, episodes: 109000, mean episode reward: -600.7562581314519, agent episode reward: [-200.2520860438173, -200.2520860438173, -200.2520860438173], time: 90.612
steps: 2724975, episodes: 109000, mean episode variance: 48.61117954444885, agent episode variance: [46.045010299682616, 1.257334716796875, 1.3088345279693603], time: 90.613
Running avgs for agent 0: q_loss: 2620759.5, p_loss: 155.401123046875, mean_rew: -7.916663830864805, variance: 184.18004119873046, lamda: 3.6843364238739014
Running avgs for agent 1: q_loss: 6.991822242736816, p_loss: -4.053956508636475, mean_rew: -7.912474795777609, variance: 5.029338836669922, lamda: 1.605420708656311
Running avgs for agent 2: q_loss: 7.3594746589660645, p_loss: -4.042868137359619, mean_rew: -7.912397002323971, variance: 5.235337734222412, lamda: 1.5390385389328003

steps: 2749975, episodes: 110000, mean episode reward: -610.7825379527113, agent episode reward: [-203.59417931757048, -203.59417931757048, -203.59417931757048], time: 92.29
steps: 2749975, episodes: 110000, mean episode variance: 48.90441508936882, agent episode variance: [46.31927336120606, 1.260729364156723, 1.3244123640060426], time: 92.291
Running avgs for agent 0: q_loss: 2700827.25, p_loss: 155.84169006347656, mean_rew: -7.918888169821568, variance: 185.27709344482423, lamda: 3.70931077003479
Running avgs for agent 1: q_loss: 6.871069431304932, p_loss: -4.058811187744141, mean_rew: -7.92333598257486, variance: 5.042916774749756, lamda: 1.6054296493530273
Running avgs for agent 2: q_loss: 7.407002925872803, p_loss: -4.039399147033691, mean_rew: -7.9211155958076285, variance: 5.297649383544922, lamda: 1.5391525030136108

steps: 2774975, episodes: 111000, mean episode reward: -605.772231891991, agent episode reward: [-201.92407729733037, -201.92407729733037, -201.92407729733037], time: 89.98
steps: 2774975, episodes: 111000, mean episode variance: 49.46457313585282, agent episode variance: [46.893474548339846, 1.2656569192409515, 1.3054416682720185], time: 89.981
Running avgs for agent 0: q_loss: 2709088.25, p_loss: 156.09121704101562, mean_rew: -7.93313473537618, variance: 187.57389819335938, lamda: 3.7342851161956787
Running avgs for agent 1: q_loss: 6.826236248016357, p_loss: -4.055874824523926, mean_rew: -7.9238208972201125, variance: 5.062627792358398, lamda: 1.6054294109344482
Running avgs for agent 2: q_loss: 7.457827568054199, p_loss: -4.056321144104004, mean_rew: -7.928347544105138, variance: 5.221766471862793, lamda: 1.5391526222229004

steps: 2799975, episodes: 112000, mean episode reward: -601.2676080456155, agent episode reward: [-200.42253601520514, -200.42253601520514, -200.42253601520514], time: 96.132
steps: 2799975, episodes: 112000, mean episode variance: 48.199085396289824, agent episode variance: [45.63098138427734, 1.2596202175617217, 1.30848379445076], time: 96.132
Running avgs for agent 0: q_loss: 2805631.25, p_loss: 156.3326873779297, mean_rew: -7.925256571036448, variance: 182.52392553710936, lamda: 3.7592594623565674
Running avgs for agent 1: q_loss: 7.075204849243164, p_loss: -4.065135955810547, mean_rew: -7.92897398368407, variance: 5.038480281829834, lamda: 1.6054648160934448
Running avgs for agent 2: q_loss: 7.447617530822754, p_loss: -4.060994625091553, mean_rew: -7.93661080169186, variance: 5.2339348793029785, lamda: 1.5391526222229004

steps: 2824975, episodes: 113000, mean episode reward: -608.9920954715573, agent episode reward: [-202.99736515718578, -202.99736515718578, -202.99736515718578], time: 87.835
steps: 2824975, episodes: 113000, mean episode variance: 49.871816996097564, agent episode variance: [47.28695957183838, 1.2629972851276399, 1.321860139131546], time: 87.836
Running avgs for agent 0: q_loss: 2869281.25, p_loss: 156.71266174316406, mean_rew: -7.941888823397591, variance: 189.14783828735352, lamda: 3.784233808517456
Running avgs for agent 1: q_loss: 6.996947288513184, p_loss: -4.063526153564453, mean_rew: -7.934630396394293, variance: 5.0519890785217285, lamda: 1.60546875
Running avgs for agent 2: q_loss: 7.444908142089844, p_loss: -4.052511215209961, mean_rew: -7.935629137725796, variance: 5.287440776824951, lamda: 1.5391525030136108

steps: 2849975, episodes: 114000, mean episode reward: -603.218845139057, agent episode reward: [-201.0729483796856, -201.0729483796856, -201.0729483796856], time: 79.945
steps: 2849975, episodes: 114000, mean episode variance: 48.818298387765886, agent episode variance: [46.22793176269531, 1.2700800976753235, 1.3202865273952484], time: 79.945
Running avgs for agent 0: q_loss: 2906357.25, p_loss: 156.94903564453125, mean_rew: -7.942506767060514, variance: 184.91172705078125, lamda: 3.8092081546783447
Running avgs for agent 1: q_loss: 7.020817279815674, p_loss: -4.0687079429626465, mean_rew: -7.950141197263676, variance: 5.080320358276367, lamda: 1.6054686307907104
Running avgs for agent 2: q_loss: 7.48850679397583, p_loss: -4.054706573486328, mean_rew: -7.93869146154313, variance: 5.281146049499512, lamda: 1.5392942428588867

steps: 2874975, episodes: 115000, mean episode reward: -603.9406375670654, agent episode reward: [-201.31354585568852, -201.31354585568852, -201.31354585568852], time: 81.328
steps: 2874975, episodes: 115000, mean episode variance: 49.13820545744896, agent episode variance: [46.56162600708008, 1.262824122905731, 1.31375532746315], time: 81.329
Running avgs for agent 0: q_loss: 2953238.25, p_loss: 157.31324768066406, mean_rew: -7.956564569117398, variance: 186.24650402832032, lamda: 3.8341825008392334
Running avgs for agent 1: q_loss: 7.147913932800293, p_loss: -4.071053504943848, mean_rew: -7.9475177106728045, variance: 5.051296234130859, lamda: 1.6054686307907104
Running avgs for agent 2: q_loss: 7.527609348297119, p_loss: -4.065349102020264, mean_rew: -7.944686453260949, variance: 5.255021572113037, lamda: 1.5394841432571411

steps: 2899975, episodes: 116000, mean episode reward: -605.0890058874629, agent episode reward: [-201.69633529582094, -201.69633529582094, -201.69633529582094], time: 82.326
steps: 2899975, episodes: 116000, mean episode variance: 48.6297049036026, agent episode variance: [46.040279800415036, 1.2667150747776033, 1.322710028409958], time: 82.326
Running avgs for agent 0: q_loss: 2995613.25, p_loss: 157.60015869140625, mean_rew: -7.960445919242056, variance: 184.16111920166014, lamda: 3.859157085418701
Running avgs for agent 1: q_loss: 6.9930243492126465, p_loss: -4.0718207359313965, mean_rew: -7.951690656265617, variance: 5.066860198974609, lamda: 1.60546875
Running avgs for agent 2: q_loss: 7.496468544006348, p_loss: -4.067225456237793, mean_rew: -7.952276674231626, variance: 5.290839672088623, lamda: 1.539523720741272

steps: 2924975, episodes: 117000, mean episode reward: -611.0435420114744, agent episode reward: [-203.68118067049144, -203.68118067049144, -203.68118067049144], time: 82.498
steps: 2924975, episodes: 117000, mean episode variance: 48.78415483021736, agent episode variance: [46.19354527282715, 1.2719763803482056, 1.3186331770420074], time: 82.499
Running avgs for agent 0: q_loss: 3021007.5, p_loss: 157.81626892089844, mean_rew: -7.963270074913731, variance: 184.7741810913086, lamda: 3.88413143157959
Running avgs for agent 1: q_loss: 7.144928455352783, p_loss: -4.081649303436279, mean_rew: -7.96764582903585, variance: 5.0879058837890625, lamda: 1.6055107116699219
Running avgs for agent 2: q_loss: 7.58534574508667, p_loss: -4.076775550842285, mean_rew: -7.963544412869199, variance: 5.274532794952393, lamda: 1.5395238399505615

steps: 2949975, episodes: 118000, mean episode reward: -614.330513205044, agent episode reward: [-204.77683773501462, -204.77683773501462, -204.77683773501462], time: 77.997
steps: 2949975, episodes: 118000, mean episode variance: 50.264858760595324, agent episode variance: [47.67301013183594, 1.2692356767654418, 1.3226129519939422], time: 77.998
Running avgs for agent 0: q_loss: 3110645.0, p_loss: 158.23110961914062, mean_rew: -7.985525533115995, variance: 190.69204052734375, lamda: 3.9091055393218994
Running avgs for agent 1: q_loss: 7.09207010269165, p_loss: -4.087075710296631, mean_rew: -7.981491740841524, variance: 5.0769429206848145, lamda: 1.6055406332015991
Running avgs for agent 2: q_loss: 7.5696258544921875, p_loss: -4.084413528442383, mean_rew: -7.980255234263371, variance: 5.290451526641846, lamda: 1.539523959159851

steps: 2974975, episodes: 119000, mean episode reward: -610.3316098627583, agent episode reward: [-203.4438699542528, -203.4438699542528, -203.4438699542528], time: 73.264
steps: 2974975, episodes: 119000, mean episode variance: 49.23751842713356, agent episode variance: [46.63849948120117, 1.2716857120990752, 1.327333233833313], time: 73.264
Running avgs for agent 0: q_loss: 3161629.25, p_loss: 158.5248565673828, mean_rew: -7.983405322865573, variance: 186.5539979248047, lamda: 3.934080123901367
Running avgs for agent 1: q_loss: 7.08783483505249, p_loss: -4.087460517883301, mean_rew: -7.984782786426448, variance: 5.086743354797363, lamda: 1.6059004068374634
Running avgs for agent 2: q_loss: 7.4867658615112305, p_loss: -4.083609580993652, mean_rew: -7.990932505288202, variance: 5.309332847595215, lamda: 1.539523720741272

steps: 2999975, episodes: 120000, mean episode reward: -620.8689106006312, agent episode reward: [-206.95630353354377, -206.95630353354377, -206.95630353354377], time: 69.831
steps: 2999975, episodes: 120000, mean episode variance: 49.023163095951084, agent episode variance: [46.42176466369629, 1.2739363334178924, 1.3274620988368988], time: 69.832
Running avgs for agent 0: q_loss: 3219771.0, p_loss: 159.042236328125, mean_rew: -7.996927358144437, variance: 185.68705865478515, lamda: 3.959054470062256
Running avgs for agent 1: q_loss: 7.277061462402344, p_loss: -4.087469100952148, mean_rew: -7.991791391726026, variance: 5.095745086669922, lamda: 1.6060551404953003
Running avgs for agent 2: q_loss: 7.510687351226807, p_loss: -4.093031406402588, mean_rew: -8.002188687683665, variance: 5.309848308563232, lamda: 1.5395238399505615

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -615.5613492250533, agent episode reward: [-205.18711640835113, -205.18711640835113, -205.18711640835113], time: 49.071
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 49.072
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -617.5221406940842, agent episode reward: [-205.84071356469474, -205.84071356469474, -205.84071356469474], time: 61.533
steps: 49975, episodes: 2000, mean episode variance: 127.67363322830201, agent episode variance: [124.92829739379883, 1.0342573432922364, 1.7110784912109376], time: 61.534
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -8.221643351424687, variance: 512.001220703125, lamda: 3.9715917110443115
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -8.219653342934997, variance: 4.2387590408325195, lamda: 1.606222152709961
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -8.223009495458642, variance: 7.012616157531738, lamda: 1.5395171642303467

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567760895.1091627: line 9: --exp_var_alpha: command not found
