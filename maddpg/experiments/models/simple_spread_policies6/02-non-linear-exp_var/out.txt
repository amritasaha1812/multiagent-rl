# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 23.72 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies6/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies6/02-non-linear-exp_var/
Job <1090575> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc163>>
arglist.u_estimation True
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -521.6130553971574, agent episode reward: [-173.87101846571912, -173.87101846571912, -173.87101846571912], time: 57.195
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 57.195
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -806.3989109117036, agent episode reward: [-268.79963697056786, -268.79963697056786, -268.79963697056786], time: 81.457
steps: 49975, episodes: 2000, mean episode variance: 7.361242549490184, agent episode variance: [3.0317428986914456, 1.2982894540652632, 3.0312101967334746], time: 81.458
Running avgs for agent 0: q_loss: 161.64952087402344, p_loss: -5.947740077972412, mean_rew: -8.069820534576882, variance: 12.425174713134766, lamda: 1.0109708309173584
Running avgs for agent 1: q_loss: 892.2347412109375, p_loss: 12.440674781799316, mean_rew: -8.05252208792762, variance: 5.32085841830026, lamda: 1.0105332136154175
Running avgs for agent 2: q_loss: 164.48484802246094, p_loss: -5.964716911315918, mean_rew: -8.070644625059984, variance: 12.42299260956342, lamda: 1.0109771490097046

steps: 74975, episodes: 3000, mean episode reward: -846.595587866399, agent episode reward: [-282.1985292887997, -282.1985292887997, -282.1985292887997], time: 80.931
steps: 74975, episodes: 3000, mean episode variance: 13.009997543334961, agent episode variance: [2.3100194206237794, 8.385873290657997, 2.3141048320531845], time: 80.932
Running avgs for agent 0: q_loss: 53.140804290771484, p_loss: -5.136045455932617, mean_rew: -9.347714840076135, variance: 9.24007797241211, lamda: 1.034162163734436
Running avgs for agent 1: q_loss: 20667.017578125, p_loss: 32.231849670410156, mean_rew: -9.36610298834437, variance: 33.54349316263199, lamda: 1.0352145433425903
Running avgs for agent 2: q_loss: 51.291046142578125, p_loss: -5.099841594696045, mean_rew: -9.364679420790072, variance: 9.256420135498047, lamda: 1.0349693298339844

steps: 99975, episodes: 4000, mean episode reward: -782.9621708242446, agent episode reward: [-260.98739027474824, -260.98739027474824, -260.98739027474824], time: 79.546
steps: 99975, episodes: 4000, mean episode variance: 26.518991075277327, agent episode variance: [2.3543514091968536, 21.82006132364273, 2.344578342437744], time: 79.547
Running avgs for agent 0: q_loss: 60.312591552734375, p_loss: -5.088352680206299, mean_rew: -9.798708371407388, variance: 9.417405128479004, lamda: 1.0529282093048096
Running avgs for agent 1: q_loss: 147593.34375, p_loss: 54.16246795654297, mean_rew: -9.808124073068232, variance: 87.28024529457092, lamda: 1.060219407081604
Running avgs for agent 2: q_loss: 44.6418342590332, p_loss: -5.003394603729248, mean_rew: -9.794711570720697, variance: 9.378312110900879, lamda: 1.0577024221420288

steps: 124975, episodes: 5000, mean episode reward: -677.2297248901124, agent episode reward: [-225.7432416300375, -225.7432416300375, -225.7432416300375], time: 79.747
steps: 124975, episodes: 5000, mean episode variance: 37.69221518051624, agent episode variance: [2.3116995168924332, 33.12599544000626, 2.2545202236175537], time: 79.748
Running avgs for agent 0: q_loss: 55.98802947998047, p_loss: -5.08147668838501, mean_rew: -9.797077953550497, variance: 9.246797561645508, lamda: 1.0725207328796387
Running avgs for agent 1: q_loss: 259784.546875, p_loss: 72.93873596191406, mean_rew: -9.799838376954565, variance: 132.50398176002503, lamda: 1.0852235555648804
Running avgs for agent 2: q_loss: 42.53192901611328, p_loss: -5.03129768371582, mean_rew: -9.794114443348684, variance: 9.01807975769043, lamda: 1.0767399072647095

steps: 149975, episodes: 6000, mean episode reward: -653.8296863829294, agent episode reward: [-217.94322879430982, -217.94322879430982, -217.94322879430982], time: 80.656
steps: 149975, episodes: 6000, mean episode variance: 40.00051120400429, agent episode variance: [2.208693575143814, 35.59558877134323, 2.1962288575172426], time: 80.656
Running avgs for agent 0: q_loss: 52.51711654663086, p_loss: -4.9893927574157715, mean_rew: -9.619429200072926, variance: 8.834774017333984, lamda: 1.0864529609680176
Running avgs for agent 1: q_loss: 364140.375, p_loss: 88.48641967773438, mean_rew: -9.60246444464301, variance: 142.38235508537292, lamda: 1.1102278232574463
Running avgs for agent 2: q_loss: 32.5274543762207, p_loss: -4.9277191162109375, mean_rew: -9.598350426263787, variance: 8.784915924072266, lamda: 1.0990691184997559

steps: 174975, episodes: 7000, mean episode reward: -706.8655641607452, agent episode reward: [-235.6218547202484, -235.6218547202484, -235.6218547202484], time: 80.637
steps: 174975, episodes: 7000, mean episode variance: 31.308028513669967, agent episode variance: [2.1746169798374178, 27.013573692798616, 2.1198378410339354], time: 80.637
Running avgs for agent 0: q_loss: 41.82411575317383, p_loss: -4.9324541091918945, mean_rew: -9.526859159338018, variance: 8.698468208312988, lamda: 1.1066877841949463
Running avgs for agent 1: q_loss: 521804.03125, p_loss: 103.26156616210938, mean_rew: -9.52584850565019, variance: 108.05429477119446, lamda: 1.135231852531433
Running avgs for agent 2: q_loss: 35.08168411254883, p_loss: -4.887673377990723, mean_rew: -9.527023401266186, variance: 8.479351997375488, lamda: 1.1216448545455933

steps: 199975, episodes: 8000, mean episode reward: -702.2617536444897, agent episode reward: [-234.0872512148299, -234.0872512148299, -234.0872512148299], time: 81.611
steps: 199975, episodes: 8000, mean episode variance: 37.532559985399246, agent episode variance: [2.104547985315323, 33.354960010528565, 2.073051989555359], time: 81.612
Running avgs for agent 0: q_loss: 42.43450164794922, p_loss: -4.9240217208862305, mean_rew: -9.535355217516386, variance: 8.418190956115723, lamda: 1.131679654121399
Running avgs for agent 1: q_loss: 572060.9375, p_loss: 114.67970275878906, mean_rew: -9.539756232717686, variance: 133.41984004211426, lamda: 1.1602360010147095
Running avgs for agent 2: q_loss: 35.674407958984375, p_loss: -4.8981614112854, mean_rew: -9.533429001666075, variance: 8.292208671569824, lamda: 1.1460210084915161

steps: 224975, episodes: 9000, mean episode reward: -667.7218473701328, agent episode reward: [-222.57394912337756, -222.57394912337756, -222.57394912337756], time: 82.277
steps: 224975, episodes: 9000, mean episode variance: 50.359725162029264, agent episode variance: [2.0575100893974305, 46.2699951505661, 2.032219922065735], time: 82.278
Running avgs for agent 0: q_loss: 35.9134407043457, p_loss: -4.864711761474609, mean_rew: -9.460321608184564, variance: 8.230040550231934, lamda: 1.156535029411316
Running avgs for agent 1: q_loss: 928495.25, p_loss: 124.56682586669922, mean_rew: -9.462673511181926, variance: 185.0799806022644, lamda: 1.1852401494979858
Running avgs for agent 2: q_loss: 33.321868896484375, p_loss: -4.807501792907715, mean_rew: -9.456329606107044, variance: 8.12887954711914, lamda: 1.170364499092102

steps: 249975, episodes: 10000, mean episode reward: -748.7833835835324, agent episode reward: [-249.59446119451079, -249.59446119451079, -249.59446119451079], time: 79.75
steps: 249975, episodes: 10000, mean episode variance: 65.42773161649704, agent episode variance: [1.9948849391937256, 61.44247422504425, 1.9903724522590638], time: 79.75
Running avgs for agent 0: q_loss: 35.8997688293457, p_loss: -4.878838062286377, mean_rew: -9.463814122803807, variance: 7.97953987121582, lamda: 1.1809055805206299
Running avgs for agent 1: q_loss: 1629879.0, p_loss: 135.0437469482422, mean_rew: -9.447183620501717, variance: 245.769896900177, lamda: 1.2102442979812622
Running avgs for agent 2: q_loss: 35.52912521362305, p_loss: -4.819187641143799, mean_rew: -9.454463242324975, variance: 7.961489677429199, lamda: 1.194997787475586

steps: 274975, episodes: 11000, mean episode reward: -672.0060888535879, agent episode reward: [-224.00202961786266, -224.00202961786266, -224.00202961786266], time: 79.983
steps: 274975, episodes: 11000, mean episode variance: 46.637213876008985, agent episode variance: [1.977751270055771, 42.69973779153824, 1.959724814414978], time: 79.983
Running avgs for agent 0: q_loss: 38.061519622802734, p_loss: -4.855825424194336, mean_rew: -9.475622917357686, variance: 7.91100549697876, lamda: 1.2058055400848389
Running avgs for agent 1: q_loss: 1241338.0, p_loss: 146.00865173339844, mean_rew: -9.48579918749335, variance: 170.79895116615296, lamda: 1.2352484464645386
Running avgs for agent 2: q_loss: 32.8017578125, p_loss: -4.827463150024414, mean_rew: -9.481142646733257, variance: 7.838899612426758, lamda: 1.2198851108551025

steps: 299975, episodes: 12000, mean episode reward: -614.5137678746977, agent episode reward: [-204.83792262489925, -204.83792262489925, -204.83792262489925], time: 82.784
steps: 299975, episodes: 12000, mean episode variance: 55.96519869732857, agent episode variance: [1.92623344373703, 52.13166595363617, 1.907299299955368], time: 82.785
Running avgs for agent 0: q_loss: 32.920555114746094, p_loss: -4.815954208374023, mean_rew: -9.39609089678431, variance: 7.7049336433410645, lamda: 1.2308459281921387
Running avgs for agent 1: q_loss: 1683807.75, p_loss: 154.83021545410156, mean_rew: -9.395767343296955, variance: 208.5266638145447, lamda: 1.2602527141571045
Running avgs for agent 2: q_loss: 31.63519287109375, p_loss: -4.792263984680176, mean_rew: -9.407742029892729, variance: 7.629197120666504, lamda: 1.2443151473999023

steps: 324975, episodes: 13000, mean episode reward: -618.0552123968015, agent episode reward: [-206.01840413226716, -206.01840413226716, -206.01840413226716], time: 86.127
steps: 324975, episodes: 13000, mean episode variance: 59.61853122138977, agent episode variance: [1.8671388087272645, 55.89751609039307, 1.8538763222694397], time: 86.128
Running avgs for agent 0: q_loss: 32.197452545166016, p_loss: -4.754297256469727, mean_rew: -9.29417191842833, variance: 7.468554496765137, lamda: 1.2555819749832153
Running avgs for agent 1: q_loss: 1385076.0, p_loss: 162.86167907714844, mean_rew: -9.315825490460597, variance: 223.59006436157227, lamda: 1.2852566242218018
Running avgs for agent 2: q_loss: 29.652637481689453, p_loss: -4.740485191345215, mean_rew: -9.298570880155559, variance: 7.415505409240723, lamda: 1.2686725854873657

steps: 349975, episodes: 14000, mean episode reward: -597.4034051132984, agent episode reward: [-199.1344683710995, -199.1344683710995, -199.1344683710995], time: 86.695
steps: 349975, episodes: 14000, mean episode variance: 61.36720340204239, agent episode variance: [1.8192282762527465, 57.750314750671386, 1.7976603751182556], time: 86.696
Running avgs for agent 0: q_loss: 27.535911560058594, p_loss: -4.709573268890381, mean_rew: -9.215936539156264, variance: 7.276913166046143, lamda: 1.2796435356140137
Running avgs for agent 1: q_loss: 1658656.25, p_loss: 170.59439086914062, mean_rew: -9.221079758535284, variance: 231.00125900268554, lamda: 1.3102608919143677
Running avgs for agent 2: q_loss: 29.33552360534668, p_loss: -4.711755275726318, mean_rew: -9.214990368131774, variance: 7.190641403198242, lamda: 1.292897343635559

steps: 374975, episodes: 15000, mean episode reward: -572.049007954628, agent episode reward: [-190.68300265154267, -190.68300265154267, -190.68300265154267], time: 86.679
steps: 374975, episodes: 15000, mean episode variance: 57.963581259727476, agent episode variance: [1.7626671528816222, 54.43029862499237, 1.7706154818534852], time: 86.679
Running avgs for agent 0: q_loss: 26.63239097595215, p_loss: -4.670650482177734, mean_rew: -9.111960056086657, variance: 7.050668239593506, lamda: 1.3026760816574097
Running avgs for agent 1: q_loss: 1913972.5, p_loss: 176.8712158203125, mean_rew: -9.116668309450688, variance: 217.72119449996947, lamda: 1.335265040397644
Running avgs for agent 2: q_loss: 26.001413345336914, p_loss: -4.646996021270752, mean_rew: -9.118176359411583, variance: 7.082461357116699, lamda: 1.3162473440170288

steps: 399975, episodes: 16000, mean episode reward: -556.0007328733703, agent episode reward: [-185.33357762445678, -185.33357762445678, -185.33357762445678], time: 84.319
steps: 399975, episodes: 16000, mean episode variance: 111.06882343292236, agent episode variance: [1.7165128302574157, 107.64086701965331, 1.7114435830116272], time: 84.319
Running avgs for agent 0: q_loss: 27.01362419128418, p_loss: -4.627941608428955, mean_rew: -9.017225848970261, variance: 6.866051197052002, lamda: 1.322895884513855
Running avgs for agent 1: q_loss: 3891994.0, p_loss: 181.36497497558594, mean_rew: -9.004031251706378, variance: 430.56346807861325, lamda: 1.3602690696716309
Running avgs for agent 2: q_loss: 25.813535690307617, p_loss: -4.613547325134277, mean_rew: -9.01612472869893, variance: 6.8457746505737305, lamda: 1.3402448892593384

steps: 424975, episodes: 17000, mean episode reward: -560.9060724877011, agent episode reward: [-186.9686908292337, -186.9686908292337, -186.9686908292337], time: 101.472
steps: 424975, episodes: 17000, mean episode variance: 117.30558853745461, agent episode variance: [1.6766088156700134, 113.97026098632813, 1.6587187354564668], time: 101.473
Running avgs for agent 0: q_loss: 23.773168563842773, p_loss: -4.585521697998047, mean_rew: -8.92310560179986, variance: 6.706435680389404, lamda: 1.3432773351669312
Running avgs for agent 1: q_loss: 4368304.0, p_loss: 185.02244567871094, mean_rew: -8.920519916381425, variance: 455.8810439453125, lamda: 1.3852733373641968
Running avgs for agent 2: q_loss: 26.581724166870117, p_loss: -4.5680670738220215, mean_rew: -8.91954100375785, variance: 6.6348748207092285, lamda: 1.3638473749160767

steps: 449975, episodes: 18000, mean episode reward: -554.1253890977492, agent episode reward: [-184.70846303258307, -184.70846303258307, -184.70846303258307], time: 121.882
steps: 449975, episodes: 18000, mean episode variance: 123.92884940481186, agent episode variance: [1.645601438999176, 120.65867623901367, 1.6245717267990112], time: 121.883
Running avgs for agent 0: q_loss: 22.969179153442383, p_loss: -4.5452656745910645, mean_rew: -8.849921321565008, variance: 6.5824055671691895, lamda: 1.3625078201293945
Running avgs for agent 1: q_loss: 4547979.0, p_loss: 187.8045196533203, mean_rew: -8.822964998825894, variance: 482.6347049560547, lamda: 1.4102776050567627
Running avgs for agent 2: q_loss: 26.238636016845703, p_loss: -4.525938987731934, mean_rew: -8.831790906214387, variance: 6.498287200927734, lamda: 1.3863041400909424

steps: 474975, episodes: 19000, mean episode reward: -546.722995957188, agent episode reward: [-182.240998652396, -182.240998652396, -182.240998652396], time: 116.021
steps: 474975, episodes: 19000, mean episode variance: 119.98730870866775, agent episode variance: [1.6090272731781006, 116.78367070960998, 1.5946107258796691], time: 116.022
Running avgs for agent 0: q_loss: 21.936670303344727, p_loss: -4.488588809967041, mean_rew: -8.761314811812742, variance: 6.4361090660095215, lamda: 1.3842228651046753
Running avgs for agent 1: q_loss: 4814536.5, p_loss: 189.96856689453125, mean_rew: -8.754874249920128, variance: 467.1346828384399, lamda: 1.43528151512146
Running avgs for agent 2: q_loss: 30.549375534057617, p_loss: -4.493502140045166, mean_rew: -8.757070674991477, variance: 6.378442764282227, lamda: 1.4070061445236206

steps: 499975, episodes: 20000, mean episode reward: -535.5361441001294, agent episode reward: [-178.51204803337646, -178.51204803337646, -178.51204803337646], time: 117.677
steps: 499975, episodes: 20000, mean episode variance: 117.30055617189407, agent episode variance: [1.5656684103012084, 114.18344030761719, 1.5514474539756775], time: 117.678
Running avgs for agent 0: q_loss: 21.25958824157715, p_loss: -4.448581218719482, mean_rew: -8.677738464256436, variance: 6.262673854827881, lamda: 1.4055956602096558
Running avgs for agent 1: q_loss: 4878973.0, p_loss: 191.11569213867188, mean_rew: -8.674513917534176, variance: 456.73376123046876, lamda: 1.4602856636047363
Running avgs for agent 2: q_loss: 28.495174407958984, p_loss: -4.445362091064453, mean_rew: -8.668807963993327, variance: 6.205790042877197, lamda: 1.4286061525344849

steps: 524975, episodes: 21000, mean episode reward: -526.730341414317, agent episode reward: [-175.57678047143906, -175.57678047143906, -175.57678047143906], time: 118.613
steps: 524975, episodes: 21000, mean episode variance: 116.83301370072365, agent episode variance: [1.531209111213684, 113.77974142456054, 1.5220631649494172], time: 118.614
Running avgs for agent 0: q_loss: 21.656394958496094, p_loss: -4.398208141326904, mean_rew: -8.587359730129073, variance: 6.124836444854736, lamda: 1.4288594722747803
Running avgs for agent 1: q_loss: 4786018.5, p_loss: 191.17616271972656, mean_rew: -8.590958520339376, variance: 455.11896569824216, lamda: 1.4852899312973022
Running avgs for agent 2: q_loss: 25.104476928710938, p_loss: -4.404240131378174, mean_rew: -8.608691387749541, variance: 6.088253021240234, lamda: 1.4476847648620605

steps: 549975, episodes: 22000, mean episode reward: -516.7485445173177, agent episode reward: [-172.2495148391059, -172.2495148391059, -172.2495148391059], time: 131.419
steps: 549975, episodes: 22000, mean episode variance: 110.60976404619217, agent episode variance: [1.4901533665657043, 107.64077682495117, 1.478833854675293], time: 131.42
Running avgs for agent 0: q_loss: 20.32550048828125, p_loss: -4.367372512817383, mean_rew: -8.513503305442297, variance: 5.960613250732422, lamda: 1.4514871835708618
Running avgs for agent 1: q_loss: 4808065.5, p_loss: 191.04489135742188, mean_rew: -8.517338646048247, variance: 430.5631072998047, lamda: 1.510293960571289
Running avgs for agent 2: q_loss: 23.97705841064453, p_loss: -4.3552446365356445, mean_rew: -8.527044040068185, variance: 5.915335655212402, lamda: 1.4715286493301392

steps: 574975, episodes: 23000, mean episode reward: -512.5508487111412, agent episode reward: [-170.85028290371372, -170.85028290371372, -170.85028290371372], time: 172.682
steps: 574975, episodes: 23000, mean episode variance: 99.41372432994842, agent episode variance: [1.471353418827057, 96.50687857055664, 1.4354923405647277], time: 172.682
Running avgs for agent 0: q_loss: 21.87531280517578, p_loss: -4.320268630981445, mean_rew: -8.448248840896204, variance: 5.885413646697998, lamda: 1.472690463066101
Running avgs for agent 1: q_loss: 4244985.5, p_loss: 189.7375946044922, mean_rew: -8.44597980433042, variance: 386.02751428222655, lamda: 1.535298228263855
Running avgs for agent 2: q_loss: 23.948274612426758, p_loss: -4.318852424621582, mean_rew: -8.447423321056618, variance: 5.741969108581543, lamda: 1.4956492185592651

steps: 599975, episodes: 24000, mean episode reward: -510.309497041019, agent episode reward: [-170.10316568033969, -170.10316568033969, -170.10316568033969], time: 167.891
steps: 599975, episodes: 24000, mean episode variance: 88.83274706792831, agent episode variance: [1.4351430838108064, 85.99409165382386, 1.4035123302936554], time: 167.892
Running avgs for agent 0: q_loss: 21.446273803710938, p_loss: -4.2899346351623535, mean_rew: -8.381627442253487, variance: 5.740572452545166, lamda: 1.4925390481948853
Running avgs for agent 1: q_loss: 3958534.25, p_loss: 188.24266052246094, mean_rew: -8.369087088736475, variance: 343.97636661529543, lamda: 1.560302495956421
Running avgs for agent 2: q_loss: 22.265851974487305, p_loss: -4.3049540519714355, mean_rew: -8.383533966826116, variance: 5.614049911499023, lamda: 1.519565463066101

steps: 624975, episodes: 25000, mean episode reward: -505.68175166367286, agent episode reward: [-168.56058388789097, -168.56058388789097, -168.56058388789097], time: 169.033
steps: 624975, episodes: 25000, mean episode variance: 96.4446984899044, agent episode variance: [1.3991887574195863, 93.6737966594696, 1.371713073015213], time: 169.033
Running avgs for agent 0: q_loss: 19.580476760864258, p_loss: -4.256793975830078, mean_rew: -8.306821476540401, variance: 5.596755027770996, lamda: 1.5150790214538574
Running avgs for agent 1: q_loss: 4500607.0, p_loss: 186.96397399902344, mean_rew: -8.320986045627434, variance: 374.6951866378784, lamda: 1.5853064060211182
Running avgs for agent 2: q_loss: 26.786985397338867, p_loss: -4.24772834777832, mean_rew: -8.312330506522386, variance: 5.486852169036865, lamda: 1.5400351285934448

steps: 649975, episodes: 26000, mean episode reward: -497.3904709354239, agent episode reward: [-165.7968236451413, -165.7968236451413, -165.7968236451413], time: 171.935
steps: 649975, episodes: 26000, mean episode variance: 99.43405512857437, agent episode variance: [1.3739491755962372, 96.70619427490234, 1.3539116780757905], time: 171.936
Running avgs for agent 0: q_loss: 21.322397232055664, p_loss: -4.233138561248779, mean_rew: -8.251407922524962, variance: 5.4957966804504395, lamda: 1.5365493297576904
Running avgs for agent 1: q_loss: 4554034.0, p_loss: 184.57630920410156, mean_rew: -8.23976795418184, variance: 386.8247770996094, lamda: 1.6103105545043945
Running avgs for agent 2: q_loss: 22.111841201782227, p_loss: -4.217423915863037, mean_rew: -8.253665054807334, variance: 5.415646553039551, lamda: 1.561655044555664

steps: 674975, episodes: 27000, mean episode reward: -500.72262116112313, agent episode reward: [-166.90754038704105, -166.90754038704105, -166.90754038704105], time: 178.244
steps: 674975, episodes: 27000, mean episode variance: 102.3940135474205, agent episode variance: [1.3411710317134857, 99.71902154541016, 1.3338209702968598], time: 178.245
Running avgs for agent 0: q_loss: 25.650882720947266, p_loss: -4.207735538482666, mean_rew: -8.18505955464037, variance: 5.364684104919434, lamda: 1.5521970987319946
Running avgs for agent 1: q_loss: 4373472.5, p_loss: 182.10821533203125, mean_rew: -8.188503131738603, variance: 398.87608618164063, lamda: 1.6353148221969604
Running avgs for agent 2: q_loss: 26.716909408569336, p_loss: -4.194393634796143, mean_rew: -8.196386070445968, variance: 5.3352837562561035, lamda: 1.5838159322738647

steps: 699975, episodes: 28000, mean episode reward: -500.52434483697056, agent episode reward: [-166.84144827899019, -166.84144827899019, -166.84144827899019], time: 179.136
steps: 699975, episodes: 28000, mean episode variance: 99.94946613144874, agent episode variance: [1.3216143777370453, 97.34018035125733, 1.2876714024543763], time: 179.137
Running avgs for agent 0: q_loss: 19.494298934936523, p_loss: -4.1751275062561035, mean_rew: -8.126482890762102, variance: 5.286457538604736, lamda: 1.5665371417999268
Running avgs for agent 1: q_loss: 4296127.5, p_loss: 179.58248901367188, mean_rew: -8.129517753032308, variance: 389.3607214050293, lamda: 1.6603188514709473
Running avgs for agent 2: q_loss: 28.382633209228516, p_loss: -4.171705722808838, mean_rew: -8.134482320796419, variance: 5.1506853103637695, lamda: 1.5974678993225098

steps: 724975, episodes: 29000, mean episode reward: -500.19267204557383, agent episode reward: [-166.73089068185794, -166.73089068185794, -166.73089068185794], time: 180.135
steps: 724975, episodes: 29000, mean episode variance: 95.67926825428009, agent episode variance: [1.3018118376731873, 93.09767874145508, 1.279777675151825], time: 180.135
Running avgs for agent 0: q_loss: 18.82509422302246, p_loss: -4.1413702964782715, mean_rew: -8.086295555464774, variance: 5.207247257232666, lamda: 1.5884582996368408
Running avgs for agent 1: q_loss: 4269649.0, p_loss: 177.5435028076172, mean_rew: -8.094421856206539, variance: 372.39071496582034, lamda: 1.6853229999542236
Running avgs for agent 2: q_loss: 27.95262908935547, p_loss: -4.135513782501221, mean_rew: -8.080261952268065, variance: 5.119110584259033, lamda: 1.6094541549682617

steps: 749975, episodes: 30000, mean episode reward: -501.7569956222092, agent episode reward: [-167.25233187406974, -167.25233187406974, -167.25233187406974], time: 167.816
steps: 749975, episodes: 30000, mean episode variance: 87.71821320486069, agent episode variance: [1.2679188239574433, 85.18338422393799, 1.2669101569652557], time: 167.817
Running avgs for agent 0: q_loss: 21.0152530670166, p_loss: -4.128736972808838, mean_rew: -8.03584167115183, variance: 5.071674823760986, lamda: 1.6065313816070557
Running avgs for agent 1: q_loss: 4063393.5, p_loss: 175.56463623046875, mean_rew: -8.037674156226691, variance: 340.73353689575197, lamda: 1.7103272676467896
Running avgs for agent 2: q_loss: 20.337867736816406, p_loss: -4.126601696014404, mean_rew: -8.031581018853608, variance: 5.067640781402588, lamda: 1.6278923749923706

steps: 774975, episodes: 31000, mean episode reward: -502.64513252520106, agent episode reward: [-167.54837750840036, -167.54837750840036, -167.54837750840036], time: 163.992
steps: 774975, episodes: 31000, mean episode variance: 49.18908843123913, agent episode variance: [1.25553058385849, 46.70362207508087, 1.2299357722997666], time: 163.992
Running avgs for agent 0: q_loss: 16.573226928710938, p_loss: -4.0994720458984375, mean_rew: -7.98983687038162, variance: 5.022122383117676, lamda: 1.6274336576461792
Running avgs for agent 1: q_loss: 2104597.75, p_loss: 173.82130432128906, mean_rew: -8.000527590435532, variance: 186.8144883003235, lamda: 1.7353312969207764
Running avgs for agent 2: q_loss: 27.90183448791504, p_loss: -4.101670742034912, mean_rew: -7.9952532741348685, variance: 4.919743061065674, lamda: 1.6488093137741089

steps: 799975, episodes: 32000, mean episode reward: -494.50077213363477, agent episode reward: [-164.83359071121157, -164.83359071121157, -164.83359071121157], time: 167.886
steps: 799975, episodes: 32000, mean episode variance: 46.94006848835945, agent episode variance: [1.2272747476100923, 44.48748550128937, 1.2253082394599915], time: 167.886
Running avgs for agent 0: q_loss: 19.178165435791016, p_loss: -4.084525108337402, mean_rew: -7.951727535231923, variance: 4.9090986251831055, lamda: 1.6474618911743164
Running avgs for agent 1: q_loss: 2070163.625, p_loss: 172.25270080566406, mean_rew: -7.957838840404929, variance: 177.94994200515748, lamda: 1.7603354454040527
Running avgs for agent 2: q_loss: 26.12647819519043, p_loss: -4.079703330993652, mean_rew: -7.9541486392230105, variance: 4.901232719421387, lamda: 1.6609528064727783

steps: 824975, episodes: 33000, mean episode reward: -489.942095796442, agent episode reward: [-163.31403193214734, -163.31403193214734, -163.31403193214734], time: 167.422
steps: 824975, episodes: 33000, mean episode variance: 57.49635902142525, agent episode variance: [1.2071550917625427, 55.098662712097166, 1.1905412175655365], time: 167.423
Running avgs for agent 0: q_loss: 17.864009857177734, p_loss: -4.062643051147461, mean_rew: -7.911188490047295, variance: 4.828619956970215, lamda: 1.6690925359725952
Running avgs for agent 1: q_loss: 2652632.5, p_loss: 170.4810333251953, mean_rew: -7.904943730190532, variance: 220.39465084838866, lamda: 1.7853397130966187
Running avgs for agent 2: q_loss: 21.124784469604492, p_loss: -4.050339221954346, mean_rew: -7.898516021318563, variance: 4.762165069580078, lamda: 1.6752771139144897

steps: 849975, episodes: 34000, mean episode reward: -489.9342772065838, agent episode reward: [-163.3114257355279, -163.3114257355279, -163.3114257355279], time: 167.549
steps: 849975, episodes: 34000, mean episode variance: 70.5285645430088, agent episode variance: [1.1835335092544557, 68.17265209960938, 1.1723789341449737], time: 167.549
Running avgs for agent 0: q_loss: 17.62278175354004, p_loss: -4.040087699890137, mean_rew: -7.8631225072301625, variance: 4.734133720397949, lamda: 1.6917728185653687
Running avgs for agent 1: q_loss: 2998584.25, p_loss: 169.29197692871094, mean_rew: -7.869543897757452, variance: 272.6906083984375, lamda: 1.810343861579895
Running avgs for agent 2: q_loss: 19.711444854736328, p_loss: -4.027279376983643, mean_rew: -7.8658959415095575, variance: 4.689515590667725, lamda: 1.697840929031372

steps: 874975, episodes: 35000, mean episode reward: -489.75512040162795, agent episode reward: [-163.25170680054265, -163.25170680054265, -163.25170680054265], time: 160.937
steps: 874975, episodes: 35000, mean episode variance: 59.75895190691948, agent episode variance: [1.162196261882782, 57.4415833568573, 1.1551722881793975], time: 160.937
Running avgs for agent 0: q_loss: 17.989131927490234, p_loss: -4.00966215133667, mean_rew: -7.82794599719441, variance: 4.64878511428833, lamda: 1.7111786603927612
Running avgs for agent 1: q_loss: 2791998.75, p_loss: 167.89381408691406, mean_rew: -7.841839313806186, variance: 229.7663334274292, lamda: 1.8353478908538818
Running avgs for agent 2: q_loss: 18.85442352294922, p_loss: -4.016798973083496, mean_rew: -7.841467521469607, variance: 4.6206889152526855, lamda: 1.7210955619812012

steps: 899975, episodes: 36000, mean episode reward: -487.0342159099168, agent episode reward: [-162.3447386366389, -162.3447386366389, -162.3447386366389], time: 173.45
steps: 899975, episodes: 36000, mean episode variance: 81.28282732796669, agent episode variance: [1.1422627892494202, 79.00610418701172, 1.1344603517055512], time: 173.45
Running avgs for agent 0: q_loss: 17.339683532714844, p_loss: -4.007280349731445, mean_rew: -7.7892555895081355, variance: 4.569051742553711, lamda: 1.733299732208252
Running avgs for agent 1: q_loss: 3880084.25, p_loss: 166.3480224609375, mean_rew: -7.789540466754342, variance: 316.02441674804686, lamda: 1.8603521585464478
Running avgs for agent 2: q_loss: 21.468019485473633, p_loss: -3.991628408432007, mean_rew: -7.803773196605418, variance: 4.537841320037842, lamda: 1.74045729637146

steps: 924975, episodes: 37000, mean episode reward: -486.3276446794838, agent episode reward: [-162.10921489316127, -162.10921489316127, -162.10921489316127], time: 172.936
steps: 924975, episodes: 37000, mean episode variance: 81.15249937701225, agent episode variance: [1.1324900529384614, 78.89409432983399, 1.1259149942398072], time: 172.936
Running avgs for agent 0: q_loss: 16.049243927001953, p_loss: -3.9783129692077637, mean_rew: -7.754065087900145, variance: 4.529960632324219, lamda: 1.7548120021820068
Running avgs for agent 1: q_loss: 3661744.25, p_loss: 164.98377990722656, mean_rew: -7.747356843953615, variance: 315.57637731933596, lamda: 1.8853561878204346
Running avgs for agent 2: q_loss: 19.657251358032227, p_loss: -3.9671006202697754, mean_rew: -7.761486037747053, variance: 4.503660202026367, lamda: 1.7506691217422485

steps: 949975, episodes: 38000, mean episode reward: -488.49311830242453, agent episode reward: [-162.8310394341415, -162.8310394341415, -162.8310394341415], time: 173.829
steps: 949975, episodes: 38000, mean episode variance: 78.82940062117576, agent episode variance: [1.1104425497055053, 76.59147718811035, 1.127480883359909], time: 173.829
Running avgs for agent 0: q_loss: 18.886770248413086, p_loss: -3.9699008464813232, mean_rew: -7.727546342275742, variance: 4.441770076751709, lamda: 1.7746366262435913
Running avgs for agent 1: q_loss: 3708153.75, p_loss: 164.09121704101562, mean_rew: -7.719461506182527, variance: 306.3659087524414, lamda: 1.910360336303711
Running avgs for agent 2: q_loss: 20.494590759277344, p_loss: -3.947350025177002, mean_rew: -7.72345034889471, variance: 4.509923934936523, lamda: 1.759784460067749

steps: 974975, episodes: 39000, mean episode reward: -489.06392845329265, agent episode reward: [-163.02130948443087, -163.02130948443087, -163.02130948443087], time: 172.453
steps: 974975, episodes: 39000, mean episode variance: 79.8727083363533, agent episode variance: [1.0964339635372162, 77.68106691741943, 1.0952074553966522], time: 172.454
Running avgs for agent 0: q_loss: 19.076366424560547, p_loss: -3.9594810009002686, mean_rew: -7.701997093339702, variance: 4.385735988616943, lamda: 1.7948658466339111
Running avgs for agent 1: q_loss: 3742250.75, p_loss: 163.52708435058594, mean_rew: -7.695985818252296, variance: 310.7242676696777, lamda: 1.9353646039962769
Running avgs for agent 2: q_loss: 18.85174560546875, p_loss: -3.934251546859741, mean_rew: -7.681546443157358, variance: 4.380829811096191, lamda: 1.7701618671417236

steps: 999975, episodes: 40000, mean episode reward: -490.7567872688996, agent episode reward: [-163.58559575629985, -163.58559575629985, -163.58559575629985], time: 171.819
steps: 999975, episodes: 40000, mean episode variance: 78.24568348050117, agent episode variance: [1.0772229027748108, 76.07233047485352, 1.0961301028728485], time: 171.82
Running avgs for agent 0: q_loss: 16.393390655517578, p_loss: -3.935250997543335, mean_rew: -7.667350693520892, variance: 4.308891773223877, lamda: 1.8146604299545288
Running avgs for agent 1: q_loss: 3681194.0, p_loss: 163.0477294921875, mean_rew: -7.657036582703095, variance: 304.28932189941406, lamda: 1.9603686332702637
Running avgs for agent 2: q_loss: 20.015277862548828, p_loss: -3.914968967437744, mean_rew: -7.661549591378373, variance: 4.384520530700684, lamda: 1.7779501676559448

steps: 1024975, episodes: 41000, mean episode reward: -488.02155548616105, agent episode reward: [-162.67385182872033, -162.67385182872033, -162.67385182872033], time: 169.197
steps: 1024975, episodes: 41000, mean episode variance: 77.58244055700303, agent episode variance: [1.0650271818637849, 75.41983534240723, 1.09757803273201], time: 169.198
Running avgs for agent 0: q_loss: 15.802237510681152, p_loss: -3.921308994293213, mean_rew: -7.637675872225253, variance: 4.260108947753906, lamda: 1.838968276977539
Running avgs for agent 1: q_loss: 3615119.0, p_loss: 162.90574645996094, mean_rew: -7.644916087173413, variance: 301.6793413696289, lamda: 1.98537278175354
Running avgs for agent 2: q_loss: 19.780136108398438, p_loss: -3.9097628593444824, mean_rew: -7.62836792546582, variance: 4.390312194824219, lamda: 1.783237338066101

steps: 1049975, episodes: 42000, mean episode reward: -491.48541829007013, agent episode reward: [-163.8284727633567, -163.8284727633567, -163.8284727633567], time: 170.972
steps: 1049975, episodes: 42000, mean episode variance: 45.316552134156225, agent episode variance: [1.038814651608467, 43.18906762886047, 1.0886698536872863], time: 170.972
Running avgs for agent 0: q_loss: 17.718687057495117, p_loss: -3.912071943283081, mean_rew: -7.600130197334222, variance: 4.155258655548096, lamda: 1.860042929649353
Running avgs for agent 1: q_loss: 2310206.5, p_loss: 162.13909912109375, mean_rew: -7.593986682451233, variance: 172.75627051544188, lamda: 2.010364532470703
Running avgs for agent 2: q_loss: 19.397823333740234, p_loss: -3.887983560562134, mean_rew: -7.589027908038505, variance: 4.354679584503174, lamda: 1.7892282009124756

steps: 1074975, episodes: 43000, mean episode reward: -493.23980768369483, agent episode reward: [-164.41326922789827, -164.41326922789827, -164.41326922789827], time: 170.746
steps: 1074975, episodes: 43000, mean episode variance: 77.53390823721885, agent episode variance: [1.020243857383728, 75.45491413879394, 1.0587502410411835], time: 170.746
Running avgs for agent 0: q_loss: 15.636736869812012, p_loss: -3.8373665809631348, mean_rew: -7.477699235207181, variance: 4.080975532531738, lamda: 1.8724751472473145
Running avgs for agent 1: q_loss: 3286220.25, p_loss: 159.98922729492188, mean_rew: -7.466792608568779, variance: 301.81965655517575, lamda: 2.035339117050171
Running avgs for agent 2: q_loss: 17.415735244750977, p_loss: -3.8372862339019775, mean_rew: -7.474498502644361, variance: 4.235001087188721, lamda: 1.794843316078186

steps: 1099975, episodes: 44000, mean episode reward: -493.3842983614914, agent episode reward: [-164.46143278716383, -164.46143278716383, -164.46143278716383], time: 172.924
steps: 1099975, episodes: 44000, mean episode variance: 73.20998902750016, agent episode variance: [0.9985684821605683, 71.15347158813476, 1.0579489572048186], time: 172.925
Running avgs for agent 0: q_loss: 12.761427879333496, p_loss: -3.787461996078491, mean_rew: -7.361886500076143, variance: 3.9942736625671387, lamda: 1.8882734775543213
Running avgs for agent 1: q_loss: 2928017.25, p_loss: 157.9313507080078, mean_rew: -7.366569881300322, variance: 284.61388635253905, lamda: 2.0603134632110596
Running avgs for agent 2: q_loss: 15.335613250732422, p_loss: -3.7800586223602295, mean_rew: -7.363201375694871, variance: 4.231795787811279, lamda: 1.7998874187469482

steps: 1124975, episodes: 45000, mean episode reward: -493.2493064642217, agent episode reward: [-164.41643548807392, -164.41643548807392, -164.41643548807392], time: 169.399
steps: 1124975, episodes: 45000, mean episode variance: 71.66074240148068, agent episode variance: [0.9773840548992156, 69.66498551940919, 1.0183728271722794], time: 169.4
Running avgs for agent 0: q_loss: 15.893855094909668, p_loss: -3.7457334995269775, mean_rew: -7.27637738358173, variance: 3.909536361694336, lamda: 1.9022043943405151
Running avgs for agent 1: q_loss: 2662066.25, p_loss: 156.3687744140625, mean_rew: -7.283540920828378, variance: 278.65994207763674, lamda: 2.085287570953369
Running avgs for agent 2: q_loss: 14.197005271911621, p_loss: -3.7485969066619873, mean_rew: -7.286407803970893, variance: 4.07349157333374, lamda: 1.8080227375030518

steps: 1149975, episodes: 46000, mean episode reward: -497.28575917372746, agent episode reward: [-165.76191972457582, -165.76191972457582, -165.76191972457582], time: 170.904
steps: 1149975, episodes: 46000, mean episode variance: 70.57524600076675, agent episode variance: [0.9623029766082764, 68.5953275756836, 1.017615448474884], time: 170.905
Running avgs for agent 0: q_loss: 13.190332412719727, p_loss: -3.7174198627471924, mean_rew: -7.2297963307446915, variance: 3.8492119312286377, lamda: 1.9080067873001099
Running avgs for agent 1: q_loss: 2479128.5, p_loss: 154.90225219726562, mean_rew: -7.234351025742406, variance: 274.3813103027344, lamda: 2.110262155532837
Running avgs for agent 2: q_loss: 13.185271263122559, p_loss: -3.7111973762512207, mean_rew: -7.231240415156676, variance: 4.070461750030518, lamda: 1.8134140968322754

steps: 1174975, episodes: 47000, mean episode reward: -493.5337173447977, agent episode reward: [-164.51123911493255, -164.51123911493255, -164.51123911493255], time: 168.439
steps: 1174975, episodes: 47000, mean episode variance: 64.47764542865754, agent episode variance: [0.953107736825943, 62.52464561462402, 0.9998920772075653], time: 168.44
Running avgs for agent 0: q_loss: 13.349847793579102, p_loss: -3.6799066066741943, mean_rew: -7.161281263356417, variance: 3.8124310970306396, lamda: 1.9150311946868896
Running avgs for agent 1: q_loss: 2319021.75, p_loss: 153.92259216308594, mean_rew: -7.173804562597233, variance: 250.09858245849608, lamda: 2.1352362632751465
Running avgs for agent 2: q_loss: 12.169482231140137, p_loss: -3.6883819103240967, mean_rew: -7.1602211872058374, variance: 3.999568462371826, lamda: 1.8174023628234863

steps: 1199975, episodes: 48000, mean episode reward: -504.81986235188293, agent episode reward: [-168.27328745062763, -168.27328745062763, -168.27328745062763], time: 170.937
steps: 1199975, episodes: 48000, mean episode variance: 64.59029753112793, agent episode variance: [0.9465878820419311, 62.64640231323242, 0.9973073358535767], time: 170.937
Running avgs for agent 0: q_loss: 11.538601875305176, p_loss: -3.649099588394165, mean_rew: -7.0993086028249985, variance: 3.7863516807556152, lamda: 1.9165124893188477
Running avgs for agent 1: q_loss: 2144873.25, p_loss: 152.31556701660156, mean_rew: -7.090525042696795, variance: 250.5856092529297, lamda: 2.1602108478546143
Running avgs for agent 2: q_loss: 10.046656608581543, p_loss: -3.6465249061584473, mean_rew: -7.0949879807660325, variance: 3.989229440689087, lamda: 1.8211127519607544

steps: 1224975, episodes: 49000, mean episode reward: -505.7680383118766, agent episode reward: [-168.58934610395886, -168.58934610395886, -168.58934610395886], time: 170.999
steps: 1224975, episodes: 49000, mean episode variance: 62.464219357013704, agent episode variance: [0.9304728569984436, 60.54743392944336, 0.9863125705718994], time: 171.0
Running avgs for agent 0: q_loss: 11.703557968139648, p_loss: -3.6245486736297607, mean_rew: -7.0449159506661125, variance: 3.721891403198242, lamda: 1.9230467081069946
Running avgs for agent 1: q_loss: 1998557.75, p_loss: 150.72384643554688, mean_rew: -7.045276447622432, variance: 242.18973571777343, lamda: 2.185184955596924
Running avgs for agent 2: q_loss: 9.682513236999512, p_loss: -3.621333599090576, mean_rew: -7.038459832078598, variance: 3.9452502727508545, lamda: 1.8342148065567017

steps: 1249975, episodes: 50000, mean episode reward: -515.6291033794831, agent episode reward: [-171.87636779316102, -171.87636779316102, -171.87636779316102], time: 171.77
steps: 1249975, episodes: 50000, mean episode variance: 58.17405542159081, agent episode variance: [0.9263596863746643, 56.27649154663086, 0.9712041885852813], time: 171.77
Running avgs for agent 0: q_loss: 7.691117286682129, p_loss: -3.5882229804992676, mean_rew: -6.979671499924647, variance: 3.7054383754730225, lamda: 1.930004596710205
Running avgs for agent 1: q_loss: 1814262.75, p_loss: 148.69488525390625, mean_rew: -6.973088265115355, variance: 225.10596618652343, lamda: 2.2101597785949707
Running avgs for agent 2: q_loss: 9.096500396728516, p_loss: -3.5961828231811523, mean_rew: -6.9790695615433975, variance: 3.8848166465759277, lamda: 1.8386696577072144

steps: 1274975, episodes: 51000, mean episode reward: -525.8480744560807, agent episode reward: [-175.2826914853602, -175.2826914853602, -175.2826914853602], time: 173.035
steps: 1274975, episodes: 51000, mean episode variance: 54.232469439744946, agent episode variance: [0.912489289522171, 52.35927958679199, 0.9607005634307861], time: 173.036
Running avgs for agent 0: q_loss: 8.466887474060059, p_loss: -3.558628559112549, mean_rew: -6.912282179659878, variance: 3.6499569416046143, lamda: 1.9394543170928955
Running avgs for agent 1: q_loss: 1610922.375, p_loss: 145.9512176513672, mean_rew: -6.908721760941455, variance: 209.43711834716797, lamda: 2.2351338863372803
Running avgs for agent 2: q_loss: 8.155917167663574, p_loss: -3.5688395500183105, mean_rew: -6.912659192580993, variance: 3.8428022861480713, lamda: 1.8389729261398315

steps: 1299975, episodes: 52000, mean episode reward: -529.3471295212164, agent episode reward: [-176.4490431737388, -176.4490431737388, -176.4490431737388], time: 175.573
steps: 1299975, episodes: 52000, mean episode variance: 53.15870116853714, agent episode variance: [0.8961991229057312, 51.31178224182129, 0.9507198038101197], time: 175.574
Running avgs for agent 0: q_loss: 8.57480525970459, p_loss: -3.5414209365844727, mean_rew: -6.872863277333003, variance: 3.58479642868042, lamda: 1.9399651288986206
Running avgs for agent 1: q_loss: 1548304.125, p_loss: 143.75650024414062, mean_rew: -6.866543353088777, variance: 205.24712896728516, lamda: 2.260108232498169
Running avgs for agent 2: q_loss: 7.733712673187256, p_loss: -3.5548195838928223, mean_rew: -6.8704773473906275, variance: 3.8028793334960938, lamda: 1.839228630065918

steps: 1324975, episodes: 53000, mean episode reward: -530.8200408222482, agent episode reward: [-176.94001360741606, -176.94001360741606, -176.94001360741606], time: 168.699
steps: 1324975, episodes: 53000, mean episode variance: 49.95326416826248, agent episode variance: [0.8946648030281067, 48.10943342590332, 0.9491659393310546], time: 168.7
Running avgs for agent 0: q_loss: 8.118603706359863, p_loss: -3.5229787826538086, mean_rew: -6.839945154059334, variance: 3.5786592960357666, lamda: 1.940133810043335
Running avgs for agent 1: q_loss: 1489069.25, p_loss: 142.26409912109375, mean_rew: -6.848705792331684, variance: 192.43773370361328, lamda: 2.2850825786590576
Running avgs for agent 2: q_loss: 7.070186614990234, p_loss: -3.5400710105895996, mean_rew: -6.844975135846212, variance: 3.796663522720337, lamda: 1.8402479887008667

steps: 1349975, episodes: 54000, mean episode reward: -537.974154793171, agent episode reward: [-179.3247182643903, -179.3247182643903, -179.3247182643903], time: 172.111
steps: 1349975, episodes: 54000, mean episode variance: 48.588319489717485, agent episode variance: [0.897484475851059, 46.7516563873291, 0.939178626537323], time: 172.112
Running avgs for agent 0: q_loss: 7.767831802368164, p_loss: -3.5056586265563965, mean_rew: -6.815499793859935, variance: 3.589937925338745, lamda: 1.9404746294021606
Running avgs for agent 1: q_loss: 1390508.625, p_loss: 141.02842712402344, mean_rew: -6.818295542251739, variance: 187.0066255493164, lamda: 2.3100569248199463
Running avgs for agent 2: q_loss: 6.699564456939697, p_loss: -3.5215444564819336, mean_rew: -6.8131486197812015, variance: 3.756714344024658, lamda: 1.8404943943023682

steps: 1374975, episodes: 55000, mean episode reward: -544.6325492858833, agent episode reward: [-181.54418309529444, -181.54418309529444, -181.54418309529444], time: 171.536
steps: 1374975, episodes: 55000, mean episode variance: 47.06980376076698, agent episode variance: [0.8886018989086151, 45.24032783508301, 0.9408740267753601], time: 171.536
Running avgs for agent 0: q_loss: 7.505967617034912, p_loss: -3.5054080486297607, mean_rew: -6.807217699901702, variance: 3.5544073581695557, lamda: 1.9407851696014404
Running avgs for agent 1: q_loss: 1326523.0, p_loss: 140.17938232421875, mean_rew: -6.802291698496772, variance: 180.96131134033203, lamda: 2.335031270980835
Running avgs for agent 2: q_loss: 6.586830139160156, p_loss: -3.521765947341919, mean_rew: -6.810434454043758, variance: 3.763496160507202, lamda: 1.8406258821487427

steps: 1399975, episodes: 56000, mean episode reward: -538.635909211288, agent episode reward: [-179.5453030704293, -179.5453030704293, -179.5453030704293], time: 173.272
steps: 1399975, episodes: 56000, mean episode variance: 46.377387061357496, agent episode variance: [0.8868627517223359, 44.55570866394043, 0.9348156456947326], time: 173.273
Running avgs for agent 0: q_loss: 7.186048984527588, p_loss: -3.4970693588256836, mean_rew: -6.795352942089597, variance: 3.5474510192871094, lamda: 1.9409801959991455
Running avgs for agent 1: q_loss: 1279450.25, p_loss: 139.27294921875, mean_rew: -6.799756395861268, variance: 178.22283465576172, lamda: 2.3600053787231445
Running avgs for agent 2: q_loss: 6.362982749938965, p_loss: -3.5163440704345703, mean_rew: -6.798825364432497, variance: 3.739262580871582, lamda: 1.8408327102661133

steps: 1424975, episodes: 57000, mean episode reward: -543.7107429770285, agent episode reward: [-181.23691432567617, -181.23691432567617, -181.23691432567617], time: 163.642
steps: 1424975, episodes: 57000, mean episode variance: 42.696416943073274, agent episode variance: [0.8863488960266114, 40.876833084106444, 0.933234962940216], time: 163.642
Running avgs for agent 0: q_loss: 6.953256130218506, p_loss: -3.4904003143310547, mean_rew: -6.786763299775175, variance: 3.545395612716675, lamda: 1.9412049055099487
Running avgs for agent 1: q_loss: 1238072.625, p_loss: 138.42987060546875, mean_rew: -6.790818235097002, variance: 163.50733233642578, lamda: 2.3849799633026123
Running avgs for agent 2: q_loss: 6.254019737243652, p_loss: -3.511676073074341, mean_rew: -6.793261267191723, variance: 3.7329399585723877, lamda: 1.8408328294754028

steps: 1449975, episodes: 58000, mean episode reward: -541.2213539094639, agent episode reward: [-180.40711796982131, -180.40711796982131, -180.40711796982131], time: 165.807
steps: 1449975, episodes: 58000, mean episode variance: 44.12750608587265, agent episode variance: [0.8905805339813232, 42.29575581359863, 0.9411697382926941], time: 165.808
Running avgs for agent 0: q_loss: 6.839509010314941, p_loss: -3.491326093673706, mean_rew: -6.782816889542631, variance: 3.5623221397399902, lamda: 1.941249966621399
Running avgs for agent 1: q_loss: 1195620.5, p_loss: 137.73715209960938, mean_rew: -6.788676698757437, variance: 169.18302325439453, lamda: 2.409954071044922
Running avgs for agent 2: q_loss: 6.1050310134887695, p_loss: -3.4952480792999268, mean_rew: -6.777204007361787, variance: 3.764678955078125, lamda: 1.8408459424972534

steps: 1474975, episodes: 59000, mean episode reward: -545.3233059900567, agent episode reward: [-181.7744353300189, -181.7744353300189, -181.7744353300189], time: 178.876
steps: 1474975, episodes: 59000, mean episode variance: 43.278465456962586, agent episode variance: [0.8883309137821197, 41.457230575561525, 0.9329039676189422], time: 178.876
Running avgs for agent 0: q_loss: 6.6411356925964355, p_loss: -3.4931106567382812, mean_rew: -6.785862082575734, variance: 3.553323745727539, lamda: 1.9412559270858765
Running avgs for agent 1: q_loss: 1125156.25, p_loss: 137.21678161621094, mean_rew: -6.7846581011422735, variance: 165.8289223022461, lamda: 2.4349286556243896
Running avgs for agent 2: q_loss: 5.89457368850708, p_loss: -3.5019266605377197, mean_rew: -6.784672817116631, variance: 3.7316157817840576, lamda: 1.8408465385437012

steps: 1499975, episodes: 60000, mean episode reward: -550.6048560038537, agent episode reward: [-183.53495200128458, -183.53495200128458, -183.53495200128458], time: 176.77
steps: 1499975, episodes: 60000, mean episode variance: 41.918616172075275, agent episode variance: [0.8864376013278961, 40.09587271118164, 0.9363058595657349], time: 176.771
Running avgs for agent 0: q_loss: 6.701837062835693, p_loss: -3.4921679496765137, mean_rew: -6.7803694636488325, variance: 3.545750379562378, lamda: 1.941256046295166
Running avgs for agent 1: q_loss: 1099365.75, p_loss: 137.24783325195312, mean_rew: -6.785057142263772, variance: 160.38349084472657, lamda: 2.459902763366699
Running avgs for agent 2: q_loss: 5.859788417816162, p_loss: -3.5008983612060547, mean_rew: -6.782236510069812, variance: 3.7452235221862793, lamda: 1.8408466577529907

steps: 1524975, episodes: 61000, mean episode reward: -540.7858512197176, agent episode reward: [-180.26195040657254, -180.26195040657254, -180.26195040657254], time: 177.886
steps: 1524975, episodes: 61000, mean episode variance: 44.214764618873595, agent episode variance: [0.8913284325599671, 42.38220869445801, 0.9412274918556214], time: 177.887
Running avgs for agent 0: q_loss: 6.658428192138672, p_loss: -3.4990427494049072, mean_rew: -6.791102967224904, variance: 3.5653140544891357, lamda: 1.9412797689437866
Running avgs for agent 1: q_loss: 1074056.25, p_loss: 137.0572967529297, mean_rew: -6.793827212269889, variance: 169.52883477783203, lamda: 2.484877109527588
Running avgs for agent 2: q_loss: 5.767286777496338, p_loss: -3.503554105758667, mean_rew: -6.790852803369229, variance: 3.7649099826812744, lamda: 1.8408464193344116

steps: 1549975, episodes: 62000, mean episode reward: -548.9985751601828, agent episode reward: [-182.99952505339425, -182.99952505339425, -182.99952505339425], time: 170.196
steps: 1549975, episodes: 62000, mean episode variance: 42.232658662557604, agent episode variance: [0.8867471575736999, 40.407935028076174, 0.9379764769077301], time: 170.197
Running avgs for agent 0: q_loss: 6.701165676116943, p_loss: -3.5031850337982178, mean_rew: -6.792019413622658, variance: 3.5469887256622314, lamda: 1.9413111209869385
Running avgs for agent 1: q_loss: 1040337.5625, p_loss: 137.0513916015625, mean_rew: -6.803966805135843, variance: 161.6317401123047, lamda: 2.5098516941070557
Running avgs for agent 2: q_loss: 5.6409454345703125, p_loss: -3.5096383094787598, mean_rew: -6.799151180657742, variance: 3.751905918121338, lamda: 1.8408465385437012

steps: 1574975, episodes: 63000, mean episode reward: -553.8529012979043, agent episode reward: [-184.61763376596812, -184.61763376596812, -184.61763376596812], time: 167.429
steps: 1574975, episodes: 63000, mean episode variance: 42.14019680404663, agent episode variance: [0.8934786098003388, 40.305380294799804, 0.9413378994464874], time: 167.429
Running avgs for agent 0: q_loss: 6.747514247894287, p_loss: -3.512326717376709, mean_rew: -6.814386902438551, variance: 3.5739145278930664, lamda: 1.9414626359939575
Running avgs for agent 1: q_loss: 1026476.8125, p_loss: 136.8901824951172, mean_rew: -6.808497991961686, variance: 161.22152117919921, lamda: 2.5348260402679443
Running avgs for agent 2: q_loss: 5.672282695770264, p_loss: -3.5078678131103516, mean_rew: -6.80948352626548, variance: 3.7653515338897705, lamda: 1.8408466577529907

steps: 1599975, episodes: 64000, mean episode reward: -562.9605945535008, agent episode reward: [-187.65353151783358, -187.65353151783358, -187.65353151783358], time: 176.505
steps: 1599975, episodes: 64000, mean episode variance: 41.65249897193909, agent episode variance: [0.8983494076728821, 39.810715423583986, 0.9434341406822204], time: 176.505
Running avgs for agent 0: q_loss: 6.791992664337158, p_loss: -3.5124945640563965, mean_rew: -6.822202154914238, variance: 3.593397378921509, lamda: 1.941523790359497
Running avgs for agent 1: q_loss: 1030088.5625, p_loss: 137.00466918945312, mean_rew: -6.8199695065543615, variance: 159.24286169433594, lamda: 2.559800386428833
Running avgs for agent 2: q_loss: 5.672220230102539, p_loss: -3.518730401992798, mean_rew: -6.82687311176414, variance: 3.7737364768981934, lamda: 1.8408464193344116

steps: 1624975, episodes: 65000, mean episode reward: -568.9310239406295, agent episode reward: [-189.6436746468765, -189.6436746468765, -189.6436746468765], time: 180.902
steps: 1624975, episodes: 65000, mean episode variance: 41.920836654663084, agent episode variance: [0.8963291046619415, 40.077358169555666, 0.9471493804454804], time: 180.903
Running avgs for agent 0: q_loss: 6.699230194091797, p_loss: -3.5252232551574707, mean_rew: -6.8483105804083895, variance: 3.5853164196014404, lamda: 1.941610336303711
Running avgs for agent 1: q_loss: 987220.4375, p_loss: 137.0880126953125, mean_rew: -6.846987611517423, variance: 160.30943267822266, lamda: 2.5847747325897217
Running avgs for agent 2: q_loss: 5.752309799194336, p_loss: -3.522862434387207, mean_rew: -6.841890579607702, variance: 3.788597345352173, lamda: 1.8408465385437012

steps: 1649975, episodes: 66000, mean episode reward: -573.7155603895351, agent episode reward: [-191.238520129845, -191.238520129845, -191.238520129845], time: 180.421
steps: 1649975, episodes: 66000, mean episode variance: 41.31498944711685, agent episode variance: [0.9045823905467987, 39.460373237609865, 0.9500338189601898], time: 180.422
Running avgs for agent 0: q_loss: 6.682494163513184, p_loss: -3.5461196899414062, mean_rew: -6.875327280027347, variance: 3.6183295249938965, lamda: 1.9416817426681519
Running avgs for agent 1: q_loss: 993360.5, p_loss: 136.88877868652344, mean_rew: -6.8705320553673435, variance: 157.84149295043946, lamda: 2.6097490787506104
Running avgs for agent 2: q_loss: 5.799347877502441, p_loss: -3.5350797176361084, mean_rew: -6.870443507858145, variance: 3.8001351356506348, lamda: 1.8408466577529907

steps: 1674975, episodes: 67000, mean episode reward: -571.7080235463058, agent episode reward: [-190.5693411821019, -190.5693411821019, -190.5693411821019], time: 180.63
steps: 1674975, episodes: 67000, mean episode variance: 40.444965520858766, agent episode variance: [0.9046060471534729, 38.582999160766605, 0.9573603129386902], time: 180.631
Running avgs for agent 0: q_loss: 6.64979887008667, p_loss: -3.5469136238098145, mean_rew: -6.895504739755772, variance: 3.618424415588379, lamda: 1.9416816234588623
Running avgs for agent 1: q_loss: 959351.1875, p_loss: 136.57020568847656, mean_rew: -6.895989775601864, variance: 154.33199664306642, lamda: 2.63472318649292
Running avgs for agent 2: q_loss: 5.7416486740112305, p_loss: -3.549969434738159, mean_rew: -6.892258120772451, variance: 3.8294408321380615, lamda: 1.8408464193344116

steps: 1699975, episodes: 68000, mean episode reward: -579.6163129969667, agent episode reward: [-193.20543766565558, -193.20543766565558, -193.20543766565558], time: 166.047
steps: 1699975, episodes: 68000, mean episode variance: 39.4917579369545, agent episode variance: [0.906389543056488, 37.62764079284668, 0.9577276010513306], time: 166.048
Running avgs for agent 0: q_loss: 6.592957973480225, p_loss: -3.568880081176758, mean_rew: -6.924733748904005, variance: 3.6255581378936768, lamda: 1.9416816234588623
Running avgs for agent 1: q_loss: 937109.4375, p_loss: 136.21820068359375, mean_rew: -6.919237693529411, variance: 150.5105631713867, lamda: 2.6596977710723877
Running avgs for agent 2: q_loss: 5.776029109954834, p_loss: -3.5584423542022705, mean_rew: -6.923939027204525, variance: 3.8309104442596436, lamda: 1.8408465385437012

steps: 1724975, episodes: 69000, mean episode reward: -576.6983324106636, agent episode reward: [-192.23277747022118, -192.23277747022118, -192.23277747022118], time: 166.462
steps: 1724975, episodes: 69000, mean episode variance: 39.98292983341217, agent episode variance: [0.9111273565292358, 38.112089233398436, 0.959713243484497], time: 166.462
Running avgs for agent 0: q_loss: 6.458475112915039, p_loss: -3.579073905944824, mean_rew: -6.944242236426155, variance: 3.6445093154907227, lamda: 1.9416817426681519
Running avgs for agent 1: q_loss: 931659.625, p_loss: 136.0204315185547, mean_rew: -6.9394067917058315, variance: 152.44835693359374, lamda: 2.6846718788146973
Running avgs for agent 2: q_loss: 5.9706339836120605, p_loss: -3.5804951190948486, mean_rew: -6.949821389263347, variance: 3.838853120803833, lamda: 1.8408466577529907

steps: 1749975, episodes: 70000, mean episode reward: -578.8643127647293, agent episode reward: [-192.95477092157645, -192.95477092157645, -192.95477092157645], time: 164.523
steps: 1749975, episodes: 70000, mean episode variance: 39.486097383737565, agent episode variance: [0.9175154547691345, 37.60358813476562, 0.9649937942028045], time: 164.524
Running avgs for agent 0: q_loss: 6.304177761077881, p_loss: -3.5824222564697266, mean_rew: -6.970091340030692, variance: 3.6700618267059326, lamda: 1.942038893699646
Running avgs for agent 1: q_loss: 932026.125, p_loss: 135.84304809570312, mean_rew: -6.9621122950800345, variance: 150.4143525390625, lamda: 2.709646463394165
Running avgs for agent 2: q_loss: 5.8636064529418945, p_loss: -3.579026937484741, mean_rew: -6.96949085950062, variance: 3.8599753379821777, lamda: 1.8408464193344116

steps: 1774975, episodes: 71000, mean episode reward: -580.8742147106533, agent episode reward: [-193.6247382368844, -193.6247382368844, -193.6247382368844], time: 166.59
steps: 1774975, episodes: 71000, mean episode variance: 37.82213834357262, agent episode variance: [0.9195070641040802, 35.93007818603515, 0.9725530934333801], time: 166.59
Running avgs for agent 0: q_loss: 4.680870056152344, p_loss: -3.595659017562866, mean_rew: -6.9902763575488525, variance: 3.6780283451080322, lamda: 1.9458518028259277
Running avgs for agent 1: q_loss: 916860.4375, p_loss: 135.80599975585938, mean_rew: -6.99380334123935, variance: 143.7203127441406, lamda: 2.7346205711364746
Running avgs for agent 2: q_loss: 6.166383743286133, p_loss: -3.5956006050109863, mean_rew: -7.000818570425775, variance: 3.890212297439575, lamda: 1.8409420251846313

steps: 1799975, episodes: 72000, mean episode reward: -583.3577347698852, agent episode reward: [-194.45257825662836, -194.45257825662836, -194.45257825662836], time: 169.077
steps: 1799975, episodes: 72000, mean episode variance: 37.478891037464145, agent episode variance: [0.9209885613918305, 35.58757647705078, 0.9703259990215302], time: 169.078
Running avgs for agent 0: q_loss: 6.457601547241211, p_loss: -3.61468243598938, mean_rew: -7.025937888813139, variance: 3.6839540004730225, lamda: 1.9467251300811768
Running avgs for agent 1: q_loss: 912498.4375, p_loss: 135.84286499023438, mean_rew: -7.0213552750289, variance: 142.35030590820313, lamda: 2.7595949172973633
Running avgs for agent 2: q_loss: 6.135849475860596, p_loss: -3.6081295013427734, mean_rew: -7.0231187553063625, variance: 3.8813040256500244, lamda: 1.841047763824463

steps: 1824975, episodes: 73000, mean episode reward: -583.5626016281859, agent episode reward: [-194.52086720939533, -194.52086720939533, -194.52086720939533], time: 182.117
steps: 1824975, episodes: 73000, mean episode variance: 38.15526075172424, agent episode variance: [0.9282075064182281, 36.25410383605957, 0.9729494092464447], time: 182.118
Running avgs for agent 0: q_loss: 6.440587043762207, p_loss: -3.628674268722534, mean_rew: -7.053802309910992, variance: 3.712830066680908, lamda: 1.9467302560806274
Running avgs for agent 1: q_loss: 920886.375, p_loss: 135.97866821289062, mean_rew: -7.056928608153015, variance: 145.01641534423828, lamda: 2.784569501876831
Running avgs for agent 2: q_loss: 6.252941608428955, p_loss: -3.6294443607330322, mean_rew: -7.058224676488335, variance: 3.8917975425720215, lamda: 1.8410476446151733

steps: 1849975, episodes: 74000, mean episode reward: -583.4069826441091, agent episode reward: [-194.46899421470306, -194.46899421470306, -194.46899421470306], time: 179.706
steps: 1849975, episodes: 74000, mean episode variance: 37.15797977972031, agent episode variance: [0.9348315181732177, 35.243914001464844, 0.9792342600822449], time: 179.706
Running avgs for agent 0: q_loss: 6.45693826675415, p_loss: -3.6430797576904297, mean_rew: -7.084422023284154, variance: 3.739326000213623, lamda: 1.9467300176620483
Running avgs for agent 1: q_loss: 909260.5625, p_loss: 136.1915740966797, mean_rew: -7.082604675634099, variance: 140.97565600585938, lamda: 2.8095436096191406
Running avgs for agent 2: q_loss: 6.248226165771484, p_loss: -3.63901948928833, mean_rew: -7.089412820203366, variance: 3.9169371128082275, lamda: 1.8410476446151733

steps: 1874975, episodes: 75000, mean episode reward: -590.392554775004, agent episode reward: [-196.7975182583347, -196.7975182583347, -196.7975182583347], time: 176.227
steps: 1874975, episodes: 75000, mean episode variance: 37.31570211482048, agent episode variance: [0.931700704574585, 35.39760243225098, 0.9863989779949188], time: 176.228
Running avgs for agent 0: q_loss: 6.668405532836914, p_loss: -3.6725754737854004, mean_rew: -7.121348497971063, variance: 3.7268028259277344, lamda: 1.946730375289917
Running avgs for agent 1: q_loss: 903231.5, p_loss: 136.3341827392578, mean_rew: -7.122296271493848, variance: 141.5904097290039, lamda: 2.8345179557800293
Running avgs for agent 2: q_loss: 6.127323150634766, p_loss: -3.652867078781128, mean_rew: -7.117617092483508, variance: 3.9455959796905518, lamda: 1.841047763824463

steps: 1899975, episodes: 76000, mean episode reward: -584.9340655259552, agent episode reward: [-194.9780218419851, -194.9780218419851, -194.9780218419851], time: 175.314
steps: 1899975, episodes: 76000, mean episode variance: 37.58532902050018, agent episode variance: [0.9357259166240692, 35.65810403442383, 0.9914990694522857], time: 175.314
Running avgs for agent 0: q_loss: 6.494021415710449, p_loss: -3.6785354614257812, mean_rew: -7.148785787870032, variance: 3.742903470993042, lamda: 1.9467302560806274
Running avgs for agent 1: q_loss: 919922.3125, p_loss: 136.54412841796875, mean_rew: -7.147807385727782, variance: 142.63241613769532, lamda: 2.859492540359497
Running avgs for agent 2: q_loss: 6.371097564697266, p_loss: -3.6717305183410645, mean_rew: -7.146187394398966, variance: 3.965996265411377, lamda: 1.8412622213363647

steps: 1924975, episodes: 77000, mean episode reward: -582.6885879640893, agent episode reward: [-194.2295293213631, -194.2295293213631, -194.2295293213631], time: 176.914
steps: 1924975, episodes: 77000, mean episode variance: 36.855142043352124, agent episode variance: [0.945223174571991, 34.918300369262695, 0.9916184995174407], time: 176.915
Running avgs for agent 0: q_loss: 6.4879841804504395, p_loss: -3.697742462158203, mean_rew: -7.182484506693969, variance: 3.7808926105499268, lamda: 1.9467302560806274
Running avgs for agent 1: q_loss: 924782.0625, p_loss: 136.8667449951172, mean_rew: -7.1766166865015855, variance: 139.67320147705078, lamda: 2.8844668865203857
Running avgs for agent 2: q_loss: 6.189523696899414, p_loss: -3.6796934604644775, mean_rew: -7.183260738117985, variance: 3.9664738178253174, lamda: 1.841448187828064

steps: 1949975, episodes: 78000, mean episode reward: -587.3866922962882, agent episode reward: [-195.79556409876275, -195.79556409876275, -195.79556409876275], time: 176.11
steps: 1949975, episodes: 78000, mean episode variance: 36.92265412926674, agent episode variance: [0.9446698520183563, 34.974772087097165, 1.0032121901512145], time: 176.111
Running avgs for agent 0: q_loss: 6.565872669219971, p_loss: -3.711703062057495, mean_rew: -7.211033185469465, variance: 3.778679370880127, lamda: 1.946730375289917
Running avgs for agent 1: q_loss: 930552.0, p_loss: 137.00384521484375, mean_rew: -7.21487153298188, variance: 139.89908834838866, lamda: 2.9094414710998535
Running avgs for agent 2: q_loss: 6.284510612487793, p_loss: -3.7065536975860596, mean_rew: -7.220160429510659, variance: 4.012848854064941, lamda: 1.8414483070373535

steps: 1974975, episodes: 79000, mean episode reward: -589.8781818891549, agent episode reward: [-196.62606062971835, -196.62606062971835, -196.62606062971835], time: 178.422
steps: 1974975, episodes: 79000, mean episode variance: 37.41754762363434, agent episode variance: [0.950400139093399, 35.4624694519043, 1.0046780326366425], time: 178.422
Running avgs for agent 0: q_loss: 6.53795051574707, p_loss: -3.7355360984802246, mean_rew: -7.246798883277841, variance: 3.801600694656372, lamda: 1.9467302560806274
Running avgs for agent 1: q_loss: 941472.0, p_loss: 137.150390625, mean_rew: -7.245593198190658, variance: 141.8498778076172, lamda: 2.934415578842163
Running avgs for agent 2: q_loss: 6.334934234619141, p_loss: -3.7250797748565674, mean_rew: -7.255230098535108, variance: 4.018712043762207, lamda: 1.841448187828064

steps: 1999975, episodes: 80000, mean episode reward: -584.1158906234333, agent episode reward: [-194.70529687447777, -194.70529687447777, -194.70529687447777], time: 176.305
steps: 1999975, episodes: 80000, mean episode variance: 37.04044297337532, agent episode variance: [0.9575430147647858, 35.07686050415039, 1.006039454460144], time: 176.306
Running avgs for agent 0: q_loss: 6.531224250793457, p_loss: -3.743297815322876, mean_rew: -7.276760207331174, variance: 3.830171823501587, lamda: 1.9467302560806274
Running avgs for agent 1: q_loss: 941014.375, p_loss: 137.46409606933594, mean_rew: -7.284570636517594, variance: 140.30744201660156, lamda: 2.9593899250030518
Running avgs for agent 2: q_loss: 6.225350379943848, p_loss: -3.733750820159912, mean_rew: -7.275368067183723, variance: 4.024157524108887, lamda: 1.841448187828064

steps: 2024975, episodes: 81000, mean episode reward: -582.9199875596404, agent episode reward: [-194.30666251988012, -194.30666251988012, -194.30666251988012], time: 174.65
steps: 2024975, episodes: 81000, mean episode variance: 37.122639779806136, agent episode variance: [0.9573576037883759, 35.149478897094724, 1.0158032789230347], time: 174.65
Running avgs for agent 0: q_loss: 6.734168529510498, p_loss: -3.767805576324463, mean_rew: -7.315098249146024, variance: 3.829430341720581, lamda: 1.946730375289917
Running avgs for agent 1: q_loss: 957401.625, p_loss: 137.8925323486328, mean_rew: -7.311514275192361, variance: 140.5979155883789, lamda: 2.9843642711639404
Running avgs for agent 2: q_loss: 6.261626720428467, p_loss: -3.7475571632385254, mean_rew: -7.309042556617874, variance: 4.063213348388672, lamda: 1.8414483070373535

steps: 2049975, episodes: 82000, mean episode reward: -596.8627708219216, agent episode reward: [-198.9542569406405, -198.9542569406405, -198.9542569406405], time: 172.455
steps: 2049975, episodes: 82000, mean episode variance: 37.03375882673264, agent episode variance: [0.9645209259986878, 35.04556414794922, 1.023673752784729], time: 172.456
Running avgs for agent 0: q_loss: 6.607937335968018, p_loss: -3.784609317779541, mean_rew: -7.345052836443958, variance: 3.858083724975586, lamda: 1.9467861652374268
Running avgs for agent 1: q_loss: 970355.5, p_loss: 138.2440948486328, mean_rew: -7.349101130257633, variance: 140.1822565917969, lamda: 3.009338617324829
Running avgs for agent 2: q_loss: 6.3189802169799805, p_loss: -3.769526481628418, mean_rew: -7.351316220442286, variance: 4.094695091247559, lamda: 1.841448187828064

steps: 2074975, episodes: 83000, mean episode reward: -588.0492441998979, agent episode reward: [-196.01641473329929, -196.01641473329929, -196.01641473329929], time: 175.882
steps: 2074975, episodes: 83000, mean episode variance: 37.11702581882477, agent episode variance: [0.9697856338024139, 35.12559936523437, 1.0216408197879792], time: 175.883
Running avgs for agent 0: q_loss: 6.662988662719727, p_loss: -3.798732042312622, mean_rew: -7.37716350865445, variance: 3.8791425228118896, lamda: 1.946881890296936
Running avgs for agent 1: q_loss: 980342.4375, p_loss: 138.53395080566406, mean_rew: -7.380760250455658, variance: 140.5023974609375, lamda: 3.0343129634857178
Running avgs for agent 2: q_loss: 6.381646633148193, p_loss: -3.7847177982330322, mean_rew: -7.374069984711489, variance: 4.0865631103515625, lamda: 1.841448187828064

steps: 2099975, episodes: 84000, mean episode reward: -591.9244809320853, agent episode reward: [-197.30816031069511, -197.30816031069511, -197.30816031069511], time: 173.987
steps: 2099975, episodes: 84000, mean episode variance: 37.11221790313721, agent episode variance: [0.9704197759628296, 35.110899032592776, 1.0308990945816041], time: 173.987
Running avgs for agent 0: q_loss: 6.745543956756592, p_loss: -3.816640853881836, mean_rew: -7.408467570266844, variance: 3.8816792964935303, lamda: 1.9468821287155151
Running avgs for agent 1: q_loss: 996797.875, p_loss: 138.85903930664062, mean_rew: -7.4101283205790045, variance: 140.4435961303711, lamda: 3.0592873096466064
Running avgs for agent 2: q_loss: 6.358483791351318, p_loss: -3.8037827014923096, mean_rew: -7.415524783137122, variance: 4.12359619140625, lamda: 1.8414483070373535

steps: 2124975, episodes: 85000, mean episode reward: -590.3415394821438, agent episode reward: [-196.78051316071458, -196.78051316071458, -196.78051316071458], time: 175.754
steps: 2124975, episodes: 85000, mean episode variance: 37.938422486305235, agent episode variance: [0.9794021141529083, 35.92329122924804, 1.0357291429042816], time: 175.755
Running avgs for agent 0: q_loss: 6.774110794067383, p_loss: -3.829638957977295, mean_rew: -7.441840733155695, variance: 3.9176084995269775, lamda: 1.9468820095062256
Running avgs for agent 1: q_loss: 1010566.625, p_loss: 139.1146697998047, mean_rew: -7.441180714300693, variance: 143.69316491699217, lamda: 3.084261655807495
Running avgs for agent 2: q_loss: 6.465588092803955, p_loss: -3.8171801567077637, mean_rew: -7.447848448519991, variance: 4.142916679382324, lamda: 1.841448187828064

steps: 2149975, episodes: 86000, mean episode reward: -596.1078373965695, agent episode reward: [-198.7026124655232, -198.7026124655232, -198.7026124655232], time: 175.241
steps: 2149975, episodes: 86000, mean episode variance: 36.889360835313795, agent episode variance: [0.9795724160671234, 34.87040979003906, 1.039378629207611], time: 175.241
Running avgs for agent 0: q_loss: 6.818439483642578, p_loss: -3.8494746685028076, mean_rew: -7.471632400460249, variance: 3.9182896614074707, lamda: 1.946881890296936
Running avgs for agent 1: q_loss: 1017874.5625, p_loss: 139.3855743408203, mean_rew: -7.475455074188299, variance: 139.48163916015625, lamda: 3.1092357635498047
Running avgs for agent 2: q_loss: 6.523031234741211, p_loss: -3.8305020332336426, mean_rew: -7.473112091897338, variance: 4.157514572143555, lamda: 1.841448187828064

steps: 2174975, episodes: 87000, mean episode reward: -588.1334084176561, agent episode reward: [-196.044469472552, -196.044469472552, -196.044469472552], time: 178.131
steps: 2174975, episodes: 87000, mean episode variance: 37.456443150281906, agent episode variance: [0.9857802107334137, 35.424510635375974, 1.0461523041725158], time: 178.132
Running avgs for agent 0: q_loss: 6.814119815826416, p_loss: -3.8638222217559814, mean_rew: -7.503020763446338, variance: 3.9431209564208984, lamda: 1.9468821287155151
Running avgs for agent 1: q_loss: 1042832.25, p_loss: 139.67697143554688, mean_rew: -7.507574763795462, variance: 141.6980425415039, lamda: 3.1342103481292725
Running avgs for agent 2: q_loss: 6.527693271636963, p_loss: -3.856870651245117, mean_rew: -7.5131432492185235, variance: 4.184609413146973, lamda: 1.8414483070373535

steps: 2199975, episodes: 88000, mean episode reward: -588.2598899604803, agent episode reward: [-196.0866299868268, -196.0866299868268, -196.0866299868268], time: 175.214
steps: 2199975, episodes: 88000, mean episode variance: 38.025343657016755, agent episode variance: [0.9947201890945434, 35.977419967651365, 1.0532035002708435], time: 175.214
Running avgs for agent 0: q_loss: 6.9791035652160645, p_loss: -3.8873984813690186, mean_rew: -7.544728941527031, variance: 3.9788804054260254, lamda: 1.9469172954559326
Running avgs for agent 1: q_loss: 1060707.875, p_loss: 139.90151977539062, mean_rew: -7.543984360601458, variance: 143.90967987060546, lamda: 3.159184455871582
Running avgs for agent 2: q_loss: 6.5668206214904785, p_loss: -3.8634800910949707, mean_rew: -7.534873001716814, variance: 4.212813854217529, lamda: 1.841448187828064

steps: 2224975, episodes: 89000, mean episode reward: -590.8706410993376, agent episode reward: [-196.95688036644586, -196.95688036644586, -196.95688036644586], time: 177.012
steps: 2224975, episodes: 89000, mean episode variance: 37.43269820594788, agent episode variance: [0.9987468214035035, 35.38142959594727, 1.052521788597107], time: 177.012
Running avgs for agent 0: q_loss: 6.83756160736084, p_loss: -3.896148681640625, mean_rew: -7.568039393373964, variance: 3.9949872493743896, lamda: 1.9469423294067383
Running avgs for agent 1: q_loss: 1072038.75, p_loss: 140.22206115722656, mean_rew: -7.573213656498391, variance: 141.52571838378907, lamda: 3.184159278869629
Running avgs for agent 2: q_loss: 6.627816200256348, p_loss: -3.8819127082824707, mean_rew: -7.566487550223368, variance: 4.210086822509766, lamda: 1.841448187828064

steps: 2249975, episodes: 90000, mean episode reward: -592.3033922305322, agent episode reward: [-197.43446407684402, -197.43446407684402, -197.43446407684402], time: 176.499
steps: 2249975, episodes: 90000, mean episode variance: 38.074858367443085, agent episode variance: [1.001487377166748, 36.016099655151365, 1.0572713351249694], time: 176.499
Running avgs for agent 0: q_loss: 6.916738510131836, p_loss: -3.9135801792144775, mean_rew: -7.597741456344236, variance: 4.0059494972229, lamda: 1.9469424486160278
Running avgs for agent 1: q_loss: 1087871.0, p_loss: 140.5317840576172, mean_rew: -7.5940415200641675, variance: 144.06439862060546, lamda: 3.2091333866119385
Running avgs for agent 2: q_loss: 6.577507019042969, p_loss: -3.8952877521514893, mean_rew: -7.5973139977243624, variance: 4.229085445404053, lamda: 1.8414483070373535

steps: 2274975, episodes: 91000, mean episode reward: -596.5835006723141, agent episode reward: [-198.86116689077136, -198.86116689077136, -198.86116689077136], time: 179.926
steps: 2274975, episodes: 91000, mean episode variance: 38.0237227602005, agent episode variance: [1.0011456270217896, 35.95847264099121, 1.0641044921875], time: 179.927
Running avgs for agent 0: q_loss: 7.062041759490967, p_loss: -3.9325029850006104, mean_rew: -7.620790831632634, variance: 4.004582405090332, lamda: 1.9469435214996338
Running avgs for agent 1: q_loss: 1111315.75, p_loss: 140.91835021972656, mean_rew: -7.6162612718212435, variance: 143.83389056396484, lamda: 3.234107732772827
Running avgs for agent 2: q_loss: 6.625183582305908, p_loss: -3.909477710723877, mean_rew: -7.6189537134287955, variance: 4.256417751312256, lamda: 1.841448187828064

steps: 2299975, episodes: 92000, mean episode reward: -593.7255476817994, agent episode reward: [-197.90851589393313, -197.90851589393313, -197.90851589393313], time: 180.954
steps: 2299975, episodes: 92000, mean episode variance: 37.382675614118575, agent episode variance: [1.0027547886371613, 35.31871536254883, 1.0612054629325867], time: 180.955
Running avgs for agent 0: q_loss: 7.081857204437256, p_loss: -3.94138765335083, mean_rew: -7.64242055578054, variance: 4.011019229888916, lamda: 1.9471791982650757
Running avgs for agent 1: q_loss: 1123881.375, p_loss: 141.25721740722656, mean_rew: -7.644621892359075, variance: 141.27486145019532, lamda: 3.259082078933716
Running avgs for agent 2: q_loss: 6.68347692489624, p_loss: -3.9212441444396973, mean_rew: -7.640018855437116, variance: 4.244821548461914, lamda: 1.841448187828064

steps: 2324975, episodes: 93000, mean episode reward: -593.0090220984746, agent episode reward: [-197.66967403282484, -197.66967403282484, -197.66967403282484], time: 179.306
steps: 2324975, episodes: 93000, mean episode variance: 37.70892104077339, agent episode variance: [1.0081206302642822, 35.63812657165527, 1.0626738388538362], time: 179.306
Running avgs for agent 0: q_loss: 7.087897300720215, p_loss: -3.955427646636963, mean_rew: -7.669116760875517, variance: 4.032482624053955, lamda: 1.9471889734268188
Running avgs for agent 1: q_loss: 1141462.125, p_loss: 141.51141357421875, mean_rew: -7.65982259557142, variance: 142.55250628662108, lamda: 3.2840564250946045
Running avgs for agent 2: q_loss: 6.712689399719238, p_loss: -3.9349820613861084, mean_rew: -7.664639308303516, variance: 4.25069522857666, lamda: 1.8414483070373535

steps: 2349975, episodes: 94000, mean episode reward: -592.2971435266071, agent episode reward: [-197.43238117553568, -197.43238117553568, -197.43238117553568], time: 170.291
steps: 2349975, episodes: 94000, mean episode variance: 37.96783080625534, agent episode variance: [1.019137870311737, 35.87333847045898, 1.0753544654846192], time: 170.292
Running avgs for agent 0: q_loss: 7.025205135345459, p_loss: -3.953908681869507, mean_rew: -7.681159342519671, variance: 4.07655143737793, lamda: 1.9471887350082397
Running avgs for agent 1: q_loss: 1162479.375, p_loss: 141.80563354492188, mean_rew: -7.679479887795664, variance: 143.49335388183593, lamda: 3.309030771255493
Running avgs for agent 2: q_loss: 6.856513500213623, p_loss: -3.949317693710327, mean_rew: -7.6867426431561485, variance: 4.301417827606201, lamda: 1.841448187828064

steps: 2374975, episodes: 95000, mean episode reward: -597.1221611572968, agent episode reward: [-199.04072038576558, -199.04072038576558, -199.04072038576558], time: 170.26
steps: 2374975, episodes: 95000, mean episode variance: 37.04365083670616, agent episode variance: [1.0136518967151642, 34.96188381958008, 1.0681151204109192], time: 170.26
Running avgs for agent 0: q_loss: 7.199639797210693, p_loss: -3.9653780460357666, mean_rew: -7.697556079811607, variance: 4.05460786819458, lamda: 1.9471887350082397
Running avgs for agent 1: q_loss: 1174978.0, p_loss: 142.09722900390625, mean_rew: -7.701483970567846, variance: 139.84753527832032, lamda: 3.3340048789978027
Running avgs for agent 2: q_loss: 6.830441474914551, p_loss: -3.9560651779174805, mean_rew: -7.700137075445102, variance: 4.2724609375, lamda: 1.841448187828064

steps: 2399975, episodes: 96000, mean episode reward: -592.6409888934297, agent episode reward: [-197.54699629780987, -197.54699629780987, -197.54699629780987], time: 167.759
steps: 2399975, episodes: 96000, mean episode variance: 37.75151009273529, agent episode variance: [1.0177109642028808, 35.65482067871094, 1.078978449821472], time: 167.76
Running avgs for agent 0: q_loss: 7.247031211853027, p_loss: -3.9756269454956055, mean_rew: -7.718495441807552, variance: 4.0708441734313965, lamda: 1.9471889734268188
Running avgs for agent 1: q_loss: 1186356.75, p_loss: 142.35333251953125, mean_rew: -7.722624177854968, variance: 142.61928271484376, lamda: 3.3589794635772705
Running avgs for agent 2: q_loss: 6.79240083694458, p_loss: -3.9638559818267822, mean_rew: -7.718291670318416, variance: 4.315914154052734, lamda: 1.8414483070373535

steps: 2424975, episodes: 97000, mean episode reward: -596.077051880718, agent episode reward: [-198.69235062690603, -198.69235062690603, -198.69235062690603], time: 166.816
steps: 2424975, episodes: 97000, mean episode variance: 38.549605246543884, agent episode variance: [1.023159636259079, 36.44305986022949, 1.083385750055313], time: 166.816
Running avgs for agent 0: q_loss: 7.183676719665527, p_loss: -3.9811081886291504, mean_rew: -7.732135083846031, variance: 4.0926384925842285, lamda: 1.9471888542175293
Running avgs for agent 1: q_loss: 1231008.5, p_loss: 142.577880859375, mean_rew: -7.733934494843843, variance: 145.77223944091796, lamda: 3.38395357131958
Running avgs for agent 2: q_loss: 6.979193210601807, p_loss: -3.9725747108459473, mean_rew: -7.7407342523019125, variance: 4.333542823791504, lamda: 1.841448187828064

steps: 2449975, episodes: 98000, mean episode reward: -597.1957041689584, agent episode reward: [-199.06523472298608, -199.06523472298608, -199.06523472298608], time: 180.746
steps: 2449975, episodes: 98000, mean episode variance: 37.17094432497024, agent episode variance: [1.029860638141632, 35.06333171081543, 1.0777519760131835], time: 180.747
Running avgs for agent 0: q_loss: 7.231579303741455, p_loss: -3.9923250675201416, mean_rew: -7.754787722557044, variance: 4.119442462921143, lamda: 1.9471887350082397
Running avgs for agent 1: q_loss: 1225130.75, p_loss: 142.89569091796875, mean_rew: -7.758082121168209, variance: 140.25332684326173, lamda: 3.408928155899048
Running avgs for agent 2: q_loss: 6.90273904800415, p_loss: -3.981823444366455, mean_rew: -7.7534890007298864, variance: 4.311007976531982, lamda: 1.841448187828064

steps: 2474975, episodes: 99000, mean episode reward: -595.3406519582306, agent episode reward: [-198.44688398607687, -198.44688398607687, -198.44688398607687], time: 179.829
steps: 2474975, episodes: 99000, mean episode variance: 37.594064565896986, agent episode variance: [1.0229394521713258, 35.487001846313476, 1.0841232674121857], time: 179.83
Running avgs for agent 0: q_loss: 7.246646881103516, p_loss: -4.004195690155029, mean_rew: -7.763658994483736, variance: 4.091757774353027, lamda: 1.9471889734268188
Running avgs for agent 1: q_loss: 1252063.5, p_loss: 143.23634338378906, mean_rew: -7.772763167228042, variance: 141.9480073852539, lamda: 3.4339025020599365
Running avgs for agent 2: q_loss: 7.006942272186279, p_loss: -3.9924216270446777, mean_rew: -7.767714211848229, variance: 4.336492538452148, lamda: 1.841467022895813

steps: 2499975, episodes: 100000, mean episode reward: -600.1174729213457, agent episode reward: [-200.0391576404486, -200.0391576404486, -200.0391576404486], time: 166.627
steps: 2499975, episodes: 100000, mean episode variance: 38.01865462946892, agent episode variance: [1.0276154103279114, 35.89974139404297, 1.0912978250980376], time: 166.628
Running avgs for agent 0: q_loss: 7.217846870422363, p_loss: -4.010800838470459, mean_rew: -7.78309939525352, variance: 4.110461711883545, lamda: 1.9471888542175293
Running avgs for agent 1: q_loss: 1274551.0, p_loss: 143.57205200195312, mean_rew: -7.789334762633395, variance: 143.59896557617188, lamda: 3.458876609802246
Running avgs for agent 2: q_loss: 6.9491095542907715, p_loss: -4.000748157501221, mean_rew: -7.7940026924516115, variance: 4.365191459655762, lamda: 1.8415061235427856

steps: 2524975, episodes: 101000, mean episode reward: -602.8803554619302, agent episode reward: [-200.96011848731, -200.96011848731, -200.96011848731], time: 163.849
steps: 2524975, episodes: 101000, mean episode variance: 37.55358516812325, agent episode variance: [1.0302723276615142, 35.434300918579105, 1.0890119218826293], time: 163.849
Running avgs for agent 0: q_loss: 7.329446315765381, p_loss: -4.017766952514648, mean_rew: -7.802354342264388, variance: 4.121089458465576, lamda: 1.9471887350082397
Running avgs for agent 1: q_loss: 1273786.25, p_loss: 143.92645263671875, mean_rew: -7.811423203657595, variance: 141.73720367431642, lamda: 3.4838509559631348
Running avgs for agent 2: q_loss: 6.963916778564453, p_loss: -4.011211395263672, mean_rew: -7.802871366400084, variance: 4.356047630310059, lamda: 1.8415062427520752

steps: 2549975, episodes: 102000, mean episode reward: -597.8274865891857, agent episode reward: [-199.27582886306192, -199.27582886306192, -199.27582886306192], time: 162.106
steps: 2549975, episodes: 102000, mean episode variance: 38.357766607284546, agent episode variance: [1.032943026304245, 36.23197416687012, 1.0928494141101837], time: 162.106
Running avgs for agent 0: q_loss: 7.372788429260254, p_loss: -4.033383846282959, mean_rew: -7.82492463169773, variance: 4.131772041320801, lamda: 1.9472546577453613
Running avgs for agent 1: q_loss: 1308344.875, p_loss: 144.20956420898438, mean_rew: -7.8315106906606, variance: 144.92789666748047, lamda: 3.5088255405426025
Running avgs for agent 2: q_loss: 7.0150275230407715, p_loss: -4.0188398361206055, mean_rew: -7.824195465068829, variance: 4.371397495269775, lamda: 1.8415063619613647

steps: 2574975, episodes: 103000, mean episode reward: -597.7281316994261, agent episode reward: [-199.24271056647535, -199.24271056647535, -199.24271056647535], time: 162.53
steps: 2574975, episodes: 103000, mean episode variance: 38.159586305856706, agent episode variance: [1.0346539092063904, 36.02794062805176, 1.0969917685985566], time: 162.53
Running avgs for agent 0: q_loss: 7.4691853523254395, p_loss: -4.04354190826416, mean_rew: -7.841462953709112, variance: 4.138615608215332, lamda: 1.9473897218704224
Running avgs for agent 1: q_loss: 1321630.125, p_loss: 144.44606018066406, mean_rew: -7.841411579269174, variance: 144.11176251220704, lamda: 3.533799886703491
Running avgs for agent 2: q_loss: 7.001312732696533, p_loss: -4.0296807289123535, mean_rew: -7.841265467340305, variance: 4.387966632843018, lamda: 1.8415061235427856

steps: 2599975, episodes: 104000, mean episode reward: -594.4305706609528, agent episode reward: [-198.14352355365094, -198.14352355365094, -198.14352355365094], time: 160.419
steps: 2599975, episodes: 104000, mean episode variance: 37.85835262417793, agent episode variance: [1.0358288304805756, 35.73423550415039, 1.0882882895469665], time: 160.42
Running avgs for agent 0: q_loss: 7.335864543914795, p_loss: -4.049058437347412, mean_rew: -7.857811237987799, variance: 4.143315315246582, lamda: 1.9473896026611328
Running avgs for agent 1: q_loss: 1324586.375, p_loss: 144.7619171142578, mean_rew: -7.856507131746401, variance: 142.93694201660156, lamda: 3.55877423286438
Running avgs for agent 2: q_loss: 7.017109394073486, p_loss: -4.033776760101318, mean_rew: -7.847615958141807, variance: 4.353152751922607, lamda: 1.8415063619613647

steps: 2624975, episodes: 105000, mean episode reward: -600.317764029426, agent episode reward: [-200.10592134314206, -200.10592134314206, -200.10592134314206], time: 166.29
steps: 2624975, episodes: 105000, mean episode variance: 37.50764117407799, agent episode variance: [1.0343444402217865, 35.37483012390137, 1.098466609954834], time: 166.291
Running avgs for agent 0: q_loss: 7.4203009605407715, p_loss: -4.053319931030273, mean_rew: -7.868270256396107, variance: 4.137377738952637, lamda: 1.947389841079712
Running avgs for agent 1: q_loss: 1347284.25, p_loss: 145.0719757080078, mean_rew: -7.858884795873607, variance: 141.49932049560547, lamda: 3.5837485790252686
Running avgs for agent 2: q_loss: 7.0376739501953125, p_loss: -4.0388503074646, mean_rew: -7.8623037099502495, variance: 4.393866539001465, lamda: 1.8415064811706543

steps: 2649975, episodes: 106000, mean episode reward: -599.2624612193584, agent episode reward: [-199.75415373978615, -199.75415373978615, -199.75415373978615], time: 167.146
steps: 2649975, episodes: 106000, mean episode variance: 37.78204122400284, agent episode variance: [1.0398186895847321, 35.63950392150879, 1.102718612909317], time: 167.146
Running avgs for agent 0: q_loss: 7.3652544021606445, p_loss: -4.055233001708984, mean_rew: -7.875096967870313, variance: 4.159275054931641, lamda: 1.9473897218704224
Running avgs for agent 1: q_loss: 1360626.125, p_loss: 145.4130096435547, mean_rew: -7.883732014815161, variance: 142.55801568603516, lamda: 3.608722686767578
Running avgs for agent 2: q_loss: 7.133561134338379, p_loss: -4.0491132736206055, mean_rew: -7.879047590289665, variance: 4.410874843597412, lamda: 1.8415063619613647

steps: 2674975, episodes: 107000, mean episode reward: -599.2340904139182, agent episode reward: [-199.74469680463946, -199.74469680463946, -199.74469680463946], time: 167.234
steps: 2674975, episodes: 107000, mean episode variance: 37.11834104037285, agent episode variance: [1.0408214907646178, 34.980092987060544, 1.0974265625476838], time: 167.234
Running avgs for agent 0: q_loss: 7.442686557769775, p_loss: -4.064967155456543, mean_rew: -7.8866609377815005, variance: 4.163286209106445, lamda: 1.9474482536315918
Running avgs for agent 1: q_loss: 1373864.25, p_loss: 145.71014404296875, mean_rew: -7.876517545408747, variance: 139.92037194824218, lamda: 3.633697271347046
Running avgs for agent 2: q_loss: 6.9583353996276855, p_loss: -4.048508644104004, mean_rew: -7.874296810402491, variance: 4.389706134796143, lamda: 1.8415063619613647

steps: 2699975, episodes: 108000, mean episode reward: -597.402580952386, agent episode reward: [-199.13419365079525, -199.13419365079525, -199.13419365079525], time: 165.778
steps: 2699975, episodes: 108000, mean episode variance: 37.3310515537262, agent episode variance: [1.036330620288849, 35.19385681152344, 1.10086412191391], time: 165.778
Running avgs for agent 0: q_loss: 7.297450065612793, p_loss: -4.064097881317139, mean_rew: -7.887943272727857, variance: 4.145322322845459, lamda: 1.9475184679031372
Running avgs for agent 1: q_loss: 1391695.125, p_loss: 146.03115844726562, mean_rew: -7.8883570864476065, variance: 140.77542724609376, lamda: 3.6586716175079346
Running avgs for agent 2: q_loss: 7.0301642417907715, p_loss: -4.057023525238037, mean_rew: -7.893762840730515, variance: 4.403456211090088, lamda: 1.8415064811706543

steps: 2724975, episodes: 109000, mean episode reward: -599.4734488426535, agent episode reward: [-199.82448294755113, -199.82448294755113, -199.82448294755113], time: 167.214
steps: 2724975, episodes: 109000, mean episode variance: 37.790933525323865, agent episode variance: [1.0459034583568574, 35.64053183746338, 1.1044982295036316], time: 167.215
Running avgs for agent 0: q_loss: 7.378108501434326, p_loss: -4.073025703430176, mean_rew: -7.90094596512052, variance: 4.1836137771606445, lamda: 1.9475270509719849
Running avgs for agent 1: q_loss: 1422822.375, p_loss: 146.3314971923828, mean_rew: -7.90302602834581, variance: 142.5621273498535, lamda: 3.6836459636688232
Running avgs for agent 2: q_loss: 6.974684715270996, p_loss: -4.056885242462158, mean_rew: -7.89100113871627, variance: 4.417993068695068, lamda: 1.8415063619613647

steps: 2749975, episodes: 110000, mean episode reward: -604.234809890578, agent episode reward: [-201.41160329685937, -201.41160329685937, -201.41160329685937], time: 163.649
steps: 2749975, episodes: 110000, mean episode variance: 37.697216608762744, agent episode variance: [1.0459597234725952, 35.54887013244629, 1.102386752843857], time: 163.649
Running avgs for agent 0: q_loss: 7.3156232833862305, p_loss: -4.069901466369629, mean_rew: -7.9046677789413815, variance: 4.183838844299316, lamda: 1.9475438594818115
Running avgs for agent 1: q_loss: 1438675.875, p_loss: 146.60745239257812, mean_rew: -7.904117271984183, variance: 142.19548052978516, lamda: 3.708620309829712
Running avgs for agent 2: q_loss: 7.017253875732422, p_loss: -4.057187557220459, mean_rew: -7.894475662389412, variance: 4.409546852111816, lamda: 1.8415063619613647

steps: 2774975, episodes: 111000, mean episode reward: -607.2124933992794, agent episode reward: [-202.40416446642647, -202.40416446642647, -202.40416446642647], time: 162.275
steps: 2774975, episodes: 111000, mean episode variance: 37.283572819709775, agent episode variance: [1.0417284986972808, 35.14034202575684, 1.101502295255661], time: 162.276
Running avgs for agent 0: q_loss: 7.403960227966309, p_loss: -4.080526351928711, mean_rew: -7.913848792792004, variance: 4.166913986206055, lamda: 1.947543978691101
Running avgs for agent 1: q_loss: 1455664.375, p_loss: 146.94363403320312, mean_rew: -7.913538139788164, variance: 140.56136810302735, lamda: 3.7335944175720215
Running avgs for agent 2: q_loss: 7.041039943695068, p_loss: -4.072411060333252, mean_rew: -7.911807671266405, variance: 4.406008720397949, lamda: 1.8415064811706543

steps: 2799975, episodes: 112000, mean episode reward: -598.7040948297315, agent episode reward: [-199.5680316099105, -199.5680316099105, -199.5680316099105], time: 163.434
steps: 2799975, episodes: 112000, mean episode variance: 37.48549571084976, agent episode variance: [1.047449821472168, 35.332040618896485, 1.1060052704811096], time: 163.435
Running avgs for agent 0: q_loss: 7.3257551193237305, p_loss: -4.086140155792236, mean_rew: -7.923830790450902, variance: 4.1897993087768555, lamda: 1.947543978691101
Running avgs for agent 1: q_loss: 1492094.875, p_loss: 147.24322509765625, mean_rew: -7.922349049366856, variance: 141.32816247558594, lamda: 3.7585690021514893
Running avgs for agent 2: q_loss: 7.075888156890869, p_loss: -4.074297904968262, mean_rew: -7.9233716342206035, variance: 4.424020767211914, lamda: 1.8415063619613647

steps: 2824975, episodes: 113000, mean episode reward: -601.7728965093198, agent episode reward: [-200.5909655031066, -200.5909655031066, -200.5909655031066], time: 170.806
steps: 2824975, episodes: 113000, mean episode variance: 37.311048507452014, agent episode variance: [1.0485796115398407, 35.158030212402345, 1.1044386835098268], time: 170.807
Running avgs for agent 0: q_loss: 7.386416912078857, p_loss: -4.08341121673584, mean_rew: -7.92951775035581, variance: 4.1943182945251465, lamda: 1.9475438594818115
Running avgs for agent 1: q_loss: 1510454.125, p_loss: 147.5553436279297, mean_rew: -7.930318656366185, variance: 140.63212084960938, lamda: 3.783543348312378
Running avgs for agent 2: q_loss: 7.152251720428467, p_loss: -4.074102401733398, mean_rew: -7.9245056451811315, variance: 4.417754650115967, lamda: 1.8415063619613647

steps: 2849975, episodes: 114000, mean episode reward: -608.0128865518025, agent episode reward: [-202.67096218393417, -202.67096218393417, -202.67096218393417], time: 156.1
steps: 2849975, episodes: 114000, mean episode variance: 38.109555029153825, agent episode variance: [1.0461841309070588, 35.95783479309082, 1.105536105155945], time: 156.101
Running avgs for agent 0: q_loss: 7.341032028198242, p_loss: -4.0896406173706055, mean_rew: -7.9325999431915895, variance: 4.184736251831055, lamda: 1.9475713968276978
Running avgs for agent 1: q_loss: 1529537.0, p_loss: 147.84353637695312, mean_rew: -7.92925182099621, variance: 143.83133917236327, lamda: 3.8085176944732666
Running avgs for agent 2: q_loss: 7.081491470336914, p_loss: -4.079606533050537, mean_rew: -7.930294407352438, variance: 4.422144412994385, lamda: 1.8415064811706543

steps: 2874975, episodes: 115000, mean episode reward: -595.697483217818, agent episode reward: [-198.5658277392726, -198.5658277392726, -198.5658277392726], time: 116.145
steps: 2874975, episodes: 115000, mean episode variance: 38.312592782020566, agent episode variance: [1.0474672570228576, 36.15435514831543, 1.1107703766822814], time: 116.146
Running avgs for agent 0: q_loss: 7.3793044090271, p_loss: -4.089212894439697, mean_rew: -7.934300829032438, variance: 4.189868927001953, lamda: 1.9476515054702759
Running avgs for agent 1: q_loss: 1560725.25, p_loss: 148.11099243164062, mean_rew: -7.93550955016877, variance: 144.61742059326173, lamda: 3.8334922790527344
Running avgs for agent 2: q_loss: 7.046530246734619, p_loss: -4.079466342926025, mean_rew: -7.939324081981812, variance: 4.443081378936768, lamda: 1.8415063619613647

steps: 2899975, episodes: 116000, mean episode reward: -603.3736374691003, agent episode reward: [-201.12454582303343, -201.12454582303343, -201.12454582303343], time: 110.596
steps: 2899975, episodes: 116000, mean episode variance: 37.18255544137955, agent episode variance: [1.0483603689670562, 35.02878137207031, 1.1054137003421785], time: 110.597
Running avgs for agent 0: q_loss: 7.472244739532471, p_loss: -4.088262557983398, mean_rew: -7.939115867796823, variance: 4.193441390991211, lamda: 1.9476513862609863
Running avgs for agent 1: q_loss: 1580679.125, p_loss: 148.49325561523438, mean_rew: -7.943816429903753, variance: 140.11512548828125, lamda: 3.858466386795044
Running avgs for agent 2: q_loss: 7.019737243652344, p_loss: -4.082712173461914, mean_rew: -7.940490166432624, variance: 4.42165470123291, lamda: 1.8415063619613647

steps: 2924975, episodes: 117000, mean episode reward: -597.1506581197903, agent episode reward: [-199.05021937326342, -199.05021937326342, -199.05021937326342], time: 116.301
steps: 2924975, episodes: 117000, mean episode variance: 37.25794518995285, agent episode variance: [1.043452121734619, 35.10712184143066, 1.107371226787567], time: 116.302
Running avgs for agent 0: q_loss: 7.356239318847656, p_loss: -4.093212604522705, mean_rew: -7.946248962167786, variance: 4.173808574676514, lamda: 1.9476515054702759
Running avgs for agent 1: q_loss: 1615764.25, p_loss: 148.79722595214844, mean_rew: -7.946945382774789, variance: 140.42848736572265, lamda: 3.8834407329559326
Running avgs for agent 2: q_loss: 7.026370048522949, p_loss: -4.085093975067139, mean_rew: -7.947331229074927, variance: 4.429484844207764, lamda: 1.8415064811706543

steps: 2949975, episodes: 118000, mean episode reward: -602.9756674029445, agent episode reward: [-200.99188913431482, -200.99188913431482, -200.99188913431482], time: 114.212
steps: 2949975, episodes: 118000, mean episode variance: 38.01343678832054, agent episode variance: [1.0520120997428895, 35.84836408996582, 1.1130605986118316], time: 114.212
Running avgs for agent 0: q_loss: 7.519781112670898, p_loss: -4.091953754425049, mean_rew: -7.948560379337621, variance: 4.208048343658447, lamda: 1.9476515054702759
Running avgs for agent 1: q_loss: 1651070.0, p_loss: 149.070556640625, mean_rew: -7.95132836868937, variance: 143.39345635986328, lamda: 3.9084150791168213
Running avgs for agent 2: q_loss: 7.136496543884277, p_loss: -4.084283828735352, mean_rew: -7.9555006448448955, variance: 4.452242374420166, lamda: 1.8415063619613647

steps: 2974975, episodes: 119000, mean episode reward: -599.389424163196, agent episode reward: [-199.7964747210653, -199.7964747210653, -199.7964747210653], time: 103.226
steps: 2974975, episodes: 119000, mean episode variance: 37.61617338943481, agent episode variance: [1.0541746122837066, 35.45556184387207, 1.1064369332790376], time: 103.227
Running avgs for agent 0: q_loss: 7.4831223487854, p_loss: -4.095048904418945, mean_rew: -7.956493377341289, variance: 4.216698169708252, lamda: 1.9477192163467407
Running avgs for agent 1: q_loss: 1674036.0, p_loss: 149.28250122070312, mean_rew: -7.957924683430182, variance: 141.82224737548827, lamda: 3.933389663696289
Running avgs for agent 2: q_loss: 7.061356544494629, p_loss: -4.087730407714844, mean_rew: -7.951211211450647, variance: 4.425747871398926, lamda: 1.8415063619613647

steps: 2999975, episodes: 120000, mean episode reward: -601.8181413507291, agent episode reward: [-200.60604711690968, -200.60604711690968, -200.60604711690968], time: 105.277
steps: 2999975, episodes: 120000, mean episode variance: 38.264800228357316, agent episode variance: [1.0557758867740632, 36.10257162475586, 1.1064527168273925], time: 105.278
Running avgs for agent 0: q_loss: 7.437982082366943, p_loss: -4.097322463989258, mean_rew: -7.964756513992823, variance: 4.2231035232543945, lamda: 1.9478501081466675
Running avgs for agent 1: q_loss: 1700318.75, p_loss: 149.5279083251953, mean_rew: -7.968671144264285, variance: 144.41028649902344, lamda: 3.9583635330200195
Running avgs for agent 2: q_loss: 7.222298622131348, p_loss: -4.092364311218262, mean_rew: -7.9603079960868515, variance: 4.42581033706665, lamda: 1.8415124416351318

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -603.8145369319753, agent episode reward: [-201.27151231065847, -201.27151231065847, -201.27151231065847], time: 84.53
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 84.53
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -598.9185528815698, agent episode reward: [-199.63951762718997, -199.63951762718997, -199.63951762718997], time: 95.453
steps: 49975, episodes: 2000, mean episode variance: 2.159107991695404, agent episode variance: [1.205066017150879, 0.0, 0.9540419745445251], time: 95.454
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -8.037458250327274, variance: 4.93879508972168, lamda: 1.947837233543396
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -8.027951268817601, variance: 0.0, lamda: 3.9709010124206543
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -8.04127459275104, variance: 3.910008192062378, lamda: 1.841623067855835

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567759519.1090575: line 9: --exp_var_alpha: command not found
