# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation False --constrained \
    False --exp-name MADDPG_simple_spread_uncons_01 --save-dir \
    models/simple_spread_policies1/01-non-linear-uncons/ --plots-dir \
    models/simple_spread_policies1/01-non-linear-uncons/
Job <1083941> is submitted to queue <x86_6h>.
arglist.u_estimation False
2019-09-06 03:01:43.807162: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -528.7398964120868, agent episode reward: [-176.24663213736227, -176.24663213736227, -176.24663213736227], time: 32.523
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 32.523
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -615.0481357232176, agent episode reward: [-205.01604524107253, -205.01604524107253, -205.01604524107253], time: 54.198
steps: 49975, episodes: 2000, mean episode variance: 2.2182039624750614, agent episode variance: [0.731757958471775, 0.7714688496440649, 0.7149771543592215], time: 54.198
Running avgs for agent 0: q_loss: 2.0047805309295654, p_loss: 11.143692016601562, mean_rew: -7.603902830801866, variance: 2.999008026523668, mean_q: -11.220989227294922, std_q: 3.319467544555664
Running avgs for agent 1: q_loss: 2.3214683532714844, p_loss: 11.102836608886719, mean_rew: -7.618519395585771, variance: 3.161757580508463, mean_q: -11.195301055908203, std_q: 3.3317391872406006
Running avgs for agent 2: q_loss: 2.28861927986145, p_loss: 10.675745010375977, mean_rew: -7.610153825081756, variance: 2.930234239177137, mean_q: -10.756811141967773, std_q: 3.031787872314453

steps: 74975, episodes: 3000, mean episode reward: -463.8786691750416, agent episode reward: [-154.62622305834722, -154.62622305834722, -154.62622305834722], time: 61.446
steps: 74975, episodes: 3000, mean episode variance: 7.053468395575881, agent episode variance: [3.8634754787385464, 1.6489080245941878, 1.5410848922431468], time: 61.447
Running avgs for agent 0: q_loss: 1.604042410850525, p_loss: 22.780792236328125, mean_rew: -7.376593122692414, variance: 15.453901914954185, mean_q: -22.930404663085938, std_q: 6.819349765777588
Running avgs for agent 1: q_loss: 1.2463462352752686, p_loss: 22.677467346191406, mean_rew: -7.383428978282618, variance: 6.595632098376751, mean_q: -22.862144470214844, std_q: 6.903475284576416
Running avgs for agent 2: q_loss: 1.213541030883789, p_loss: 22.17537498474121, mean_rew: -7.3804687785349525, variance: 6.164339568972587, mean_q: -22.375656127929688, std_q: 6.510690212249756

steps: 99975, episodes: 4000, mean episode reward: -429.65673579361555, agent episode reward: [-143.21891193120518, -143.21891193120518, -143.21891193120518], time: 63.548
steps: 99975, episodes: 4000, mean episode variance: 14.363323701143266, agent episode variance: [7.897787042617798, 3.5018897523880006, 2.9636469061374666], time: 63.549
Running avgs for agent 0: q_loss: 2.1091372966766357, p_loss: 33.11598587036133, mean_rew: -6.970133309393844, variance: 31.591148170471193, mean_q: -33.379844665527344, std_q: 9.78175163269043
Running avgs for agent 1: q_loss: 1.3789504766464233, p_loss: 32.893001556396484, mean_rew: -6.974912306127201, variance: 14.007559009552002, mean_q: -33.20659255981445, std_q: 9.926933288574219
Running avgs for agent 2: q_loss: 1.4241077899932861, p_loss: 32.59699249267578, mean_rew: -6.982794767820586, variance: 11.854587624549866, mean_q: -32.970279693603516, std_q: 9.683752059936523

steps: 124975, episodes: 5000, mean episode reward: -397.186940669558, agent episode reward: [-132.39564688985269, -132.39564688985269, -132.39564688985269], time: 67.861
steps: 124975, episodes: 5000, mean episode variance: 20.166508022785187, agent episode variance: [10.723509860992431, 4.6567264370918275, 4.786271724700928], time: 67.861
Running avgs for agent 0: q_loss: 2.349461317062378, p_loss: 41.20536422729492, mean_rew: -6.638296911465645, variance: 42.894039443969724, mean_q: -41.57246017456055, std_q: 11.866609573364258
Running avgs for agent 1: q_loss: 1.4781893491744995, p_loss: 40.84653091430664, mean_rew: -6.624835059282026, variance: 18.62690574836731, mean_q: -41.241615295410156, std_q: 11.930068016052246
Running avgs for agent 2: q_loss: 1.5170373916625977, p_loss: 40.646339416503906, mean_rew: -6.633329820734368, variance: 19.14508689880371, mean_q: -41.20186996459961, std_q: 11.727228164672852

steps: 149975, episodes: 6000, mean episode reward: -387.6964216493057, agent episode reward: [-129.23214054976862, -129.23214054976862, -129.23214054976862], time: 67.553
steps: 149975, episodes: 6000, mean episode variance: 27.409375505447386, agent episode variance: [13.117113235473633, 6.157084236145019, 8.135178033828735], time: 67.553
Running avgs for agent 0: q_loss: 2.5165817737579346, p_loss: 47.460453033447266, mean_rew: -6.381625351608238, variance: 52.46845294189453, mean_q: -47.870269775390625, std_q: 12.946941375732422
Running avgs for agent 1: q_loss: 1.6616181135177612, p_loss: 47.154476165771484, mean_rew: -6.378944658941371, variance: 24.628336944580077, mean_q: -47.59153747558594, std_q: 13.092164039611816
Running avgs for agent 2: q_loss: 1.9258220195770264, p_loss: 46.969181060791016, mean_rew: -6.38656593557182, variance: 32.54071213531494, mean_q: -47.59397506713867, std_q: 12.894118309020996

steps: 174975, episodes: 7000, mean episode reward: -380.8976633535689, agent episode reward: [-126.96588778452298, -126.96588778452298, -126.96588778452298], time: 67.562
steps: 174975, episodes: 7000, mean episode variance: 36.47735607910156, agent episode variance: [15.265399490356446, 7.483934196472168, 13.72802239227295], time: 67.562
Running avgs for agent 0: q_loss: 2.6640336513519287, p_loss: 52.716617584228516, mean_rew: -6.196143449865472, variance: 61.06159796142578, mean_q: -53.12702178955078, std_q: 13.489655494689941
Running avgs for agent 1: q_loss: 1.7709407806396484, p_loss: 52.48211669921875, mean_rew: -6.190008620401114, variance: 29.935736785888672, mean_q: -52.9232292175293, std_q: 13.861515998840332
Running avgs for agent 2: q_loss: 2.543328046798706, p_loss: 52.193904876708984, mean_rew: -6.194941048660051, variance: 54.9120895690918, mean_q: -52.83938980102539, std_q: 13.4783296585083

steps: 199975, episodes: 8000, mean episode reward: -376.7736185422834, agent episode reward: [-125.59120618076112, -125.59120618076112, -125.59120618076112], time: 65.559
steps: 199975, episodes: 8000, mean episode variance: 39.47187322616577, agent episode variance: [16.219909034729003, 8.356198356628418, 14.89576583480835], time: 65.559
Running avgs for agent 0: q_loss: 2.7564218044281006, p_loss: 57.20501708984375, mean_rew: -6.046047057866821, variance: 64.87963613891601, mean_q: -57.60004425048828, std_q: 13.749358177185059
Running avgs for agent 1: q_loss: 1.9062390327453613, p_loss: 56.89885330200195, mean_rew: -6.0308797552020845, variance: 33.42479342651367, mean_q: -57.323673248291016, std_q: 13.968375205993652
Running avgs for agent 2: q_loss: 2.651935338973999, p_loss: 56.64878463745117, mean_rew: -6.034322963719923, variance: 59.5830633392334, mean_q: -57.2680778503418, std_q: 13.677098274230957

steps: 224975, episodes: 9000, mean episode reward: -371.0529130231231, agent episode reward: [-123.68430434104104, -123.68430434104104, -123.68430434104104], time: 71.522
steps: 224975, episodes: 9000, mean episode variance: 42.745108619689944, agent episode variance: [17.884376174926757, 9.114943161010743, 15.745789283752442], time: 71.522
Running avgs for agent 0: q_loss: 2.797299385070801, p_loss: 60.97275924682617, mean_rew: -5.912476733208136, variance: 71.53750469970703, mean_q: -61.352787017822266, std_q: 13.572480201721191
Running avgs for agent 1: q_loss: 1.9334479570388794, p_loss: 60.64373779296875, mean_rew: -5.91325761053441, variance: 36.45977264404297, mean_q: -61.07432174682617, std_q: 13.975177764892578
Running avgs for agent 2: q_loss: 2.6738598346710205, p_loss: 60.46394348144531, mean_rew: -5.912403022734226, variance: 62.98315713500977, mean_q: -61.057701110839844, std_q: 13.583867073059082

steps: 249975, episodes: 10000, mean episode reward: -367.0055024453222, agent episode reward: [-122.33516748177405, -122.33516748177405, -122.33516748177405], time: 66.886
steps: 249975, episodes: 10000, mean episode variance: 44.29444191169739, agent episode variance: [18.389548980712892, 8.999136884689332, 16.905756046295167], time: 66.886
Running avgs for agent 0: q_loss: 2.8234338760375977, p_loss: 64.26631164550781, mean_rew: -5.812926865013717, variance: 73.55819592285157, mean_q: -64.64231872558594, std_q: 13.62574577331543
Running avgs for agent 1: q_loss: 1.9641779661178589, p_loss: 63.82691955566406, mean_rew: -5.8076397163578495, variance: 35.996547538757326, mean_q: -64.2530288696289, std_q: 13.74729061126709
Running avgs for agent 2: q_loss: 2.677765130996704, p_loss: 63.656681060791016, mean_rew: -5.807001475603587, variance: 67.62302418518067, mean_q: -64.22501373291016, std_q: 13.40432071685791

steps: 274975, episodes: 11000, mean episode reward: -366.26878279882345, agent episode reward: [-122.08959426627449, -122.08959426627449, -122.08959426627449], time: 67.681
steps: 274975, episodes: 11000, mean episode variance: 47.48446654891968, agent episode variance: [19.060970123291014, 10.1034298286438, 18.320066596984862], time: 67.682
Running avgs for agent 0: q_loss: 2.871467113494873, p_loss: 67.03384399414062, mean_rew: -5.723813446019068, variance: 76.24388049316406, mean_q: -67.40013885498047, std_q: 13.38856029510498
Running avgs for agent 1: q_loss: 1.9147331714630127, p_loss: 66.55884552001953, mean_rew: -5.721656703474215, variance: 40.4137193145752, mean_q: -66.9697494506836, std_q: 13.461837768554688
Running avgs for agent 2: q_loss: 2.684882879257202, p_loss: 66.40323638916016, mean_rew: -5.722158127137665, variance: 73.28026638793945, mean_q: -66.93731689453125, std_q: 13.498943328857422

steps: 299975, episodes: 12000, mean episode reward: -363.75150705581905, agent episode reward: [-121.25050235193967, -121.25050235193967, -121.25050235193967], time: 68.336
steps: 299975, episodes: 12000, mean episode variance: 48.12376494121551, agent episode variance: [19.963468421936035, 10.201409155845642, 17.95888736343384], time: 68.336
Running avgs for agent 0: q_loss: 2.855560541152954, p_loss: 69.37294006347656, mean_rew: -5.648247588972661, variance: 79.85387368774414, mean_q: -69.73139953613281, std_q: 13.186799049377441
Running avgs for agent 1: q_loss: 1.918500542640686, p_loss: 68.92198181152344, mean_rew: -5.645995191849403, variance: 40.80563662338257, mean_q: -69.31547546386719, std_q: 13.18382453918457
Running avgs for agent 2: q_loss: 2.6609251499176025, p_loss: 68.757080078125, mean_rew: -5.648140400443921, variance: 71.83554945373535, mean_q: -69.25936126708984, std_q: 13.162452697753906

steps: 324975, episodes: 13000, mean episode reward: -364.56846776675224, agent episode reward: [-121.5228225889174, -121.5228225889174, -121.5228225889174], time: 67.455
steps: 324975, episodes: 13000, mean episode variance: 49.80440603637695, agent episode variance: [20.42497766113281, 10.823208827972412, 18.556219547271727], time: 67.455
Running avgs for agent 0: q_loss: 2.8319506645202637, p_loss: 71.47840881347656, mean_rew: -5.583291554864812, variance: 81.69991064453124, mean_q: -71.82368469238281, std_q: 13.113341331481934
Running avgs for agent 1: q_loss: 1.9149363040924072, p_loss: 70.91864013671875, mean_rew: -5.579340313794537, variance: 43.29283531188965, mean_q: -71.29624938964844, std_q: 12.883373260498047
Running avgs for agent 2: q_loss: 2.6802115440368652, p_loss: 70.77238464355469, mean_rew: -5.581699412345112, variance: 74.22487818908691, mean_q: -71.245361328125, std_q: 13.15225887298584

steps: 349975, episodes: 14000, mean episode reward: -364.50601657116124, agent episode reward: [-121.50200552372038, -121.50200552372038, -121.50200552372038], time: 67.783
steps: 349975, episodes: 14000, mean episode variance: 51.064053676605226, agent episode variance: [20.759643478393556, 10.897638217926026, 19.406771980285644], time: 67.783
Running avgs for agent 0: q_loss: 2.8353142738342285, p_loss: 73.18281555175781, mean_rew: -5.528479032776086, variance: 83.03857391357423, mean_q: -73.51504516601562, std_q: 12.819708824157715
Running avgs for agent 1: q_loss: 1.9073889255523682, p_loss: 72.75271606445312, mean_rew: -5.523813878115355, variance: 43.590552871704105, mean_q: -73.11782836914062, std_q: 12.827851295471191
Running avgs for agent 2: q_loss: 2.642890691757202, p_loss: 72.47505950927734, mean_rew: -5.5279531463455065, variance: 77.62708792114258, mean_q: -72.92850494384766, std_q: 12.801966667175293

steps: 374975, episodes: 15000, mean episode reward: -362.958434924328, agent episode reward: [-120.98614497477598, -120.98614497477598, -120.98614497477598], time: 67.882
steps: 374975, episodes: 15000, mean episode variance: 50.8359077167511, agent episode variance: [20.929763076782226, 10.990393877029419, 18.915750762939453], time: 67.882
Running avgs for agent 0: q_loss: 2.830798864364624, p_loss: 74.80099487304688, mean_rew: -5.481439925315826, variance: 83.7190523071289, mean_q: -75.1274642944336, std_q: 12.706626892089844
Running avgs for agent 1: q_loss: 1.8817360401153564, p_loss: 74.40744018554688, mean_rew: -5.488374004316434, variance: 43.961575508117676, mean_q: -74.76534271240234, std_q: 12.70894718170166
Running avgs for agent 2: q_loss: 2.6083126068115234, p_loss: 74.03266906738281, mean_rew: -5.4864665793961045, variance: 75.66300305175781, mean_q: -74.46683502197266, std_q: 12.720023155212402

steps: 399975, episodes: 16000, mean episode reward: -360.00638212955266, agent episode reward: [-120.00212737651756, -120.00212737651756, -120.00212737651756], time: 67.16
steps: 399975, episodes: 16000, mean episode variance: 52.6595170211792, agent episode variance: [21.892325061798097, 11.470136631011963, 19.29705532836914], time: 67.16
Running avgs for agent 0: q_loss: 2.7949471473693848, p_loss: 76.08732604980469, mean_rew: -5.440348539752095, variance: 87.56930024719239, mean_q: -76.40603637695312, std_q: 12.55605411529541
Running avgs for agent 1: q_loss: 1.8429127931594849, p_loss: 75.65687561035156, mean_rew: -5.4365042726352115, variance: 45.88054652404785, mean_q: -76.01292419433594, std_q: 12.251354217529297
Running avgs for agent 2: q_loss: 2.6192328929901123, p_loss: 75.27131652832031, mean_rew: -5.441056087392059, variance: 77.18822131347656, mean_q: -75.68888092041016, std_q: 12.465697288513184

steps: 424975, episodes: 17000, mean episode reward: -359.06160473160367, agent episode reward: [-119.68720157720121, -119.68720157720121, -119.68720157720121], time: 67.401
steps: 424975, episodes: 17000, mean episode variance: 58.24375758266449, agent episode variance: [21.23988285064697, 17.515656447410585, 19.488218284606933], time: 67.402
Running avgs for agent 0: q_loss: 2.7753453254699707, p_loss: 77.22132873535156, mean_rew: -5.396462476899676, variance: 84.95953140258788, mean_q: -77.52555847167969, std_q: 12.125617027282715
Running avgs for agent 1: q_loss: 2.456787109375, p_loss: 76.80874633789062, mean_rew: -5.407976690647509, variance: 70.06262578964234, mean_q: -77.15935516357422, std_q: 12.395162582397461
Running avgs for agent 2: q_loss: 2.586097240447998, p_loss: 76.27899932861328, mean_rew: -5.395879784820927, variance: 77.95287313842773, mean_q: -76.68280029296875, std_q: 12.217533111572266

steps: 449975, episodes: 18000, mean episode reward: -360.57519153489454, agent episode reward: [-120.19173051163149, -120.19173051163149, -120.19173051163149], time: 67.364
steps: 449975, episodes: 18000, mean episode variance: 62.47076512145996, agent episode variance: [21.556439102172853, 21.542950248718263, 19.371375770568847], time: 67.364
Running avgs for agent 0: q_loss: 2.7267210483551025, p_loss: 78.22676849365234, mean_rew: -5.372140419090691, variance: 86.22575640869141, mean_q: -78.52127838134766, std_q: 11.922220230102539
Running avgs for agent 1: q_loss: 2.794403314590454, p_loss: 77.7753677368164, mean_rew: -5.3604881169605525, variance: 86.17180099487305, mean_q: -78.10887145996094, std_q: 11.994233131408691
Running avgs for agent 2: q_loss: 2.542954206466675, p_loss: 77.2881851196289, mean_rew: -5.369105397540169, variance: 77.48550308227539, mean_q: -77.67530059814453, std_q: 11.868629455566406

steps: 474975, episodes: 19000, mean episode reward: -357.3328852899892, agent episode reward: [-119.11096176332973, -119.11096176332973, -119.11096176332973], time: 63.29
steps: 474975, episodes: 19000, mean episode variance: 63.04194592285156, agent episode variance: [21.747225814819338, 21.655806091308595, 19.638914016723632], time: 63.29
Running avgs for agent 0: q_loss: 2.700526475906372, p_loss: 79.11111450195312, mean_rew: -5.337211404154111, variance: 86.98890325927735, mean_q: -79.40069580078125, std_q: 11.756755828857422
Running avgs for agent 1: q_loss: 2.717623472213745, p_loss: 78.69951629638672, mean_rew: -5.340098850150056, variance: 86.62322436523438, mean_q: -79.02984619140625, std_q: 11.843123435974121
Running avgs for agent 2: q_loss: 2.5006792545318604, p_loss: 78.0570297241211, mean_rew: -5.334157995985048, variance: 78.55565606689453, mean_q: -78.43232727050781, std_q: 11.742220878601074

steps: 499975, episodes: 20000, mean episode reward: -356.4020896928982, agent episode reward: [-118.8006965642994, -118.8006965642994, -118.8006965642994], time: 64.805
steps: 499975, episodes: 20000, mean episode variance: 62.326094104766845, agent episode variance: [21.282117553710936, 21.48939702987671, 19.5545795211792], time: 64.806
Running avgs for agent 0: q_loss: 2.626351833343506, p_loss: 79.83464050292969, mean_rew: -5.312863157708318, variance: 85.12847021484374, mean_q: -80.11726379394531, std_q: 11.6658296585083
Running avgs for agent 1: q_loss: 2.6997294425964355, p_loss: 79.45372772216797, mean_rew: -5.308819950623227, variance: 85.95758811950684, mean_q: -79.77490997314453, std_q: 11.600975036621094
Running avgs for agent 2: q_loss: 2.4731647968292236, p_loss: 78.79889678955078, mean_rew: -5.311773051449849, variance: 78.2183180847168, mean_q: -79.16703033447266, std_q: 11.68110179901123

steps: 524975, episodes: 21000, mean episode reward: -356.65993861007706, agent episode reward: [-118.886646203359, -118.886646203359, -118.886646203359], time: 68.087
steps: 524975, episodes: 21000, mean episode variance: 62.722026573181154, agent episode variance: [21.741283309936524, 21.659158409118653, 19.321584854125977], time: 68.087
Running avgs for agent 0: q_loss: 2.5952320098876953, p_loss: 80.40923309326172, mean_rew: -5.277801073011471, variance: 86.9651332397461, mean_q: -80.68150329589844, std_q: 11.45411205291748
Running avgs for agent 1: q_loss: 2.6441493034362793, p_loss: 80.12574768066406, mean_rew: -5.2816390483138855, variance: 86.63663363647461, mean_q: -80.4432601928711, std_q: 11.292301177978516
Running avgs for agent 2: q_loss: 2.451903820037842, p_loss: 79.44269561767578, mean_rew: -5.286677220765287, variance: 77.2863394165039, mean_q: -79.80623626708984, std_q: 11.514554977416992

steps: 549975, episodes: 22000, mean episode reward: -355.8126699851484, agent episode reward: [-118.60422332838277, -118.60422332838277, -118.60422332838277], time: 68.984
steps: 549975, episodes: 22000, mean episode variance: 62.194053749084475, agent episode variance: [20.99600710296631, 21.860876953125, 19.337169692993164], time: 68.985
Running avgs for agent 0: q_loss: 2.5700595378875732, p_loss: 80.98767852783203, mean_rew: -5.2597388489758865, variance: 83.98402841186524, mean_q: -81.25872039794922, std_q: 11.290522575378418
Running avgs for agent 1: q_loss: 2.6134657859802246, p_loss: 80.5516128540039, mean_rew: -5.253639950793449, variance: 87.4435078125, mean_q: -80.86456298828125, std_q: 10.914302825927734
Running avgs for agent 2: q_loss: 2.41715669631958, p_loss: 79.95608520507812, mean_rew: -5.260380469648827, variance: 77.34867877197266, mean_q: -80.31407928466797, std_q: 11.10279655456543

steps: 574975, episodes: 23000, mean episode reward: -357.4731938594668, agent episode reward: [-119.15773128648894, -119.15773128648894, -119.15773128648894], time: 69.877
steps: 574975, episodes: 23000, mean episode variance: 61.43886898803711, agent episode variance: [20.698176155090334, 21.216935218811035, 19.523757614135743], time: 69.878
Running avgs for agent 0: q_loss: 2.5151402950286865, p_loss: 81.47222137451172, mean_rew: -5.241024132673587, variance: 82.79270462036133, mean_q: -81.7366714477539, std_q: 11.085289001464844
Running avgs for agent 1: q_loss: 2.5908329486846924, p_loss: 81.07444763183594, mean_rew: -5.232309660683895, variance: 84.86774087524414, mean_q: -81.38130950927734, std_q: 10.688804626464844
Running avgs for agent 2: q_loss: 2.404872417449951, p_loss: 80.41695404052734, mean_rew: -5.2388837318728525, variance: 78.09503045654297, mean_q: -80.77217864990234, std_q: 10.982958793640137

steps: 599975, episodes: 24000, mean episode reward: -356.13933004287134, agent episode reward: [-118.71311001429045, -118.71311001429045, -118.71311001429045], time: 75.151
steps: 599975, episodes: 24000, mean episode variance: 61.274373569488525, agent episode variance: [20.988348495483397, 21.24653826522827, 19.039486808776857], time: 75.152
Running avgs for agent 0: q_loss: 2.439329147338867, p_loss: 81.7892074584961, mean_rew: -5.216538125732895, variance: 83.95339398193359, mean_q: -82.05367279052734, std_q: 10.868210792541504
Running avgs for agent 1: q_loss: 2.562382221221924, p_loss: 81.46881103515625, mean_rew: -5.211153323936613, variance: 84.98615306091308, mean_q: -81.77033996582031, std_q: 10.649371147155762
Running avgs for agent 2: q_loss: 2.377675771713257, p_loss: 80.78394317626953, mean_rew: -5.211422954209265, variance: 76.15794723510743, mean_q: -81.12797546386719, std_q: 10.510641098022461

steps: 624975, episodes: 25000, mean episode reward: -357.17034695012865, agent episode reward: [-119.05678231670954, -119.05678231670954, -119.05678231670954], time: 72.207
steps: 624975, episodes: 25000, mean episode variance: 62.25965878295899, agent episode variance: [20.9792939453125, 21.488064292907715, 19.79230054473877], time: 72.207
Running avgs for agent 0: q_loss: 2.400054693222046, p_loss: 82.09320068359375, mean_rew: -5.196157786229623, variance: 83.91717578125, mean_q: -82.35606384277344, std_q: 10.762417793273926
Running avgs for agent 1: q_loss: 2.5138959884643555, p_loss: 81.78120422363281, mean_rew: -5.198163791871152, variance: 85.95225717163086, mean_q: -82.08004760742188, std_q: 10.390399932861328
Running avgs for agent 2: q_loss: 2.3920094966888428, p_loss: 81.07274627685547, mean_rew: -5.195590486270692, variance: 79.16920217895508, mean_q: -81.41690063476562, std_q: 10.284646034240723

steps: 649975, episodes: 26000, mean episode reward: -354.25348518304855, agent episode reward: [-118.08449506101617, -118.08449506101617, -118.08449506101617], time: 71.15
steps: 649975, episodes: 26000, mean episode variance: 61.5195080947876, agent episode variance: [20.219993225097657, 21.401097831726073, 19.898417037963867], time: 71.15
Running avgs for agent 0: q_loss: 2.3933255672454834, p_loss: 82.29193115234375, mean_rew: -5.174848152631166, variance: 80.87997290039063, mean_q: -82.55140686035156, std_q: 10.482982635498047
Running avgs for agent 1: q_loss: 2.4881768226623535, p_loss: 82.0280990600586, mean_rew: -5.178313614384988, variance: 85.60439132690429, mean_q: -82.33238220214844, std_q: 10.286458969116211
Running avgs for agent 2: q_loss: 2.3700473308563232, p_loss: 81.3404541015625, mean_rew: -5.184070363606659, variance: 79.59366815185547, mean_q: -81.67759704589844, std_q: 10.270025253295898

steps: 674975, episodes: 27000, mean episode reward: -354.054726066741, agent episode reward: [-118.018242022247, -118.018242022247, -118.018242022247], time: 70.742
steps: 674975, episodes: 27000, mean episode variance: 59.885502708435055, agent episode variance: [20.714855155944825, 20.435680419921876, 18.73496713256836], time: 70.743
Running avgs for agent 0: q_loss: 2.3265371322631836, p_loss: 82.49100494384766, mean_rew: -5.163162033821964, variance: 82.8594206237793, mean_q: -82.7450180053711, std_q: 10.537361145019531
Running avgs for agent 1: q_loss: 2.474843740463257, p_loss: 82.19912719726562, mean_rew: -5.156887474821432, variance: 81.7427216796875, mean_q: -82.49995422363281, std_q: 10.204867362976074
Running avgs for agent 2: q_loss: 2.3144707679748535, p_loss: 81.61305236816406, mean_rew: -5.163919746268796, variance: 74.93986853027344, mean_q: -81.94071960449219, std_q: 9.966111183166504

steps: 699975, episodes: 28000, mean episode reward: -355.92900566758624, agent episode reward: [-118.64300188919539, -118.64300188919539, -118.64300188919539], time: 72.661
steps: 699975, episodes: 28000, mean episode variance: 59.79770422744751, agent episode variance: [19.75129391479492, 20.82194203567505, 19.22446827697754], time: 72.661
Running avgs for agent 0: q_loss: 2.287421703338623, p_loss: 82.54351806640625, mean_rew: -5.139718722514688, variance: 79.00517565917968, mean_q: -82.80036926269531, std_q: 10.351333618164062
Running avgs for agent 1: q_loss: 2.460925579071045, p_loss: 82.34441375732422, mean_rew: -5.145274343573988, variance: 83.2877681427002, mean_q: -82.64070129394531, std_q: 10.015416145324707
Running avgs for agent 2: q_loss: 2.303715467453003, p_loss: 81.80193328857422, mean_rew: -5.144146834972284, variance: 76.89787310791016, mean_q: -82.13117218017578, std_q: 9.819252967834473

steps: 724975, episodes: 29000, mean episode reward: -354.4289868333048, agent episode reward: [-118.1429956111016, -118.1429956111016, -118.1429956111016], time: 74.955
steps: 724975, episodes: 29000, mean episode variance: 59.988248901367186, agent episode variance: [19.8684139251709, 20.987630043029785, 19.132204933166506], time: 74.955
Running avgs for agent 0: q_loss: 2.235379219055176, p_loss: 82.67305755615234, mean_rew: -5.1342799645370985, variance: 79.4736557006836, mean_q: -82.92620086669922, std_q: 10.390382766723633
Running avgs for agent 1: q_loss: 2.444052219390869, p_loss: 82.51495361328125, mean_rew: -5.137276630785141, variance: 83.95052017211914, mean_q: -82.81563568115234, std_q: 10.131863594055176
Running avgs for agent 2: q_loss: 2.326968193054199, p_loss: 81.9868392944336, mean_rew: -5.13121790732157, variance: 76.52881973266602, mean_q: -82.30941772460938, std_q: 9.640464782714844

steps: 749975, episodes: 30000, mean episode reward: -355.59457743231474, agent episode reward: [-118.5315258107716, -118.5315258107716, -118.5315258107716], time: 75.142
steps: 749975, episodes: 30000, mean episode variance: 58.57974375915527, agent episode variance: [19.52538272857666, 20.334699501037598, 18.719661529541014], time: 75.143
Running avgs for agent 0: q_loss: 2.192502498626709, p_loss: 82.70415496826172, mean_rew: -5.112681534923864, variance: 78.10153091430664, mean_q: -82.9510498046875, std_q: 10.272809028625488
Running avgs for agent 1: q_loss: 2.4169790744781494, p_loss: 82.6045913696289, mean_rew: -5.12207190800243, variance: 81.33879800415039, mean_q: -82.89547729492188, std_q: 10.111327171325684
Running avgs for agent 2: q_loss: 2.2683489322662354, p_loss: 82.18495178222656, mean_rew: -5.117722600768424, variance: 74.87864611816406, mean_q: -82.5000228881836, std_q: 9.599262237548828

steps: 774975, episodes: 31000, mean episode reward: -351.5884239184907, agent episode reward: [-117.19614130616358, -117.19614130616358, -117.19614130616358], time: 73.487
steps: 774975, episodes: 31000, mean episode variance: 58.27096997070313, agent episode variance: [19.42504711151123, 20.27228318786621, 18.573639671325683], time: 73.488
Running avgs for agent 0: q_loss: 2.1773126125335693, p_loss: 82.77491760253906, mean_rew: -5.107231654065979, variance: 77.70018844604492, mean_q: -83.01771545410156, std_q: 10.328768730163574
Running avgs for agent 1: q_loss: 2.401369571685791, p_loss: 82.67623138427734, mean_rew: -5.110638385320601, variance: 81.08913275146485, mean_q: -82.97602844238281, std_q: 9.978795051574707
Running avgs for agent 2: q_loss: 2.257938861846924, p_loss: 82.3851318359375, mean_rew: -5.110716267535252, variance: 74.29455868530273, mean_q: -82.6926498413086, std_q: 9.684762001037598

steps: 799975, episodes: 32000, mean episode reward: -352.38320657379086, agent episode reward: [-117.46106885793029, -117.46106885793029, -117.46106885793029], time: 71.742
steps: 799975, episodes: 32000, mean episode variance: 58.536694770812986, agent episode variance: [19.310981132507326, 20.455931427001953, 18.76978221130371], time: 71.743
Running avgs for agent 0: q_loss: 2.1500775814056396, p_loss: 82.798828125, mean_rew: -5.097596931346446, variance: 77.2439245300293, mean_q: -83.0349349975586, std_q: 9.982951164245605
Running avgs for agent 1: q_loss: 2.3731656074523926, p_loss: 82.62738800048828, mean_rew: -5.092269101526683, variance: 81.82372570800781, mean_q: -82.92266845703125, std_q: 9.676817893981934
Running avgs for agent 2: q_loss: 2.211374044418335, p_loss: 82.47845458984375, mean_rew: -5.093754784020358, variance: 75.07912884521484, mean_q: -82.78387451171875, std_q: 9.504413604736328

steps: 824975, episodes: 33000, mean episode reward: -351.49674602291265, agent episode reward: [-117.16558200763757, -117.16558200763757, -117.16558200763757], time: 71.466
steps: 824975, episodes: 33000, mean episode variance: 57.437740898132326, agent episode variance: [19.597873905181885, 19.68126692199707, 18.15860007095337], time: 71.467
Running avgs for agent 0: q_loss: 2.1297783851623535, p_loss: 82.74744415283203, mean_rew: -5.080007944993716, variance: 78.39149562072754, mean_q: -82.98099517822266, std_q: 10.02863883972168
Running avgs for agent 1: q_loss: 2.5257623195648193, p_loss: 82.66996002197266, mean_rew: -5.0851123106928915, variance: 78.72506768798829, mean_q: -82.95516967773438, std_q: 9.911216735839844
Running avgs for agent 2: q_loss: 2.1790707111358643, p_loss: 82.52568817138672, mean_rew: -5.078752856895124, variance: 72.63440028381348, mean_q: -82.83050537109375, std_q: 9.34624195098877

steps: 849975, episodes: 34000, mean episode reward: -352.87348030614817, agent episode reward: [-117.62449343538276, -117.62449343538276, -117.62449343538276], time: 72.246
steps: 849975, episodes: 34000, mean episode variance: 57.957256534576416, agent episode variance: [19.301392482757567, 20.279095893859864, 18.376768157958985], time: 72.246
Running avgs for agent 0: q_loss: 2.1333723068237305, p_loss: 82.69316864013672, mean_rew: -5.070138485549811, variance: 77.20556993103027, mean_q: -82.92684173583984, std_q: 9.843266487121582
Running avgs for agent 1: q_loss: 2.354665994644165, p_loss: 82.52766418457031, mean_rew: -5.069978478922869, variance: 81.11638357543946, mean_q: -82.80884552001953, std_q: 9.578357696533203
Running avgs for agent 2: q_loss: 2.1755659580230713, p_loss: 82.5826187133789, mean_rew: -5.067030030813047, variance: 73.50707263183594, mean_q: -82.88304901123047, std_q: 9.271718978881836

steps: 874975, episodes: 35000, mean episode reward: -353.12791903635576, agent episode reward: [-117.70930634545192, -117.70930634545192, -117.70930634545192], time: 72.451
steps: 874975, episodes: 35000, mean episode variance: 56.628686721801756, agent episode variance: [18.357330184936522, 19.811341346740722, 18.460015190124512], time: 72.451
Running avgs for agent 0: q_loss: 2.1022064685821533, p_loss: 82.6191635131836, mean_rew: -5.052341514164413, variance: 73.42932073974609, mean_q: -82.84552001953125, std_q: 9.730673789978027
Running avgs for agent 1: q_loss: 2.3150088787078857, p_loss: 82.48395538330078, mean_rew: -5.061661633913512, variance: 79.24536538696289, mean_q: -82.76268005371094, std_q: 9.701211929321289
Running avgs for agent 2: q_loss: 2.1823363304138184, p_loss: 82.59729766845703, mean_rew: -5.059654531584851, variance: 73.84006076049805, mean_q: -82.89752197265625, std_q: 9.297612190246582

steps: 899975, episodes: 36000, mean episode reward: -352.8836867103586, agent episode reward: [-117.62789557011955, -117.62789557011955, -117.62789557011955], time: 72.492
steps: 899975, episodes: 36000, mean episode variance: 56.15312097167969, agent episode variance: [18.70108229827881, 19.373480911254884, 18.078557762145998], time: 72.492
Running avgs for agent 0: q_loss: 2.106982469558716, p_loss: 82.61030578613281, mean_rew: -5.051871672148923, variance: 74.80432919311524, mean_q: -82.82825469970703, std_q: 9.651062965393066
Running avgs for agent 1: q_loss: 2.348193407058716, p_loss: 82.37350463867188, mean_rew: -5.049669554799403, variance: 77.49392364501954, mean_q: -82.65555572509766, std_q: 9.641335487365723
Running avgs for agent 2: q_loss: 2.2145659923553467, p_loss: 82.65465545654297, mean_rew: -5.052398766116879, variance: 72.31423104858399, mean_q: -82.95475006103516, std_q: 9.390915870666504

steps: 924975, episodes: 37000, mean episode reward: -348.6165350442745, agent episode reward: [-116.20551168142484, -116.20551168142484, -116.20551168142484], time: 71.574
steps: 924975, episodes: 37000, mean episode variance: 56.37548064804077, agent episode variance: [18.613911590576173, 19.748635116577148, 18.01293394088745], time: 71.575
Running avgs for agent 0: q_loss: 2.085768222808838, p_loss: 82.52888488769531, mean_rew: -5.037247783019657, variance: 74.45564636230469, mean_q: -82.74832153320312, std_q: 9.649192810058594
Running avgs for agent 1: q_loss: 2.301043748855591, p_loss: 82.17877960205078, mean_rew: -5.036304636138238, variance: 78.99454046630859, mean_q: -82.45922088623047, std_q: 9.431517601013184
Running avgs for agent 2: q_loss: 2.1271581649780273, p_loss: 82.60181427001953, mean_rew: -5.037647617284509, variance: 72.0517357635498, mean_q: -82.89751434326172, std_q: 9.191943168640137

steps: 949975, episodes: 38000, mean episode reward: -350.23119164564554, agent episode reward: [-116.74373054854851, -116.74373054854851, -116.74373054854851], time: 70.837
steps: 949975, episodes: 38000, mean episode variance: 55.43309077835083, agent episode variance: [18.27886777496338, 19.03622211456299, 18.11800088882446], time: 70.838
Running avgs for agent 0: q_loss: 2.0459980964660645, p_loss: 82.45006561279297, mean_rew: -5.030525310960868, variance: 73.11547109985352, mean_q: -82.66268157958984, std_q: 9.555337905883789
Running avgs for agent 1: q_loss: 2.3092281818389893, p_loss: 82.1285171508789, mean_rew: -5.032558399979294, variance: 76.14488845825196, mean_q: -82.40425872802734, std_q: 9.432883262634277
Running avgs for agent 2: q_loss: 2.1453933715820312, p_loss: 82.5440444946289, mean_rew: -5.030694254059409, variance: 72.47200355529785, mean_q: -82.83865356445312, std_q: 9.281648635864258

steps: 974975, episodes: 39000, mean episode reward: -349.8856872721379, agent episode reward: [-116.62856242404597, -116.62856242404597, -116.62856242404597], time: 71.175
steps: 974975, episodes: 39000, mean episode variance: 54.62943322372436, agent episode variance: [18.11990640258789, 18.94512813949585, 17.564398681640625], time: 71.176
Running avgs for agent 0: q_loss: 2.0423150062561035, p_loss: 82.35086822509766, mean_rew: -5.022245355501697, variance: 72.47962561035156, mean_q: -82.56301879882812, std_q: 9.583401679992676
Running avgs for agent 1: q_loss: 2.2855749130249023, p_loss: 81.93165588378906, mean_rew: -5.020086781979214, variance: 75.7805125579834, mean_q: -82.20606231689453, std_q: 9.45061206817627
Running avgs for agent 2: q_loss: 2.0610129833221436, p_loss: 82.4673843383789, mean_rew: -5.017110439143511, variance: 70.2575947265625, mean_q: -82.760498046875, std_q: 9.090002059936523

steps: 999975, episodes: 40000, mean episode reward: -346.31383183316706, agent episode reward: [-115.43794394438903, -115.43794394438903, -115.43794394438903], time: 80.924
steps: 999975, episodes: 40000, mean episode variance: 49.04849308252334, agent episode variance: [18.084468955993653, 18.67709223175049, 12.286931894779205], time: 80.925
Running avgs for agent 0: q_loss: 2.0576722621917725, p_loss: 82.16838836669922, mean_rew: -5.005870045011567, variance: 72.33787582397461, mean_q: -82.37635803222656, std_q: 9.363176345825195
Running avgs for agent 1: q_loss: 2.202305793762207, p_loss: 81.782958984375, mean_rew: -5.012061232269471, variance: 74.70836892700196, mean_q: -82.06342315673828, std_q: 9.208192825317383
Running avgs for agent 2: q_loss: 1.8253705501556396, p_loss: 82.45846557617188, mean_rew: -5.013361371841706, variance: 49.14772757911682, mean_q: -82.7528305053711, std_q: 9.193134307861328

steps: 1024975, episodes: 41000, mean episode reward: -347.1427142315407, agent episode reward: [-115.71423807718023, -115.71423807718023, -115.71423807718023], time: 70.842
steps: 1024975, episodes: 41000, mean episode variance: 45.655844564437864, agent episode variance: [17.492645378112794, 18.788050941467286, 9.375148244857789], time: 70.843
Running avgs for agent 0: q_loss: 2.028656005859375, p_loss: 81.91000366210938, mean_rew: -4.977042930366379, variance: 69.97058151245118, mean_q: -82.11328125, std_q: 9.368891716003418
Running avgs for agent 1: q_loss: 2.1659231185913086, p_loss: 81.46951293945312, mean_rew: -4.975989139074078, variance: 75.15220376586915, mean_q: -81.73731994628906, std_q: 9.121527671813965
Running avgs for agent 2: q_loss: 1.5448122024536133, p_loss: 82.2293930053711, mean_rew: -4.976635467288557, variance: 37.500592979431154, mean_q: -82.51398468017578, std_q: 8.932623863220215

steps: 1049975, episodes: 42000, mean episode reward: -346.60054584223013, agent episode reward: [-115.53351528074339, -115.53351528074339, -115.53351528074339], time: 71.077
steps: 1049975, episodes: 42000, mean episode variance: 44.4458818283081, agent episode variance: [17.12776075744629, 17.76326146697998, 9.554859603881836], time: 71.077
Running avgs for agent 0: q_loss: 1.9010605812072754, p_loss: 81.18258666992188, mean_rew: -4.893256552901566, variance: 68.51104302978516, mean_q: -81.35626983642578, std_q: 7.256693363189697
Running avgs for agent 1: q_loss: 2.0654447078704834, p_loss: 80.80210876464844, mean_rew: -4.892646541304417, variance: 71.05304586791992, mean_q: -81.03121948242188, std_q: 7.157925128936768
Running avgs for agent 2: q_loss: 1.4010025262832642, p_loss: 81.56011199951172, mean_rew: -4.89107615276295, variance: 38.21943841552734, mean_q: -81.81668090820312, std_q: 6.813884735107422

steps: 1074975, episodes: 43000, mean episode reward: -347.54782759049067, agent episode reward: [-115.84927586349691, -115.84927586349691, -115.84927586349691], time: 70.68
steps: 1074975, episodes: 43000, mean episode variance: 43.635094818115235, agent episode variance: [16.733741531372072, 17.58051695251465, 9.320836334228515], time: 70.68
Running avgs for agent 0: q_loss: 1.7909494638442993, p_loss: 80.70562744140625, mean_rew: -4.830053872224707, variance: 66.93496612548829, mean_q: -80.85780334472656, std_q: 6.372964859008789
Running avgs for agent 1: q_loss: 1.9570841789245605, p_loss: 80.39508056640625, mean_rew: -4.839487218256025, variance: 70.3220678100586, mean_q: -80.59993743896484, std_q: 6.162901401519775
Running avgs for agent 2: q_loss: 1.3291445970535278, p_loss: 81.13035583496094, mean_rew: -4.833850089910358, variance: 37.28334533691406, mean_q: -81.36644744873047, std_q: 5.828885078430176

steps: 1099975, episodes: 44000, mean episode reward: -346.9651903295625, agent episode reward: [-115.65506344318749, -115.65506344318749, -115.65506344318749], time: 69.882
steps: 1099975, episodes: 44000, mean episode variance: 42.380783107757566, agent episode variance: [16.159276992797853, 17.114692558288574, 9.106813556671142], time: 69.882
Running avgs for agent 0: q_loss: 1.7305073738098145, p_loss: 80.4776611328125, mean_rew: -4.797966361593957, variance: 64.63710797119141, mean_q: -80.62602996826172, std_q: 6.298290729522705
Running avgs for agent 1: q_loss: 1.8697069883346558, p_loss: 80.20610809326172, mean_rew: -4.79900258067385, variance: 68.4587702331543, mean_q: -80.39645385742188, std_q: 6.04102897644043
Running avgs for agent 2: q_loss: 1.2800912857055664, p_loss: 80.93233489990234, mean_rew: -4.8023042155713425, variance: 36.42725422668457, mean_q: -81.15495300292969, std_q: 5.709605693817139

steps: 1124975, episodes: 45000, mean episode reward: -348.8076859160398, agent episode reward: [-116.26922863867992, -116.26922863867992, -116.26922863867992], time: 83.115
steps: 1124975, episodes: 45000, mean episode variance: 42.38340481567383, agent episode variance: [16.14727838897705, 16.669493431091308, 9.566632995605469], time: 83.116
Running avgs for agent 0: q_loss: 1.6994754076004028, p_loss: 80.3121566772461, mean_rew: -4.781777721656985, variance: 64.5891135559082, mean_q: -80.4541244506836, std_q: 6.300724983215332
Running avgs for agent 1: q_loss: 1.8282307386398315, p_loss: 80.10894012451172, mean_rew: -4.779168655924835, variance: 66.67797372436523, mean_q: -80.29023742675781, std_q: 5.994241237640381
Running avgs for agent 2: q_loss: 1.227475881576538, p_loss: 80.77027893066406, mean_rew: -4.780110600700136, variance: 38.266531982421874, mean_q: -80.98780059814453, std_q: 5.687501430511475

steps: 1149975, episodes: 46000, mean episode reward: -348.1589542248113, agent episode reward: [-116.05298474160375, -116.05298474160375, -116.05298474160375], time: 113.803
steps: 1149975, episodes: 46000, mean episode variance: 42.11689437294007, agent episode variance: [16.051035388946534, 16.759487075805666, 9.306371908187867], time: 113.804
Running avgs for agent 0: q_loss: 1.649017572402954, p_loss: 80.14875793457031, mean_rew: -4.769900149404251, variance: 64.20414155578614, mean_q: -80.28351593017578, std_q: 6.289571285247803
Running avgs for agent 1: q_loss: 1.7984389066696167, p_loss: 80.0172119140625, mean_rew: -4.766725968482475, variance: 67.03794830322266, mean_q: -80.19158172607422, std_q: 5.978786945343018
Running avgs for agent 2: q_loss: 1.2129040956497192, p_loss: 80.65042114257812, mean_rew: -4.76366002537913, variance: 37.22548763275147, mean_q: -80.86184692382812, std_q: 5.676659107208252

steps: 1174975, episodes: 47000, mean episode reward: -346.7989522084901, agent episode reward: [-115.59965073616338, -115.59965073616338, -115.59965073616338], time: 104.121
steps: 1174975, episodes: 47000, mean episode variance: 41.29984962463379, agent episode variance: [15.37094171142578, 16.625721118927, 9.303186794281006], time: 104.122
Running avgs for agent 0: q_loss: 1.6223745346069336, p_loss: 79.95165252685547, mean_rew: -4.747149237158857, variance: 61.48376684570312, mean_q: -80.08015441894531, std_q: 6.219226360321045
Running avgs for agent 1: q_loss: 1.7601464986801147, p_loss: 79.93034362792969, mean_rew: -4.756190066073919, variance: 66.502884475708, mean_q: -80.10231018066406, std_q: 5.951481819152832
Running avgs for agent 2: q_loss: 1.1966475248336792, p_loss: 80.5498275756836, mean_rew: -4.7537361996239795, variance: 37.21274717712402, mean_q: -80.75656127929688, std_q: 5.636231422424316

steps: 1199975, episodes: 48000, mean episode reward: -346.74803848913604, agent episode reward: [-115.58267949637867, -115.58267949637867, -115.58267949637867], time: 72.821
steps: 1199975, episodes: 48000, mean episode variance: 40.968778802871704, agent episode variance: [15.634938194274902, 16.311582878112795, 9.022257730484009], time: 72.821
Running avgs for agent 0: q_loss: 1.608527421951294, p_loss: 79.7846908569336, mean_rew: -4.742466975496307, variance: 62.53975277709961, mean_q: -79.9092025756836, std_q: 6.2361063957214355
Running avgs for agent 1: q_loss: 1.7426247596740723, p_loss: 79.76567840576172, mean_rew: -4.739572607619627, variance: 65.24633151245118, mean_q: -79.93353271484375, std_q: 5.879024982452393
Running avgs for agent 2: q_loss: 1.1851160526275635, p_loss: 80.42903137207031, mean_rew: -4.739670197004384, variance: 36.089030921936036, mean_q: -80.62377166748047, std_q: 5.620884418487549

steps: 1224975, episodes: 49000, mean episode reward: -346.71174326941104, agent episode reward: [-115.57058108980367, -115.57058108980367, -115.57058108980367], time: 74.193
steps: 1224975, episodes: 49000, mean episode variance: 41.18273708343506, agent episode variance: [15.316996429443359, 16.584876541137696, 9.280864112854005], time: 74.194
Running avgs for agent 0: q_loss: 1.5908167362213135, p_loss: 79.66300201416016, mean_rew: -4.735121928975267, variance: 61.267985717773435, mean_q: -79.78118133544922, std_q: 6.1802520751953125
Running avgs for agent 1: q_loss: 1.6982511281967163, p_loss: 79.64432525634766, mean_rew: -4.736315742722958, variance: 66.33950616455078, mean_q: -79.80606079101562, std_q: 5.907308578491211
Running avgs for agent 2: q_loss: 1.1663042306900024, p_loss: 80.31937408447266, mean_rew: -4.735336771463875, variance: 37.12345645141602, mean_q: -80.51020812988281, std_q: 5.630315780639648

steps: 1249975, episodes: 50000, mean episode reward: -348.3826022840085, agent episode reward: [-116.12753409466947, -116.12753409466947, -116.12753409466947], time: 74.308
steps: 1249975, episodes: 50000, mean episode variance: 39.75723466491699, agent episode variance: [14.833216396331787, 15.99654303741455, 8.927475231170654], time: 74.308
Running avgs for agent 0: q_loss: 1.576694130897522, p_loss: 79.5416488647461, mean_rew: -4.72452598081874, variance: 59.33286558532715, mean_q: -79.65489196777344, std_q: 6.142579078674316
Running avgs for agent 1: q_loss: 1.6726549863815308, p_loss: 79.5112533569336, mean_rew: -4.727627974207822, variance: 63.9861721496582, mean_q: -79.66897583007812, std_q: 5.881353855133057
Running avgs for agent 2: q_loss: 1.1504814624786377, p_loss: 80.21050262451172, mean_rew: -4.727797436996428, variance: 35.709900924682614, mean_q: -80.40059661865234, std_q: 5.683609485626221

steps: 1274975, episodes: 51000, mean episode reward: -347.25958766597984, agent episode reward: [-115.75319588865993, -115.75319588865993, -115.75319588865993], time: 73.227
steps: 1274975, episodes: 51000, mean episode variance: 39.805762517929075, agent episode variance: [15.125945297241211, 15.779529022216797, 8.900288198471069], time: 73.228
Running avgs for agent 0: q_loss: 1.549269676208496, p_loss: 79.4281997680664, mean_rew: -4.720488210066551, variance: 60.503781188964844, mean_q: -79.5391845703125, std_q: 6.156380653381348
Running avgs for agent 1: q_loss: 1.6631956100463867, p_loss: 79.40290832519531, mean_rew: -4.721663379315409, variance: 63.11811608886719, mean_q: -79.55599975585938, std_q: 5.885737419128418
Running avgs for agent 2: q_loss: 1.1509923934936523, p_loss: 80.05653381347656, mean_rew: -4.716566011562372, variance: 35.601152793884275, mean_q: -80.24315643310547, std_q: 5.679508686065674

steps: 1299975, episodes: 52000, mean episode reward: -344.5515785734298, agent episode reward: [-114.85052619114329, -114.85052619114329, -114.85052619114329], time: 74.036
steps: 1299975, episodes: 52000, mean episode variance: 39.425292427062985, agent episode variance: [14.816801776885987, 15.700894737243653, 8.907595912933349], time: 74.036
Running avgs for agent 0: q_loss: 1.5379400253295898, p_loss: 79.33673095703125, mean_rew: -4.7189207443355405, variance: 59.267207107543946, mean_q: -79.44817352294922, std_q: 6.166707992553711
Running avgs for agent 1: q_loss: 1.6429330110549927, p_loss: 79.34367370605469, mean_rew: -4.7175955475912605, variance: 62.80357894897461, mean_q: -79.49009704589844, std_q: 5.894514560699463
Running avgs for agent 2: q_loss: 1.1516927480697632, p_loss: 79.9686279296875, mean_rew: -4.716978316963254, variance: 35.630383651733396, mean_q: -80.15540313720703, std_q: 5.737767696380615

steps: 1324975, episodes: 53000, mean episode reward: -347.30507783169406, agent episode reward: [-115.76835927723134, -115.76835927723134, -115.76835927723134], time: 74.641
steps: 1324975, episodes: 53000, mean episode variance: 39.15520432662964, agent episode variance: [14.70551692199707, 15.577772495269775, 8.871914909362793], time: 74.642
Running avgs for agent 0: q_loss: 1.5226856470108032, p_loss: 79.21189880371094, mean_rew: -4.709375622035033, variance: 58.82206768798828, mean_q: -79.32233428955078, std_q: 6.173264026641846
Running avgs for agent 1: q_loss: 1.6176327466964722, p_loss: 79.23522186279297, mean_rew: -4.705722733492333, variance: 62.3110899810791, mean_q: -79.37884521484375, std_q: 5.827981472015381
Running avgs for agent 2: q_loss: 1.14762282371521, p_loss: 79.79972076416016, mean_rew: -4.702186734946454, variance: 35.48765963745117, mean_q: -79.98428344726562, std_q: 5.737685680389404

steps: 1349975, episodes: 54000, mean episode reward: -345.9706457315412, agent episode reward: [-115.3235485771804, -115.3235485771804, -115.3235485771804], time: 71.8
steps: 1349975, episodes: 54000, mean episode variance: 38.989405937194825, agent episode variance: [14.912144412994385, 15.438815773010253, 8.638445751190186], time: 71.801
Running avgs for agent 0: q_loss: 1.5196677446365356, p_loss: 79.07698059082031, mean_rew: -4.701226181696539, variance: 59.64857765197754, mean_q: -79.18576049804688, std_q: 6.152079105377197
Running avgs for agent 1: q_loss: 1.619598150253296, p_loss: 79.15929412841797, mean_rew: -4.7046428673327645, variance: 61.75526309204101, mean_q: -79.3025894165039, std_q: 5.834739685058594
Running avgs for agent 2: q_loss: 1.1610500812530518, p_loss: 79.68394470214844, mean_rew: -4.703847600839904, variance: 34.553783004760746, mean_q: -79.86751556396484, std_q: 5.781238079071045

steps: 1374975, episodes: 55000, mean episode reward: -345.361428920779, agent episode reward: [-115.12047630692635, -115.12047630692635, -115.12047630692635], time: 81.667
steps: 1374975, episodes: 55000, mean episode variance: 38.93825120162964, agent episode variance: [14.47270655822754, 15.511305152893067, 8.954239490509034], time: 81.667
Running avgs for agent 0: q_loss: 1.5228723287582397, p_loss: 78.97190856933594, mean_rew: -4.6975776626876256, variance: 57.89082623291016, mean_q: -79.08074951171875, std_q: 6.123627662658691
Running avgs for agent 1: q_loss: 1.593727469444275, p_loss: 79.0221939086914, mean_rew: -4.696274091862388, variance: 62.04522061157227, mean_q: -79.16436767578125, std_q: 5.8357834815979
Running avgs for agent 2: q_loss: 1.1543229818344116, p_loss: 79.49047088623047, mean_rew: -4.6967848498808085, variance: 35.816957962036135, mean_q: -79.67061614990234, std_q: 5.819045066833496

steps: 1399975, episodes: 56000, mean episode reward: -343.00482446621345, agent episode reward: [-114.33494148873783, -114.33494148873783, -114.33494148873783], time: 78.597
steps: 1399975, episodes: 56000, mean episode variance: 38.619037284851075, agent episode variance: [14.61409903717041, 15.323084335327149, 8.681853912353516], time: 78.598
Running avgs for agent 0: q_loss: 1.5183823108673096, p_loss: 78.8983154296875, mean_rew: -4.693702494544488, variance: 58.45639614868164, mean_q: -79.00700378417969, std_q: 6.156909942626953
Running avgs for agent 1: q_loss: 1.5805599689483643, p_loss: 78.8984375, mean_rew: -4.692148373983505, variance: 61.292337341308595, mean_q: -79.03770446777344, std_q: 5.8399248123168945
Running avgs for agent 2: q_loss: 1.1276671886444092, p_loss: 79.35540771484375, mean_rew: -4.69085224623569, variance: 34.72741564941406, mean_q: -79.53369903564453, std_q: 5.848942279815674

steps: 1424975, episodes: 57000, mean episode reward: -344.68346586675267, agent episode reward: [-114.89448862225089, -114.89448862225089, -114.89448862225089], time: 75.899
steps: 1424975, episodes: 57000, mean episode variance: 37.933928253173825, agent episode variance: [14.731951438903808, 14.766883247375489, 8.435093566894531], time: 75.9
Running avgs for agent 0: q_loss: 1.489365816116333, p_loss: 78.70426940917969, mean_rew: -4.685161327114611, variance: 58.92780575561523, mean_q: -78.81222534179688, std_q: 6.1564226150512695
Running avgs for agent 1: q_loss: 1.5651971101760864, p_loss: 78.76443481445312, mean_rew: -4.683581962348153, variance: 59.067532989501956, mean_q: -78.90193176269531, std_q: 5.840780735015869
Running avgs for agent 2: q_loss: 1.1418150663375854, p_loss: 79.20890808105469, mean_rew: -4.683188074163345, variance: 33.740374267578126, mean_q: -79.3820571899414, std_q: 5.870482921600342

steps: 1449975, episodes: 58000, mean episode reward: -341.740520941765, agent episode reward: [-113.91350698058832, -113.91350698058832, -113.91350698058832], time: 75.659
steps: 1449975, episodes: 58000, mean episode variance: 38.1948856048584, agent episode variance: [14.26965726852417, 15.098212432861327, 8.827015903472901], time: 75.659
Running avgs for agent 0: q_loss: 1.5019025802612305, p_loss: 78.59601593017578, mean_rew: -4.677497862622014, variance: 57.07862907409668, mean_q: -78.70030975341797, std_q: 6.154571056365967
Running avgs for agent 1: q_loss: 1.5441635847091675, p_loss: 78.59423065185547, mean_rew: -4.679412320432304, variance: 60.39284973144531, mean_q: -78.73389434814453, std_q: 5.847159385681152
Running avgs for agent 2: q_loss: 1.1307109594345093, p_loss: 79.04000091552734, mean_rew: -4.681126657382942, variance: 35.308063613891605, mean_q: -79.21493530273438, std_q: 5.888643741607666

steps: 1474975, episodes: 59000, mean episode reward: -341.73003410291244, agent episode reward: [-113.91001136763747, -113.91001136763747, -113.91001136763747], time: 78.112
steps: 1474975, episodes: 59000, mean episode variance: 37.75581135559082, agent episode variance: [14.144701984405518, 14.742217823028565, 8.868891548156737], time: 78.113
Running avgs for agent 0: q_loss: 1.477961540222168, p_loss: 78.47420501708984, mean_rew: -4.67719380939198, variance: 56.578807937622074, mean_q: -78.57987976074219, std_q: 6.191593647003174
Running avgs for agent 1: q_loss: 1.5343443155288696, p_loss: 78.4850845336914, mean_rew: -4.674465536236603, variance: 58.96887129211426, mean_q: -78.62203979492188, std_q: 5.860100746154785
Running avgs for agent 2: q_loss: 1.100342035293579, p_loss: 78.85415649414062, mean_rew: -4.668785739921619, variance: 35.47556619262695, mean_q: -79.01786804199219, std_q: 5.894792079925537

steps: 1499975, episodes: 60000, mean episode reward: -341.67377442619716, agent episode reward: [-113.89125814206572, -113.89125814206572, -113.89125814206572], time: 76.143
steps: 1499975, episodes: 60000, mean episode variance: 36.99655911064148, agent episode variance: [14.147729610443115, 14.578699768066405, 8.270129732131958], time: 76.144
Running avgs for agent 0: q_loss: 1.475457787513733, p_loss: 78.30554962158203, mean_rew: -4.669707323263838, variance: 56.59091844177246, mean_q: -78.41500091552734, std_q: 6.216735363006592
Running avgs for agent 1: q_loss: 1.52738356590271, p_loss: 78.43365478515625, mean_rew: -4.673017773324069, variance: 58.31479907226562, mean_q: -78.56839752197266, std_q: 5.8552045822143555
Running avgs for agent 2: q_loss: 1.1247014999389648, p_loss: 78.7610855102539, mean_rew: -4.669559624162348, variance: 33.08051892852783, mean_q: -78.92462158203125, std_q: 5.938881874084473

...Finished total of 60001 episodes... Now freezing policy and running for 10000 more episodes to get cvar estimation
steps: 24975, episodes: 1000, mean episode reward: -340.73449794453325, agent episode reward: [-113.57816598151109, -113.57816598151109, -113.57816598151109], time: 53.21
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 53.211
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -340.65382206010315, agent episode reward: [-113.55127402003441, -113.55127402003441, -113.55127402003441], time: 67.088
steps: 49975, episodes: 2000, mean episode variance: 51.985179862976075, agent episode variance: [0.0, 30.14905677032471, 21.836123092651366], time: 67.088
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.547058267293068, variance: 0.0, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -78.48503112792969, std_q: 6.2844767570495605
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.547185110326607, variance: 123.56170654296875, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -77.20454406738281, std_q: 5.775175094604492
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.541731066321057, variance: 89.49230194091797, cvar: -3.9979302883148193, v: -3.9979302883148193, mean_q: -77.71611022949219, std_q: 5.869213581085205

steps: 74975, episodes: 3000, mean episode reward: -339.08240774174584, agent episode reward: [-113.0274692472486, -113.0274692472486, -113.0274692472486], time: 67.112
steps: 74975, episodes: 3000, mean episode variance: 52.82036227416992, agent episode variance: [0.0, 30.70213603210449, 22.11822624206543], time: 67.113
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.538794052115189, variance: 0.0, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -78.40816497802734, std_q: 6.250983238220215
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.54656306601959, variance: 122.80854797363281, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -77.14590454101562, std_q: 5.818881034851074
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.539282256084804, variance: 88.47290802001953, cvar: -9.752158164978027, v: -9.752158164978027, mean_q: -77.66301727294922, std_q: 5.879004001617432

steps: 99975, episodes: 4000, mean episode reward: -337.6336671043385, agent episode reward: [-112.54455570144616, -112.54455570144616, -112.54455570144616], time: 66.602
steps: 99975, episodes: 4000, mean episode variance: 52.412021911621096, agent episode variance: [0.0, 30.406917137145996, 22.005104774475097], time: 66.603
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.530726943012716, variance: 0.0, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -78.39900970458984, std_q: 6.22601842880249
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.536400492741932, variance: 121.62767028808594, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -77.12528991699219, std_q: 5.802506923675537
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.533359356940608, variance: 88.02041625976562, cvar: -13.811443328857422, v: -13.811443328857422, mean_q: -77.67570495605469, std_q: 5.866638660430908

steps: 124975, episodes: 5000, mean episode reward: -340.14737046999875, agent episode reward: [-113.3824568233329, -113.3824568233329, -113.3824568233329], time: 66.928
steps: 124975, episodes: 5000, mean episode variance: 52.3963021774292, agent episode variance: [0.0, 30.364777641296385, 22.031524536132814], time: 66.928
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.535792020750303, variance: 0.0, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -78.45071411132812, std_q: 6.235229969024658
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.529153941862909, variance: 121.4591064453125, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -77.09764099121094, std_q: 5.744750499725342
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.535998849587915, variance: 88.12609100341797, cvar: -17.280132293701172, v: -17.280132293701172, mean_q: -77.69371795654297, std_q: 5.894415855407715

steps: 149975, episodes: 6000, mean episode reward: -340.7244428847923, agent episode reward: [-113.57481429493079, -113.57481429493079, -113.57481429493079], time: 67.204
steps: 149975, episodes: 6000, mean episode variance: 52.79726678466797, agent episode variance: [0.0, 30.45167096710205, 22.34559581756592], time: 67.205
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.535480459696126, variance: 0.0, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -78.47184753417969, std_q: 6.252119541168213
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.532431923199669, variance: 121.80668640136719, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -77.12142181396484, std_q: 5.7797322273254395
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.538499698315431, variance: 89.38239288330078, cvar: -20.437917709350586, v: -20.437917709350586, mean_q: -77.67922973632812, std_q: 5.892014503479004

steps: 174975, episodes: 7000, mean episode reward: -339.1066310282062, agent episode reward: [-113.03554367606873, -113.03554367606873, -113.03554367606873], time: 68.297
steps: 174975, episodes: 7000, mean episode variance: 52.73033502197266, agent episode variance: [0.0, 30.596110107421875, 22.13422491455078], time: 68.297
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5381874507746165, variance: 0.0, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -78.51460266113281, std_q: 6.263092994689941
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.536554628506066, variance: 122.38443756103516, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -77.14482879638672, std_q: 5.802881240844727
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.532238714294076, variance: 88.53689575195312, cvar: -23.40530014038086, v: -23.40530014038086, mean_q: -77.68780517578125, std_q: 5.868936538696289

steps: 199975, episodes: 8000, mean episode reward: -340.13946265734467, agent episode reward: [-113.37982088578157, -113.37982088578157, -113.37982088578157], time: 69.947
steps: 199975, episodes: 8000, mean episode variance: 52.589150260925294, agent episode variance: [0.0, 30.449971466064454, 22.13917879486084], time: 69.947
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.530579498874344, variance: 0.0, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -78.4664077758789, std_q: 6.224140644073486
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.534199219636706, variance: 121.79988098144531, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -77.14637756347656, std_q: 5.775115013122559
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.531378181409237, variance: 88.55671691894531, cvar: -26.246538162231445, v: -26.246538162231445, mean_q: -77.68321990966797, std_q: 5.8650593757629395

steps: 224975, episodes: 9000, mean episode reward: -340.4693690962678, agent episode reward: [-113.48978969875596, -113.48978969875596, -113.48978969875596], time: 66.267
steps: 224975, episodes: 9000, mean episode variance: 52.80428720855713, agent episode variance: [0.0, 30.536444664001465, 22.267842544555663], time: 66.267
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.535146859279935, variance: 0.0, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -78.47789001464844, std_q: 6.237977981567383
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.533222481037145, variance: 122.14578247070312, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -77.13739013671875, std_q: 5.791356563568115
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.539860250840629, variance: 89.07136535644531, cvar: -29.000120162963867, v: -29.000120162963867, mean_q: -77.71634674072266, std_q: 5.8627753257751465

steps: 249975, episodes: 10000, mean episode reward: -339.0954573515969, agent episode reward: [-113.03181911719898, -113.03181911719898, -113.03181911719898], time: 63.147
steps: 249975, episodes: 10000, mean episode variance: 52.63473432159424, agent episode variance: [0.0, 30.467268928527833, 22.167465393066408], time: 63.148
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.534486195794827, variance: 0.0, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -78.47869873046875, std_q: 6.2565388679504395
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.540090345453067, variance: 121.86907958984375, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -77.14714050292969, std_q: 5.78396463394165
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.532502799573789, variance: 88.66986083984375, cvar: -31.69087028503418, v: -31.69087028503418, mean_q: -77.68392181396484, std_q: 5.851887226104736

steps: 274975, episodes: 11000, mean episode reward: -338.36222258618545, agent episode reward: [-112.7874075287285, -112.7874075287285, -112.7874075287285], time: 64.007
steps: 274975, episodes: 11000, mean episode variance: 52.48210735321045, agent episode variance: [0.0, 30.2945682220459, 22.187539131164552], time: 64.008
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.531536034596889, variance: 0.0, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -78.47840881347656, std_q: 6.222700119018555
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.534513908095523, variance: 121.17827606201172, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -77.13216400146484, std_q: 5.785998821258545
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.533944203218041, variance: 88.75015258789062, cvar: -34.33561706542969, v: -34.33561706542969, mean_q: -77.71282958984375, std_q: 5.892247200012207

steps: 299975, episodes: 12000, mean episode reward: -342.66524515736313, agent episode reward: [-114.2217483857877, -114.2217483857877, -114.2217483857877], time: 64.37
steps: 299975, episodes: 12000, mean episode variance: 52.39481785583496, agent episode variance: [0.0, 30.28889762878418, 22.10592022705078], time: 64.371
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.535897890860942, variance: 0.0, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -78.47177124023438, std_q: 6.276570796966553
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.535750356165044, variance: 121.15559387207031, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -77.13552856445312, std_q: 5.800055980682373
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.53417820762727, variance: 88.4236831665039, cvar: -36.94614791870117, v: -36.94614791870117, mean_q: -77.70694732666016, std_q: 5.853023529052734

steps: 324975, episodes: 13000, mean episode reward: -339.5681370951748, agent episode reward: [-113.18937903172494, -113.18937903172494, -113.18937903172494], time: 63.664
steps: 324975, episodes: 13000, mean episode variance: 52.24472958374024, agent episode variance: [0.0, 30.188423881530763, 22.05630570220947], time: 63.664
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.535172116699544, variance: 0.0, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -78.48938751220703, std_q: 6.269796848297119
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.534678030650093, variance: 120.75369262695312, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -77.14574432373047, std_q: 5.795024871826172
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.537856029543231, variance: 88.2252197265625, cvar: -39.53093719482422, v: -39.53093719482422, mean_q: -77.7425765991211, std_q: 5.878814220428467

steps: 349975, episodes: 14000, mean episode reward: -339.2779282375162, agent episode reward: [-113.09264274583873, -113.09264274583873, -113.09264274583873], time: 64.773
steps: 349975, episodes: 14000, mean episode variance: 52.33355233001709, agent episode variance: [0.0, 30.253418006896972, 22.080134323120117], time: 64.773
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.542123613225076, variance: 0.0, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -78.51762390136719, std_q: 6.286142349243164
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.534210235449311, variance: 121.013671875, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -77.15377807617188, std_q: 5.821342945098877
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.532289598964577, variance: 88.32054138183594, cvar: -42.09621810913086, v: -42.09621810913086, mean_q: -77.71056365966797, std_q: 5.851138591766357

steps: 374975, episodes: 15000, mean episode reward: -339.0039033665099, agent episode reward: [-113.00130112217, -113.00130112217, -113.00130112217], time: 63.92
steps: 374975, episodes: 15000, mean episode variance: 52.32055392456055, agent episode variance: [0.0, 30.193532539367677, 22.127021385192872], time: 63.921
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.535185886169982, variance: 0.0, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -78.48880767822266, std_q: 6.285470008850098
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.533272031528923, variance: 120.77413177490234, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -77.14617919921875, std_q: 5.827180862426758
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.532490268886626, variance: 88.50808715820312, cvar: -44.6466064453125, v: -44.6466064453125, mean_q: -77.71971130371094, std_q: 5.862033843994141

steps: 399975, episodes: 16000, mean episode reward: -341.3101744646072, agent episode reward: [-113.77005815486905, -113.77005815486905, -113.77005815486905], time: 65.458
steps: 399975, episodes: 16000, mean episode variance: 52.239078994750976, agent episode variance: [0.0, 30.212908851623535, 22.02617014312744], time: 65.458
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.534328799851263, variance: 0.0, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -78.49760437011719, std_q: 6.278886795043945
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.533756637619841, variance: 120.85163879394531, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -77.1286392211914, std_q: 5.817324161529541
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.532496404292149, variance: 88.10468292236328, cvar: -47.185569763183594, v: -47.185569763183594, mean_q: -77.74288177490234, std_q: 5.888697624206543

steps: 424975, episodes: 17000, mean episode reward: -340.60835954859976, agent episode reward: [-113.53611984953324, -113.53611984953324, -113.53611984953324], time: 68.334
steps: 424975, episodes: 17000, mean episode variance: 52.323844360351565, agent episode variance: [0.0, 30.31188823699951, 22.01195612335205], time: 68.334
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.54033018573566, variance: 0.0, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -78.49777221679688, std_q: 6.297361850738525
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.543118061274577, variance: 121.24755096435547, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -77.1746597290039, std_q: 5.851809024810791
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.536645030000168, variance: 88.0478286743164, cvar: -49.71575927734375, v: -49.71575927734375, mean_q: -77.74748229980469, std_q: 5.8905487060546875

steps: 449975, episodes: 18000, mean episode reward: -340.05339936554196, agent episode reward: [-113.35113312184734, -113.35113312184734, -113.35113312184734], time: 63.95
steps: 449975, episodes: 18000, mean episode variance: 52.2830338973999, agent episode variance: [0.0, 30.17937186431885, 22.103662033081054], time: 63.951
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.533289354191105, variance: 0.0, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -78.47210693359375, std_q: 6.2870774269104
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.536150466078509, variance: 120.71748352050781, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -77.15037536621094, std_q: 5.8375372886657715
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.535352126159982, variance: 88.4146499633789, cvar: -52.23917007446289, v: -52.23917007446289, mean_q: -77.75074005126953, std_q: 5.897315979003906

steps: 474975, episodes: 19000, mean episode reward: -340.2034666087221, agent episode reward: [-113.40115553624071, -113.40115553624071, -113.40115553624071], time: 63.647
steps: 474975, episodes: 19000, mean episode variance: 52.2154197845459, agent episode variance: [0.0, 30.182976692199706, 22.032443092346192], time: 63.647
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.53817466964043, variance: 0.0, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -78.49388122558594, std_q: 6.29914665222168
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5329909800246275, variance: 120.73190307617188, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -77.14696502685547, std_q: 5.849285125732422
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.539547711463291, variance: 88.12977600097656, cvar: -54.75734329223633, v: -54.75734329223633, mean_q: -77.78176879882812, std_q: 5.9158735275268555

steps: 499975, episodes: 20000, mean episode reward: -339.4121368685026, agent episode reward: [-113.13737895616752, -113.13737895616752, -113.13737895616752], time: 61.923
steps: 499975, episodes: 20000, mean episode variance: 52.02022302246094, agent episode variance: [0.0, 30.145243408203125, 21.874979614257814], time: 61.924
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.535435596861231, variance: 0.0, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -78.48207092285156, std_q: 6.303471088409424
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.53309590355666, variance: 120.58097076416016, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -77.141845703125, std_q: 5.843349456787109
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5360149281098145, variance: 87.49992370605469, cvar: -57.271453857421875, v: -57.271453857421875, mean_q: -77.7701416015625, std_q: 5.905433654785156

steps: 524975, episodes: 21000, mean episode reward: -342.77479197480966, agent episode reward: [-114.25826399160323, -114.25826399160323, -114.25826399160323], time: 62.343
steps: 524975, episodes: 21000, mean episode variance: 52.29258332061767, agent episode variance: [0.0, 30.228684913635252, 22.06389840698242], time: 62.343
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5392979820288, variance: 0.0, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -78.48857879638672, std_q: 6.300149440765381
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.537601782225856, variance: 120.91473388671875, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -77.16413879394531, std_q: 5.8391594886779785
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5339717877665615, variance: 88.2555923461914, cvar: -59.78242874145508, v: -59.78242874145508, mean_q: -77.74546813964844, std_q: 5.878299236297607

steps: 549975, episodes: 22000, mean episode reward: -339.56216760595726, agent episode reward: [-113.18738920198577, -113.18738920198577, -113.18738920198577], time: 66.142
steps: 549975, episodes: 22000, mean episode variance: 52.53303043365479, agent episode variance: [0.0, 30.332868217468263, 22.200162216186524], time: 66.143
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5385557512134636, variance: 0.0, cvar: -62.290958404541016, v: -62.290958404541016, mean_q: -78.47996520996094, std_q: 6.28643274307251
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.542603164844908, variance: 121.33146667480469, cvar: -62.290958404541016, v: -62.290958404541016, mean_q: -77.17652893066406, std_q: 5.86481237411499
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.542262138138954, variance: 88.8006591796875, cvar: -62.290958404541016, v: -62.290958404541016, mean_q: -77.77662658691406, std_q: 5.905677318572998

steps: 574975, episodes: 23000, mean episode reward: -339.4905595435097, agent episode reward: [-113.16351984783657, -113.16351984783657, -113.16351984783657], time: 62.905
steps: 574975, episodes: 23000, mean episode variance: 52.34384020996094, agent episode variance: [0.0, 30.26741307067871, 22.076427139282227], time: 62.906
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5388589078129495, variance: 0.0, cvar: -64.79768371582031, v: -64.79768371582031, mean_q: -78.50858306884766, std_q: 6.27611780166626
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.53859709772958, variance: 121.06965637207031, cvar: -64.79768371582031, v: -64.79768371582031, mean_q: -77.16525268554688, std_q: 5.820761203765869
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.537397639742565, variance: 88.30570220947266, cvar: -64.7975845336914, v: -64.79768371582031, mean_q: -77.76284790039062, std_q: 5.873341083526611

steps: 599975, episodes: 24000, mean episode reward: -341.40982776179186, agent episode reward: [-113.80327592059729, -113.80327592059729, -113.80327592059729], time: 62.177
steps: 599975, episodes: 24000, mean episode variance: 52.34213389587402, agent episode variance: [0.0, 30.1783090133667, 22.163824882507324], time: 62.178
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5378031251784785, variance: 0.0, cvar: -67.29226684570312, v: -67.3027572631836, mean_q: -78.51202392578125, std_q: 6.284242153167725
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.542262845216665, variance: 120.7132339477539, cvar: -67.28945922851562, v: -67.3027572631836, mean_q: -77.17982482910156, std_q: 5.853171348571777
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.539660002013841, variance: 88.65530395507812, cvar: -67.26872253417969, v: -67.3027572631836, mean_q: -77.75975036621094, std_q: 5.890357494354248

steps: 624975, episodes: 25000, mean episode reward: -340.87045804448894, agent episode reward: [-113.62348601482964, -113.62348601482964, -113.62348601482964], time: 61.961
steps: 624975, episodes: 25000, mean episode variance: 52.25290637207031, agent episode variance: [0.0, 30.163281715393065, 22.089624656677245], time: 61.961
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.536643186176838, variance: 0.0, cvar: -69.4647445678711, v: -69.80147552490234, mean_q: -78.4776840209961, std_q: 6.267392635345459
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.538536148523088, variance: 120.65312957763672, cvar: -69.3431625366211, v: -69.76033782958984, mean_q: -77.17379760742188, std_q: 5.847238063812256
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.538360769468525, variance: 88.3584976196289, cvar: -69.19310760498047, v: -69.75718688964844, mean_q: -77.75717163085938, std_q: 5.909754753112793

steps: 649975, episodes: 26000, mean episode reward: -339.6068329583404, agent episode reward: [-113.20227765278013, -113.20227765278013, -113.20227765278013], time: 62.86
steps: 649975, episodes: 26000, mean episode variance: 52.27292598724365, agent episode variance: [0.0, 30.193451438903807, 22.079474548339842], time: 62.86
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.5391691219501284, variance: 0.0, cvar: -70.27558898925781, v: -71.51518249511719, mean_q: -78.49543762207031, std_q: 6.280872821807861
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.542895646958595, variance: 120.77381134033203, cvar: -69.82955169677734, v: -70.9286117553711, mean_q: -77.17980194091797, std_q: 5.8263468742370605
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.534671259622737, variance: 88.3178939819336, cvar: -69.67597961425781, v: -71.11885833740234, mean_q: -77.7428970336914, std_q: 5.885478496551514

steps: 674975, episodes: 27000, mean episode reward: -339.57781319201393, agent episode reward: [-113.19260439733799, -113.19260439733799, -113.19260439733799], time: 66.991
steps: 674975, episodes: 27000, mean episode variance: 52.415879432678224, agent episode variance: [0.0, 30.353162132263183, 22.062717300415038], time: 66.991
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.540299322739887, variance: 0.0, cvar: -70.31913757324219, v: -71.81327056884766, mean_q: -78.51361083984375, std_q: 6.2892165184021
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.543462124730479, variance: 121.41265869140625, cvar: -69.81312561035156, v: -70.9924087524414, mean_q: -77.17801666259766, std_q: 5.844708442687988
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.53894650622756, variance: 88.2508773803711, cvar: -69.730712890625, v: -71.37680053710938, mean_q: -77.77029418945312, std_q: 5.907836437225342

Traceback (most recent call last):
  File "train.py", line 493, in <module>
    train(arglist)
  File "train.py", line 216, in train
    obs_n = env.reset()
  File "../../multiagent-particle-envs/multiagent/environment.py", line 108, in reset
    self.reset_callback(self.world)
  File "../../multiagent-particle-envs/multiagent/scenarios/simple_spread.py", line 66, in reset_world
    save([world.agents, world.landmarks], 'ss_state')
  File "../../multiagent-particle-envs/multiagent/scenarios/simple_spread.py", line 9, in save
    with open('obj/' + filename + '.pkl', 'wb') as handle:
KeyboardInterrupt
