# bsub -q x86_6h -g /stamilse/_/default -M 102400 -hl -n 1 -R \
    rusage[mem=108544,ngpus_excl_p=1] affinity[core(1)] -Is \
    /u/stamilse/miniconda3/bin/python train.py --scenario simple_spread \
    --num-agents 3 --num-adversaries 0 --u_estimation True --constrained \
    True --constraint_type Exp_Var
--exp_var_alpha 23.72 --exp-name MADDPG_simple_spread_exp_var_02 \
    --save-dir models/simple_spread_policies1/02-non-linear-exp_var/ \
    --plots-dir models/simple_spread_policies1/02-non-linear-exp_var/
Job <1090486> is submitted to queue <x86_6h>.
<<Waiting for dispatch ...>>
<<Starting on dccxc276>>
arglist.u_estimation True
2019-09-06 04:43:23.465217: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
good agent:  4.2
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -521.8167841836264, agent episode reward: [-173.93892806120877, -173.93892806120877, -173.93892806120877], time: 47.224
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 47.224
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/u/stamilse/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -684.2938505118711, agent episode reward: [-228.0979501706237, -228.0979501706237, -228.0979501706237], time: 72.939
steps: 49975, episodes: 2000, mean episode variance: 8.737566238000989, agent episode variance: [2.9485964714288713, 2.9370951542407275, 2.8518746123313905], time: 72.94
Running avgs for agent 0: q_loss: 116.06542205810547, p_loss: -5.543495178222656, mean_rew: -7.6979501889381545, variance: 12.084411768151112, lamda: 1.0108145475387573
Running avgs for agent 1: q_loss: 115.82318115234375, p_loss: -5.627845764160156, mean_rew: -7.699679030698515, variance: 12.037275314331055, lamda: 1.010284185409546
Running avgs for agent 2: q_loss: 300.27874755859375, p_loss: -1.1071105003356934, mean_rew: -7.687917212913479, variance: 11.688010215759277, lamda: 1.010858178138733

steps: 74975, episodes: 3000, mean episode reward: -597.7109541189959, agent episode reward: [-199.23698470633198, -199.23698470633198, -199.23698470633198], time: 78.993
steps: 74975, episodes: 3000, mean episode variance: 5.711994909584522, agent episode variance: [1.974058524131775, 1.9722850050926208, 1.7656513803601266], time: 78.993
Running avgs for agent 0: q_loss: 19.53350830078125, p_loss: -4.406683921813965, mean_rew: -8.005126443835298, variance: 7.896234512329102, lamda: 1.0340299606323242
Running avgs for agent 1: q_loss: 21.120800018310547, p_loss: -4.401679515838623, mean_rew: -8.01181349997098, variance: 7.889140605926514, lamda: 1.0329216718673706
Running avgs for agent 2: q_loss: 675.920654296875, p_loss: 1.9432556629180908, mean_rew: -7.995286629298301, variance: 7.062605521440506, lamda: 1.0355271100997925

steps: 99975, episodes: 4000, mean episode reward: -704.2992755528278, agent episode reward: [-234.7664251842759, -234.7664251842759, -234.7664251842759], time: 74.045
steps: 99975, episodes: 4000, mean episode variance: 7.5325722563266755, agent episode variance: [1.9373710169792175, 1.962213860988617, 3.632987378358841], time: 74.045
Running avgs for agent 0: q_loss: 22.808086395263672, p_loss: -4.280221939086914, mean_rew: -8.192177002856534, variance: 7.749484539031982, lamda: 1.0574406385421753
Running avgs for agent 1: q_loss: 25.086153030395508, p_loss: -4.291362285614014, mean_rew: -8.18763565421715, variance: 7.848855018615723, lamda: 1.0569143295288086
Running avgs for agent 2: q_loss: 8571.419921875, p_loss: 6.384976387023926, mean_rew: -8.199787741720119, variance: 14.531949513435364, lamda: 1.0605312585830688

steps: 124975, episodes: 5000, mean episode reward: -807.3567912721225, agent episode reward: [-269.11893042404085, -269.11893042404085, -269.11893042404085], time: 73.718
steps: 124975, episodes: 5000, mean episode variance: 12.038179975271225, agent episode variance: [1.9947893691062928, 1.9931164002418518, 8.050274205923081], time: 73.718
Running avgs for agent 0: q_loss: 28.82146644592285, p_loss: -4.452856540679932, mean_rew: -8.606512315370717, variance: 7.979157447814941, lamda: 1.081056833267212
Running avgs for agent 1: q_loss: 29.454336166381836, p_loss: -4.403021812438965, mean_rew: -8.605421585188305, variance: 7.972465991973877, lamda: 1.0816150903701782
Running avgs for agent 2: q_loss: 45113.28515625, p_loss: 17.57082176208496, mean_rew: -8.59767210484351, variance: 32.201096823692325, lamda: 1.0855354070663452

steps: 149975, episodes: 6000, mean episode reward: -779.7203577080646, agent episode reward: [-259.90678590268817, -259.90678590268817, -259.90678590268817], time: 76.598
steps: 149975, episodes: 6000, mean episode variance: 15.10608847641945, agent episode variance: [2.048868287563324, 2.0475264999866485, 11.009693688869476], time: 76.598
Running avgs for agent 0: q_loss: 29.01085090637207, p_loss: -4.591152191162109, mean_rew: -8.984561132536836, variance: 8.195473670959473, lamda: 1.1053372621536255
Running avgs for agent 1: q_loss: 38.720245361328125, p_loss: -4.553807258605957, mean_rew: -8.985659789448171, variance: 8.190106391906738, lamda: 1.10640287399292
Running avgs for agent 2: q_loss: 90189.9765625, p_loss: 28.18473243713379, mean_rew: -8.990235384925564, variance: 44.0387747554779, lamda: 1.1105395555496216

steps: 174975, episodes: 7000, mean episode reward: -834.9760654139028, agent episode reward: [-278.3253551379676, -278.3253551379676, -278.3253551379676], time: 76.216
steps: 174975, episodes: 7000, mean episode variance: 19.981865855932234, agent episode variance: [2.0557205200195314, 2.0583215248584747, 15.86782381105423], time: 76.216
Running avgs for agent 0: q_loss: 30.21949005126953, p_loss: -4.723305702209473, mean_rew: -9.26808049011288, variance: 8.222882270812988, lamda: 1.1278897523880005
Running avgs for agent 1: q_loss: 41.22386932373047, p_loss: -4.696505069732666, mean_rew: -9.25824681363855, variance: 8.233285903930664, lamda: 1.1296151876449585
Running avgs for agent 2: q_loss: 232921.53125, p_loss: 36.24505615234375, mean_rew: -9.268855597825022, variance: 63.47129524421692, lamda: 1.1355438232421875

steps: 199975, episodes: 8000, mean episode reward: -819.4421076912918, agent episode reward: [-273.1473692304306, -273.1473692304306, -273.1473692304306], time: 76.502
steps: 199975, episodes: 8000, mean episode variance: 30.94628561902046, agent episode variance: [2.0624521465301515, 2.0931223595142363, 26.790711112976073], time: 76.503
Running avgs for agent 0: q_loss: 29.580673217773438, p_loss: -4.853509902954102, mean_rew: -9.511966049932868, variance: 8.249808311462402, lamda: 1.1501704454421997
Running avgs for agent 1: q_loss: 50.33780288696289, p_loss: -4.816934585571289, mean_rew: -9.494073132636302, variance: 8.372489929199219, lamda: 1.1482831239700317
Running avgs for agent 2: q_loss: 471966.40625, p_loss: 44.57709503173828, mean_rew: -9.491282951561292, variance: 107.16284445190429, lamda: 1.1605478525161743

steps: 224975, episodes: 9000, mean episode reward: -865.9521459980114, agent episode reward: [-288.6507153326705, -288.6507153326705, -288.6507153326705], time: 75.977
steps: 224975, episodes: 9000, mean episode variance: 29.648833891391753, agent episode variance: [2.104867464542389, 2.1078571887016295, 25.436109238147736], time: 75.977
Running avgs for agent 0: q_loss: 28.410524368286133, p_loss: -4.920457363128662, mean_rew: -9.708200171640891, variance: 8.419469833374023, lamda: 1.1657524108886719
Running avgs for agent 1: q_loss: 36.774166107177734, p_loss: -4.930243968963623, mean_rew: -9.714350921898038, variance: 8.431428909301758, lamda: 1.1629753112792969
Running avgs for agent 2: q_loss: 690974.0625, p_loss: 56.354801177978516, mean_rew: -9.683493648888648, variance: 101.74443695259095, lamda: 1.1855520009994507

steps: 249975, episodes: 10000, mean episode reward: -836.0613638079944, agent episode reward: [-278.6871212693315, -278.6871212693315, -278.6871212693315], time: 77.594
steps: 249975, episodes: 10000, mean episode variance: 48.884959413051604, agent episode variance: [2.128932388305664, 2.105627796649933, 44.650399228096006], time: 77.595
Running avgs for agent 0: q_loss: 34.345638275146484, p_loss: -5.0164079666137695, mean_rew: -9.890278240283234, variance: 8.515730857849121, lamda: 1.1816720962524414
Running avgs for agent 1: q_loss: 33.06212615966797, p_loss: -5.029392242431641, mean_rew: -9.897047988396702, variance: 8.42251205444336, lamda: 1.1801425218582153
Running avgs for agent 2: q_loss: 1341711.375, p_loss: 70.75238800048828, mean_rew: -9.891624555168015, variance: 178.60159691238403, lamda: 1.210556149482727

steps: 274975, episodes: 11000, mean episode reward: -748.4512028117583, agent episode reward: [-249.4837342705861, -249.4837342705861, -249.4837342705861], time: 78.179
steps: 274975, episodes: 11000, mean episode variance: 68.83291081953048, agent episode variance: [2.101992910385132, 2.127996980190277, 64.60292092895507], time: 78.179
Running avgs for agent 0: q_loss: 29.9239444732666, p_loss: -5.045730113983154, mean_rew: -9.954352169505558, variance: 8.40797233581543, lamda: 1.1960259675979614
Running avgs for agent 1: q_loss: 30.208284378051758, p_loss: -5.045360565185547, mean_rew: -9.947881801597982, variance: 8.511988639831543, lamda: 1.1935938596725464
Running avgs for agent 2: q_loss: 1761406.75, p_loss: 84.10167694091797, mean_rew: -9.946811268786503, variance: 258.4116837158203, lamda: 1.2355602979660034

steps: 299975, episodes: 12000, mean episode reward: -743.5115974686677, agent episode reward: [-247.83719915622257, -247.83719915622257, -247.83719915622257], time: 78.995
steps: 299975, episodes: 12000, mean episode variance: 77.5201330242157, agent episode variance: [2.080837133407593, 2.0714445724487303, 73.36785131835937], time: 78.996
Running avgs for agent 0: q_loss: 31.521154403686523, p_loss: -5.0252580642700195, mean_rew: -9.938196537269986, variance: 8.323348999023438, lamda: 1.2134668827056885
Running avgs for agent 1: q_loss: 30.62016487121582, p_loss: -5.043277263641357, mean_rew: -9.94266231683347, variance: 8.285778045654297, lamda: 1.208613634109497
Running avgs for agent 2: q_loss: 2116504.75, p_loss: 93.20796966552734, mean_rew: -9.937192376956236, variance: 293.4714052734375, lamda: 1.2605644464492798

steps: 324975, episodes: 13000, mean episode reward: -715.1394061732781, agent episode reward: [-238.3798020577594, -238.3798020577594, -238.3798020577594], time: 74.587
steps: 324975, episodes: 13000, mean episode variance: 79.13462039470673, agent episode variance: [2.0496369876861573, 2.060231209754944, 75.02475219726563], time: 74.588
Running avgs for agent 0: q_loss: 42.17786407470703, p_loss: -5.028567314147949, mean_rew: -9.935518714100349, variance: 8.198548316955566, lamda: 1.2329548597335815
Running avgs for agent 1: q_loss: 30.95774269104004, p_loss: -5.043818950653076, mean_rew: -9.929827359653567, variance: 8.240924835205078, lamda: 1.2215908765792847
Running avgs for agent 2: q_loss: 2354600.0, p_loss: 98.99667358398438, mean_rew: -9.935622884452382, variance: 300.0990087890625, lamda: 1.2855685949325562

steps: 349975, episodes: 14000, mean episode reward: -678.9700438114464, agent episode reward: [-226.3233479371488, -226.3233479371488, -226.3233479371488], time: 82.665
steps: 349975, episodes: 14000, mean episode variance: 77.20894299459458, agent episode variance: [2.0037193450927733, 2.0190793929100037, 73.1861442565918], time: 82.665
Running avgs for agent 0: q_loss: 31.19082260131836, p_loss: -5.020074367523193, mean_rew: -9.886940757467205, variance: 8.014877319335938, lamda: 1.2462037801742554
Running avgs for agent 1: q_loss: 28.352998733520508, p_loss: -5.025738716125488, mean_rew: -9.885290818135898, variance: 8.07631778717041, lamda: 1.2371491193771362
Running avgs for agent 2: q_loss: 2375711.25, p_loss: 102.80689239501953, mean_rew: -9.90617286912234, variance: 292.7445770263672, lamda: 1.3105727434158325

steps: 374975, episodes: 15000, mean episode reward: -604.9910505992851, agent episode reward: [-201.66368353309505, -201.66368353309505, -201.66368353309505], time: 90.691
steps: 374975, episodes: 15000, mean episode variance: 78.16863988924027, agent episode variance: [1.9706766395568847, 1.991808678150177, 74.2061545715332], time: 90.692
Running avgs for agent 0: q_loss: 28.13178062438965, p_loss: -4.966622829437256, mean_rew: -9.787914605289217, variance: 7.882706642150879, lamda: 1.2608739137649536
Running avgs for agent 1: q_loss: 31.165647506713867, p_loss: -4.993459701538086, mean_rew: -9.805577605742496, variance: 7.967235088348389, lamda: 1.2481447458267212
Running avgs for agent 2: q_loss: 2381407.75, p_loss: 107.57721710205078, mean_rew: -9.804855738860613, variance: 296.8246182861328, lamda: 1.3355768918991089

steps: 399975, episodes: 16000, mean episode reward: -591.8536091485936, agent episode reward: [-197.28453638286453, -197.28453638286453, -197.28453638286453], time: 91.12
steps: 399975, episodes: 16000, mean episode variance: 79.04611531448364, agent episode variance: [1.9201556882858277, 1.944508454322815, 75.181451171875], time: 91.121
Running avgs for agent 0: q_loss: 31.905616760253906, p_loss: -4.918221950531006, mean_rew: -9.67198355643596, variance: 7.6806230545043945, lamda: 1.2775542736053467
Running avgs for agent 1: q_loss: 24.893402099609375, p_loss: -4.913818359375, mean_rew: -9.666218704435249, variance: 7.778034210205078, lamda: 1.2582987546920776
Running avgs for agent 2: q_loss: 2465329.75, p_loss: 112.00206756591797, mean_rew: -9.688579020657178, variance: 300.7258046875, lamda: 1.3605810403823853

steps: 424975, episodes: 17000, mean episode reward: -565.4364708768818, agent episode reward: [-188.47882362562726, -188.47882362562726, -188.47882362562726], time: 92.387
steps: 424975, episodes: 17000, mean episode variance: 75.98723683404923, agent episode variance: [1.8713876886367797, 1.9060128464698791, 72.20983629894256], time: 92.387
Running avgs for agent 0: q_loss: 25.885168075561523, p_loss: -4.841653347015381, mean_rew: -9.547040810944047, variance: 7.485550880432129, lamda: 1.2958422899246216
Running avgs for agent 1: q_loss: 24.669702529907227, p_loss: -4.863192558288574, mean_rew: -9.54368654194029, variance: 7.624051094055176, lamda: 1.270035743713379
Running avgs for agent 2: q_loss: 2786107.0, p_loss: 116.3864974975586, mean_rew: -9.563768100131703, variance: 288.83934519577025, lamda: 1.3855851888656616

steps: 449975, episodes: 18000, mean episode reward: -551.0890452128186, agent episode reward: [-183.6963484042729, -183.6963484042729, -183.6963484042729], time: 95.176
steps: 449975, episodes: 18000, mean episode variance: 79.5598253288269, agent episode variance: [1.8388553876876832, 1.8640512399673461, 75.85691870117188], time: 95.176
Running avgs for agent 0: q_loss: 34.158321380615234, p_loss: -4.803742408752441, mean_rew: -9.447784568959618, variance: 7.355421543121338, lamda: 1.3105336427688599
Running avgs for agent 1: q_loss: 22.430700302124023, p_loss: -4.7861008644104, mean_rew: -9.446493016052639, variance: 7.456204891204834, lamda: 1.2820143699645996
Running avgs for agent 2: q_loss: 2571925.5, p_loss: 119.63520812988281, mean_rew: -9.443348093271862, variance: 303.4276748046875, lamda: 1.410589337348938

steps: 474975, episodes: 19000, mean episode reward: -537.8968921285528, agent episode reward: [-179.29896404285097, -179.29896404285097, -179.29896404285097], time: 95.333
steps: 474975, episodes: 19000, mean episode variance: 69.04852437639236, agent episode variance: [1.8017856864929198, 1.825643313884735, 65.4210953760147], time: 95.334
Running avgs for agent 0: q_loss: 30.194292068481445, p_loss: -4.751104354858398, mean_rew: -9.334281561098976, variance: 7.207142353057861, lamda: 1.3167883157730103
Running avgs for agent 1: q_loss: 30.164348602294922, p_loss: -4.731398582458496, mean_rew: -9.32010230756357, variance: 7.302573204040527, lamda: 1.29347562789917
Running avgs for agent 2: q_loss: 2666679.5, p_loss: 122.89141845703125, mean_rew: -9.320725003527128, variance: 261.6843815040588, lamda: 1.4355934858322144

steps: 499975, episodes: 20000, mean episode reward: -514.2881510127731, agent episode reward: [-171.4293836709244, -171.4293836709244, -171.4293836709244], time: 127.241
steps: 499975, episodes: 20000, mean episode variance: 74.23885400676727, agent episode variance: [1.7620131845474243, 1.789510576248169, 70.68733024597168], time: 127.241
Running avgs for agent 0: q_loss: 29.079164505004883, p_loss: -4.697574615478516, mean_rew: -9.211825748302322, variance: 7.048052787780762, lamda: 1.3325698375701904
Running avgs for agent 1: q_loss: 22.520538330078125, p_loss: -4.675815582275391, mean_rew: -9.214418889123765, variance: 7.1580424308776855, lamda: 1.3024377822875977
Running avgs for agent 2: q_loss: 2624838.5, p_loss: 126.34634399414062, mean_rew: -9.219753960953295, variance: 282.7493209838867, lamda: 1.4605976343154907

steps: 524975, episodes: 21000, mean episode reward: -499.7870381215578, agent episode reward: [-166.59567937385262, -166.59567937385262, -166.59567937385262], time: 123.364
steps: 524975, episodes: 21000, mean episode variance: 64.03094056916237, agent episode variance: [1.7197713429927826, 1.7552543377876282, 60.555914888381956], time: 123.365
Running avgs for agent 0: q_loss: 33.57808303833008, p_loss: -4.633483409881592, mean_rew: -9.081702650389749, variance: 6.879085063934326, lamda: 1.3403401374816895
Running avgs for agent 1: q_loss: 30.188161849975586, p_loss: -4.591546058654785, mean_rew: -9.08541644484282, variance: 7.021017551422119, lamda: 1.3175045251846313
Running avgs for agent 2: q_loss: 2753866.5, p_loss: 129.88214111328125, mean_rew: -9.090660095613071, variance: 242.22365955352782, lamda: 1.485601782798767

steps: 549975, episodes: 22000, mean episode reward: -504.00852946902097, agent episode reward: [-168.0028431563403, -168.0028431563403, -168.0028431563403], time: 122.936
steps: 549975, episodes: 22000, mean episode variance: 71.71718032646179, agent episode variance: [1.688232861995697, 1.7091602482795716, 68.31978721618653], time: 122.936
Running avgs for agent 0: q_loss: 27.31694984436035, p_loss: -4.579548358917236, mean_rew: -8.984345677404812, variance: 6.752931594848633, lamda: 1.3497521877288818
Running avgs for agent 1: q_loss: 21.28838539123535, p_loss: -4.561416149139404, mean_rew: -8.975256100117022, variance: 6.836641311645508, lamda: 1.3311448097229004
Running avgs for agent 2: q_loss: 2639618.5, p_loss: 132.0204315185547, mean_rew: -8.975597647854018, variance: 273.2791488647461, lamda: 1.5106059312820435

steps: 574975, episodes: 23000, mean episode reward: -521.7173063021263, agent episode reward: [-173.90576876737543, -173.90576876737543, -173.90576876737543], time: 125.499
steps: 574975, episodes: 23000, mean episode variance: 58.211350885391234, agent episode variance: [1.644388195991516, 1.676804409980774, 54.89015827941895], time: 125.499
Running avgs for agent 0: q_loss: 22.370149612426758, p_loss: -4.510947227478027, mean_rew: -8.885071076062548, variance: 6.577552795410156, lamda: 1.3664911985397339
Running avgs for agent 1: q_loss: 18.588253021240234, p_loss: -4.526353359222412, mean_rew: -8.890664875142999, variance: 6.707217216491699, lamda: 1.3404865264892578
Running avgs for agent 2: q_loss: 2522889.75, p_loss: 133.7648468017578, mean_rew: -8.877543009358734, variance: 219.5606331176758, lamda: 1.5356100797653198

steps: 599975, episodes: 24000, mean episode reward: -513.1983525149961, agent episode reward: [-171.0661175049987, -171.0661175049987, -171.0661175049987], time: 147.873
steps: 599975, episodes: 24000, mean episode variance: 42.216014169454574, agent episode variance: [1.611713033914566, 1.6621605339050294, 38.94214060163498], time: 147.874
Running avgs for agent 0: q_loss: 23.788299560546875, p_loss: -4.487514972686768, mean_rew: -8.789538487658568, variance: 6.446852207183838, lamda: 1.383812427520752
Running avgs for agent 1: q_loss: 19.29776954650879, p_loss: -4.467837810516357, mean_rew: -8.791385882811285, variance: 6.648642063140869, lamda: 1.3493750095367432
Running avgs for agent 2: q_loss: 2274273.25, p_loss: 136.44769287109375, mean_rew: -8.788346379793756, variance: 155.76856240653993, lamda: 1.5606142282485962

steps: 624975, episodes: 25000, mean episode reward: -534.530030504816, agent episode reward: [-178.17667683493863, -178.17667683493863, -178.17667683493863], time: 160.477
steps: 624975, episodes: 25000, mean episode variance: 40.67174643111229, agent episode variance: [1.5691225137710572, 1.6306443355083466, 37.47197958183288], time: 160.477
Running avgs for agent 0: q_loss: 21.69143295288086, p_loss: -4.459763526916504, mean_rew: -8.72906839910091, variance: 6.276490211486816, lamda: 1.400510311126709
Running avgs for agent 1: q_loss: 30.070749282836914, p_loss: -4.431330680847168, mean_rew: -8.707844967878337, variance: 6.522577285766602, lamda: 1.3591346740722656
Running avgs for agent 2: q_loss: 1547918.125, p_loss: 138.85792541503906, mean_rew: -8.70803711343285, variance: 149.88791832733153, lamda: 1.5856183767318726

steps: 649975, episodes: 26000, mean episode reward: -525.5042832701021, agent episode reward: [-175.16809442336736, -175.16809442336736, -175.16809442336736], time: 161.628
steps: 649975, episodes: 26000, mean episode variance: 61.6821174929142, agent episode variance: [1.5553919739723205, 1.6190857636928557, 58.50763975524902], time: 161.629
Running avgs for agent 0: q_loss: 23.708173751831055, p_loss: -4.406009197235107, mean_rew: -8.6452338924714, variance: 6.221567153930664, lamda: 1.4187780618667603
Running avgs for agent 1: q_loss: 22.820390701293945, p_loss: -4.411100387573242, mean_rew: -8.668663813631403, variance: 6.476343154907227, lamda: 1.3640356063842773
Running avgs for agent 2: q_loss: 2663469.75, p_loss: 140.43882751464844, mean_rew: -8.648474521341559, variance: 234.0305590209961, lamda: 1.610622525215149

steps: 674975, episodes: 27000, mean episode reward: -547.3818078239368, agent episode reward: [-182.46060260797896, -182.46060260797896, -182.46060260797896], time: 158.764
steps: 674975, episodes: 27000, mean episode variance: 48.41094346547127, agent episode variance: [1.5314023847579956, 1.5653382642269134, 45.31420281648636], time: 158.764
Running avgs for agent 0: q_loss: 29.560834884643555, p_loss: -4.373239040374756, mean_rew: -8.59296440465483, variance: 6.125609874725342, lamda: 1.4304800033569336
Running avgs for agent 1: q_loss: 21.253448486328125, p_loss: -4.371624946594238, mean_rew: -8.58274515689194, variance: 6.261353015899658, lamda: 1.379052758216858
Running avgs for agent 2: q_loss: 2232500.25, p_loss: 141.52183532714844, mean_rew: -8.588053109607367, variance: 181.25681126594543, lamda: 1.6356266736984253

steps: 699975, episodes: 28000, mean episode reward: -564.7501158549351, agent episode reward: [-188.25003861831166, -188.25003861831166, -188.25003861831166], time: 155.197
steps: 699975, episodes: 28000, mean episode variance: 60.16629956746102, agent episode variance: [1.5098874659538268, 1.5595256078243256, 57.09688649368286], time: 155.197
Running avgs for agent 0: q_loss: 28.570629119873047, p_loss: -4.368072032928467, mean_rew: -8.550954661400338, variance: 6.039549827575684, lamda: 1.4388662576675415
Running avgs for agent 1: q_loss: 19.143409729003906, p_loss: -4.36821174621582, mean_rew: -8.564795597618417, variance: 6.238102436065674, lamda: 1.3968346118927002
Running avgs for agent 2: q_loss: 2773043.25, p_loss: 143.51124572753906, mean_rew: -8.56418597144773, variance: 228.38754597473144, lamda: 1.6606308221817017

steps: 724975, episodes: 29000, mean episode reward: -545.3387037945124, agent episode reward: [-181.77956793150418, -181.77956793150418, -181.77956793150418], time: 156.45
steps: 724975, episodes: 29000, mean episode variance: 46.837079570770264, agent episode variance: [1.4900981855392457, 1.5223836250305176, 43.8245977602005], time: 156.45
Running avgs for agent 0: q_loss: 29.820756912231445, p_loss: -4.351468563079834, mean_rew: -8.52538472893718, variance: 5.960392475128174, lamda: 1.4461206197738647
Running avgs for agent 1: q_loss: 17.712730407714844, p_loss: -4.3414082527160645, mean_rew: -8.511745474206261, variance: 6.089534759521484, lamda: 1.4132229089736938
Running avgs for agent 2: q_loss: 2581531.25, p_loss: 146.2158660888672, mean_rew: -8.519926835940904, variance: 175.298391040802, lamda: 1.685634970664978

steps: 749975, episodes: 30000, mean episode reward: -525.4924720844488, agent episode reward: [-175.16415736148292, -175.16415736148292, -175.16415736148292], time: 159.531
steps: 749975, episodes: 30000, mean episode variance: 41.483374642372134, agent episode variance: [1.4804583258628845, 1.5119207968711854, 38.49099551963806], time: 159.532
Running avgs for agent 0: q_loss: 20.99827766418457, p_loss: -4.319306373596191, mean_rew: -8.464153222334161, variance: 5.921833038330078, lamda: 1.457007884979248
Running avgs for agent 1: q_loss: 16.141332626342773, p_loss: -4.3247294425964355, mean_rew: -8.476778605651441, variance: 6.047683238983154, lamda: 1.4267741441726685
Running avgs for agent 2: q_loss: 1855732.125, p_loss: 148.65347290039062, mean_rew: -8.46768705320126, variance: 153.96398207855225, lamda: 1.7106391191482544

steps: 774975, episodes: 31000, mean episode reward: -511.54551819946266, agent episode reward: [-170.51517273315423, -170.51517273315423, -170.51517273315423], time: 160.518
steps: 774975, episodes: 31000, mean episode variance: 58.68540132188797, agent episode variance: [1.4500231094360352, 1.49432067155838, 55.74105754089356], time: 160.518
Running avgs for agent 0: q_loss: 18.667335510253906, p_loss: -4.2911248207092285, mean_rew: -8.41711494694692, variance: 5.800092697143555, lamda: 1.4773128032684326
Running avgs for agent 1: q_loss: 17.047876358032227, p_loss: -4.30661153793335, mean_rew: -8.432265862122243, variance: 5.977283000946045, lamda: 1.4390387535095215
Running avgs for agent 2: q_loss: 2505130.5, p_loss: 150.52354431152344, mean_rew: -8.414378233508222, variance: 222.96423016357423, lamda: 1.7356432676315308

steps: 799975, episodes: 32000, mean episode reward: -500.93033622314755, agent episode reward: [-166.97677874104923, -166.97677874104923, -166.97677874104923], time: 159.894
steps: 799975, episodes: 32000, mean episode variance: 53.974668537378314, agent episode variance: [1.4234949510097503, 1.467884907245636, 51.08328867912292], time: 159.894
Running avgs for agent 0: q_loss: 22.859331130981445, p_loss: -4.277218818664551, mean_rew: -8.379017311202166, variance: 5.6939802169799805, lamda: 1.4928178787231445
Running avgs for agent 1: q_loss: 23.165102005004883, p_loss: -4.26372766494751, mean_rew: -8.364900331128599, variance: 5.871540069580078, lamda: 1.4508159160614014
Running avgs for agent 2: q_loss: 2398644.75, p_loss: 151.6542510986328, mean_rew: -8.368828345187852, variance: 204.3331547164917, lamda: 1.7606474161148071

steps: 824975, episodes: 33000, mean episode reward: -489.8603598510246, agent episode reward: [-163.2867866170082, -163.2867866170082, -163.2867866170082], time: 155.29
steps: 824975, episodes: 33000, mean episode variance: 35.82222283577919, agent episode variance: [1.3933310298919677, 1.4550136144161225, 32.9738781914711], time: 155.291
Running avgs for agent 0: q_loss: 27.823795318603516, p_loss: -4.252503871917725, mean_rew: -8.313189713748931, variance: 5.573324203491211, lamda: 1.5074853897094727
Running avgs for agent 1: q_loss: 24.204572677612305, p_loss: -4.242164611816406, mean_rew: -8.312792234157053, variance: 5.820054531097412, lamda: 1.4553852081298828
Running avgs for agent 2: q_loss: 1826740.375, p_loss: 152.49388122558594, mean_rew: -8.306189046217572, variance: 131.8955127658844, lamda: 1.7856515645980835

steps: 849975, episodes: 34000, mean episode reward: -486.71113040241806, agent episode reward: [-162.23704346747272, -162.23704346747272, -162.23704346747272], time: 157.26
steps: 849975, episodes: 34000, mean episode variance: 42.00221872854233, agent episode variance: [1.3972543995380402, 1.4425200178623199, 39.16244431114197], time: 157.26
Running avgs for agent 0: q_loss: 24.578439712524414, p_loss: -4.207827091217041, mean_rew: -8.257331308469517, variance: 5.589017391204834, lamda: 1.5153822898864746
Running avgs for agent 1: q_loss: 23.662527084350586, p_loss: -4.211526393890381, mean_rew: -8.25010309108714, variance: 5.770080089569092, lamda: 1.4591689109802246
Running avgs for agent 2: q_loss: 2315635.25, p_loss: 152.93692016601562, mean_rew: -8.244885194524239, variance: 156.64977724456787, lamda: 1.8106557130813599

steps: 874975, episodes: 35000, mean episode reward: -494.3694114997747, agent episode reward: [-164.7898038332582, -164.7898038332582, -164.7898038332582], time: 159.051
steps: 874975, episodes: 35000, mean episode variance: 59.41316447854042, agent episode variance: [1.3645954353809358, 1.4260699453353882, 56.6224990978241], time: 159.051
Running avgs for agent 0: q_loss: 26.362581253051758, p_loss: -4.188625812530518, mean_rew: -8.206537020188646, variance: 5.458381652832031, lamda: 1.5272927284240723
Running avgs for agent 1: q_loss: 21.96950340270996, p_loss: -4.194852352142334, mean_rew: -8.213502071552481, variance: 5.704279899597168, lamda: 1.4653016328811646
Running avgs for agent 2: q_loss: 2974774.75, p_loss: 153.14370727539062, mean_rew: -8.220252338816435, variance: 226.4899963912964, lamda: 1.8356598615646362

steps: 899975, episodes: 36000, mean episode reward: -497.8376020392071, agent episode reward: [-165.94586734640237, -165.94586734640237, -165.94586734640237], time: 158.357
steps: 899975, episodes: 36000, mean episode variance: 57.83665074300766, agent episode variance: [1.3458314549922943, 1.4219649255275726, 55.06885436248779], time: 158.357
Running avgs for agent 0: q_loss: 21.267309188842773, p_loss: -4.166651725769043, mean_rew: -8.159073956876004, variance: 5.383325576782227, lamda: 1.5396018028259277
Running avgs for agent 1: q_loss: 22.390422821044922, p_loss: -4.155839443206787, mean_rew: -8.156733044788034, variance: 5.687859535217285, lamda: 1.4690147638320923
Running avgs for agent 2: q_loss: 3048920.0, p_loss: 152.46885681152344, mean_rew: -8.177473861243723, variance: 220.27541744995116, lamda: 1.8606640100479126

steps: 924975, episodes: 37000, mean episode reward: -495.9976214447216, agent episode reward: [-165.33254048157386, -165.33254048157386, -165.33254048157386], time: 160.314
steps: 924975, episodes: 37000, mean episode variance: 38.05016375255585, agent episode variance: [1.3314682409763337, 1.4082274749279022, 35.31046803665161], time: 160.315
Running avgs for agent 0: q_loss: 26.866615295410156, p_loss: -4.142717361450195, mean_rew: -8.117628201076759, variance: 5.325872898101807, lamda: 1.5549440383911133
Running avgs for agent 1: q_loss: 15.780710220336914, p_loss: -4.152622222900391, mean_rew: -8.133947017969032, variance: 5.632909774780273, lamda: 1.475679874420166
Running avgs for agent 2: q_loss: 1969462.75, p_loss: 151.20858764648438, mean_rew: -8.133573092844738, variance: 141.24187214660645, lamda: 1.885668158531189

steps: 949975, episodes: 38000, mean episode reward: -493.3635924135721, agent episode reward: [-164.45453080452407, -164.45453080452407, -164.45453080452407], time: 164.237
steps: 949975, episodes: 38000, mean episode variance: 36.836978975057605, agent episode variance: [1.3020106353759766, 1.3841478788852692, 34.15082046079635], time: 164.237
Running avgs for agent 0: q_loss: 19.86908531188965, p_loss: -4.137087821960449, mean_rew: -8.079406622486852, variance: 5.208042621612549, lamda: 1.5687768459320068
Running avgs for agent 1: q_loss: 14.18757438659668, p_loss: -4.124629974365234, mean_rew: -8.085477531494723, variance: 5.53659200668335, lamda: 1.4901341199874878
Running avgs for agent 2: q_loss: 2283845.5, p_loss: 150.16360473632812, mean_rew: -8.07340650764013, variance: 136.6032818431854, lamda: 1.9106723070144653

steps: 974975, episodes: 39000, mean episode reward: -496.4118508333284, agent episode reward: [-165.47061694444278, -165.47061694444278, -165.47061694444278], time: 164.883
steps: 974975, episodes: 39000, mean episode variance: 31.85666987991333, agent episode variance: [1.290261079311371, 1.3663151698112488, 29.20009363079071], time: 164.883
Running avgs for agent 0: q_loss: 17.94208335876465, p_loss: -4.1149773597717285, mean_rew: -8.050223707904074, variance: 5.161044120788574, lamda: 1.5906951427459717
Running avgs for agent 1: q_loss: 15.536863327026367, p_loss: -4.113033771514893, mean_rew: -8.03947863643144, variance: 5.4652605056762695, lamda: 1.5028190612792969
Running avgs for agent 2: q_loss: 1690938.625, p_loss: 149.70606994628906, mean_rew: -8.041021977996833, variance: 116.80037452316284, lamda: 1.9356764554977417

steps: 999975, episodes: 40000, mean episode reward: -491.59465841543545, agent episode reward: [-163.86488613847854, -163.86488613847854, -163.86488613847854], time: 161.961
steps: 999975, episodes: 40000, mean episode variance: 38.38892460608482, agent episode variance: [1.274340695619583, 1.3557031226158143, 35.75888078784943], time: 161.962
Running avgs for agent 0: q_loss: 18.459918975830078, p_loss: -4.1108574867248535, mean_rew: -8.019604003181833, variance: 5.097362995147705, lamda: 1.611972451210022
Running avgs for agent 1: q_loss: 20.470722198486328, p_loss: -4.098511219024658, mean_rew: -8.010926512097615, variance: 5.422812461853027, lamda: 1.5108407735824585
Running avgs for agent 2: q_loss: 1977307.875, p_loss: 149.1927490234375, mean_rew: -8.003304516121599, variance: 143.0355231513977, lamda: 1.9606807231903076

steps: 1024975, episodes: 41000, mean episode reward: -495.49310331358026, agent episode reward: [-165.1643677711934, -165.1643677711934, -165.1643677711934], time: 162.351
steps: 1024975, episodes: 41000, mean episode variance: 39.3187092730999, agent episode variance: [1.2486959760189056, 1.3386310863494872, 36.7313822107315], time: 162.351
Running avgs for agent 0: q_loss: 20.98567771911621, p_loss: -4.084421157836914, mean_rew: -7.980376671909816, variance: 4.994784355163574, lamda: 1.6313366889953613
Running avgs for agent 1: q_loss: 14.661643981933594, p_loss: -4.066208362579346, mean_rew: -7.972512585209922, variance: 5.354524612426758, lamda: 1.5208187103271484
Running avgs for agent 2: q_loss: 2185581.75, p_loss: 149.2383270263672, mean_rew: -7.987564638014918, variance: 146.925528842926, lamda: 1.9856846332550049

steps: 1049975, episodes: 42000, mean episode reward: -493.2650844698706, agent episode reward: [-164.42169482329018, -164.42169482329018, -164.42169482329018], time: 162.47
steps: 1049975, episodes: 42000, mean episode variance: 37.961035982370376, agent episode variance: [1.241977098941803, 1.3265627791881562, 35.392496104240415], time: 162.47
Running avgs for agent 0: q_loss: 24.40329360961914, p_loss: -4.0688276290893555, mean_rew: -7.940653737795455, variance: 4.96790885925293, lamda: 1.6396151781082153
Running avgs for agent 1: q_loss: 14.757833480834961, p_loss: -4.070793628692627, mean_rew: -7.960490096419587, variance: 5.306251049041748, lamda: 1.5358418226242065
Running avgs for agent 2: q_loss: 2207408.75, p_loss: 149.1283416748047, mean_rew: -7.94878265411033, variance: 141.56998441696166, lamda: 2.010676145553589

steps: 1074975, episodes: 43000, mean episode reward: -504.637700144067, agent episode reward: [-168.21256671468902, -168.21256671468902, -168.21256671468902], time: 164.558
steps: 1074975, episodes: 43000, mean episode variance: 32.285024394989016, agent episode variance: [1.226702456474304, 1.3051587886810303, 29.75316314983368], time: 164.559
Running avgs for agent 0: q_loss: 22.048765182495117, p_loss: -4.047696113586426, mean_rew: -7.9152497973223905, variance: 4.9068098068237305, lamda: 1.6465578079223633
Running avgs for agent 1: q_loss: 13.561102867126465, p_loss: -4.0458855628967285, mean_rew: -7.910813660980462, variance: 5.220635414123535, lamda: 1.5516597032546997
Running avgs for agent 2: q_loss: 1705079.5, p_loss: 148.32691955566406, mean_rew: -7.893152084689692, variance: 119.01265259933471, lamda: 2.0356504917144775

steps: 1099975, episodes: 44000, mean episode reward: -501.7847307154272, agent episode reward: [-167.26157690514242, -167.26157690514242, -167.26157690514242], time: 161.163
steps: 1099975, episodes: 44000, mean episode variance: 37.36561426639557, agent episode variance: [1.2071130335330964, 1.2808722970485686, 34.8776289358139], time: 161.164
Running avgs for agent 0: q_loss: 23.229448318481445, p_loss: -4.032491683959961, mean_rew: -7.861718829264878, variance: 4.828452110290527, lamda: 1.6607803106307983
Running avgs for agent 1: q_loss: 13.51183795928955, p_loss: -4.006328105926514, mean_rew: -7.8494544208755395, variance: 5.1234893798828125, lamda: 1.5670512914657593
Running avgs for agent 2: q_loss: 2258866.75, p_loss: 147.7261962890625, mean_rew: -7.842408939412793, variance: 139.5105157432556, lamda: 2.0606250762939453

steps: 1124975, episodes: 45000, mean episode reward: -496.46132793554386, agent episode reward: [-165.48710931184792, -165.48710931184792, -165.48710931184792], time: 161.43
steps: 1124975, episodes: 45000, mean episode variance: 38.72832490134239, agent episode variance: [1.188521856546402, 1.2614797263145447, 36.27832331848145], time: 161.43
Running avgs for agent 0: q_loss: 22.704357147216797, p_loss: -3.981813430786133, mean_rew: -7.76551031092208, variance: 4.754087924957275, lamda: 1.6691465377807617
Running avgs for agent 1: q_loss: 12.786785125732422, p_loss: -4.001000881195068, mean_rew: -7.787984219626858, variance: 5.045918941497803, lamda: 1.5768896341323853
Running avgs for agent 2: q_loss: 2587927.75, p_loss: 147.69187927246094, mean_rew: -7.771827824540831, variance: 145.1132932739258, lamda: 2.085599184036255

steps: 1149975, episodes: 46000, mean episode reward: -498.82901742297094, agent episode reward: [-166.27633914099027, -166.27633914099027, -166.27633914099027], time: 160.1
steps: 1149975, episodes: 46000, mean episode variance: 29.240831599235534, agent episode variance: [1.1679413151741027, 1.2287435193061829, 26.844146764755248], time: 160.1
Running avgs for agent 0: q_loss: 17.974641799926758, p_loss: -3.9329833984375, mean_rew: -7.6666571459951065, variance: 4.6717658042907715, lamda: 1.678452968597412
Running avgs for agent 1: q_loss: 13.191234588623047, p_loss: -3.9231646060943604, mean_rew: -7.670856543855256, variance: 4.914974212646484, lamda: 1.5885260105133057
Running avgs for agent 2: q_loss: 1642046.75, p_loss: 147.17666625976562, mean_rew: -7.672806532826858, variance: 107.37658705902099, lamda: 2.1105735301971436

steps: 1174975, episodes: 47000, mean episode reward: -497.104345184595, agent episode reward: [-165.701448394865, -165.701448394865, -165.701448394865], time: 161.475
steps: 1174975, episodes: 47000, mean episode variance: 35.94610701680183, agent episode variance: [1.1341623978614808, 1.1989723327159882, 33.61297228622436], time: 161.475
Running avgs for agent 0: q_loss: 16.66172981262207, p_loss: -3.8781774044036865, mean_rew: -7.561880963305183, variance: 4.536649703979492, lamda: 1.6951876878738403
Running avgs for agent 1: q_loss: 12.836371421813965, p_loss: -3.8814902305603027, mean_rew: -7.55573997919053, variance: 4.795889377593994, lamda: 1.601391315460205
Running avgs for agent 2: q_loss: 1919875.25, p_loss: 146.2598114013672, mean_rew: -7.569234494054137, variance: 134.45188914489745, lamda: 2.1355478763580322

steps: 1199975, episodes: 48000, mean episode reward: -502.03795104327656, agent episode reward: [-167.34598368109215, -167.34598368109215, -167.34598368109215], time: 159.926
steps: 1199975, episodes: 48000, mean episode variance: 36.19355912601948, agent episode variance: [1.109758583545685, 1.175291183590889, 33.9085093588829], time: 159.926
Running avgs for agent 0: q_loss: 19.061185836791992, p_loss: -3.8307583332061768, mean_rew: -7.44704290452753, variance: 4.4390339851379395, lamda: 1.7029926776885986
Running avgs for agent 1: q_loss: 16.075435638427734, p_loss: -3.838016986846924, mean_rew: -7.454196800771485, variance: 4.701165199279785, lamda: 1.6107922792434692
Running avgs for agent 2: q_loss: 1618235.625, p_loss: 145.61814880371094, mean_rew: -7.45871404201428, variance: 135.6340374355316, lamda: 2.1605224609375

steps: 1224975, episodes: 49000, mean episode reward: -500.12230470333736, agent episode reward: [-166.70743490111246, -166.70743490111246, -166.70743490111246], time: 160.117
steps: 1224975, episodes: 49000, mean episode variance: 33.749454201698306, agent episode variance: [1.0957102324962615, 1.1596008970737457, 31.494143072128296], time: 160.117
Running avgs for agent 0: q_loss: 16.771141052246094, p_loss: -3.7650861740112305, mean_rew: -7.337876784865228, variance: 4.382840633392334, lamda: 1.708405613899231
Running avgs for agent 1: q_loss: 14.420476913452148, p_loss: -3.775789499282837, mean_rew: -7.337024392446061, variance: 4.638403415679932, lamda: 1.6126612424850464
Running avgs for agent 2: q_loss: 1316804.875, p_loss: 144.6333770751953, mean_rew: -7.336797232526438, variance: 125.97657228851318, lamda: 2.1854965686798096

steps: 1249975, episodes: 50000, mean episode reward: -499.3095249490778, agent episode reward: [-166.43650831635927, -166.43650831635927, -166.43650831635927], time: 157.654
steps: 1249975, episodes: 50000, mean episode variance: 28.318311790704726, agent episode variance: [1.0695524582862854, 1.1362007777690888, 26.112558554649354], time: 157.655
Running avgs for agent 0: q_loss: 12.800436973571777, p_loss: -3.7142021656036377, mean_rew: -7.217748340830676, variance: 4.278210163116455, lamda: 1.7230725288391113
Running avgs for agent 1: q_loss: 13.632786750793457, p_loss: -3.7200722694396973, mean_rew: -7.222768680433718, variance: 4.544802665710449, lamda: 1.6144208908081055
Running avgs for agent 2: q_loss: 1015897.4375, p_loss: 143.28565979003906, mean_rew: -7.221044440323132, variance: 104.45023421859742, lamda: 2.2104711532592773

steps: 1274975, episodes: 51000, mean episode reward: -506.71691436582825, agent episode reward: [-168.90563812194276, -168.90563812194276, -168.90563812194276], time: 157.902
steps: 1274975, episodes: 51000, mean episode variance: 43.67500837564469, agent episode variance: [1.0472020266056061, 1.1395722267627717, 41.4882341222763], time: 157.902
Running avgs for agent 0: q_loss: 13.755101203918457, p_loss: -3.6671738624572754, mean_rew: -7.130147792257695, variance: 4.188807964324951, lamda: 1.7366786003112793
Running avgs for agent 1: q_loss: 12.33816909790039, p_loss: -3.669233798980713, mean_rew: -7.128581053580999, variance: 4.558289051055908, lamda: 1.6160510778427124
Running avgs for agent 2: q_loss: 1540853.375, p_loss: 142.02731323242188, mean_rew: -7.125007669360951, variance: 165.9529364891052, lamda: 2.235445261001587

steps: 1299975, episodes: 52000, mean episode reward: -503.993205452273, agent episode reward: [-167.99773515075765, -167.99773515075765, -167.99773515075765], time: 162.444
steps: 1299975, episodes: 52000, mean episode variance: 51.5663435075283, agent episode variance: [1.0262411341667175, 1.1119426138401032, 49.428159759521485], time: 162.444
Running avgs for agent 0: q_loss: 11.564690589904785, p_loss: -3.628368854522705, mean_rew: -7.040988890110237, variance: 4.104964733123779, lamda: 1.7485525608062744
Running avgs for agent 1: q_loss: 11.57631778717041, p_loss: -3.6358931064605713, mean_rew: -7.04438297316443, variance: 4.447770595550537, lamda: 1.617172360420227
Running avgs for agent 2: q_loss: 1670766.875, p_loss: 140.75238037109375, mean_rew: -7.048064377981347, variance: 197.71263903808594, lamda: 2.2604193687438965

steps: 1324975, episodes: 53000, mean episode reward: -508.32219444526896, agent episode reward: [-169.44073148175633, -169.44073148175633, -169.44073148175633], time: 161.666
steps: 1324975, episodes: 53000, mean episode variance: 49.938285031318664, agent episode variance: [1.014271980047226, 1.1054654819965362, 47.818547569274905], time: 161.667
Running avgs for agent 0: q_loss: 12.743453025817871, p_loss: -3.5978457927703857, mean_rew: -6.973272620672239, variance: 4.0570878982543945, lamda: 1.7527964115142822
Running avgs for agent 1: q_loss: 11.229628562927246, p_loss: -3.6044583320617676, mean_rew: -6.980790461593476, variance: 4.4218621253967285, lamda: 1.6223430633544922
Running avgs for agent 2: q_loss: 1524326.625, p_loss: 139.78018188476562, mean_rew: -6.970959370147527, variance: 191.27419027709962, lamda: 2.2853939533233643

steps: 1349975, episodes: 54000, mean episode reward: -512.6274923704617, agent episode reward: [-170.87583079015388, -170.87583079015388, -170.87583079015388], time: 163.193
steps: 1349975, episodes: 54000, mean episode variance: 46.488515290737155, agent episode variance: [1.0076385707855224, 1.0853855242729187, 44.39549119567871], time: 163.194
Running avgs for agent 0: q_loss: 11.644593238830566, p_loss: -3.563080072402954, mean_rew: -6.914150498918579, variance: 4.030554294586182, lamda: 1.7537310123443604
Running avgs for agent 1: q_loss: 10.464489936828613, p_loss: -3.5714120864868164, mean_rew: -6.913044322791391, variance: 4.341541767120361, lamda: 1.624158501625061
Running avgs for agent 2: q_loss: 1387916.0, p_loss: 138.79403686523438, mean_rew: -6.910425305138875, variance: 177.58196478271483, lamda: 2.310368061065674

steps: 1374975, episodes: 55000, mean episode reward: -513.7447087289271, agent episode reward: [-171.24823624297568, -171.24823624297568, -171.24823624297568], time: 169.523
steps: 1374975, episodes: 55000, mean episode variance: 46.53131497263908, agent episode variance: [0.9975714373588562, 1.0825041630268097, 44.45123937225342], time: 169.524
Running avgs for agent 0: q_loss: 10.79845142364502, p_loss: -3.5394630432128906, mean_rew: -6.865076095880198, variance: 3.990285634994507, lamda: 1.7545347213745117
Running avgs for agent 1: q_loss: 10.06807804107666, p_loss: -3.5485284328460693, mean_rew: -6.868208072794803, variance: 4.330016613006592, lamda: 1.6253845691680908
Running avgs for agent 2: q_loss: 1294641.0, p_loss: 137.7467498779297, mean_rew: -6.865768976425679, variance: 177.80495748901367, lamda: 2.3353428840637207

steps: 1399975, episodes: 56000, mean episode reward: -522.629547796108, agent episode reward: [-174.2098492653693, -174.2098492653693, -174.2098492653693], time: 160.715
steps: 1399975, episodes: 56000, mean episode variance: 45.88835263085365, agent episode variance: [0.9900971713066101, 1.0721130950450897, 43.826142364501955], time: 160.715
Running avgs for agent 0: q_loss: 10.376971244812012, p_loss: -3.5280017852783203, mean_rew: -6.837996186783299, variance: 3.960388660430908, lamda: 1.7551721334457397
Running avgs for agent 1: q_loss: 9.522644996643066, p_loss: -3.5347039699554443, mean_rew: -6.836587592102162, variance: 4.2884521484375, lamda: 1.6261281967163086
Running avgs for agent 2: q_loss: 1235200.0, p_loss: 136.9007568359375, mean_rew: -6.841150762037318, variance: 175.30456945800782, lamda: 2.3603172302246094

steps: 1424975, episodes: 57000, mean episode reward: -524.2748125748157, agent episode reward: [-174.7582708582719, -174.7582708582719, -174.7582708582719], time: 160.573
steps: 1424975, episodes: 57000, mean episode variance: 43.08494306349754, agent episode variance: [0.9941214218139648, 1.0700231339931487, 41.02079850769043], time: 160.573
Running avgs for agent 0: q_loss: 9.877992630004883, p_loss: -3.509671688079834, mean_rew: -6.818803919858696, variance: 3.97648549079895, lamda: 1.7558178901672363
Running avgs for agent 1: q_loss: 8.998767852783203, p_loss: -3.527402400970459, mean_rew: -6.822657207686581, variance: 4.280092716217041, lamda: 1.626836895942688
Running avgs for agent 2: q_loss: 1156174.625, p_loss: 136.0809326171875, mean_rew: -6.829823272814781, variance: 164.08319403076172, lamda: 2.385291576385498

steps: 1449975, episodes: 58000, mean episode reward: -540.6109081918953, agent episode reward: [-180.20363606396512, -180.20363606396512, -180.20363606396512], time: 160.774
steps: 1449975, episodes: 58000, mean episode variance: 42.01126169323921, agent episode variance: [0.9821967272758484, 1.0738795869350433, 39.95518537902832], time: 160.774
Running avgs for agent 0: q_loss: 9.626300811767578, p_loss: -3.5084404945373535, mean_rew: -6.81563655847632, variance: 3.9287867546081543, lamda: 1.7564951181411743
Running avgs for agent 1: q_loss: 8.673025131225586, p_loss: -3.527646541595459, mean_rew: -6.815641809963494, variance: 4.295518398284912, lamda: 1.627423882484436
Running avgs for agent 2: q_loss: 1101956.5, p_loss: 134.16819763183594, mean_rew: -6.813225568948299, variance: 159.8207415161133, lamda: 2.4102656841278076

steps: 1474975, episodes: 59000, mean episode reward: -545.2931839821639, agent episode reward: [-181.7643946607213, -181.7643946607213, -181.7643946607213], time: 155.698
steps: 1474975, episodes: 59000, mean episode variance: 40.62000553894043, agent episode variance: [0.982716228723526, 1.060293407201767, 38.57699590301514], time: 155.699
Running avgs for agent 0: q_loss: 6.9152398109436035, p_loss: -3.4906911849975586, mean_rew: -6.8116082508215765, variance: 3.9308648109436035, lamda: 1.7598804235458374
Running avgs for agent 1: q_loss: 8.314412117004395, p_loss: -3.529848098754883, mean_rew: -6.815470382511379, variance: 4.241173267364502, lamda: 1.6279418468475342
Running avgs for agent 2: q_loss: 1051174.125, p_loss: 132.34312438964844, mean_rew: -6.80442387784626, variance: 154.30798361206055, lamda: 2.4352400302886963

steps: 1499975, episodes: 60000, mean episode reward: -553.4929484347939, agent episode reward: [-184.49764947826463, -184.49764947826463, -184.49764947826463], time: 155.784
steps: 1499975, episodes: 60000, mean episode variance: 39.02942214345932, agent episode variance: [0.9861584024429322, 1.0630765690803528, 36.98018717193604], time: 155.784
Running avgs for agent 0: q_loss: 6.049694061279297, p_loss: -3.495816230773926, mean_rew: -6.816976012303677, variance: 3.944633722305298, lamda: 1.7643681764602661
Running avgs for agent 1: q_loss: 7.997398376464844, p_loss: -3.5280604362487793, mean_rew: -6.81824582866433, variance: 4.25230598449707, lamda: 1.627955436706543
Running avgs for agent 2: q_loss: 1000542.875, p_loss: 130.9071807861328, mean_rew: -6.8227455480156545, variance: 147.92074868774415, lamda: 2.460214376449585

steps: 1524975, episodes: 61000, mean episode reward: -559.2059126182145, agent episode reward: [-186.40197087273816, -186.40197087273816, -186.40197087273816], time: 157.197
steps: 1524975, episodes: 61000, mean episode variance: 38.21176076960564, agent episode variance: [0.9843208618164062, 1.0675623977184296, 36.1598775100708], time: 157.197
Running avgs for agent 0: q_loss: 5.7079315185546875, p_loss: -3.5032241344451904, mean_rew: -6.837490389363592, variance: 3.937283754348755, lamda: 1.766833782196045
Running avgs for agent 1: q_loss: 7.955937385559082, p_loss: -3.535031318664551, mean_rew: -6.836852731801561, variance: 4.270249366760254, lamda: 1.6279553174972534
Running avgs for agent 2: q_loss: 1019255.5, p_loss: 129.88516235351562, mean_rew: -6.836539665201017, variance: 144.6395100402832, lamda: 2.4851889610290527

steps: 1549975, episodes: 62000, mean episode reward: -555.1310425149285, agent episode reward: [-185.04368083830946, -185.04368083830946, -185.04368083830946], time: 156.342
steps: 1549975, episodes: 62000, mean episode variance: 37.413679553985595, agent episode variance: [0.985076164484024, 1.0682762715816498, 35.36032711791992], time: 156.343
Running avgs for agent 0: q_loss: 5.580572605133057, p_loss: -3.5081310272216797, mean_rew: -6.858986562354408, variance: 3.94030499458313, lamda: 1.7683956623077393
Running avgs for agent 1: q_loss: 7.858901500701904, p_loss: -3.547079086303711, mean_rew: -6.8580664900680075, variance: 4.273105144500732, lamda: 1.6280282735824585
Running avgs for agent 2: q_loss: 983359.375, p_loss: 128.77069091796875, mean_rew: -6.851924439120575, variance: 141.4413084716797, lamda: 2.5101630687713623

steps: 1574975, episodes: 63000, mean episode reward: -559.214948861945, agent episode reward: [-186.40498295398163, -186.40498295398163, -186.40498295398163], time: 157.314
steps: 1574975, episodes: 63000, mean episode variance: 36.31008952593803, agent episode variance: [0.9858124713897705, 1.0706698920726776, 34.25360716247559], time: 157.314
Running avgs for agent 0: q_loss: 5.504850387573242, p_loss: -3.5165443420410156, mean_rew: -6.869059269429987, variance: 3.9432499408721924, lamda: 1.7700517177581787
Running avgs for agent 1: q_loss: 7.711836814880371, p_loss: -3.5569849014282227, mean_rew: -6.8727685845422295, variance: 4.282679080963135, lamda: 1.6284396648406982
Running avgs for agent 2: q_loss: 939513.125, p_loss: 128.0293426513672, mean_rew: -6.868332366066736, variance: 137.01442864990236, lamda: 2.535137414932251

steps: 1599975, episodes: 64000, mean episode reward: -570.2464770939383, agent episode reward: [-190.08215903131273, -190.08215903131273, -190.08215903131273], time: 161.873
steps: 1599975, episodes: 64000, mean episode variance: 36.53243747329712, agent episode variance: [0.9836111209392547, 1.0714659016132355, 34.47736045074463], time: 161.873
Running avgs for agent 0: q_loss: 5.814032077789307, p_loss: -3.525299549102783, mean_rew: -6.88448632247366, variance: 3.9344446659088135, lamda: 1.772748589515686
Running avgs for agent 1: q_loss: 7.7122802734375, p_loss: -3.564500331878662, mean_rew: -6.8848326713443715, variance: 4.285863399505615, lamda: 1.6286653280258179
Running avgs for agent 2: q_loss: 940386.5625, p_loss: 127.51869201660156, mean_rew: -6.882138159642469, variance: 137.9094418029785, lamda: 2.5601117610931396

steps: 1624975, episodes: 65000, mean episode reward: -574.7967312466876, agent episode reward: [-191.5989104155625, -191.5989104155625, -191.5989104155625], time: 160.542
steps: 1624975, episodes: 65000, mean episode variance: 35.85417727470398, agent episode variance: [0.9860876669883728, 1.0700523381233216, 33.798037269592285], time: 160.543
Running avgs for agent 0: q_loss: 5.6275105476379395, p_loss: -3.5207879543304443, mean_rew: -6.900427177308491, variance: 3.944350481033325, lamda: 1.7772570848464966
Running avgs for agent 1: q_loss: 7.621874809265137, p_loss: -3.5718328952789307, mean_rew: -6.902484654734756, variance: 4.280209541320801, lamda: 1.6289987564086914
Running avgs for agent 2: q_loss: 959960.8125, p_loss: 127.28865814208984, mean_rew: -6.906536313147213, variance: 135.19214907836914, lamda: 2.5850861072540283

steps: 1649975, episodes: 66000, mean episode reward: -577.4479858580432, agent episode reward: [-192.48266195268107, -192.48266195268107, -192.48266195268107], time: 158.186
steps: 1649975, episodes: 66000, mean episode variance: 35.083803431749345, agent episode variance: [0.9858978576660157, 1.0776874649524688, 33.02021810913086], time: 158.187
Running avgs for agent 0: q_loss: 6.879499912261963, p_loss: -3.535317897796631, mean_rew: -6.926636439871212, variance: 3.943591356277466, lamda: 1.7797882556915283
Running avgs for agent 1: q_loss: 7.551768779754639, p_loss: -3.577208995819092, mean_rew: -6.921849913217078, variance: 4.3107500076293945, lamda: 1.6289989948272705
Running avgs for agent 2: q_loss: 932049.4375, p_loss: 127.26323699951172, mean_rew: -6.918405754396846, variance: 132.08087243652344, lamda: 2.610060453414917

steps: 1674975, episodes: 67000, mean episode reward: -582.7779205572109, agent episode reward: [-194.25930685240363, -194.25930685240363, -194.25930685240363], time: 158.823
steps: 1674975, episodes: 67000, mean episode variance: 35.32280994272232, agent episode variance: [0.9824245715141297, 1.0825768842697143, 33.25780848693848], time: 158.824
Running avgs for agent 0: q_loss: 8.479998588562012, p_loss: -3.540515899658203, mean_rew: -6.927435220079465, variance: 3.9296982288360596, lamda: 1.7813701629638672
Running avgs for agent 1: q_loss: 7.596108436584473, p_loss: -3.5842559337615967, mean_rew: -6.939493680049028, variance: 4.3303070068359375, lamda: 1.6289987564086914
Running avgs for agent 2: q_loss: 926109.4375, p_loss: 127.39910888671875, mean_rew: -6.93697425248458, variance: 133.03123394775392, lamda: 2.6350347995758057

steps: 1699975, episodes: 68000, mean episode reward: -581.3794987842934, agent episode reward: [-193.79316626143117, -193.79316626143117, -193.79316626143117], time: 160.553
steps: 1699975, episodes: 68000, mean episode variance: 34.939522227525714, agent episode variance: [0.9866421794891358, 1.0794662678241729, 32.8734137802124], time: 160.553
Running avgs for agent 0: q_loss: 8.273743629455566, p_loss: -3.5481154918670654, mean_rew: -6.945084345101941, variance: 3.946568489074707, lamda: 1.7813730239868164
Running avgs for agent 1: q_loss: 7.321964740753174, p_loss: -3.592369318008423, mean_rew: -6.934753575694708, variance: 4.317865371704102, lamda: 1.6291664838790894
Running avgs for agent 2: q_loss: 946173.6875, p_loss: 127.7751235961914, mean_rew: -6.942537185019672, variance: 131.4936551208496, lamda: 2.6600093841552734

steps: 1724975, episodes: 69000, mean episode reward: -582.7874487357487, agent episode reward: [-194.26248291191627, -194.26248291191627, -194.26248291191627], time: 161.16
steps: 1724975, episodes: 69000, mean episode variance: 34.5464624736309, agent episode variance: [0.9883983616828919, 1.0818540213108063, 32.47621009063721], time: 161.16
Running avgs for agent 0: q_loss: 8.086791038513184, p_loss: -3.5568766593933105, mean_rew: -6.954500210413, variance: 3.9535932540893555, lamda: 1.7813730239868164
Running avgs for agent 1: q_loss: 5.488936424255371, p_loss: -3.5926928520202637, mean_rew: -6.950661076400411, variance: 4.327415943145752, lamda: 1.6301606893539429
Running avgs for agent 2: q_loss: 908050.3125, p_loss: 128.14328002929688, mean_rew: -6.951285664602731, variance: 129.90484036254884, lamda: 2.684983491897583

steps: 1749975, episodes: 70000, mean episode reward: -577.8627503937018, agent episode reward: [-192.6209167979006, -192.6209167979006, -192.6209167979006], time: 158.961
steps: 1749975, episodes: 70000, mean episode variance: 34.069427424192426, agent episode variance: [0.9871487948894501, 1.0848809776306152, 31.99739765167236], time: 158.962
Running avgs for agent 0: q_loss: 7.948122978210449, p_loss: -3.554894208908081, mean_rew: -6.96431131291717, variance: 3.9485950469970703, lamda: 1.7813886404037476
Running avgs for agent 1: q_loss: 4.523507595062256, p_loss: -3.6000897884368896, mean_rew: -6.964565915265368, variance: 4.339524269104004, lamda: 1.6318871974945068
Running avgs for agent 2: q_loss: 916698.375, p_loss: 128.68142700195312, mean_rew: -6.963334867344645, variance: 127.98959060668945, lamda: 2.709958076477051

steps: 1774975, episodes: 71000, mean episode reward: -580.3603401678328, agent episode reward: [-193.45344672261092, -193.45344672261092, -193.45344672261092], time: 158.276
steps: 1774975, episodes: 71000, mean episode variance: 34.07074389529228, agent episode variance: [0.992770860671997, 1.0820831954479218, 31.995889839172364], time: 158.277
Running avgs for agent 0: q_loss: 7.90104341506958, p_loss: -3.570756673812866, mean_rew: -6.985719178340869, variance: 3.971083402633667, lamda: 1.7815200090408325
Running avgs for agent 1: q_loss: 4.269871234893799, p_loss: -3.6101675033569336, mean_rew: -6.985311596117709, variance: 4.328332901000977, lamda: 1.633207082748413
Running avgs for agent 2: q_loss: 925088.875, p_loss: 129.34996032714844, mean_rew: -6.984566158891435, variance: 127.98355935668945, lamda: 2.7349321842193604

steps: 1799975, episodes: 72000, mean episode reward: -578.8100641847716, agent episode reward: [-192.93668806159053, -192.93668806159053, -192.93668806159053], time: 163.668
steps: 1799975, episodes: 72000, mean episode variance: 34.09957861757278, agent episode variance: [0.999784677028656, 1.0914048490524293, 32.0083890914917], time: 163.669
Running avgs for agent 0: q_loss: 7.802588939666748, p_loss: -3.579101085662842, mean_rew: -7.013538782671255, variance: 3.999138593673706, lamda: 1.7815208435058594
Running avgs for agent 1: q_loss: 4.241026401519775, p_loss: -3.612731456756592, mean_rew: -7.00795544520098, variance: 4.36561918258667, lamda: 1.6342602968215942
Running avgs for agent 2: q_loss: 940770.75, p_loss: 130.17068481445312, mean_rew: -7.009350928894894, variance: 128.0335563659668, lamda: 2.759906768798828

steps: 1824975, episodes: 73000, mean episode reward: -582.6103476815848, agent episode reward: [-194.2034492271949, -194.2034492271949, -194.2034492271949], time: 162.88
steps: 1824975, episodes: 73000, mean episode variance: 35.09047891521454, agent episode variance: [1.0023404457569123, 1.0932656819820403, 32.994872787475586], time: 162.88
Running avgs for agent 0: q_loss: 7.826251029968262, p_loss: -3.598186731338501, mean_rew: -7.037802685017949, variance: 4.009361743927002, lamda: 1.7815207242965698
Running avgs for agent 1: q_loss: 4.167547702789307, p_loss: -3.631228446960449, mean_rew: -7.037333193770164, variance: 4.373062610626221, lamda: 1.6348872184753418
Running avgs for agent 2: q_loss: 940226.125, p_loss: 131.0338134765625, mean_rew: -7.037625835470644, variance: 131.97949114990234, lamda: 2.7848808765411377

steps: 1849975, episodes: 74000, mean episode reward: -589.4351509573369, agent episode reward: [-196.47838365244561, -196.47838365244561, -196.47838365244561], time: 162.049
steps: 1849975, episodes: 74000, mean episode variance: 35.61588182783127, agent episode variance: [1.0065517094135283, 1.105120805978775, 33.50420931243897], time: 162.05
Running avgs for agent 0: q_loss: 7.808599472045898, p_loss: -3.6141152381896973, mean_rew: -7.065566735247686, variance: 4.026206970214844, lamda: 1.7815558910369873
Running avgs for agent 1: q_loss: 4.292177200317383, p_loss: -3.646984577178955, mean_rew: -7.075296193963708, variance: 4.420483589172363, lamda: 1.6353460550308228
Running avgs for agent 2: q_loss: 991682.5625, p_loss: 132.01089477539062, mean_rew: -7.070325370838349, variance: 134.01683724975587, lamda: 2.8098552227020264

steps: 1874975, episodes: 75000, mean episode reward: -585.1179803310176, agent episode reward: [-195.03932677700587, -195.03932677700587, -195.03932677700587], time: 160.01
steps: 1874975, episodes: 75000, mean episode variance: 34.64984832000732, agent episode variance: [1.0144487874507904, 1.1043107950687407, 32.53108873748779], time: 160.011
Running avgs for agent 0: q_loss: 7.77972412109375, p_loss: -3.6325626373291016, mean_rew: -7.105351092570352, variance: 4.05779504776001, lamda: 1.7817922830581665
Running avgs for agent 1: q_loss: 4.201701641082764, p_loss: -3.6611597537994385, mean_rew: -7.103897663504024, variance: 4.417243003845215, lamda: 1.6357113122940063
Running avgs for agent 2: q_loss: 1031483.625, p_loss: 133.22105407714844, mean_rew: -7.105962531792389, variance: 130.12435494995117, lamda: 2.834829568862915

steps: 1899975, episodes: 76000, mean episode reward: -576.5433805542065, agent episode reward: [-192.18112685140215, -192.18112685140215, -192.18112685140215], time: 157.4
steps: 1899975, episodes: 76000, mean episode variance: 36.92846523332596, agent episode variance: [1.013752905845642, 1.1084873061180114, 34.8062250213623], time: 157.4
Running avgs for agent 0: q_loss: 7.764813423156738, p_loss: -3.6454007625579834, mean_rew: -7.126087190626506, variance: 4.05501127243042, lamda: 1.7819980382919312
Running avgs for agent 1: q_loss: 4.465697765350342, p_loss: -3.66231632232666, mean_rew: -7.124765885068574, variance: 4.433948993682861, lamda: 1.638061761856079
Running avgs for agent 2: q_loss: 1037348.25, p_loss: 134.0337677001953, mean_rew: -7.131882055817263, variance: 139.2249000854492, lamda: 2.8598039150238037

steps: 1924975, episodes: 77000, mean episode reward: -577.1531377772986, agent episode reward: [-192.3843792590996, -192.3843792590996, -192.3843792590996], time: 158.434
steps: 1924975, episodes: 77000, mean episode variance: 35.41399190473557, agent episode variance: [1.0208014283180238, 1.1090009469985962, 33.284189529418946], time: 158.434
Running avgs for agent 0: q_loss: 7.742220878601074, p_loss: -3.658193826675415, mean_rew: -7.153857034588396, variance: 4.083205699920654, lamda: 1.7819982767105103
Running avgs for agent 1: q_loss: 4.2259745597839355, p_loss: -3.686180591583252, mean_rew: -7.152136297794643, variance: 4.436003684997559, lamda: 1.6400643587112427
Running avgs for agent 2: q_loss: 1078063.125, p_loss: 134.88217163085938, mean_rew: -7.168221605396616, variance: 133.13675811767578, lamda: 2.8847782611846924

steps: 1949975, episodes: 78000, mean episode reward: -587.1917375793066, agent episode reward: [-195.73057919310222, -195.73057919310222, -195.73057919310222], time: 167.695
steps: 1949975, episodes: 78000, mean episode variance: 35.838385781288146, agent episode variance: [1.0258453743457794, 1.1126263139247894, 33.699914093017576], time: 167.696
Running avgs for agent 0: q_loss: 7.7143964767456055, p_loss: -3.673682689666748, mean_rew: -7.183898837081394, variance: 4.103381633758545, lamda: 1.7820008993148804
Running avgs for agent 1: q_loss: 4.4632368087768555, p_loss: -3.7021684646606445, mean_rew: -7.184710412797321, variance: 4.450504779815674, lamda: 1.642187237739563
Running avgs for agent 2: q_loss: 1100156.25, p_loss: 135.7723388671875, mean_rew: -7.1859973664070695, variance: 134.7996563720703, lamda: 2.909752607345581

steps: 1974975, episodes: 79000, mean episode reward: -579.4894977409443, agent episode reward: [-193.16316591364816, -193.16316591364816, -193.16316591364816], time: 166.782
steps: 1974975, episodes: 79000, mean episode variance: 37.029277113199235, agent episode variance: [1.0306236345767974, 1.1196954708099365, 34.8789580078125], time: 166.783
Running avgs for agent 0: q_loss: 7.776306629180908, p_loss: -3.693995952606201, mean_rew: -7.21420164783425, variance: 4.122494697570801, lamda: 1.7820608615875244
Running avgs for agent 1: q_loss: 4.177145004272461, p_loss: -3.7170569896698, mean_rew: -7.220000676191393, variance: 4.4787821769714355, lamda: 1.6438329219818115
Running avgs for agent 2: q_loss: 1132522.625, p_loss: 136.577880859375, mean_rew: -7.216414132747094, variance: 139.51583203125, lamda: 2.9347269535064697

steps: 1999975, episodes: 80000, mean episode reward: -589.9640645278897, agent episode reward: [-196.65468817596326, -196.65468817596326, -196.65468817596326], time: 168.221
steps: 1999975, episodes: 80000, mean episode variance: 38.210696453571316, agent episode variance: [1.0335660972595215, 1.1178746800422668, 36.05925567626953], time: 168.222
Running avgs for agent 0: q_loss: 7.829434394836426, p_loss: -3.7154903411865234, mean_rew: -7.256827634631992, variance: 4.1342644691467285, lamda: 1.7821779251098633
Running avgs for agent 1: q_loss: 4.282108783721924, p_loss: -3.737169027328491, mean_rew: -7.2477247982076385, variance: 4.471498966217041, lamda: 1.6444758176803589
Running avgs for agent 2: q_loss: 1165180.75, p_loss: 137.39378356933594, mean_rew: -7.247862720300127, variance: 144.23702270507812, lamda: 2.9597010612487793

steps: 2024975, episodes: 81000, mean episode reward: -581.9754328099909, agent episode reward: [-193.99181093666365, -193.99181093666365, -193.99181093666365], time: 162.608
steps: 2024975, episodes: 81000, mean episode variance: 36.049876195430755, agent episode variance: [1.042598972082138, 1.130711412191391, 33.87656581115723], time: 162.609
Running avgs for agent 0: q_loss: 7.773616313934326, p_loss: -3.7284328937530518, mean_rew: -7.279017640594182, variance: 4.170395851135254, lamda: 1.782216191291809
Running avgs for agent 1: q_loss: 4.223693370819092, p_loss: -3.749549150466919, mean_rew: -7.281443797615332, variance: 4.52284574508667, lamda: 1.6444787979125977
Running avgs for agent 2: q_loss: 1186263.125, p_loss: 138.30245971679688, mean_rew: -7.27400964519753, variance: 135.5062632446289, lamda: 2.984675884246826

steps: 2049975, episodes: 82000, mean episode reward: -589.1799494583775, agent episode reward: [-196.3933164861258, -196.3933164861258, -196.3933164861258], time: 161.97
steps: 2049975, episodes: 82000, mean episode variance: 38.84726625752449, agent episode variance: [1.0454299278259278, 1.1288856613636016, 36.67295066833496], time: 161.971
Running avgs for agent 0: q_loss: 7.714005947113037, p_loss: -3.7391676902770996, mean_rew: -7.30080380653318, variance: 4.181719779968262, lamda: 1.7822201251983643
Running avgs for agent 1: q_loss: 4.4423370361328125, p_loss: -3.765028476715088, mean_rew: -7.311107245015526, variance: 4.515542984008789, lamda: 1.6444929838180542
Running avgs for agent 2: q_loss: 1200906.75, p_loss: 138.84231567382812, mean_rew: -7.307346601803926, variance: 146.69180267333985, lamda: 3.009650230407715

steps: 2074975, episodes: 83000, mean episode reward: -584.1743516419889, agent episode reward: [-194.72478388066293, -194.72478388066293, -194.72478388066293], time: 167.308
steps: 2074975, episodes: 83000, mean episode variance: 38.29784501171112, agent episode variance: [1.0496622219085694, 1.1397173738479613, 36.10846541595459], time: 167.308
Running avgs for agent 0: q_loss: 7.677051544189453, p_loss: -3.765063524246216, mean_rew: -7.340340153333738, variance: 4.198648929595947, lamda: 1.7822203636169434
Running avgs for agent 1: q_loss: 4.632260799407959, p_loss: -3.773000955581665, mean_rew: -7.3318317493390435, variance: 4.5588698387146, lamda: 1.6447169780731201
Running avgs for agent 2: q_loss: 1218589.875, p_loss: 139.5162353515625, mean_rew: -7.343160372493523, variance: 144.43386166381836, lamda: 3.0346245765686035

steps: 2099975, episodes: 84000, mean episode reward: -586.6433316654932, agent episode reward: [-195.54777722183104, -195.54777722183104, -195.54777722183104], time: 164.113
steps: 2099975, episodes: 84000, mean episode variance: 38.206697030067446, agent episode variance: [1.0537696907520293, 1.1448878037929535, 36.008039535522464], time: 164.114
Running avgs for agent 0: q_loss: 7.685391426086426, p_loss: -3.7776803970336914, mean_rew: -7.36193481485175, variance: 4.215079307556152, lamda: 1.7824879884719849
Running avgs for agent 1: q_loss: 5.685195446014404, p_loss: -3.790090322494507, mean_rew: -7.3647578091345425, variance: 4.5795512199401855, lamda: 1.6482350826263428
Running avgs for agent 2: q_loss: 1243628.75, p_loss: 140.2247314453125, mean_rew: -7.364095739242178, variance: 144.03215814208986, lamda: 3.059598684310913

steps: 2124975, episodes: 85000, mean episode reward: -592.1887017011163, agent episode reward: [-197.39623390037212, -197.39623390037212, -197.39623390037212], time: 164.998
steps: 2124975, episodes: 85000, mean episode variance: 38.52192522740364, agent episode variance: [1.0602028336524962, 1.140496845960617, 36.321225547790526], time: 164.998
Running avgs for agent 0: q_loss: 7.778416156768799, p_loss: -3.793454647064209, mean_rew: -7.400245882612073, variance: 4.240810871124268, lamda: 1.783055305480957
Running avgs for agent 1: q_loss: 7.903426647186279, p_loss: -3.8098175525665283, mean_rew: -7.39653831311585, variance: 4.561987400054932, lamda: 1.6493558883666992
Running avgs for agent 2: q_loss: 1274240.25, p_loss: 140.83432006835938, mean_rew: -7.399966912646586, variance: 145.2849021911621, lamda: 3.0845730304718018

steps: 2149975, episodes: 86000, mean episode reward: -584.5217636680795, agent episode reward: [-194.84058788935982, -194.84058788935982, -194.84058788935982], time: 162.852
steps: 2149975, episodes: 86000, mean episode variance: 38.79561600518227, agent episode variance: [1.0593546693325042, 1.1483591446876527, 36.58790219116211], time: 162.852
Running avgs for agent 0: q_loss: 7.713328838348389, p_loss: -3.815230369567871, mean_rew: -7.435021651554235, variance: 4.237419128417969, lamda: 1.783376693725586
Running avgs for agent 1: q_loss: 7.998481273651123, p_loss: -3.822197437286377, mean_rew: -7.425444851157235, variance: 4.5934367179870605, lamda: 1.6493560075759888
Running avgs for agent 2: q_loss: 1285464.5, p_loss: 141.54547119140625, mean_rew: -7.426552145774652, variance: 146.35160876464843, lamda: 3.1095473766326904

steps: 2174975, episodes: 87000, mean episode reward: -589.7052252540851, agent episode reward: [-196.56840841802838, -196.56840841802838, -196.56840841802838], time: 165.432
steps: 2174975, episodes: 87000, mean episode variance: 38.77921794581413, agent episode variance: [1.073426096200943, 1.1591693978309632, 36.54662245178223], time: 165.433
Running avgs for agent 0: q_loss: 7.666393756866455, p_loss: -3.8184289932250977, mean_rew: -7.451355833781199, variance: 4.293704509735107, lamda: 1.783458948135376
Running avgs for agent 1: q_loss: 7.929924964904785, p_loss: -3.8315041065216064, mean_rew: -7.454338671896303, variance: 4.6366777420043945, lamda: 1.6493557691574097
Running avgs for agent 2: q_loss: 1286697.25, p_loss: 142.08790588378906, mean_rew: -7.453226327221075, variance: 146.1864898071289, lamda: 3.134521961212158

steps: 2199975, episodes: 88000, mean episode reward: -588.1423127433281, agent episode reward: [-196.04743758110942, -196.04743758110942, -196.04743758110942], time: 161.825
steps: 2199975, episodes: 88000, mean episode variance: 38.506797955274585, agent episode variance: [1.0783158087730407, 1.160268248796463, 36.26821389770508], time: 161.825
Running avgs for agent 0: q_loss: 7.565425395965576, p_loss: -3.8394827842712402, mean_rew: -7.48383318537781, variance: 4.313263416290283, lamda: 1.7836568355560303
Running avgs for agent 1: q_loss: 7.9756083488464355, p_loss: -3.852163553237915, mean_rew: -7.48340375763061, variance: 4.641073226928711, lamda: 1.6493558883666992
Running avgs for agent 2: q_loss: 1309429.625, p_loss: 142.68690490722656, mean_rew: -7.487754762887695, variance: 145.07285559082032, lamda: 3.1594960689544678

steps: 2224975, episodes: 89000, mean episode reward: -598.7939644883272, agent episode reward: [-199.59798816277575, -199.59798816277575, -199.59798816277575], time: 164.282
steps: 2224975, episodes: 89000, mean episode variance: 38.91914345431328, agent episode variance: [1.075937203168869, 1.1551007823944093, 36.68810546875], time: 164.283
Running avgs for agent 0: q_loss: 7.574439525604248, p_loss: -3.860785722732544, mean_rew: -7.515633574352818, variance: 4.303749084472656, lamda: 1.7839267253875732
Running avgs for agent 1: q_loss: 7.988579750061035, p_loss: -3.867873430252075, mean_rew: -7.516076290629082, variance: 4.620403289794922, lamda: 1.6493560075759888
Running avgs for agent 2: q_loss: 1317062.125, p_loss: 143.216064453125, mean_rew: -7.518174176671204, variance: 146.752421875, lamda: 3.1844704151153564

steps: 2249975, episodes: 90000, mean episode reward: -591.4453515204189, agent episode reward: [-197.14845050680626, -197.14845050680626, -197.14845050680626], time: 162.433
steps: 2249975, episodes: 90000, mean episode variance: 40.068754809618, agent episode variance: [1.0860611965656282, 1.1747311038970947, 37.807962509155274], time: 162.434
Running avgs for agent 0: q_loss: 7.634875297546387, p_loss: -3.87778377532959, mean_rew: -7.552312727653387, variance: 4.344244480133057, lamda: 1.7842283248901367
Running avgs for agent 1: q_loss: 8.088191986083984, p_loss: -3.8793935775756836, mean_rew: -7.543871032229612, variance: 4.698925018310547, lamda: 1.6493691205978394
Running avgs for agent 2: q_loss: 1310691.0, p_loss: 143.64068603515625, mean_rew: -7.548638726246243, variance: 151.2318500366211, lamda: 3.209444761276245

steps: 2274975, episodes: 91000, mean episode reward: -586.5238338037567, agent episode reward: [-195.50794460125226, -195.50794460125226, -195.50794460125226], time: 163.639
steps: 2274975, episodes: 91000, mean episode variance: 38.55192052102089, agent episode variance: [1.0899821379184722, 1.1829113445281982, 36.27902703857422], time: 163.64
Running avgs for agent 0: q_loss: 7.605390548706055, p_loss: -3.898137092590332, mean_rew: -7.582137527110974, variance: 4.359928607940674, lamda: 1.7844185829162598
Running avgs for agent 1: q_loss: 7.669629096984863, p_loss: -3.8905820846557617, mean_rew: -7.57384586332358, variance: 4.731645584106445, lamda: 1.6499840021133423
Running avgs for agent 2: q_loss: 1351200.25, p_loss: 144.12655639648438, mean_rew: -7.574953017732418, variance: 145.11610815429688, lamda: 3.2344188690185547

steps: 2299975, episodes: 92000, mean episode reward: -587.5587844521541, agent episode reward: [-195.85292815071804, -195.85292815071804, -195.85292815071804], time: 164.692
steps: 2299975, episodes: 92000, mean episode variance: 38.99883190059662, agent episode variance: [1.0908894820213317, 1.1797053275108338, 36.72823709106445], time: 164.693
Running avgs for agent 0: q_loss: 7.605045795440674, p_loss: -3.9055967330932617, mean_rew: -7.5999765407587345, variance: 4.363558292388916, lamda: 1.7844843864440918
Running avgs for agent 1: q_loss: 7.951261520385742, p_loss: -3.9126508235931396, mean_rew: -7.6035752325352295, variance: 4.7188215255737305, lamda: 1.651090383529663
Running avgs for agent 2: q_loss: 1375335.375, p_loss: 144.62393188476562, mean_rew: -7.608897788772978, variance: 146.9129483642578, lamda: 3.2593934535980225

steps: 2324975, episodes: 93000, mean episode reward: -591.4125357304874, agent episode reward: [-197.13751191016246, -197.13751191016246, -197.13751191016246], time: 164.102
steps: 2324975, episodes: 93000, mean episode variance: 38.97651544904709, agent episode variance: [1.0957068157196046, 1.1798845763206482, 36.70092405700684], time: 164.103
Running avgs for agent 0: q_loss: 7.633947372436523, p_loss: -3.921853542327881, mean_rew: -7.625971163731216, variance: 4.382827281951904, lamda: 1.7847776412963867
Running avgs for agent 1: q_loss: 8.080336570739746, p_loss: -3.92584228515625, mean_rew: -7.632013177640483, variance: 4.71953821182251, lamda: 1.651090383529663
Running avgs for agent 2: q_loss: 1356487.375, p_loss: 145.15939331054688, mean_rew: -7.621413464074305, variance: 146.80369622802735, lamda: 3.2843680381774902

steps: 2349975, episodes: 94000, mean episode reward: -586.1867284252495, agent episode reward: [-195.39557614174976, -195.39557614174976, -195.39557614174976], time: 161.02
steps: 2349975, episodes: 94000, mean episode variance: 38.1246467628479, agent episode variance: [1.099799910545349, 1.1961164903640746, 35.828730361938476], time: 161.02
Running avgs for agent 0: q_loss: 7.333908557891846, p_loss: -3.939289093017578, mean_rew: -7.654098345571321, variance: 4.39919900894165, lamda: 1.7858984470367432
Running avgs for agent 1: q_loss: 7.97893762588501, p_loss: -3.929372549057007, mean_rew: -7.651364948551024, variance: 4.784465789794922, lamda: 1.6510905027389526
Running avgs for agent 2: q_loss: 1387950.375, p_loss: 145.71444702148438, mean_rew: -7.6628550360334895, variance: 143.3149214477539, lamda: 3.309342384338379

steps: 2374975, episodes: 95000, mean episode reward: -584.9700494116796, agent episode reward: [-194.99001647055988, -194.99001647055988, -194.99001647055988], time: 162.203
steps: 2374975, episodes: 95000, mean episode variance: 38.808320491313935, agent episode variance: [1.1015974190235138, 1.1948885691165925, 36.51183450317383], time: 162.203
Running avgs for agent 0: q_loss: 7.5786590576171875, p_loss: -3.9505603313446045, mean_rew: -7.684185608461747, variance: 4.4063897132873535, lamda: 1.788983941078186
Running avgs for agent 1: q_loss: 7.943239212036133, p_loss: -3.9534711837768555, mean_rew: -7.680230665523042, variance: 4.77955436706543, lamda: 1.651090383529663
Running avgs for agent 2: q_loss: 1427446.25, p_loss: 146.19680786132812, mean_rew: -7.682943047191339, variance: 146.04733801269532, lamda: 3.3343167304992676

steps: 2399975, episodes: 96000, mean episode reward: -585.1109457965803, agent episode reward: [-195.03698193219338, -195.03698193219338, -195.03698193219338], time: 165.345
steps: 2399975, episodes: 96000, mean episode variance: 37.76552064394951, agent episode variance: [1.1038796570301055, 1.1914508776664734, 35.47019010925293], time: 165.345
Running avgs for agent 0: q_loss: 7.500082015991211, p_loss: -3.9578073024749756, mean_rew: -7.693994709378583, variance: 4.415518760681152, lamda: 1.789129614830017
Running avgs for agent 1: q_loss: 8.001725196838379, p_loss: -3.9665932655334473, mean_rew: -7.7075886900867685, variance: 4.765803813934326, lamda: 1.651090383529663
Running avgs for agent 2: q_loss: 1410813.5, p_loss: 146.59303283691406, mean_rew: -7.705813891055368, variance: 141.88076043701173, lamda: 3.359290838241577

steps: 2424975, episodes: 97000, mean episode reward: -584.1541796387013, agent episode reward: [-194.71805987956708, -194.71805987956708, -194.71805987956708], time: 163.914
steps: 2424975, episodes: 97000, mean episode variance: 38.017386295080186, agent episode variance: [1.10510023021698, 1.1964468009471894, 35.71583926391602], time: 163.915
Running avgs for agent 0: q_loss: 7.493804454803467, p_loss: -3.979963541030884, mean_rew: -7.72845513817378, variance: 4.420401096343994, lamda: 1.790783166885376
Running avgs for agent 1: q_loss: 7.949769496917725, p_loss: -3.972817897796631, mean_rew: -7.725279652404379, variance: 4.785787105560303, lamda: 1.6510905027389526
Running avgs for agent 2: q_loss: 1380629.75, p_loss: 147.08233642578125, mean_rew: -7.721164661195391, variance: 142.86335705566407, lamda: 3.384265184402466

steps: 2449975, episodes: 98000, mean episode reward: -582.3784838392309, agent episode reward: [-194.1261612797436, -194.1261612797436, -194.1261612797436], time: 164.835
steps: 2449975, episodes: 98000, mean episode variance: 38.574623616695405, agent episode variance: [1.1135007166862487, 1.2005561122894286, 36.260566787719725], time: 164.835
Running avgs for agent 0: q_loss: 7.727753162384033, p_loss: -3.9818007946014404, mean_rew: -7.742099439784508, variance: 4.454002857208252, lamda: 1.7923388481140137
Running avgs for agent 1: q_loss: 8.014780044555664, p_loss: -3.981642961502075, mean_rew: -7.748082512411153, variance: 4.802224636077881, lamda: 1.651090383529663
Running avgs for agent 2: q_loss: 1361443.375, p_loss: 147.35699462890625, mean_rew: -7.7397662921078805, variance: 145.0422671508789, lamda: 3.4092397689819336

steps: 2474975, episodes: 99000, mean episode reward: -587.294788146649, agent episode reward: [-195.76492938221637, -195.76492938221637, -195.76492938221637], time: 167.868
steps: 2474975, episodes: 99000, mean episode variance: 37.00320372223854, agent episode variance: [1.1106555140018464, 1.2017269115447997, 34.690821296691894], time: 167.869
Running avgs for agent 0: q_loss: 7.5478010177612305, p_loss: -3.9897310733795166, mean_rew: -7.751805601638195, variance: 4.442622184753418, lamda: 1.7935136556625366
Running avgs for agent 1: q_loss: 7.777107238769531, p_loss: -3.979923963546753, mean_rew: -7.752293301210739, variance: 4.8069071769714355, lamda: 1.6510905027389526
Running avgs for agent 2: q_loss: 1415218.75, p_loss: 147.75079345703125, mean_rew: -7.748277914857996, variance: 138.76328518676758, lamda: 3.434213876724243

steps: 2499975, episodes: 100000, mean episode reward: -589.7159328690237, agent episode reward: [-196.5719776230079, -196.5719776230079, -196.5719776230079], time: 160.275
steps: 2499975, episodes: 100000, mean episode variance: 37.57119820141792, agent episode variance: [1.1119837245941162, 1.208858062028885, 35.25035641479492], time: 160.276
Running avgs for agent 0: q_loss: 7.327200889587402, p_loss: -4.002667427062988, mean_rew: -7.777364618226336, variance: 4.447934627532959, lamda: 1.7946903705596924
Running avgs for agent 1: q_loss: 7.771251678466797, p_loss: -3.992560863494873, mean_rew: -7.770250830925192, variance: 4.835432529449463, lamda: 1.6510906219482422
Running avgs for agent 2: q_loss: 1400852.25, p_loss: 148.02615356445312, mean_rew: -7.768528857963983, variance: 141.0014256591797, lamda: 3.4591879844665527

steps: 2524975, episodes: 101000, mean episode reward: -587.737697050563, agent episode reward: [-195.91256568352105, -195.91256568352105, -195.91256568352105], time: 165.891
steps: 2524975, episodes: 101000, mean episode variance: 37.22415108084679, agent episode variance: [1.116112934589386, 1.2066998283863068, 34.901338317871094], time: 165.892
Running avgs for agent 0: q_loss: 7.651858806610107, p_loss: -4.001783847808838, mean_rew: -7.777556465939312, variance: 4.464451789855957, lamda: 1.795915961265564
Running avgs for agent 1: q_loss: 7.783002853393555, p_loss: -3.9970927238464355, mean_rew: -7.77672039158979, variance: 4.826799392700195, lamda: 1.6511167287826538
Running avgs for agent 2: q_loss: 1392183.75, p_loss: 148.1622772216797, mean_rew: -7.780287577740138, variance: 139.60535327148438, lamda: 3.4841625690460205

steps: 2549975, episodes: 102000, mean episode reward: -583.0086050979072, agent episode reward: [-194.33620169930242, -194.33620169930242, -194.33620169930242], time: 167.196
steps: 2549975, episodes: 102000, mean episode variance: 36.29510902547836, agent episode variance: [1.1071336145401, 1.210741295337677, 33.97723411560059], time: 167.197
Running avgs for agent 0: q_loss: 7.615945816040039, p_loss: -4.010218143463135, mean_rew: -7.788665442006244, variance: 4.428534030914307, lamda: 1.796617031097412
Running avgs for agent 1: q_loss: 7.831881999969482, p_loss: -4.009195804595947, mean_rew: -7.796373626771723, variance: 4.842965126037598, lamda: 1.6511507034301758
Running avgs for agent 2: q_loss: 1377019.75, p_loss: 148.31661987304688, mean_rew: -7.786358819791103, variance: 135.90893646240235, lamda: 3.509136915206909

steps: 2574975, episodes: 103000, mean episode reward: -591.7850363502647, agent episode reward: [-197.2616787834216, -197.2616787834216, -197.2616787834216], time: 164.758
steps: 2574975, episodes: 103000, mean episode variance: 36.12999156332016, agent episode variance: [1.1089509131908417, 1.2097282264232636, 33.811312423706056], time: 164.758
Running avgs for agent 0: q_loss: 7.608579158782959, p_loss: -4.013730049133301, mean_rew: -7.797241346779164, variance: 4.4358038902282715, lamda: 1.7976868152618408
Running avgs for agent 1: q_loss: 7.634101390838623, p_loss: -4.006806373596191, mean_rew: -7.7946985090754115, variance: 4.8389129638671875, lamda: 1.6511508226394653
Running avgs for agent 2: q_loss: 1386105.375, p_loss: 148.5101318359375, mean_rew: -7.796983899107927, variance: 135.24524969482422, lamda: 3.534111261367798

steps: 2599975, episodes: 104000, mean episode reward: -587.8218112459355, agent episode reward: [-195.94060374864517, -195.94060374864517, -195.94060374864517], time: 164.002
steps: 2599975, episodes: 104000, mean episode variance: 36.94932183241844, agent episode variance: [1.117194462776184, 1.2164548690319061, 34.61567250061035], time: 164.003
Running avgs for agent 0: q_loss: 7.715206146240234, p_loss: -4.015188217163086, mean_rew: -7.808428882967481, variance: 4.468777656555176, lamda: 1.7984172105789185
Running avgs for agent 1: q_loss: 7.734447002410889, p_loss: -4.014965057373047, mean_rew: -7.808021692725649, variance: 4.865819454193115, lamda: 1.6513117551803589
Running avgs for agent 2: q_loss: 1398812.625, p_loss: 148.8361053466797, mean_rew: -7.813492620842809, variance: 138.4626900024414, lamda: 3.5590856075286865

steps: 2624975, episodes: 105000, mean episode reward: -594.4481698321223, agent episode reward: [-198.14938994404076, -198.14938994404076, -198.14938994404076], time: 173.04
steps: 2624975, episodes: 105000, mean episode variance: 35.86201639151573, agent episode variance: [1.113739547252655, 1.2163274347782136, 33.53194940948487], time: 173.041
Running avgs for agent 0: q_loss: 7.640771865844727, p_loss: -4.023358345031738, mean_rew: -7.8077973413586585, variance: 4.454957962036133, lamda: 1.7991772890090942
Running avgs for agent 1: q_loss: 7.6591620445251465, p_loss: -4.009384632110596, mean_rew: -7.810018133106605, variance: 4.865309715270996, lamda: 1.6516131162643433
Running avgs for agent 2: q_loss: 1385801.25, p_loss: 149.14602661132812, mean_rew: -7.81905475343892, variance: 134.12779763793947, lamda: 3.584059953689575

steps: 2649975, episodes: 106000, mean episode reward: -591.7987180866538, agent episode reward: [-197.26623936221793, -197.26623936221793, -197.26623936221793], time: 176.836
steps: 2649975, episodes: 106000, mean episode variance: 36.01731920361519, agent episode variance: [1.1153696916103364, 1.2098071780204773, 33.69214233398438], time: 176.837
Running avgs for agent 0: q_loss: 5.294516086578369, p_loss: -4.029652118682861, mean_rew: -7.8209941812790476, variance: 4.4614787101745605, lamda: 1.8017946481704712
Running avgs for agent 1: q_loss: 7.718836784362793, p_loss: -4.014546871185303, mean_rew: -7.817915576334778, variance: 4.839228630065918, lamda: 1.6521925926208496
Running avgs for agent 2: q_loss: 1406876.875, p_loss: 149.2845458984375, mean_rew: -7.813439983438562, variance: 134.7685693359375, lamda: 3.609034538269043

steps: 2674975, episodes: 107000, mean episode reward: -601.2340772006014, agent episode reward: [-200.41135906686716, -200.41135906686716, -200.41135906686716], time: 162.966
steps: 2674975, episodes: 107000, mean episode variance: 36.216346017360685, agent episode variance: [1.110846584558487, 1.2205364201068878, 33.884963012695316], time: 162.967
Running avgs for agent 0: q_loss: 5.288300037384033, p_loss: -4.026554107666016, mean_rew: -7.821092676852869, variance: 4.443386077880859, lamda: 1.8069719076156616
Running avgs for agent 1: q_loss: 7.582736492156982, p_loss: -4.016949653625488, mean_rew: -7.823346417820314, variance: 4.882145404815674, lamda: 1.6524163484573364
Running avgs for agent 2: q_loss: 1429591.0, p_loss: 149.52000427246094, mean_rew: -7.823181557754849, variance: 135.53985205078126, lamda: 3.6340088844299316

steps: 2699975, episodes: 108000, mean episode reward: -601.53379275719, agent episode reward: [-200.51126425239664, -200.51126425239664, -200.51126425239664], time: 152.397
steps: 2699975, episodes: 108000, mean episode variance: 35.7520740146637, agent episode variance: [1.1192362678050995, 1.212981072664261, 33.419856674194335], time: 152.397
Running avgs for agent 0: q_loss: 6.60372257232666, p_loss: -4.019509315490723, mean_rew: -7.829279706404799, variance: 4.476945400238037, lamda: 1.8105580806732178
Running avgs for agent 1: q_loss: 7.590195178985596, p_loss: -4.029010772705078, mean_rew: -7.835429739492569, variance: 4.851924419403076, lamda: 1.6528205871582031
Running avgs for agent 2: q_loss: 1423428.875, p_loss: 149.64022827148438, mean_rew: -7.836090721392955, variance: 133.67942669677734, lamda: 3.658982992172241

steps: 2724975, episodes: 109000, mean episode reward: -599.3437522633778, agent episode reward: [-199.78125075445934, -199.78125075445934, -199.78125075445934], time: 158.177
steps: 2724975, episodes: 109000, mean episode variance: 35.71607270503044, agent episode variance: [1.1091897308826446, 1.2153951201438904, 33.391487854003906], time: 158.177
Running avgs for agent 0: q_loss: 7.6259942054748535, p_loss: -4.0330400466918945, mean_rew: -7.831338096801599, variance: 4.436758995056152, lamda: 1.8122822046279907
Running avgs for agent 1: q_loss: 7.51496696472168, p_loss: -4.02935266494751, mean_rew: -7.840231409658821, variance: 4.861579895019531, lamda: 1.652835488319397
Running avgs for agent 2: q_loss: 1427229.5, p_loss: 149.64649963378906, mean_rew: -7.828602247584011, variance: 133.56595141601562, lamda: 3.683957576751709

steps: 2749975, episodes: 110000, mean episode reward: -590.3098203369053, agent episode reward: [-196.76994011230178, -196.76994011230178, -196.76994011230178], time: 158.253
steps: 2749975, episodes: 110000, mean episode variance: 36.200812705278395, agent episode variance: [1.109190808057785, 1.2210584969520568, 33.87056340026855], time: 158.254
Running avgs for agent 0: q_loss: 6.924609184265137, p_loss: -4.035758972167969, mean_rew: -7.8402951598789805, variance: 4.436763286590576, lamda: 1.8139816522598267
Running avgs for agent 1: q_loss: 7.595579624176025, p_loss: -4.025066375732422, mean_rew: -7.841757233004305, variance: 4.8842339515686035, lamda: 1.6529892683029175
Running avgs for agent 2: q_loss: 1444026.875, p_loss: 149.7064971923828, mean_rew: -7.8404843028289415, variance: 135.4822536010742, lamda: 3.7089316844940186

steps: 2774975, episodes: 111000, mean episode reward: -594.6706127113938, agent episode reward: [-198.2235375704646, -198.2235375704646, -198.2235375704646], time: 154.476
steps: 2774975, episodes: 111000, mean episode variance: 35.78983731484413, agent episode variance: [1.1052438893318177, 1.215251949071884, 33.46934147644043], time: 154.476
Running avgs for agent 0: q_loss: 5.420461654663086, p_loss: -4.0411248207092285, mean_rew: -7.847411620994486, variance: 4.420975685119629, lamda: 1.8197699785232544
Running avgs for agent 1: q_loss: 7.553768634796143, p_loss: -4.031046390533447, mean_rew: -7.844745815036405, variance: 4.8610076904296875, lamda: 1.6531176567077637
Running avgs for agent 2: q_loss: 1456775.25, p_loss: 149.83680725097656, mean_rew: -7.849241222397043, variance: 133.8773659057617, lamda: 3.7339062690734863

steps: 2799975, episodes: 112000, mean episode reward: -601.5743264799069, agent episode reward: [-200.5247754933023, -200.5247754933023, -200.5247754933023], time: 158.724
steps: 2799975, episodes: 112000, mean episode variance: 36.03100413250923, agent episode variance: [1.0989411420822144, 1.2193257772922517, 33.712737213134766], time: 158.725
Running avgs for agent 0: q_loss: 5.262445449829102, p_loss: -4.035243034362793, mean_rew: -7.8483624751853585, variance: 4.3957648277282715, lamda: 1.8259695768356323
Running avgs for agent 1: q_loss: 7.549839019775391, p_loss: -4.030539035797119, mean_rew: -7.847067489726011, variance: 4.877303123474121, lamda: 1.653213381767273
Running avgs for agent 2: q_loss: 1466066.125, p_loss: 149.97341918945312, mean_rew: -7.849042425511523, variance: 134.85094885253906, lamda: 3.758880376815796

steps: 2824975, episodes: 113000, mean episode reward: -598.1260954532657, agent episode reward: [-199.37536515108854, -199.37536515108854, -199.37536515108854], time: 159.865
steps: 2824975, episodes: 113000, mean episode variance: 36.69681714296341, agent episode variance: [1.0997216498851776, 1.2212619512081146, 34.375833541870115], time: 159.866
Running avgs for agent 0: q_loss: 5.444910049438477, p_loss: -4.039336681365967, mean_rew: -7.860366443366316, variance: 4.398886203765869, lamda: 1.8320600986480713
Running avgs for agent 1: q_loss: 7.634828090667725, p_loss: -4.033623695373535, mean_rew: -7.861728633169159, variance: 4.885047912597656, lamda: 1.6533195972442627
Running avgs for agent 2: q_loss: 1476943.125, p_loss: 150.08253479003906, mean_rew: -7.858629796658304, variance: 137.50333416748046, lamda: 3.7838547229766846

steps: 2849975, episodes: 114000, mean episode reward: -593.7076050250346, agent episode reward: [-197.90253500834487, -197.90253500834487, -197.90253500834487], time: 159.569
steps: 2849975, episodes: 114000, mean episode variance: 35.49806094646454, agent episode variance: [1.1030090618133546, 1.2200997819900512, 33.17495210266113], time: 159.569
Running avgs for agent 0: q_loss: 5.610161304473877, p_loss: -4.043785572052002, mean_rew: -7.8708561358306275, variance: 4.412035942077637, lamda: 1.8385342359542847
Running avgs for agent 1: q_loss: 7.467585563659668, p_loss: -4.037374973297119, mean_rew: -7.861054325403324, variance: 4.880398273468018, lamda: 1.6536085605621338
Running avgs for agent 2: q_loss: 1481989.125, p_loss: 150.166748046875, mean_rew: -7.865194891827748, variance: 132.69980841064452, lamda: 3.8088293075561523

steps: 2874975, episodes: 115000, mean episode reward: -595.8547387875257, agent episode reward: [-198.61824626250856, -198.61824626250856, -198.61824626250856], time: 159.392
steps: 2874975, episodes: 115000, mean episode variance: 35.520823735952376, agent episode variance: [1.093596108198166, 1.2172782716751098, 33.2099493560791], time: 159.393
Running avgs for agent 0: q_loss: 5.337253093719482, p_loss: -4.0411787033081055, mean_rew: -7.860369286658875, variance: 4.374384880065918, lamda: 1.8419872522354126
Running avgs for agent 1: q_loss: 7.461111545562744, p_loss: -4.038970947265625, mean_rew: -7.864634327390435, variance: 4.869112968444824, lamda: 1.6539626121520996
Running avgs for agent 2: q_loss: 1511123.375, p_loss: 150.1467742919922, mean_rew: -7.860084231327956, variance: 132.8397974243164, lamda: 3.833803415298462

steps: 2899975, episodes: 116000, mean episode reward: -590.9164682757466, agent episode reward: [-196.97215609191554, -196.97215609191554, -196.97215609191554], time: 157.477
steps: 2899975, episodes: 116000, mean episode variance: 36.176924996852875, agent episode variance: [1.0937518301010132, 1.2187961831092835, 33.86437698364258], time: 157.477
Running avgs for agent 0: q_loss: 5.581299781799316, p_loss: -4.041108131408691, mean_rew: -7.865705621470118, variance: 4.375007152557373, lamda: 1.8481154441833496
Running avgs for agent 1: q_loss: 5.839658260345459, p_loss: -4.04459285736084, mean_rew: -7.869816134871526, variance: 4.875184535980225, lamda: 1.6554433107376099
Running avgs for agent 2: q_loss: 1515998.5, p_loss: 150.1592254638672, mean_rew: -7.861482841458785, variance: 135.45750793457032, lamda: 3.8587777614593506

steps: 2924975, episodes: 117000, mean episode reward: -589.7126020834302, agent episode reward: [-196.57086736114334, -196.57086736114334, -196.57086736114334], time: 160.866
steps: 2924975, episodes: 117000, mean episode variance: 35.1407979195118, agent episode variance: [1.0912897839546203, 1.2170981166362762, 32.832410018920896], time: 160.866
Running avgs for agent 0: q_loss: 5.612967491149902, p_loss: -4.048501968383789, mean_rew: -7.876144324022349, variance: 4.365159034729004, lamda: 1.8537548780441284
Running avgs for agent 1: q_loss: 4.999153137207031, p_loss: -4.039228439331055, mean_rew: -7.87664614621211, variance: 4.868391990661621, lamda: 1.6605815887451172
Running avgs for agent 2: q_loss: 1547844.5, p_loss: 150.18426513671875, mean_rew: -7.8706828018795685, variance: 131.32964007568359, lamda: 3.88375186920166

steps: 2949975, episodes: 118000, mean episode reward: -597.7679459893831, agent episode reward: [-199.255981996461, -199.255981996461, -199.255981996461], time: 157.139
steps: 2949975, episodes: 118000, mean episode variance: 35.70046416544914, agent episode variance: [1.0902376117706298, 1.2144991824626923, 33.39572737121582], time: 157.14
Running avgs for agent 0: q_loss: 5.439474105834961, p_loss: -4.046992778778076, mean_rew: -7.876219375807709, variance: 4.360949993133545, lamda: 1.855566143989563
Running avgs for agent 1: q_loss: 4.804313659667969, p_loss: -4.0410637855529785, mean_rew: -7.877323485675533, variance: 4.857996940612793, lamda: 1.6629067659378052
Running avgs for agent 2: q_loss: 1549789.75, p_loss: 150.18544006347656, mean_rew: -7.880699446753987, variance: 133.58290948486328, lamda: 3.908726453781128

steps: 2974975, episodes: 119000, mean episode reward: -592.2693138225019, agent episode reward: [-197.42310460750062, -197.42310460750062, -197.42310460750062], time: 155.895
steps: 2974975, episodes: 119000, mean episode variance: 35.72593005633354, agent episode variance: [1.084566739320755, 1.2178190817832948, 33.42354423522949], time: 155.895
Running avgs for agent 0: q_loss: 5.460342884063721, p_loss: -4.050480365753174, mean_rew: -7.879299843719824, variance: 4.338266372680664, lamda: 1.8580608367919922
Running avgs for agent 1: q_loss: 4.718163967132568, p_loss: -4.047138690948486, mean_rew: -7.887858742419922, variance: 4.871276378631592, lamda: 1.6638479232788086
Running avgs for agent 2: q_loss: 1572712.75, p_loss: 150.21212768554688, mean_rew: -7.882425033903073, variance: 133.69417694091797, lamda: 3.9337007999420166

steps: 2999975, episodes: 120000, mean episode reward: -604.4942827870947, agent episode reward: [-201.4980942623649, -201.4980942623649, -201.4980942623649], time: 159.319
steps: 2999975, episodes: 120000, mean episode variance: 35.344682977199554, agent episode variance: [1.0850044379234314, 1.2084253425598144, 33.05125319671631], time: 159.319
Running avgs for agent 0: q_loss: 5.700576305389404, p_loss: -4.050535678863525, mean_rew: -7.884432145831794, variance: 4.340017795562744, lamda: 1.8640788793563843
Running avgs for agent 1: q_loss: 6.645188331604004, p_loss: -4.049068927764893, mean_rew: -7.884943869983146, variance: 4.8337016105651855, lamda: 1.6657726764678955
Running avgs for agent 2: q_loss: 1594258.375, p_loss: 150.14878845214844, mean_rew: -7.8845055598119105, variance: 132.20501278686524, lamda: 3.9586753845214844

...Finished total of 120001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -596.0454367959318, agent episode reward: [-198.68181226531055, -198.68181226531055, -198.68181226531055], time: 138.171
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0], time: 138.172
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -601.7269499439732, agent episode reward: [-200.57564998132437, -200.57564998132437, -200.57564998132437], time: 133.655
steps: 49975, episodes: 2000, mean episode variance: 94.55762908506394, agent episode variance: [1.0278438172340394, 0.7864065752029419, 92.74337869262695], time: 133.656
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.964848758685647, variance: 4.212474346160889, lamda: 1.8663995265960693
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.967934717076002, variance: 3.222977876663208, lamda: 1.6664360761642456
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -7.964845424502113, variance: 380.0957946777344, lamda: 3.9712133407592773

...Finished total of 2001 episodes with the fixed policy.
/u/stamilse/.lsbatch/1567759391.1090486: line 9: --exp_var_alpha: command not found
