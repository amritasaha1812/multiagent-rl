WARNING: Logging before flag parsing goes to stderr.
W0827 01:22:24.236733 4631463360 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0827 01:22:24.236945 4631463360 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-27 01:22:24.237290: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0827 01:22:24.241318 4631463360 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0827 01:22:24.243252 4631463360 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0827 01:22:24.243367 4631463360 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0827 01:22:24.243438 4631463360 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0827 01:22:24.690818 4631463360 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0827 01:22:24.873597 4631463360 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0827 01:22:24.881139 4631463360 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0827 01:22:25.327524 4631463360 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation True
adversary agent:  -0.25
adversary agent:  -0.25
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -11.97060395891263, agent episode reward: [2.92, 2.92, -9.016947453046358, -8.79365650586627], time: 37.319
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 37.319
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -156.06937464008288, agent episode reward: [3.4, 3.4, -93.36905095870684, -69.50032368137606], time: 70.057
steps: 49975, episodes: 2000, mean episode variance: 1.6196846286281943, agent episode variance: [0.22306868542917072, 0.1823612474631518, 0.6872630624696613, 0.5269916332662106], time: 70.058
Running avgs for agent 0: q_loss: 2.4783220291137695, p_loss: 0.7070770263671875, mean_rew: 0.12659291752049182, variance: 0.9142159238900439, lamda: 1.0123884677886963
Running avgs for agent 1: q_loss: 2.0736656188964844, p_loss: 0.7708712816238403, mean_rew: 0.12787365522540983, variance: 0.7473821617342288, lamda: 1.011924147605896
Running avgs for agent 2: q_loss: 17.699018478393555, p_loss: -0.4290043115615845, mean_rew: -1.22968490892582, variance: 2.816651895367464, lamda: 1.0123165845870972
Running avgs for agent 3: q_loss: 13.336731910705566, p_loss: 0.10237672179937363, mean_rew: -0.8845523267749282, variance: 2.1598017756811907, lamda: 1.0123130083084106

steps: 74975, episodes: 3000, mean episode reward: -215.32188901210733, agent episode reward: [3.13, 3.13, -103.52636157781865, -118.05552743428869], time: 67.258
steps: 74975, episodes: 3000, mean episode variance: 2.6634573968052866, agent episode variance: [0.31798750615119936, 0.31469813430309296, 1.0527725034952164, 0.9779992528557777], time: 67.258
Running avgs for agent 0: q_loss: 3.1365365982055664, p_loss: 0.9185359477996826, mean_rew: 0.1265234375, variance: 1.271950125694275, lamda: 1.0376554727554321
Running avgs for agent 1: q_loss: 2.7679672241210938, p_loss: 0.8646331429481506, mean_rew: 0.1271484375, variance: 1.2587926387786865, lamda: 1.0371171236038208
Running avgs for agent 2: q_loss: 67.50238800048828, p_loss: -1.4873151779174805, mean_rew: -2.4157707483773883, variance: 4.211089611053467, lamda: 1.0371156930923462
Running avgs for agent 3: q_loss: 87.97880554199219, p_loss: -1.029617428779602, mean_rew: -2.1510471243455056, variance: 3.911997079849243, lamda: 1.0370627641677856

steps: 99975, episodes: 4000, mean episode reward: -181.06059314202062, agent episode reward: [2.84, 2.84, -80.38460593260659, -106.35598720941404], time: 67.902
steps: 99975, episodes: 4000, mean episode variance: 2.867319828093052, agent episode variance: [0.3419364185333252, 0.3461377879977226, 1.0972542064189912, 1.081991415143013], time: 67.903
Running avgs for agent 0: q_loss: 2.756295919418335, p_loss: 1.209040880203247, mean_rew: 0.1248046875, variance: 1.3677457571029663, lamda: 1.0628647804260254
Running avgs for agent 1: q_loss: 2.692211866378784, p_loss: 1.1724618673324585, mean_rew: 0.125546875, variance: 1.3845511674880981, lamda: 1.0623136758804321
Running avgs for agent 2: q_loss: 251.00936889648438, p_loss: -2.028416395187378, mean_rew: -2.8196732515091494, variance: 4.389016628265381, lamda: 1.0621531009674072
Running avgs for agent 3: q_loss: 279.0506896972656, p_loss: -1.704072117805481, mean_rew: -2.8482224417367643, variance: 4.32796573638916, lamda: 1.0620747804641724

steps: 124975, episodes: 5000, mean episode reward: -180.93192380116398, agent episode reward: [2.62, 2.62, -81.74547719119626, -104.42644660996774], time: 68.034
steps: 124975, episodes: 5000, mean episode variance: 2.896546048402786, agent episode variance: [0.34714660769701006, 0.3477679342031479, 1.0229431959986686, 1.1786883105039596], time: 68.035
Running avgs for agent 0: q_loss: 2.6365036964416504, p_loss: 1.2854658365249634, mean_rew: 0.1205859375, variance: 1.388586401939392, lamda: 1.0879818201065063
Running avgs for agent 1: q_loss: 2.692861795425415, p_loss: 1.2917886972427368, mean_rew: 0.124453125, variance: 1.3910717964172363, lamda: 1.08742356300354
Running avgs for agent 2: q_loss: 356.8236083984375, p_loss: -2.051098108291626, mean_rew: -2.9063661042960667, variance: 4.09177303314209, lamda: 1.0871726274490356
Running avgs for agent 3: q_loss: 509.0693664550781, p_loss: -2.0125927925109863, mean_rew: -3.1389998425615633, variance: 4.7147532420158385, lamda: 1.0870789289474487

steps: 149975, episodes: 6000, mean episode reward: -154.6397939042719, agent episode reward: [2.77, 2.77, -60.444432325188565, -99.73536157908336], time: 68.15
steps: 149975, episodes: 6000, mean episode variance: 3.008780721664429, agent episode variance: [0.3609561975002289, 0.3571827968358994, 1.0772833527326584, 1.213358374595642], time: 68.15
Running avgs for agent 0: q_loss: 2.798570394515991, p_loss: 1.3310855627059937, mean_rew: 0.1219140625, variance: 1.4438248872756958, lamda: 1.113055944442749
Running avgs for agent 1: q_loss: 2.592334747314453, p_loss: 1.3670437335968018, mean_rew: 0.12015625, variance: 1.4287312030792236, lamda: 1.112493872642517
Running avgs for agent 2: q_loss: 605.7417602539062, p_loss: -2.0636346340179443, mean_rew: -2.922438362400195, variance: 4.309133410930634, lamda: 1.1121766567230225
Running avgs for agent 3: q_loss: 921.8795776367188, p_loss: -2.2181880474090576, mean_rew: -3.3337704401104884, variance: 4.853433498382568, lamda: 1.1120829582214355

steps: 174975, episodes: 7000, mean episode reward: -178.81816161912496, agent episode reward: [1.96, 1.96, -76.39247747031048, -106.34568414881447], time: 68.174
steps: 174975, episodes: 7000, mean episode variance: 3.036450723648071, agent episode variance: [0.370167893409729, 0.377014560341835, 1.0690655335485935, 1.2202027363479138], time: 68.174
Running avgs for agent 0: q_loss: 2.318730354309082, p_loss: 1.3590277433395386, mean_rew: 0.11265625, variance: 1.4806716442108154, lamda: 1.138107419013977
Running avgs for agent 1: q_loss: 2.435142993927002, p_loss: 1.3477240800857544, mean_rew: 0.1180859375, variance: 1.5080580711364746, lamda: 1.137540578842163
Running avgs for agent 2: q_loss: 811.335693359375, p_loss: -2.0470800399780273, mean_rew: -2.8690246804011106, variance: 4.276262134194374, lamda: 1.1371809244155884
Running avgs for agent 3: q_loss: 1156.1322021484375, p_loss: -2.450819969177246, mean_rew: -3.4265307241568497, variance: 4.880810945391655, lamda: 1.1370872259140015

steps: 199975, episodes: 8000, mean episode reward: -177.3938201344121, agent episode reward: [1.87, 1.87, -78.02562062233078, -103.10819951208128], time: 68.327
steps: 199975, episodes: 8000, mean episode variance: 2.9273803518116472, agent episode variance: [0.3754118684530258, 0.3792046923041344, 1.0874055753648282, 1.085358215689659], time: 68.327
Running avgs for agent 0: q_loss: 2.3431217670440674, p_loss: 1.341722011566162, mean_rew: 0.1115625, variance: 1.5016474723815918, lamda: 1.1631416082382202
Running avgs for agent 1: q_loss: 2.2756357192993164, p_loss: 1.3322991132736206, mean_rew: 0.11375, variance: 1.516818881034851, lamda: 1.1625744104385376
Running avgs for agent 2: q_loss: 702.1630249023438, p_loss: -2.0755467414855957, mean_rew: -2.8932002883831647, variance: 4.349622301459313, lamda: 1.1621850728988647
Running avgs for agent 3: q_loss: 1494.2447509765625, p_loss: -2.5478386878967285, mean_rew: -3.554286637314234, variance: 4.341432862758636, lamda: 1.1620913743972778

steps: 224975, episodes: 9000, mean episode reward: -151.68840152602462, agent episode reward: [2.05, 2.05, -75.1776085566134, -80.61079296941121], time: 68.571
steps: 224975, episodes: 9000, mean episode variance: 3.016696015432477, agent episode variance: [0.380912212729454, 0.3762973643541336, 1.0439701893627644, 1.215516248986125], time: 68.571
Running avgs for agent 0: q_loss: 2.3766143321990967, p_loss: 1.3018149137496948, mean_rew: 0.1043359375, variance: 1.5236488580703735, lamda: 1.1881755590438843
Running avgs for agent 1: q_loss: 2.2189488410949707, p_loss: 1.300412654876709, mean_rew: 0.10609375, variance: 1.5051894187927246, lamda: 1.1876081228256226
Running avgs for agent 2: q_loss: 451.3403625488281, p_loss: -2.000985622406006, mean_rew: -2.9169858453527175, variance: 4.175880757451058, lamda: 1.1871892213821411
Running avgs for agent 3: q_loss: 1460.39013671875, p_loss: -2.5805418491363525, mean_rew: -3.5829879567345517, variance: 4.8620649959445, lamda: 1.1870955228805542

steps: 249975, episodes: 10000, mean episode reward: -148.7581155837029, agent episode reward: [2.06, 2.06, -111.79440637184962, -41.083709211853275], time: 68.454
steps: 249975, episodes: 10000, mean episode variance: 3.014563103765249, agent episode variance: [0.3810090191364288, 0.3805183081626892, 1.092205528318882, 1.1608302481472492], time: 68.454
Running avgs for agent 0: q_loss: 2.372465133666992, p_loss: 1.2480517625808716, mean_rew: 0.1016015625, variance: 1.524036169052124, lamda: 1.2131973505020142
Running avgs for agent 1: q_loss: 2.0961854457855225, p_loss: 1.2315144538879395, mean_rew: 0.10203125, variance: 1.5220732688903809, lamda: 1.2126243114471436
Running avgs for agent 2: q_loss: 1115.2452392578125, p_loss: -2.186885118484497, mean_rew: -2.9898488509885395, variance: 4.368822113275528, lamda: 1.2121933698654175
Running avgs for agent 3: q_loss: 1878.614013671875, p_loss: -2.644855260848999, mean_rew: -3.4189239409441425, variance: 4.643320992588997, lamda: 1.2120997905731201

steps: 274975, episodes: 11000, mean episode reward: -229.38146527253255, agent episode reward: [1.95, 1.95, -120.73547034801798, -112.54599492451456], time: 69.161
steps: 274975, episodes: 11000, mean episode variance: 2.997060909591615, agent episode variance: [0.3889941860437393, 0.3850585209131241, 1.089141697883606, 1.1338665047511458], time: 69.161
Running avgs for agent 0: q_loss: 2.5611155033111572, p_loss: 1.2166833877563477, mean_rew: 0.0998828125, variance: 1.5559767484664917, lamda: 1.2382017374038696
Running avgs for agent 1: q_loss: 2.1921939849853516, p_loss: 1.1972492933273315, mean_rew: 0.10109375, variance: 1.540234088897705, lamda: 1.23762845993042
Running avgs for agent 2: q_loss: 796.8675537109375, p_loss: -2.417891263961792, mean_rew: -3.146817716037639, variance: 4.356566791534424, lamda: 1.2371975183486938
Running avgs for agent 3: q_loss: 1327.8052978515625, p_loss: -2.5847270488739014, mean_rew: -3.388422422423245, variance: 4.535466019004583, lamda: 1.2371039390563965

steps: 299975, episodes: 12000, mean episode reward: -261.78536827277924, agent episode reward: [1.58, 1.58, -126.7297553796321, -138.2156128931471], time: 68.895
steps: 299975, episodes: 12000, mean episode variance: 2.9326616668999197, agent episode variance: [0.38400315737724305, 0.38237268829345705, 1.0747124547958373, 1.091573366433382], time: 68.896
Running avgs for agent 0: q_loss: 2.2840678691864014, p_loss: 1.217539668083191, mean_rew: 0.099296875, variance: 1.5360124111175537, lamda: 1.2632057666778564
Running avgs for agent 1: q_loss: 2.368581771850586, p_loss: 1.1944841146469116, mean_rew: 0.09859375, variance: 1.5294907093048096, lamda: 1.2626326084136963
Running avgs for agent 2: q_loss: 150.12364196777344, p_loss: -2.3448398113250732, mean_rew: -3.3181928212047236, variance: 4.298850059509277, lamda: 1.2622015476226807
Running avgs for agent 3: q_loss: 1682.233154296875, p_loss: -2.604160785675049, mean_rew: -3.5367864620417486, variance: 4.366293465733528, lamda: 1.2621079683303833

steps: 324975, episodes: 13000, mean episode reward: -202.48150004123016, agent episode reward: [1.62, 1.62, -65.58870704740264, -140.13279299382754], time: 68.731
steps: 324975, episodes: 13000, mean episode variance: 3.009632339954376, agent episode variance: [0.39047600746154787, 0.38726267373561857, 1.0498707401752472, 1.1820229185819626], time: 68.732
Running avgs for agent 0: q_loss: 2.295398473739624, p_loss: 1.170109510421753, mean_rew: 0.094765625, variance: 1.5619040727615356, lamda: 1.2882099151611328
Running avgs for agent 1: q_loss: 2.3526415824890137, p_loss: 1.1521060466766357, mean_rew: 0.0956640625, variance: 1.5490506887435913, lamda: 1.287636637687683
Running avgs for agent 2: q_loss: 46.153724670410156, p_loss: -2.331258773803711, mean_rew: -3.379644798133811, variance: 4.1994829177856445, lamda: 1.2872058153152466
Running avgs for agent 3: q_loss: 1288.6392822265625, p_loss: -2.619438409805298, mean_rew: -3.719470325760406, variance: 4.728091716766357, lamda: 1.2871122360229492

steps: 349975, episodes: 14000, mean episode reward: -177.6629601570933, agent episode reward: [1.62, 1.62, -55.7036177520949, -125.19934240499843], time: 68.726
steps: 349975, episodes: 14000, mean episode variance: 2.9922996753677724, agent episode variance: [0.3946917415857315, 0.39896125614643096, 1.0157192106246948, 1.1829274670109153], time: 68.727
Running avgs for agent 0: q_loss: 2.4480140209198, p_loss: 1.1174148321151733, mean_rew: 0.0926953125, variance: 1.578766942024231, lamda: 1.3132104873657227
Running avgs for agent 1: q_loss: 2.308680772781372, p_loss: 1.0650725364685059, mean_rew: 0.0928125, variance: 1.5958449840545654, lamda: 1.312640905380249
Running avgs for agent 2: q_loss: 31.917287826538086, p_loss: -2.1603927612304688, mean_rew: -3.2982344214747945, variance: 4.0628767013549805, lamda: 1.312209963798523
Running avgs for agent 3: q_loss: 1132.967041015625, p_loss: -2.730997323989868, mean_rew: -3.8322859966466543, variance: 4.731709868043661, lamda: 1.3121163845062256

steps: 374975, episodes: 15000, mean episode reward: -188.5794418395119, agent episode reward: [1.36, 1.36, -61.794015327925635, -129.50542651158628], time: 68.882
steps: 374975, episodes: 15000, mean episode variance: 2.982748883932829, agent episode variance: [0.3924349809885025, 0.38951530134677886, 1.002196834564209, 1.1986017670333386], time: 68.882
Running avgs for agent 0: q_loss: 2.5428242683410645, p_loss: 1.1172231435775757, mean_rew: 0.0941015625, variance: 1.5697399377822876, lamda: 1.338209867477417
Running avgs for agent 1: q_loss: 2.318390130996704, p_loss: 1.0751603841781616, mean_rew: 0.0915234375, variance: 1.5580612421035767, lamda: 1.3376438617706299
Running avgs for agent 2: q_loss: 18.937767028808594, p_loss: -2.0523993968963623, mean_rew: -3.2176948213482257, variance: 4.008787631988525, lamda: 1.3372141122817993
Running avgs for agent 3: q_loss: 1664.1082763671875, p_loss: -2.658456802368164, mean_rew: -3.926003517634701, variance: 4.794407068133355, lamda: 1.3371204137802124

steps: 399975, episodes: 16000, mean episode reward: -173.51980452008758, agent episode reward: [0.99, 0.99, -56.61088435861628, -118.88892016147126], time: 68.737
steps: 399975, episodes: 16000, mean episode variance: 2.847250510431826, agent episode variance: [0.39465480506420136, 0.3948583871126175, 0.9931422057151794, 1.0645951125398279], time: 68.738
Running avgs for agent 0: q_loss: 2.5243656635284424, p_loss: 1.0952101945877075, mean_rew: 0.08828125, variance: 1.5786192417144775, lamda: 1.3632127046585083
Running avgs for agent 1: q_loss: 2.2091548442840576, p_loss: 1.046735405921936, mean_rew: 0.086015625, variance: 1.579433560371399, lamda: 1.362644910812378
Running avgs for agent 2: q_loss: 15.502211570739746, p_loss: -2.066739320755005, mean_rew: -3.173999571847398, variance: 3.9725687503814697, lamda: 1.3622182607650757
Running avgs for agent 3: q_loss: 1089.778076171875, p_loss: -2.35546612739563, mean_rew: -3.985016890553855, variance: 4.258380450159311, lamda: 1.3621245622634888

steps: 424975, episodes: 17000, mean episode reward: -155.6886183634161, agent episode reward: [1.39, 1.39, -48.05047890114836, -110.41813946226775], time: 68.596
steps: 424975, episodes: 17000, mean episode variance: 2.878444756105542, agent episode variance: [0.3974853810071945, 0.3928985148668289, 0.9560518095493317, 1.1320090506821872], time: 68.596
Running avgs for agent 0: q_loss: 2.4645659923553467, p_loss: 1.0724653005599976, mean_rew: 0.0851171875, variance: 1.5899415016174316, lamda: 1.3882007598876953
Running avgs for agent 1: q_loss: 2.159579038619995, p_loss: 1.0449106693267822, mean_rew: 0.0873828125, variance: 1.571593999862671, lamda: 1.3876471519470215
Running avgs for agent 2: q_loss: 15.567651748657227, p_loss: -2.0925962924957275, mean_rew: -3.106404263471932, variance: 3.824207305908203, lamda: 1.387222409248352
Running avgs for agent 3: q_loss: 587.142822265625, p_loss: -2.356213331222534, mean_rew: -4.015226342870214, variance: 4.528036202728749, lamda: 1.3871288299560547

steps: 449975, episodes: 18000, mean episode reward: -162.55180068563658, agent episode reward: [1.76, 1.76, -62.82090406621185, -103.25089661942475], time: 68.708
steps: 449975, episodes: 18000, mean episode variance: 2.8528563888072966, agent episode variance: [0.3991872358322144, 0.39616214418411255, 0.9344709837436675, 1.1230360250473022], time: 68.708
Running avgs for agent 0: q_loss: 2.5076656341552734, p_loss: 1.0440012216567993, mean_rew: 0.08296875, variance: 1.5967490673065186, lamda: 1.4131557941436768
Running avgs for agent 1: q_loss: 2.1838910579681396, p_loss: 1.0355015993118286, mean_rew: 0.0841796875, variance: 1.5846487283706665, lamda: 1.4126405715942383
Running avgs for agent 2: q_loss: 19.202293395996094, p_loss: -2.090085029602051, mean_rew: -3.0534473555201216, variance: 3.7378838062286377, lamda: 1.4122264385223389
Running avgs for agent 3: q_loss: 488.8661193847656, p_loss: -2.3524255752563477, mean_rew: -4.0482136334018195, variance: 4.492144100189209, lamda: 1.4121328592300415

steps: 474975, episodes: 19000, mean episode reward: -132.68100966954285, agent episode reward: [1.51, 1.51, -59.474060653268296, -76.22694901627456], time: 68.889
steps: 474975, episodes: 19000, mean episode variance: 2.840438735604286, agent episode variance: [0.4018272033929825, 0.3946210856437683, 0.9239306173324585, 1.120059829235077], time: 68.89
Running avgs for agent 0: q_loss: 2.6319892406463623, p_loss: 1.0511112213134766, mean_rew: 0.0826953125, variance: 1.6073088645935059, lamda: 1.4381399154663086
Running avgs for agent 1: q_loss: 2.1770668029785156, p_loss: 1.0614027976989746, mean_rew: 0.0821484375, variance: 1.578484296798706, lamda: 1.4375958442687988
Running avgs for agent 2: q_loss: 15.267616271972656, p_loss: -2.0706627368927, mean_rew: -3.018997117436466, variance: 3.6957223415374756, lamda: 1.4372236728668213
Running avgs for agent 3: q_loss: 538.5103759765625, p_loss: -2.2843329906463623, mean_rew: -4.011164321226625, variance: 4.480239391326904, lamda: 1.4371370077133179

steps: 499975, episodes: 20000, mean episode reward: -116.0307268016514, agent episode reward: [1.74, 1.74, -57.34022546296291, -62.170501338688496], time: 68.738
steps: 499975, episodes: 20000, mean episode variance: 2.8141030716896056, agent episode variance: [0.3967154483795166, 0.4006386361122131, 0.9146910700798034, 1.1020579171180724], time: 68.738
Running avgs for agent 0: q_loss: 2.417484760284424, p_loss: 1.0659862756729126, mean_rew: 0.082265625, variance: 1.5868618488311768, lamda: 1.4631203413009644
Running avgs for agent 1: q_loss: 2.292956590652466, p_loss: 1.0385231971740723, mean_rew: 0.0853515625, variance: 1.6025545597076416, lamda: 1.4625593423843384
Running avgs for agent 2: q_loss: 19.221187591552734, p_loss: -2.0511369705200195, mean_rew: -2.9841600822769787, variance: 3.658764123916626, lamda: 1.4621447324752808
Running avgs for agent 3: q_loss: 328.8333740234375, p_loss: -2.221082925796509, mean_rew: -3.945720025067098, variance: 4.408231258392334, lamda: 1.4621412754058838

steps: 524975, episodes: 21000, mean episode reward: -105.24678133145714, agent episode reward: [1.2, 1.2, -54.55995693625431, -53.086824395202825], time: 68.703
steps: 524975, episodes: 21000, mean episode variance: 2.768948829770088, agent episode variance: [0.40036012828350065, 0.3998757356405258, 0.9030080363750458, 1.065704929471016], time: 68.703
Running avgs for agent 0: q_loss: 2.4475810527801514, p_loss: 1.0557856559753418, mean_rew: 0.0807421875, variance: 1.6014405488967896, lamda: 1.488109827041626
Running avgs for agent 1: q_loss: 2.495206356048584, p_loss: 1.0445623397827148, mean_rew: 0.0851953125, variance: 1.5995029211044312, lamda: 1.4875143766403198
Running avgs for agent 2: q_loss: 12.994813919067383, p_loss: -2.0298962593078613, mean_rew: -2.948582299920771, variance: 3.612032175064087, lamda: 1.4868558645248413
Running avgs for agent 3: q_loss: 187.69895935058594, p_loss: -2.0423922538757324, mean_rew: -3.868152938719871, variance: 4.262819766998291, lamda: 1.4871445894241333

steps: 549975, episodes: 22000, mean episode reward: -110.71792886385177, agent episode reward: [1.78, 1.78, -58.928569120443896, -55.34935974340786], time: 67.877
steps: 549975, episodes: 22000, mean episode variance: 2.758869554281235, agent episode variance: [0.4007338093519211, 0.4013901318311691, 0.8964484281539917, 1.060297184944153], time: 67.877
Running avgs for agent 0: q_loss: 2.5698940753936768, p_loss: 1.0572980642318726, mean_rew: 0.083359375, variance: 1.6029353141784668, lamda: 1.5131213665008545
Running avgs for agent 1: q_loss: 2.3005166053771973, p_loss: 1.0305770635604858, mean_rew: 0.077421875, variance: 1.605560541152954, lamda: 1.5124958753585815
Running avgs for agent 2: q_loss: 15.577576637268066, p_loss: -2.000084400177002, mean_rew: -2.9038976197601105, variance: 3.5857937335968018, lamda: 1.5118408203125
Running avgs for agent 3: q_loss: 123.81843566894531, p_loss: -1.8570561408996582, mean_rew: -3.813565950832992, variance: 4.241189002990723, lamda: 1.5121432542800903

steps: 574975, episodes: 23000, mean episode reward: -133.06277643721182, agent episode reward: [1.74, 1.74, -58.79594734640224, -77.74682909080957], time: 66.953
steps: 574975, episodes: 23000, mean episode variance: 2.7543948551416397, agent episode variance: [0.4051768188476563, 0.4046975048780441, 0.8885698325634003, 1.0559506988525391], time: 66.953
Running avgs for agent 0: q_loss: 2.51350998878479, p_loss: 1.046451449394226, mean_rew: 0.08109375, variance: 1.6207072734832764, lamda: 1.538112759590149
Running avgs for agent 1: q_loss: 2.17665958404541, p_loss: 1.0104399919509888, mean_rew: 0.08046875, variance: 1.6187900304794312, lamda: 1.537451982498169
Running avgs for agent 2: q_loss: 12.248988151550293, p_loss: -1.9993280172348022, mean_rew: -2.878213444387047, variance: 3.554279327392578, lamda: 1.5367289781570435
Running avgs for agent 3: q_loss: 83.48726654052734, p_loss: -1.8790788650512695, mean_rew: -3.726066895768071, variance: 4.22380256652832, lamda: 1.536480188369751

steps: 599975, episodes: 24000, mean episode reward: -135.2146026179298, agent episode reward: [1.28, 1.28, -56.45048983711373, -81.32411278081607], time: 65.679
steps: 599975, episodes: 24000, mean episode variance: 2.710270484209061, agent episode variance: [0.4036221159696579, 0.3992575298547745, 0.8774294707775115, 1.0299613676071167], time: 65.679
Running avgs for agent 0: q_loss: 2.4354794025421143, p_loss: 1.02408766746521, mean_rew: 0.07890625, variance: 1.6144884824752808, lamda: 1.563111424446106
Running avgs for agent 1: q_loss: 2.3224124908447266, p_loss: 1.0226556062698364, mean_rew: 0.08015625, variance: 1.5970301628112793, lamda: 1.562398910522461
Running avgs for agent 2: q_loss: 13.143754005432129, p_loss: -2.002403974533081, mean_rew: -2.8622200323759937, variance: 3.5097177028656006, lamda: 1.5613281726837158
Running avgs for agent 3: q_loss: 61.23799514770508, p_loss: -1.7608675956726074, mean_rew: -3.7300010257290257, variance: 4.119845867156982, lamda: 1.560283899307251

steps: 624975, episodes: 25000, mean episode reward: -109.5002633326133, agent episode reward: [1.73, 1.73, -56.11584214621456, -56.84442118639874], time: 66.623
steps: 624975, episodes: 25000, mean episode variance: 2.6894368624687193, agent episode variance: [0.4039065835475922, 0.4022415430545807, 0.8626335287094116, 1.020655207157135], time: 66.623
Running avgs for agent 0: q_loss: 2.813398838043213, p_loss: 1.038102626800537, mean_rew: 0.07890625, variance: 1.6156262159347534, lamda: 1.5881142616271973
Running avgs for agent 1: q_loss: 2.244154930114746, p_loss: 0.9894113540649414, mean_rew: 0.07703125, variance: 1.6089662313461304, lamda: 1.5874031782150269
Running avgs for agent 2: q_loss: 9.909263610839844, p_loss: -1.984291434288025, mean_rew: -2.8389006273359296, variance: 3.4505341053009033, lamda: 1.5852234363555908
Running avgs for agent 3: q_loss: 53.56389236450195, p_loss: -1.7330176830291748, mean_rew: -3.679509866764674, variance: 4.082620620727539, lamda: 1.5833048820495605

steps: 649975, episodes: 26000, mean episode reward: -92.48447608784618, agent episode reward: [1.16, 1.16, -54.96980054658922, -39.83467554125696], time: 65.538
steps: 649975, episodes: 26000, mean episode variance: 2.681812659621239, agent episode variance: [0.4060439699888229, 0.40339920318126676, 0.8638708341121674, 1.0084986523389816], time: 65.539
Running avgs for agent 0: q_loss: 2.6796634197235107, p_loss: 1.0108678340911865, mean_rew: 0.0735546875, variance: 1.6241759061813354, lamda: 1.613111138343811
Running avgs for agent 1: q_loss: 2.402249336242676, p_loss: 0.9539212584495544, mean_rew: 0.075546875, variance: 1.613596796989441, lamda: 1.612396478652954
Running avgs for agent 2: q_loss: 8.947139739990234, p_loss: -1.9743766784667969, mean_rew: -2.8294119107054185, variance: 3.4554834365844727, lamda: 1.6086894273757935
Running avgs for agent 3: q_loss: 36.15972137451172, p_loss: -1.793560266494751, mean_rew: -3.6165176812291273, variance: 4.033994674682617, lamda: 1.6050918102264404

steps: 674975, episodes: 27000, mean episode reward: -98.51261004921167, agent episode reward: [1.67, 1.67, -61.913920871268736, -39.93868917794293], time: 65.5
steps: 674975, episodes: 27000, mean episode variance: 2.634444634437561, agent episode variance: [0.4047363497018814, 0.39861920845508575, 0.8539184019565582, 0.9771706743240356], time: 65.501
Running avgs for agent 0: q_loss: 2.9183285236358643, p_loss: 1.0111157894134521, mean_rew: 0.075546875, variance: 1.6189454793930054, lamda: 1.6381103992462158
Running avgs for agent 1: q_loss: 2.468640089035034, p_loss: 0.9533883333206177, mean_rew: 0.0778125, variance: 1.5944768190383911, lamda: 1.6374094486236572
Running avgs for agent 2: q_loss: 6.573888301849365, p_loss: -1.9614201784133911, mean_rew: -2.804965156252015, variance: 3.4156734943389893, lamda: 1.6320255994796753
Running avgs for agent 3: q_loss: 43.54202651977539, p_loss: -1.699371337890625, mean_rew: -3.5335351253815883, variance: 3.9086825847625732, lamda: 1.6255320310592651

steps: 699975, episodes: 28000, mean episode reward: -119.605527194613, agent episode reward: [1.75, 1.75, -70.65170311999769, -52.45382407461532], time: 65.455
steps: 699975, episodes: 28000, mean episode variance: 2.601166440963745, agent episode variance: [0.40550073873996734, 0.4005742698907852, 0.8447175595760346, 0.9503738727569581], time: 65.455
Running avgs for agent 0: q_loss: 3.318519353866577, p_loss: 0.9813507199287415, mean_rew: 0.0744921875, variance: 1.6220029592514038, lamda: 1.6631145477294922
Running avgs for agent 1: q_loss: 2.7464029788970947, p_loss: 0.9486345648765564, mean_rew: 0.0772265625, variance: 1.602297067642212, lamda: 1.6624399423599243
Running avgs for agent 2: q_loss: 7.799300670623779, p_loss: -1.93349289894104, mean_rew: -2.7867188004589396, variance: 3.3788700103759766, lamda: 1.6554595232009888
Running avgs for agent 3: q_loss: 81.23661041259766, p_loss: -1.7250266075134277, mean_rew: -3.469250898581044, variance: 3.8014955520629883, lamda: 1.6471073627471924

steps: 724975, episodes: 29000, mean episode reward: -101.00648936210811, agent episode reward: [1.57, 1.57, -70.18036317747051, -33.96612618463759], time: 65.53
steps: 724975, episodes: 29000, mean episode variance: 2.5802593709230424, agent episode variance: [0.39902007830142977, 0.4012174379825592, 0.8427485251426696, 0.9372733294963836], time: 65.531
Running avgs for agent 0: q_loss: 3.3338828086853027, p_loss: 1.0130679607391357, mean_rew: 0.0763671875, variance: 1.5960803031921387, lamda: 1.6881186962127686
Running avgs for agent 1: q_loss: 2.70369815826416, p_loss: 0.9493098855018616, mean_rew: 0.079296875, variance: 1.6048697233200073, lamda: 1.6874516010284424
Running avgs for agent 2: q_loss: 7.188961982727051, p_loss: -1.9556199312210083, mean_rew: -2.7968201187973087, variance: 3.3709940910339355, lamda: 1.6795783042907715
Running avgs for agent 3: q_loss: 61.100589752197266, p_loss: -1.6983795166015625, mean_rew: -3.4066594350957287, variance: 3.7490932941436768, lamda: 1.6705244779586792

steps: 749975, episodes: 30000, mean episode reward: -98.79010039513433, agent episode reward: [1.28, 1.28, -69.33211049929592, -32.01798989583842], time: 65.629
steps: 749975, episodes: 30000, mean episode variance: 2.570524348139763, agent episode variance: [0.40491304194927213, 0.4109193749427795, 0.8346734976768494, 0.9200184335708618], time: 65.63
Running avgs for agent 0: q_loss: 3.4299299716949463, p_loss: 0.986563503742218, mean_rew: 0.0752734375, variance: 1.6196521520614624, lamda: 1.713122844696045
Running avgs for agent 1: q_loss: 2.5183188915252686, p_loss: 0.8627281785011292, mean_rew: 0.0737890625, variance: 1.6436774730682373, lamda: 1.7124384641647339
Running avgs for agent 2: q_loss: 7.109983921051025, p_loss: -1.9985032081604004, mean_rew: -2.809680023155245, variance: 3.338693857192993, lamda: 1.7004722356796265
Running avgs for agent 3: q_loss: 25.147869110107422, p_loss: -1.656313180923462, mean_rew: -3.3280269568055036, variance: 3.6800737380981445, lamda: 1.6926363706588745

steps: 774975, episodes: 31000, mean episode reward: -114.71374295615726, agent episode reward: [1.45, 1.45, -71.89558495507401, -45.71815800108326], time: 66.258
steps: 774975, episodes: 31000, mean episode variance: 2.5347360472679137, agent episode variance: [0.40367190408706666, 0.40266679430007934, 0.8268334081172943, 0.9015639407634735], time: 66.259
Running avgs for agent 0: q_loss: 3.231045722961426, p_loss: 0.9469733834266663, mean_rew: 0.07609375, variance: 1.6146876811981201, lamda: 1.7381267547607422
Running avgs for agent 1: q_loss: 2.650771379470825, p_loss: 0.8665273189544678, mean_rew: 0.0748046875, variance: 1.6106672286987305, lamda: 1.7373501062393188
Running avgs for agent 2: q_loss: 8.269954681396484, p_loss: -1.9901089668273926, mean_rew: -2.794783865546972, variance: 3.307333469390869, lamda: 1.7187548875808716
Running avgs for agent 3: q_loss: 25.04180335998535, p_loss: -1.7763134241104126, mean_rew: -3.2703247135506155, variance: 3.6062557697296143, lamda: 1.7155393362045288

steps: 799975, episodes: 32000, mean episode reward: -109.85494511776467, agent episode reward: [1.46, 1.46, -60.376519122069936, -52.39842599569472], time: 65.763
steps: 799975, episodes: 32000, mean episode variance: 2.526544675111771, agent episode variance: [0.4046325099468231, 0.40460961174964905, 0.8295553233623505, 0.887747230052948], time: 65.764
Running avgs for agent 0: q_loss: 2.9575910568237305, p_loss: 0.9473408460617065, mean_rew: 0.07671875, variance: 1.618530035018921, lamda: 1.7631165981292725
Running avgs for agent 1: q_loss: 2.948690176010132, p_loss: 0.8688541650772095, mean_rew: 0.0751171875, variance: 1.618438482284546, lamda: 1.7623661756515503
Running avgs for agent 2: q_loss: 6.659019470214844, p_loss: -1.9856746196746826, mean_rew: -2.7989708535760447, variance: 3.3182213306427, lamda: 1.7339904308319092
Running avgs for agent 3: q_loss: 46.62287902832031, p_loss: -1.664495587348938, mean_rew: -3.2314760766043027, variance: 3.5509889125823975, lamda: 1.7368865013122559

steps: 824975, episodes: 33000, mean episode reward: -116.44475976894775, agent episode reward: [1.29, 1.29, -45.04380666494936, -73.98095310399839], time: 65.728
steps: 824975, episodes: 33000, mean episode variance: 2.5188256633281707, agent episode variance: [0.40639015698432923, 0.4058782122135162, 0.8250288574695587, 0.8815284366607666], time: 65.728
Running avgs for agent 0: q_loss: 2.871833562850952, p_loss: 0.9218161106109619, mean_rew: 0.0705859375, variance: 1.6255606412887573, lamda: 1.7881194353103638
Running avgs for agent 1: q_loss: 3.067105531692505, p_loss: 0.8560019731521606, mean_rew: 0.0772265625, variance: 1.623512864112854, lamda: 1.787366509437561
Running avgs for agent 2: q_loss: 5.576656818389893, p_loss: -1.9745393991470337, mean_rew: -2.7780457631571336, variance: 3.3001155853271484, lamda: 1.7515703439712524
Running avgs for agent 3: q_loss: 26.997665405273438, p_loss: -1.762227177619934, mean_rew: -3.206658169110385, variance: 3.526113748550415, lamda: 1.7572141885757446

steps: 849975, episodes: 34000, mean episode reward: -112.32994237936745, agent episode reward: [1.88, 1.88, -57.5424191089686, -58.54752327039884], time: 65.936
steps: 849975, episodes: 34000, mean episode variance: 2.490504513382912, agent episode variance: [0.40549336969852445, 0.4049509978294373, 0.8149581968784332, 0.8651019489765167], time: 65.937
Running avgs for agent 0: q_loss: 2.915557384490967, p_loss: 0.9088698625564575, mean_rew: 0.0744921875, variance: 1.6219733953475952, lamda: 1.8131029605865479
Running avgs for agent 1: q_loss: 2.806356191635132, p_loss: 0.8373650908470154, mean_rew: 0.074453125, variance: 1.6198039054870605, lamda: 1.812371015548706
Running avgs for agent 2: q_loss: 6.345559597015381, p_loss: -1.9559687376022339, mean_rew: -2.7595409561805937, variance: 3.2598328590393066, lamda: 1.7698262929916382
Running avgs for agent 3: q_loss: 18.51700210571289, p_loss: -1.6646451950073242, mean_rew: -3.1892764250100827, variance: 3.4604077339172363, lamda: 1.777069091796875

steps: 874975, episodes: 35000, mean episode reward: -120.84916658439622, agent episode reward: [1.8, 1.8, -55.50572250957677, -68.94344407481945], time: 65.907
steps: 874975, episodes: 35000, mean episode variance: 2.492311137318611, agent episode variance: [0.4077689244747162, 0.4030239590406418, 0.809222288608551, 0.8722959651947021], time: 65.907
Running avgs for agent 0: q_loss: 3.038522481918335, p_loss: 0.8893236517906189, mean_rew: 0.0740234375, variance: 1.6310757398605347, lamda: 1.8380314111709595
Running avgs for agent 1: q_loss: 2.932957410812378, p_loss: 0.8652341365814209, mean_rew: 0.07203125, variance: 1.612095832824707, lamda: 1.8373804092407227
Running avgs for agent 2: q_loss: 6.073442459106445, p_loss: -1.9354454278945923, mean_rew: -2.7444902530402158, variance: 3.236889123916626, lamda: 1.7784868478775024
Running avgs for agent 3: q_loss: 15.721320152282715, p_loss: -1.641719937324524, mean_rew: -3.191023184909756, variance: 3.4891841411590576, lamda: 1.7971211671829224

steps: 899975, episodes: 36000, mean episode reward: -136.9340127920676, agent episode reward: [1.84, 1.84, -55.780607746270235, -84.83340504579736], time: 66.365
steps: 899975, episodes: 36000, mean episode variance: 2.488725016951561, agent episode variance: [0.4079192281961441, 0.40602610969543457, 0.804467493057251, 0.8703121860027313], time: 66.366
Running avgs for agent 0: q_loss: 2.9697675704956055, p_loss: 0.8611969947814941, mean_rew: 0.074296875, variance: 1.6316767930984497, lamda: 1.8630151748657227
Running avgs for agent 1: q_loss: 2.9486372470855713, p_loss: 0.8492250442504883, mean_rew: 0.0755859375, variance: 1.6241044998168945, lamda: 1.8623684644699097
Running avgs for agent 2: q_loss: 5.484401226043701, p_loss: -1.9375927448272705, mean_rew: -2.721084966136506, variance: 3.2178702354431152, lamda: 1.797061562538147
Running avgs for agent 3: q_loss: 25.850582122802734, p_loss: -1.653857946395874, mean_rew: -3.1976417760149585, variance: 3.4812488555908203, lamda: 1.8143341541290283

steps: 924975, episodes: 37000, mean episode reward: -115.51011773145143, agent episode reward: [1.74, 1.74, -51.67341861304463, -67.31669911840682], time: 65.842
steps: 924975, episodes: 37000, mean episode variance: 2.467819877386093, agent episode variance: [0.4074375052452087, 0.40378308486938475, 0.7971343612670898, 0.8594649260044098], time: 65.842
Running avgs for agent 0: q_loss: 2.828427314758301, p_loss: 0.8684205412864685, mean_rew: 0.070703125, variance: 1.6297500133514404, lamda: 1.8880146741867065
Running avgs for agent 1: q_loss: 3.060127019882202, p_loss: 0.8700941801071167, mean_rew: 0.0754296875, variance: 1.6151323318481445, lamda: 1.8873802423477173
Running avgs for agent 2: q_loss: 5.522993087768555, p_loss: -1.9308308362960815, mean_rew: -2.7041798226900893, variance: 3.18853759765625, lamda: 1.8172014951705933
Running avgs for agent 3: q_loss: 18.65127944946289, p_loss: -1.7613996267318726, mean_rew: -3.1700850166101873, variance: 3.437859535217285, lamda: 1.8302710056304932

steps: 949975, episodes: 38000, mean episode reward: -121.92882545405622, agent episode reward: [1.36, 1.36, -47.35143731197995, -77.29738814207626], time: 65.85
steps: 949975, episodes: 38000, mean episode variance: 2.4557521950006485, agent episode variance: [0.40814607644081113, 0.40642375123500823, 0.7874847919940948, 0.8536975753307342], time: 65.85
Running avgs for agent 0: q_loss: 2.920187473297119, p_loss: 0.8469848036766052, mean_rew: 0.0740625, variance: 1.6325842142105103, lamda: 1.9129990339279175
Running avgs for agent 1: q_loss: 2.8066792488098145, p_loss: 0.8720876574516296, mean_rew: 0.0721484375, variance: 1.6256951093673706, lamda: 1.912392020225525
Running avgs for agent 2: q_loss: 4.955322265625, p_loss: -1.930814504623413, mean_rew: -2.688327389431099, variance: 3.14993953704834, lamda: 1.8390947580337524
Running avgs for agent 3: q_loss: 32.1041259765625, p_loss: -1.7150146961212158, mean_rew: -3.1875213807878224, variance: 3.414790153503418, lamda: 1.8483843803405762

steps: 974975, episodes: 39000, mean episode reward: -132.1309142079511, agent episode reward: [2.04, 2.04, -50.34436824108863, -85.86654596686247], time: 65.946
steps: 974975, episodes: 39000, mean episode variance: 2.4495655802488328, agent episode variance: [0.40551512145996094, 0.4071110862493515, 0.780799485206604, 0.8561398873329162], time: 65.946
Running avgs for agent 0: q_loss: 2.94443941116333, p_loss: 0.8751855492591858, mean_rew: 0.0749609375, variance: 1.6220604181289673, lamda: 1.9379678964614868
Running avgs for agent 1: q_loss: 3.037313461303711, p_loss: 0.8718798756599426, mean_rew: 0.0711328125, variance: 1.6284443140029907, lamda: 1.9373767375946045
Running avgs for agent 2: q_loss: 5.756786346435547, p_loss: -1.9556083679199219, mean_rew: -2.6731314008475673, variance: 3.1231980323791504, lamda: 1.8563915491104126
Running avgs for agent 3: q_loss: 18.75258445739746, p_loss: -1.817184567451477, mean_rew: -3.163878933493194, variance: 3.4245595932006836, lamda: 1.8667649030685425

steps: 999975, episodes: 40000, mean episode reward: -130.74960555312788, agent episode reward: [2.07, 2.07, -49.581999836325856, -85.30760571680203], time: 65.767
steps: 999975, episodes: 40000, mean episode variance: 2.44753714120388, agent episode variance: [0.4091437598466873, 0.40596594858169555, 0.7814802100658417, 0.8509472227096557], time: 65.768
Running avgs for agent 0: q_loss: 2.8984498977661133, p_loss: 0.8559628129005432, mean_rew: 0.0722265625, variance: 1.6365749835968018, lamda: 1.962965488433838
Running avgs for agent 1: q_loss: 2.820559024810791, p_loss: 0.8899750113487244, mean_rew: 0.074375, variance: 1.6238638162612915, lamda: 1.9623417854309082
Running avgs for agent 2: q_loss: 5.519237041473389, p_loss: -1.9391160011291504, mean_rew: -2.662536156479489, variance: 3.1259210109710693, lamda: 1.8635326623916626
Running avgs for agent 3: q_loss: 14.488329887390137, p_loss: -1.7760676145553589, mean_rew: -3.1677208031064374, variance: 3.4037888050079346, lamda: 1.8804028034210205

steps: 1024975, episodes: 41000, mean episode reward: -125.15970881299292, agent episode reward: [2.0, 2.0, -48.835535226999944, -80.32417358599298], time: 65.935
steps: 1024975, episodes: 41000, mean episode variance: 2.4490691673755647, agent episode variance: [0.41010552644729614, 0.40731779420375824, 0.7795660986900329, 0.8520797480344773], time: 65.936
Running avgs for agent 0: q_loss: 3.131164789199829, p_loss: 0.8489754796028137, mean_rew: 0.0716015625, variance: 1.6404221057891846, lamda: 1.9879672527313232
Running avgs for agent 1: q_loss: 2.8640189170837402, p_loss: 0.9068146347999573, mean_rew: 0.071484375, variance: 1.6292712688446045, lamda: 1.9873347282409668
Running avgs for agent 2: q_loss: 5.328991889953613, p_loss: -1.9241912364959717, mean_rew: -2.6643657570483747, variance: 3.1182641983032227, lamda: 1.8800407648086548
Running avgs for agent 3: q_loss: 13.935091018676758, p_loss: -1.7992298603057861, mean_rew: -3.205449993726623, variance: 3.4083189964294434, lamda: 1.8934526443481445

steps: 1049975, episodes: 42000, mean episode reward: -127.32595412950201, agent episode reward: [2.15, 2.15, -59.4982297662667, -72.12772436323532], time: 66.058
steps: 1049975, episodes: 42000, mean episode variance: 2.4537977966070175, agent episode variance: [0.4091938407421112, 0.41421165883541106, 0.776063912153244, 0.8543283848762512], time: 66.059
Running avgs for agent 0: q_loss: 2.9774906635284424, p_loss: 0.8544467687606812, mean_rew: 0.0729296875, variance: 1.6367753744125366, lamda: 2.012965679168701
Running avgs for agent 1: q_loss: 3.0897421836853027, p_loss: 0.8435657620429993, mean_rew: 0.0726953125, variance: 1.6568466424942017, lamda: 2.01234769821167
Running avgs for agent 2: q_loss: 5.505016326904297, p_loss: -1.9114253520965576, mean_rew: -2.6687901555972307, variance: 3.1042556762695312, lamda: 1.8988311290740967
Running avgs for agent 3: q_loss: 8.056584358215332, p_loss: -1.8528733253479004, mean_rew: -3.2570562751496723, variance: 3.417313575744629, lamda: 1.9017951488494873

steps: 1074975, episodes: 43000, mean episode reward: -145.7245326908371, agent episode reward: [1.78, 1.78, -67.81861160289029, -81.4659210879468], time: 66.998
steps: 1074975, episodes: 43000, mean episode variance: 2.440462806344032, agent episode variance: [0.4115966509580612, 0.4105878131389618, 0.7674156033992767, 0.8508627388477326], time: 66.998
Running avgs for agent 0: q_loss: 2.871802568435669, p_loss: 0.8269773125648499, mean_rew: 0.0707421875, variance: 1.646386742591858, lamda: 2.037935733795166
Running avgs for agent 1: q_loss: 2.8777036666870117, p_loss: 0.8167359828948975, mean_rew: 0.0698828125, variance: 1.6423512697219849, lamda: 2.037341833114624
Running avgs for agent 2: q_loss: 5.626312732696533, p_loss: -1.8981757164001465, mean_rew: -2.6308060740396257, variance: 3.06966233253479, lamda: 1.9109506607055664
Running avgs for agent 3: q_loss: 5.904162883758545, p_loss: -1.8793708086013794, mean_rew: -3.2436625961520247, variance: 3.4034512042999268, lamda: 1.9025676250457764

steps: 1099975, episodes: 44000, mean episode reward: -141.48320834087338, agent episode reward: [1.83, 1.83, -73.59401586427991, -71.5491924765935], time: 66.186
steps: 1099975, episodes: 44000, mean episode variance: 2.4241878377199173, agent episode variance: [0.40967076289653775, 0.4097217316627502, 0.7641040873527527, 0.8406912558078766], time: 66.187
Running avgs for agent 0: q_loss: 2.8591501712799072, p_loss: 0.845976710319519, mean_rew: 0.0705859375, variance: 1.6386829614639282, lamda: 2.0628933906555176
Running avgs for agent 1: q_loss: 2.6936697959899902, p_loss: 0.84465491771698, mean_rew: 0.068359375, variance: 1.6388869285583496, lamda: 2.062303304672241
Running avgs for agent 2: q_loss: 5.431722640991211, p_loss: -1.864736795425415, mean_rew: -2.612715427782883, variance: 3.0564165115356445, lamda: 1.931716799736023
Running avgs for agent 3: q_loss: 5.555300712585449, p_loss: -1.8795620203018188, mean_rew: -3.191862501825166, variance: 3.362764835357666, lamda: 1.9027832746505737

steps: 1124975, episodes: 45000, mean episode reward: -136.24536374036182, agent episode reward: [2.04, 2.04, -84.5887438782317, -55.73661986213013], time: 66.19
steps: 1124975, episodes: 45000, mean episode variance: 2.4236505653858185, agent episode variance: [0.4121322469711304, 0.40886838698387146, 0.7570173702239991, 0.8456325612068176], time: 66.19
Running avgs for agent 0: q_loss: 2.7591512203216553, p_loss: 0.8341013193130493, mean_rew: 0.06875, variance: 1.648529052734375, lamda: 2.0878593921661377
Running avgs for agent 1: q_loss: 2.883803129196167, p_loss: 0.8714705109596252, mean_rew: 0.0683203125, variance: 1.635473608970642, lamda: 2.087222099304199
Running avgs for agent 2: q_loss: 5.630835056304932, p_loss: -1.889392375946045, mean_rew: -2.5988342292442868, variance: 3.028069257736206, lamda: 1.9540495872497559
Running avgs for agent 3: q_loss: 5.462105751037598, p_loss: -1.8923697471618652, mean_rew: -3.1658698170251927, variance: 3.3825302124023438, lamda: 1.903135061264038

steps: 1149975, episodes: 46000, mean episode reward: -141.9816988121746, agent episode reward: [2.29, 2.29, -96.73051013548096, -49.831188676693664], time: 66.023
steps: 1149975, episodes: 46000, mean episode variance: 2.4113893480300903, agent episode variance: [0.41119420433044435, 0.41194972586631773, 0.7542954745292664, 0.8339499433040619], time: 66.024
Running avgs for agent 0: q_loss: 2.648865222930908, p_loss: 0.8189405202865601, mean_rew: 0.064921875, variance: 1.6447768211364746, lamda: 2.112755537033081
Running avgs for agent 1: q_loss: 2.5101730823516846, p_loss: 0.8583946824073792, mean_rew: 0.0655859375, variance: 1.6477988958358765, lamda: 2.1121513843536377
Running avgs for agent 2: q_loss: 5.418877124786377, p_loss: -1.9123872518539429, mean_rew: -2.6368378531318095, variance: 3.017181873321533, lamda: 1.9748588800430298
Running avgs for agent 3: q_loss: 5.612876892089844, p_loss: -1.892853856086731, mean_rew: -3.1235538468244357, variance: 3.3357996940612793, lamda: 1.9037394523620605

steps: 1174975, episodes: 47000, mean episode reward: -144.3611563638915, agent episode reward: [2.09, 2.09, -99.58017648273535, -48.96097988115616], time: 66.849
steps: 1174975, episodes: 47000, mean episode variance: 2.4062506145238878, agent episode variance: [0.41053316223621367, 0.4131308157444, 0.7581007425785065, 0.8244858939647675], time: 66.85
Running avgs for agent 0: q_loss: 2.84281325340271, p_loss: 0.8481283187866211, mean_rew: 0.0700390625, variance: 1.6421326398849487, lamda: 2.1377782821655273
Running avgs for agent 1: q_loss: 2.690329074859619, p_loss: 0.8499678373336792, mean_rew: 0.0705078125, variance: 1.652523159980774, lamda: 2.1370692253112793
Running avgs for agent 2: q_loss: 5.019747257232666, p_loss: -1.8812252283096313, mean_rew: -2.641298711223261, variance: 3.032402753829956, lamda: 1.985945224761963
Running avgs for agent 3: q_loss: 5.339391708374023, p_loss: -1.858028531074524, mean_rew: -3.0640720419520253, variance: 3.297943592071533, lamda: 1.9041314125061035

steps: 1199975, episodes: 48000, mean episode reward: -126.40326591579745, agent episode reward: [2.08, 2.08, -89.51607259622551, -41.04719331957193], time: 66.15
steps: 1199975, episodes: 48000, mean episode variance: 2.4016857650279997, agent episode variance: [0.4120997096300125, 0.40931639754772187, 0.7558467161655426, 0.8244229416847229], time: 66.15
Running avgs for agent 0: q_loss: 2.700241804122925, p_loss: 0.8143711686134338, mean_rew: 0.068125, variance: 1.648398756980896, lamda: 2.162778854370117
Running avgs for agent 1: q_loss: 2.4848146438598633, p_loss: 0.8592199087142944, mean_rew: 0.0665234375, variance: 1.637265682220459, lamda: 2.1620354652404785
Running avgs for agent 2: q_loss: 5.169591903686523, p_loss: -1.890383005142212, mean_rew: -2.674258911112677, variance: 3.0233869552612305, lamda: 2.0055387020111084
Running avgs for agent 3: q_loss: 5.132250785827637, p_loss: -1.8390226364135742, mean_rew: -2.9913669508181764, variance: 3.297691822052002, lamda: 1.9043800830841064

steps: 1224975, episodes: 49000, mean episode reward: -135.42949004655512, agent episode reward: [1.99, 1.99, -98.56207860815805, -40.847411438397074], time: 66.043
steps: 1224975, episodes: 49000, mean episode variance: 2.392411904454231, agent episode variance: [0.4118623498678207, 0.4147874209880829, 0.7568268711566926, 0.8089352624416352], time: 66.044
Running avgs for agent 0: q_loss: 2.6166486740112305, p_loss: 0.8305164575576782, mean_rew: 0.0655078125, variance: 1.6474494934082031, lamda: 2.1877503395080566
Running avgs for agent 1: q_loss: 2.7190957069396973, p_loss: 0.8459760546684265, mean_rew: 0.0656640625, variance: 1.6591496467590332, lamda: 2.186939001083374
Running avgs for agent 2: q_loss: 5.452982425689697, p_loss: -1.8976296186447144, mean_rew: -2.695025278387813, variance: 3.0273072719573975, lamda: 2.0172278881073
Running avgs for agent 3: q_loss: 5.231893539428711, p_loss: -1.8239797353744507, mean_rew: -2.947026620994585, variance: 3.235741138458252, lamda: 1.9046964645385742

steps: 1249975, episodes: 50000, mean episode reward: -136.05133772465499, agent episode reward: [1.94, 1.94, -102.64610403327318, -37.2852336913818], time: 66.255
steps: 1249975, episodes: 50000, mean episode variance: 2.389753604054451, agent episode variance: [0.4114466052055359, 0.4127824317216873, 0.7596173930168152, 0.8059071741104126], time: 66.255
Running avgs for agent 0: q_loss: 2.5878026485443115, p_loss: 0.8585932850837708, mean_rew: 0.0653125, variance: 1.6457864046096802, lamda: 2.2127304077148438
Running avgs for agent 1: q_loss: 2.6281991004943848, p_loss: 0.8311900496482849, mean_rew: 0.069765625, variance: 1.6511297225952148, lamda: 2.211902379989624
Running avgs for agent 2: q_loss: 5.116584777832031, p_loss: -1.875298023223877, mean_rew: -2.7024511089001115, variance: 3.0384697914123535, lamda: 2.0268783569335938
Running avgs for agent 3: q_loss: 5.001317501068115, p_loss: -1.7946034669876099, mean_rew: -2.90519102641294, variance: 3.2236287593841553, lamda: 1.905063271522522

steps: 1274975, episodes: 51000, mean episode reward: -143.89048772037913, agent episode reward: [1.47, 1.47, -104.54589768444063, -42.28459003593851], time: 66.439
steps: 1274975, episodes: 51000, mean episode variance: 2.3911511070728304, agent episode variance: [0.41302973628044126, 0.41685716819763186, 0.756071474313736, 0.8051927282810211], time: 66.44
Running avgs for agent 0: q_loss: 2.663597822189331, p_loss: 0.8503947257995605, mean_rew: 0.066328125, variance: 1.6521189212799072, lamda: 2.237680196762085
Running avgs for agent 1: q_loss: 2.7132294178009033, p_loss: 0.8048444390296936, mean_rew: 0.0698046875, variance: 1.667428731918335, lamda: 2.236942768096924
Running avgs for agent 2: q_loss: 4.912156105041504, p_loss: -1.863927960395813, mean_rew: -2.684186462762893, variance: 3.0242857933044434, lamda: 2.0339138507843018
Running avgs for agent 3: q_loss: 4.9329833984375, p_loss: -1.7950063943862915, mean_rew: -2.8973041898264915, variance: 3.220771074295044, lamda: 1.9055112600326538

steps: 1299975, episodes: 52000, mean episode reward: -153.7552025169097, agent episode reward: [1.75, 1.75, -108.28469110686987, -48.97051141003982], time: 66.35
steps: 1299975, episodes: 52000, mean episode variance: 2.367478421449661, agent episode variance: [0.41166606295108793, 0.4149505490064621, 0.7532855224609375, 0.7875762870311737], time: 66.351
Running avgs for agent 0: q_loss: 2.7802183628082275, p_loss: 0.876700222492218, mean_rew: 0.066171875, variance: 1.6466642618179321, lamda: 2.2625937461853027
Running avgs for agent 1: q_loss: 2.5324325561523438, p_loss: 0.7571590542793274, mean_rew: 0.066015625, variance: 1.6598021984100342, lamda: 2.2619106769561768
Running avgs for agent 2: q_loss: 5.60883903503418, p_loss: -1.8697940111160278, mean_rew: -2.673468740573142, variance: 3.0131421089172363, lamda: 2.0478391647338867
Running avgs for agent 3: q_loss: 4.823276519775391, p_loss: -1.7621997594833374, mean_rew: -2.7955277812332486, variance: 3.1503052711486816, lamda: 1.9058986902236938

steps: 1324975, episodes: 53000, mean episode reward: -150.22775379108216, agent episode reward: [1.71, 1.71, -105.66052608665554, -47.98722770442664], time: 66.557
steps: 1324975, episodes: 53000, mean episode variance: 2.3497737489938735, agent episode variance: [0.41041689658164976, 0.4137381304502487, 0.7462390053272248, 0.7793797166347504], time: 66.558
Running avgs for agent 0: q_loss: 2.6063249111175537, p_loss: 0.8893259167671204, mean_rew: 0.0680078125, variance: 1.6416676044464111, lamda: 2.2875025272369385
Running avgs for agent 1: q_loss: 2.6116957664489746, p_loss: 0.7962104678153992, mean_rew: 0.0675, variance: 1.6549525260925293, lamda: 2.286881923675537
Running avgs for agent 2: q_loss: 5.029398441314697, p_loss: -1.8627703189849854, mean_rew: -2.6679593437677442, variance: 2.9849560260772705, lamda: 2.0635592937469482
Running avgs for agent 3: q_loss: 4.834738254547119, p_loss: -1.7282466888427734, mean_rew: -2.713210759804673, variance: 3.1175191402435303, lamda: 1.9097355604171753

steps: 1349975, episodes: 54000, mean episode reward: -148.78627114089224, agent episode reward: [1.58, 1.58, -107.50565101107388, -44.44062012981834], time: 66.43
steps: 1349975, episodes: 54000, mean episode variance: 2.3525086109638216, agent episode variance: [0.415114314198494, 0.4146637033224106, 0.7544001388549805, 0.7683304545879364], time: 66.431
Running avgs for agent 0: q_loss: 2.9886419773101807, p_loss: 0.8670812249183655, mean_rew: 0.0686328125, variance: 1.6604572534561157, lamda: 2.312500476837158
Running avgs for agent 1: q_loss: 2.5987796783447266, p_loss: 0.7955588102340698, mean_rew: 0.0658203125, variance: 1.658654808998108, lamda: 2.3117759227752686
Running avgs for agent 2: q_loss: 4.7246222496032715, p_loss: -1.883788824081421, mean_rew: -2.725821246351257, variance: 3.0176005363464355, lamda: 2.0819928646087646
Running avgs for agent 3: q_loss: 4.705327033996582, p_loss: -1.6730796098709106, mean_rew: -2.6315825134637127, variance: 3.07332181930542, lamda: 1.9141411781311035

steps: 1374975, episodes: 55000, mean episode reward: -147.08868041560206, agent episode reward: [1.27, 1.27, -101.1343972033337, -48.49428321226836], time: 66.369
steps: 1374975, episodes: 55000, mean episode variance: 2.3342554862499236, agent episode variance: [0.4151931653022766, 0.4108921639919281, 0.7555883271694184, 0.7525818297863006], time: 66.37
Running avgs for agent 0: q_loss: 2.742842674255371, p_loss: 0.8431059718132019, mean_rew: 0.06578125, variance: 1.660772681236267, lamda: 2.3374197483062744
Running avgs for agent 1: q_loss: 2.79101300239563, p_loss: 0.8276664018630981, mean_rew: 0.069296875, variance: 1.6435686349868774, lamda: 2.3367724418640137
Running avgs for agent 2: q_loss: 4.862845420837402, p_loss: -1.888578176498413, mean_rew: -2.77886682696972, variance: 3.022353410720825, lamda: 2.098771572113037
Running avgs for agent 3: q_loss: 4.383099555969238, p_loss: -1.6255171298980713, mean_rew: -2.5463312125730053, variance: 3.010327100753784, lamda: 1.9144448041915894

steps: 1399975, episodes: 56000, mean episode reward: -147.9625757007779, agent episode reward: [1.54, 1.54, -99.63480712086209, -51.4077685799158], time: 66.409
steps: 1399975, episodes: 56000, mean episode variance: 2.3254628542661666, agent episode variance: [0.41227978086471556, 0.41146691381931305, 0.7584323735237122, 0.7432837860584259], time: 66.41
Running avgs for agent 0: q_loss: 2.8668220043182373, p_loss: 0.8780828714370728, mean_rew: 0.0693359375, variance: 1.649119257926941, lamda: 2.362339735031128
Running avgs for agent 1: q_loss: 2.6052825450897217, p_loss: 0.8708844184875488, mean_rew: 0.067109375, variance: 1.6458677053451538, lamda: 2.361732006072998
Running avgs for agent 2: q_loss: 4.663578033447266, p_loss: -1.9006506204605103, mean_rew: -2.8135140391987834, variance: 3.0337295532226562, lamda: 2.1125130653381348
Running avgs for agent 3: q_loss: 4.52846097946167, p_loss: -1.5924456119537354, mean_rew: -2.4725785346900593, variance: 2.973135232925415, lamda: 1.9156955480575562

steps: 1424975, episodes: 57000, mean episode reward: -162.5037561269296, agent episode reward: [2.23, 2.23, -108.07959961609616, -58.88415651083345], time: 66.649
steps: 1424975, episodes: 57000, mean episode variance: 2.3244470571279527, agent episode variance: [0.4131900215148926, 0.4122118822336197, 0.7636081855297089, 0.7354369678497314], time: 66.649
Running avgs for agent 0: q_loss: 2.853044033050537, p_loss: 0.8770995140075684, mean_rew: 0.068515625, variance: 1.6527601480484009, lamda: 2.3871800899505615
Running avgs for agent 1: q_loss: 2.8013222217559814, p_loss: 0.8763651251792908, mean_rew: 0.0673828125, variance: 1.6488476991653442, lamda: 2.3866045475006104
Running avgs for agent 2: q_loss: 5.291014194488525, p_loss: -1.940231442451477, mean_rew: -2.8711891127590796, variance: 3.0544326305389404, lamda: 2.1205947399139404
Running avgs for agent 3: q_loss: 4.239401340484619, p_loss: -1.5464999675750732, mean_rew: -2.4105149978401825, variance: 2.9417481422424316, lamda: 1.9163098335266113

steps: 1449975, episodes: 58000, mean episode reward: -166.61760131783015, agent episode reward: [2.38, 2.38, -104.44765309056787, -66.92994822726227], time: 66.45
steps: 1449975, episodes: 58000, mean episode variance: 2.332201371669769, agent episode variance: [0.41315165436267853, 0.4131016763448715, 0.7728643391132355, 0.7330837018489837], time: 66.45
Running avgs for agent 0: q_loss: 2.8215126991271973, p_loss: 0.9002147316932678, mean_rew: 0.0678515625, variance: 1.6526066064834595, lamda: 2.4121596813201904
Running avgs for agent 1: q_loss: 2.8061983585357666, p_loss: 0.8874592185020447, mean_rew: 0.067421875, variance: 1.6524066925048828, lamda: 2.4116156101226807
Running avgs for agent 2: q_loss: 4.158790588378906, p_loss: -1.9486106634140015, mean_rew: -2.927751073203265, variance: 3.0914573669433594, lamda: 2.126488447189331
Running avgs for agent 3: q_loss: 4.340844631195068, p_loss: -1.4995678663253784, mean_rew: -2.3602782542272562, variance: 2.9323346614837646, lamda: 1.9170175790786743

steps: 1474975, episodes: 59000, mean episode reward: -174.23697361773446, agent episode reward: [1.99, 1.99, -106.54369283550312, -71.67328078223133], time: 66.541
steps: 1474975, episodes: 59000, mean episode variance: 2.3323781695365904, agent episode variance: [0.4114780263900757, 0.41389017474651335, 0.7752947161197662, 0.7317152522802353], time: 66.541
Running avgs for agent 0: q_loss: 2.834291696548462, p_loss: 0.9331360459327698, mean_rew: 0.0697265625, variance: 1.6459119319915771, lamda: 2.43715500831604
Running avgs for agent 1: q_loss: 3.0929906368255615, p_loss: 0.9044625759124756, mean_rew: 0.07265625, variance: 1.6555604934692383, lamda: 2.4365878105163574
Running avgs for agent 2: q_loss: 4.2555460929870605, p_loss: -1.942185878753662, mean_rew: -2.9637730481470324, variance: 3.1011786460876465, lamda: 2.1390509605407715
Running avgs for agent 3: q_loss: 4.34406852722168, p_loss: -1.514748454093933, mean_rew: -2.3469909399619207, variance: 2.926861047744751, lamda: 1.9182286262512207

steps: 1499975, episodes: 60000, mean episode reward: -176.9428944568523, agent episode reward: [1.66, 1.66, -99.24788621634092, -81.01500824051132], time: 66.46
steps: 1499975, episodes: 60000, mean episode variance: 2.3335053944587707, agent episode variance: [0.41354163718223574, 0.4161510443687439, 0.7752155663967133, 0.7285971465110779], time: 66.46
Running avgs for agent 0: q_loss: 2.632671356201172, p_loss: 0.9425398111343384, mean_rew: 0.0686328125, variance: 1.6541664600372314, lamda: 2.46211314201355
Running avgs for agent 1: q_loss: 2.659571647644043, p_loss: 0.8797855377197266, mean_rew: 0.07125, variance: 1.6646043062210083, lamda: 2.4616050720214844
Running avgs for agent 2: q_loss: 3.7338321208953857, p_loss: -1.9555240869522095, mean_rew: -3.0050789364885615, variance: 3.1008622646331787, lamda: 2.151705741882324
Running avgs for agent 3: q_loss: 4.405654430389404, p_loss: -1.5178261995315552, mean_rew: -2.3580139312058406, variance: 2.914388656616211, lamda: 1.9201536178588867

steps: 1524975, episodes: 61000, mean episode reward: -168.5333432719756, agent episode reward: [1.55, 1.55, -88.79032666315531, -82.84301660882029], time: 66.597
steps: 1524975, episodes: 61000, mean episode variance: 2.343221625804901, agent episode variance: [0.41486979329586027, 0.4158765071630478, 0.7781007752418518, 0.7343745501041412], time: 66.597
Running avgs for agent 0: q_loss: 2.54105281829834, p_loss: 0.9498695135116577, mean_rew: 0.06703125, variance: 1.6594792604446411, lamda: 2.486950159072876
Running avgs for agent 1: q_loss: 2.865579605102539, p_loss: 0.8776097297668457, mean_rew: 0.070859375, variance: 1.663506031036377, lamda: 2.48651385307312
Running avgs for agent 2: q_loss: 3.94551420211792, p_loss: -1.974656343460083, mean_rew: -3.0434510055594837, variance: 3.112403392791748, lamda: 2.1633217334747314
Running avgs for agent 3: q_loss: 4.321640491485596, p_loss: -1.5216113328933716, mean_rew: -2.3930374697495864, variance: 2.937498092651367, lamda: 1.9211115837097168

steps: 1549975, episodes: 62000, mean episode reward: -160.45478852982941, agent episode reward: [1.36, 1.36, -76.95391730520022, -86.22087122462919], time: 66.609
steps: 1549975, episodes: 62000, mean episode variance: 2.3386921285390856, agent episode variance: [0.41262631940841676, 0.41735167944431306, 0.7751135323047638, 0.7336005973815918], time: 66.609
Running avgs for agent 0: q_loss: 2.8743984699249268, p_loss: 0.9703677892684937, mean_rew: 0.0698828125, variance: 1.6505054235458374, lamda: 2.5119175910949707
Running avgs for agent 1: q_loss: 2.994203567504883, p_loss: 0.8506129384040833, mean_rew: 0.066328125, variance: 1.669406771659851, lamda: 2.511444330215454
Running avgs for agent 2: q_loss: 5.148609161376953, p_loss: -1.971604824066162, mean_rew: -3.063921134056673, variance: 3.100454092025757, lamda: 2.171995162963867
Running avgs for agent 3: q_loss: 4.478630065917969, p_loss: -1.4618487358093262, mean_rew: -2.4052807326278387, variance: 2.9344022274017334, lamda: 1.9235581159591675

steps: 1574975, episodes: 63000, mean episode reward: -161.5624076234652, agent episode reward: [1.7, 1.7, -76.251786502885, -88.71062112058021], time: 66.734
steps: 1574975, episodes: 63000, mean episode variance: 2.3480191560983656, agent episode variance: [0.41384779489040374, 0.4131444923877716, 0.7810164654254913, 0.7400104033946991], time: 66.734
Running avgs for agent 0: q_loss: 3.1890981197357178, p_loss: 0.9650024175643921, mean_rew: 0.069921875, variance: 1.6553910970687866, lamda: 2.536957025527954
Running avgs for agent 1: q_loss: 2.857943296432495, p_loss: 0.86772620677948, mean_rew: 0.068046875, variance: 1.6525779962539673, lamda: 2.536353826522827
Running avgs for agent 2: q_loss: 3.7478394508361816, p_loss: -1.9627997875213623, mean_rew: -3.0796597750084915, variance: 3.12406587600708, lamda: 2.180497407913208
Running avgs for agent 3: q_loss: 3.8637943267822266, p_loss: -1.490768551826477, mean_rew: -2.4441534144645076, variance: 2.9600415229797363, lamda: 1.9281080961227417

steps: 1599975, episodes: 64000, mean episode reward: -164.02227691259554, agent episode reward: [1.58, 1.58, -84.38546235947766, -82.79681455311788], time: 66.697
steps: 1599975, episodes: 64000, mean episode variance: 2.365285529613495, agent episode variance: [0.41631150937080386, 0.4198312427997589, 0.7873061056137085, 0.7418366718292236], time: 66.698
Running avgs for agent 0: q_loss: 2.9395415782928467, p_loss: 0.9457499980926514, mean_rew: 0.0683984375, variance: 1.6652460098266602, lamda: 2.5619852542877197
Running avgs for agent 1: q_loss: 3.0086987018585205, p_loss: 0.8346956968307495, mean_rew: 0.0678125, variance: 1.679324984550476, lamda: 2.5611135959625244
Running avgs for agent 2: q_loss: 5.336385726928711, p_loss: -1.96348237991333, mean_rew: -3.120508022394192, variance: 3.149224281311035, lamda: 2.1900925636291504
Running avgs for agent 3: q_loss: 4.172504901885986, p_loss: -1.5027098655700684, mean_rew: -2.433659307153079, variance: 2.967346668243408, lamda: 1.9325634241104126

steps: 1624975, episodes: 65000, mean episode reward: -140.14869679766963, agent episode reward: [1.77, 1.77, -81.62181376212243, -62.066883035547214], time: 67.139
steps: 1624975, episodes: 65000, mean episode variance: 2.3543086050748827, agent episode variance: [0.4147149647474289, 0.4132545499801636, 0.787710673570633, 0.7386284167766571], time: 67.14
Running avgs for agent 0: q_loss: 2.93019437789917, p_loss: 0.9496643543243408, mean_rew: 0.0674609375, variance: 1.6588598489761353, lamda: 2.5870249271392822
Running avgs for agent 1: q_loss: 2.828280210494995, p_loss: 0.8749973773956299, mean_rew: 0.07140625, variance: 1.6530182361602783, lamda: 2.5861191749572754
Running avgs for agent 2: q_loss: 5.636507034301758, p_loss: -1.9827017784118652, mean_rew: -3.1583970429573367, variance: 3.1508424282073975, lamda: 2.192811965942383
Running avgs for agent 3: q_loss: 4.125633716583252, p_loss: -1.5093209743499756, mean_rew: -2.433370395917561, variance: 2.9545137882232666, lamda: 1.9329841136932373

steps: 1649975, episodes: 66000, mean episode reward: -124.03309927254726, agent episode reward: [1.7, 1.7, -78.46477655537447, -48.968322717172796], time: 66.952
steps: 1649975, episodes: 66000, mean episode variance: 2.357591069340706, agent episode variance: [0.4135049238204956, 0.41553285443782806, 0.7896767821311951, 0.7388765089511872], time: 66.953
Running avgs for agent 0: q_loss: 2.9527499675750732, p_loss: 0.9555003643035889, mean_rew: 0.071328125, variance: 1.654019832611084, lamda: 2.6119890213012695
Running avgs for agent 1: q_loss: 2.906900405883789, p_loss: 0.8929843902587891, mean_rew: 0.0682421875, variance: 1.6621313095092773, lamda: 2.6111044883728027
Running avgs for agent 2: q_loss: 5.59305477142334, p_loss: -1.9719904661178589, mean_rew: -3.1628155535512525, variance: 3.1587071418762207, lamda: 2.1951723098754883
Running avgs for agent 3: q_loss: 4.084730625152588, p_loss: -1.5054230690002441, mean_rew: -2.443636322362991, variance: 2.9555060863494873, lamda: 1.9352173805236816

steps: 1674975, episodes: 67000, mean episode reward: -127.86608100010889, agent episode reward: [1.95, 1.95, -74.03823165031145, -57.72784934979746], time: 66.876
steps: 1674975, episodes: 67000, mean episode variance: 2.362434874176979, agent episode variance: [0.41850513792037963, 0.41589098632335664, 0.7865299754142762, 0.7415087745189667], time: 66.877
Running avgs for agent 0: q_loss: 3.064208507537842, p_loss: 0.9318047761917114, mean_rew: 0.0712890625, variance: 1.674020528793335, lamda: 2.6369380950927734
Running avgs for agent 1: q_loss: 3.096255302429199, p_loss: 0.8914039134979248, mean_rew: 0.070703125, variance: 1.6635639667510986, lamda: 2.6361048221588135
Running avgs for agent 2: q_loss: 5.5831499099731445, p_loss: -1.9817861318588257, mean_rew: -3.1773368130953443, variance: 3.1461195945739746, lamda: 2.195563316345215
Running avgs for agent 3: q_loss: 4.033509731292725, p_loss: -1.4650702476501465, mean_rew: -2.467795869095494, variance: 2.9660351276397705, lamda: 1.9374616146087646

steps: 1699975, episodes: 68000, mean episode reward: -128.02703185340368, agent episode reward: [1.9, 1.9, -81.9399017944636, -49.88713005894009], time: 66.919
steps: 1699975, episodes: 68000, mean episode variance: 2.365436541080475, agent episode variance: [0.42022410428524015, 0.41496340358257294, 0.786862821340561, 0.7433862118721009], time: 66.92
Running avgs for agent 0: q_loss: 2.9088125228881836, p_loss: 0.8849888443946838, mean_rew: 0.069765625, variance: 1.680896520614624, lamda: 2.6618802547454834
Running avgs for agent 1: q_loss: 2.894834518432617, p_loss: 0.921337366104126, mean_rew: 0.07109375, variance: 1.659853458404541, lamda: 2.661137342453003
Running avgs for agent 2: q_loss: 5.513676643371582, p_loss: -1.9781649112701416, mean_rew: -3.1964116729126513, variance: 3.147451162338257, lamda: 2.195784568786621
Running avgs for agent 3: q_loss: 3.776247978210449, p_loss: -1.4570364952087402, mean_rew: -2.47755565998607, variance: 2.9735445976257324, lamda: 1.9430298805236816

steps: 1724975, episodes: 69000, mean episode reward: -148.17962934971163, agent episode reward: [1.7, 1.7, -92.29356519899336, -59.28606415071825], time: 66.838
steps: 1724975, episodes: 69000, mean episode variance: 2.3647811529636384, agent episode variance: [0.41512359654903414, 0.413762482047081, 0.7900908889770508, 0.7458041853904724], time: 66.839
Running avgs for agent 0: q_loss: 3.0236520767211914, p_loss: 0.8928332328796387, mean_rew: 0.0739453125, variance: 1.660494327545166, lamda: 2.686818838119507
Running avgs for agent 1: q_loss: 2.9317681789398193, p_loss: 0.9601375460624695, mean_rew: 0.0729296875, variance: 1.6550499200820923, lamda: 2.6861279010772705
Running avgs for agent 2: q_loss: 5.059061527252197, p_loss: -1.9792646169662476, mean_rew: -3.1990640667651102, variance: 3.1603636741638184, lamda: 2.198143243789673
Running avgs for agent 3: q_loss: 3.187722682952881, p_loss: -1.4604014158248901, mean_rew: -2.484620322508468, variance: 2.9832165241241455, lamda: 1.947411298751831

steps: 1749975, episodes: 70000, mean episode reward: -162.45001842303176, agent episode reward: [1.94, 1.94, -99.22627145246393, -67.10374697056783], time: 66.785
steps: 1749975, episodes: 70000, mean episode variance: 2.378777963399887, agent episode variance: [0.4192988586425781, 0.4194668471813202, 0.7922106268405914, 0.7478016307353973], time: 66.785
Running avgs for agent 0: q_loss: 2.9029481410980225, p_loss: 0.8751004934310913, mean_rew: 0.0696875, variance: 1.6771955490112305, lamda: 2.7116992473602295
Running avgs for agent 1: q_loss: 2.9391257762908936, p_loss: 0.9239582419395447, mean_rew: 0.0707421875, variance: 1.6778674125671387, lamda: 2.7111079692840576
Running avgs for agent 2: q_loss: 4.604726791381836, p_loss: -2.010598659515381, mean_rew: -3.2354434936957768, variance: 3.1688425540924072, lamda: 2.2116401195526123
Running avgs for agent 3: q_loss: 2.924623489379883, p_loss: -1.4496574401855469, mean_rew: -2.508824149152053, variance: 2.991206645965576, lamda: 1.9526418447494507

steps: 1774975, episodes: 71000, mean episode reward: -167.42053964611446, agent episode reward: [1.93, 1.93, -103.77310124873745, -67.507438397377], time: 67.224
steps: 1774975, episodes: 71000, mean episode variance: 2.378221411585808, agent episode variance: [0.41671976017951967, 0.41738419568538665, 0.7901135258674622, 0.7540039298534393], time: 67.224
Running avgs for agent 0: q_loss: 3.105009078979492, p_loss: 0.8906061053276062, mean_rew: 0.073671875, variance: 1.6668790578842163, lamda: 2.736637592315674
Running avgs for agent 1: q_loss: 2.6843373775482178, p_loss: 0.8914775848388672, mean_rew: 0.0731640625, variance: 1.6695367097854614, lamda: 2.7360074520111084
Running avgs for agent 2: q_loss: 5.355605602264404, p_loss: -1.9805185794830322, mean_rew: -3.2673811912460695, variance: 3.160454034805298, lamda: 2.2225654125213623
Running avgs for agent 3: q_loss: 3.98898983001709, p_loss: -1.481695294380188, mean_rew: -2.5446988354808044, variance: 3.0160155296325684, lamda: 1.9578641653060913

steps: 1799975, episodes: 72000, mean episode reward: -165.35272577308092, agent episode reward: [1.64, 1.64, -102.27069592964345, -66.36202984343748], time: 66.916
steps: 1799975, episodes: 72000, mean episode variance: 2.383130159020424, agent episode variance: [0.41970829927921294, 0.41600336837768553, 0.7939504702091217, 0.7534680211544037], time: 66.917
Running avgs for agent 0: q_loss: 2.8775954246520996, p_loss: 0.8620693683624268, mean_rew: 0.069375, variance: 1.678833246231079, lamda: 2.7616682052612305
Running avgs for agent 1: q_loss: 2.9431793689727783, p_loss: 0.9329484701156616, mean_rew: 0.0746875, variance: 1.664013385772705, lamda: 2.761000394821167
Running avgs for agent 2: q_loss: 5.310798645019531, p_loss: -1.9752100706100464, mean_rew: -3.30751657602138, variance: 3.1758017539978027, lamda: 2.226116180419922
Running avgs for agent 3: q_loss: 3.690718173980713, p_loss: -1.4781551361083984, mean_rew: -2.547567867103601, variance: 3.0138721466064453, lamda: 1.958858847618103

steps: 1824975, episodes: 73000, mean episode reward: -170.20824700092479, agent episode reward: [1.72, 1.72, -95.09584370793705, -78.55240329298772], time: 67.23
steps: 1824975, episodes: 73000, mean episode variance: 2.3946853606700897, agent episode variance: [0.41722835528850555, 0.4175780516862869, 0.8013640279769898, 0.7585149257183075], time: 67.23
Running avgs for agent 0: q_loss: 2.8440842628479004, p_loss: 0.8756107091903687, mean_rew: 0.073203125, variance: 1.66891348361969, lamda: 2.786609411239624
Running avgs for agent 1: q_loss: 2.861344575881958, p_loss: 0.9202207326889038, mean_rew: 0.075, variance: 1.6703122854232788, lamda: 2.7860684394836426
Running avgs for agent 2: q_loss: 4.382547855377197, p_loss: -1.978658676147461, mean_rew: -3.350904876697916, variance: 3.205456018447876, lamda: 2.2373886108398438
Running avgs for agent 3: q_loss: 4.101667404174805, p_loss: -1.4919203519821167, mean_rew: -2.5802315667567886, variance: 3.034059524536133, lamda: 1.9602197408676147

steps: 1849975, episodes: 74000, mean episode reward: -182.23724093483662, agent episode reward: [1.6, 1.6, -98.80140461054779, -86.6358363242888], time: 66.886
steps: 1849975, episodes: 74000, mean episode variance: 2.395396273970604, agent episode variance: [0.4156124738454819, 0.4173028359413147, 0.8041998264789582, 0.7582811377048493], time: 66.886
Running avgs for agent 0: q_loss: 3.0808515548706055, p_loss: 0.8973469138145447, mean_rew: 0.0738671875, variance: 1.6624499559402466, lamda: 2.811500072479248
Running avgs for agent 1: q_loss: 2.856505870819092, p_loss: 0.9282463192939758, mean_rew: 0.073125, variance: 1.6692113876342773, lamda: 2.811000108718872/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

Running avgs for agent 2: q_loss: 5.921574115753174, p_loss: -2.00398588180542, mean_rew: -3.4091822148420374, variance: 3.216799259185791, lamda: 2.242234706878662
Running avgs for agent 3: q_loss: 4.409987449645996, p_loss: -1.4640417098999023, mean_rew: -2.5822376239281244, variance: 3.0331242084503174, lamda: 1.9640511274337769

steps: 1874975, episodes: 75000, mean episode reward: -194.10280926869308, agent episode reward: [1.26, 1.26, -102.82153869625932, -93.80127057243376], time: 67.159
steps: 1874975, episodes: 75000, mean episode variance: 2.4001845746040344, agent episode variance: [0.41696050548553465, 0.4199155924320221, 0.807601256608963, 0.7557072200775147], time: 67.159
Running avgs for agent 0: q_loss: 3.0002853870391846, p_loss: 0.9369568824768066, mean_rew: 0.072421875, variance: 1.6678420305252075, lamda: 2.8364667892456055
Running avgs for agent 1: q_loss: 3.0882728099823, p_loss: 0.8919646143913269, mean_rew: 0.0708203125, variance: 1.6796623468399048, lamda: 2.8360517024993896
Running avgs for agent 2: q_loss: 5.301017761230469, p_loss: -2.005997896194458, mean_rew: -3.432785384259094, variance: 3.23040509223938, lamda: 2.243800163269043
Running avgs for agent 3: q_loss: 4.18800163269043, p_loss: -1.4157543182373047, mean_rew: -2.6103054160971397, variance: 3.022829055786133, lamda: 1.9656400680541992

steps: 1899975, episodes: 76000, mean episode reward: -201.66516743788156, agent episode reward: [1.54, 1.54, -109.6725890896024, -95.07257834827917], time: 67.205
steps: 1899975, episodes: 76000, mean episode variance: 2.4095992126464845, agent episode variance: [0.4183851019144058, 0.4180199736356735, 0.8132084031105041, 0.7599857339859009], time: 67.205
Running avgs for agent 0: q_loss: 2.908219337463379, p_loss: 0.9379745125770569, mean_rew: 0.0707421875, variance: 1.6735403537750244, lamda: 2.861509323120117
Running avgs for agent 1: q_loss: 2.8925132751464844, p_loss: 0.89512038230896, mean_rew: 0.07015625, variance: 1.6720798015594482, lamda: 2.861102342605591
Running avgs for agent 2: q_loss: 5.965301036834717, p_loss: -2.0368101596832275, mean_rew: -3.4971067917338963, variance: 3.252833604812622, lamda: 2.248617649078369
Running avgs for agent 3: q_loss: 4.38541316986084, p_loss: -1.3838179111480713, mean_rew: -2.627893196645422, variance: 3.039942979812622, lamda: 1.9684538841247559

steps: 1924975, episodes: 77000, mean episode reward: -197.0243250382717, agent episode reward: [1.43, 1.43, -107.36195764823651, -92.5223673900352], time: 67.173
steps: 1924975, episodes: 77000, mean episode variance: 2.421559064865112, agent episode variance: [0.4180391401052475, 0.41805183470249174, 0.8228116481304169, 0.7626564419269561], time: 67.174
Running avgs for agent 0: q_loss: 2.842960834503174, p_loss: 0.9364195466041565, mean_rew: 0.070859375, variance: 1.672156572341919, lamda: 2.8863539695739746
Running avgs for agent 1: q_loss: 2.858064889907837, p_loss: 0.9180371165275574, mean_rew: 0.0697265625, variance: 1.672207236289978, lamda: 2.886012315750122
Running avgs for agent 2: q_loss: 5.68626070022583, p_loss: -2.0522103309631348, mean_rew: -3.5559582876223548, variance: 3.2912466526031494, lamda: 2.24941349029541
Running avgs for agent 3: q_loss: 4.526012897491455, p_loss: -1.3718453645706177, mean_rew: -2.6607068842219834, variance: 3.050625801086426, lamda: 1.9696944952011108

steps: 1949975, episodes: 78000, mean episode reward: -208.9737720380865, agent episode reward: [1.29, 1.29, -108.38321011423221, -103.1705619238543], time: 67.198
steps: 1949975, episodes: 78000, mean episode variance: 2.4280622979402544, agent episode variance: [0.418778529882431, 0.4162943814992905, 0.8272600390911102, 0.7657293474674225], time: 67.199
Running avgs for agent 0: q_loss: 3.1233325004577637, p_loss: 0.9294609427452087, mean_rew: 0.0722265625, variance: 1.6751140356063843, lamda: 2.9113504886627197
Running avgs for agent 1: q_loss: 2.8170175552368164, p_loss: 0.9370222091674805, mean_rew: 0.0694140625, variance: 1.6651774644851685, lamda: 2.9108829498291016
Running avgs for agent 2: q_loss: 5.502353668212891, p_loss: -2.0803909301757812, mean_rew: -3.6069134875432955, variance: 3.3090403079986572, lamda: 2.2585766315460205
Running avgs for agent 3: q_loss: 4.6261115074157715, p_loss: -1.3584709167480469, mean_rew: -2.674470853655338, variance: 3.062917470932007, lamda: 1.9723820686340332

steps: 1974975, episodes: 79000, mean episode reward: -209.88204514071177, agent episode reward: [1.23, 1.23, -110.36284426928034, -101.97920087143142], time: 67.108
steps: 1974975, episodes: 79000, mean episode variance: 2.4235104197263717, agent episode variance: [0.4167533119916916, 0.41813874197006223, 0.8251392996311188, 0.7634790661334991], time: 67.108
Running avgs for agent 0: q_loss: 3.0671961307525635, p_loss: 0.9560160040855408, mean_rew: 0.0715625, variance: 1.667013168334961, lamda: 2.936332941055298
Running avgs for agent 1: q_loss: 2.8544647693634033, p_loss: 0.9559035897254944, mean_rew: 0.0680859375, variance: 1.6725549697875977, lamda: 2.935847282409668
Running avgs for agent 2: q_loss: 7.687612533569336, p_loss: -2.1166653633117676, mean_rew: -3.6580503492246703, variance: 3.3005573749542236, lamda: 2.2640764713287354
Running avgs for agent 3: q_loss: 4.566636085510254, p_loss: -1.3359087705612183, mean_rew: -2.688529140686543, variance: 3.0539164543151855, lamda: 1.9745795726776123

steps: 1999975, episodes: 80000, mean episode reward: -210.72531483806745, agent episode reward: [1.49, 1.49, -108.54767716488114, -105.15763767318629], time: 67.047
steps: 1999975, episodes: 80000, mean episode variance: 2.4353212703466416, agent episode variance: [0.4166468695402145, 0.4202515273094177, 0.8317678861618042, 0.7666549873352051], time: 67.048
Running avgs for agent 0: q_loss: 3.0129899978637695, p_loss: 0.9817860126495361, mean_rew: 0.069765625, variance: 1.6665873527526855, lamda: 2.9611802101135254
Running avgs for agent 1: q_loss: 2.7419159412384033, p_loss: 0.9291204810142517, mean_rew: 0.0687890625, variance: 1.6810060739517212, lamda: 2.9608511924743652
Running avgs for agent 2: q_loss: 4.943111419677734, p_loss: -2.127246618270874, mean_rew: -3.7237398030094235, variance: 3.32707142829895, lamda: 2.2669880390167236
Running avgs for agent 3: q_loss: 4.5371503829956055, p_loss: -1.348021388053894, mean_rew: -2.70760355071449, variance: 3.066619873046875, lamda: 1.9752763509750366

...Finished total of 80001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -217.1793384028392, agent episode reward: [2.18, 2.18, -106.70978093361394, -114.82955746922528], time: 46.613
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 46.614
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -214.9921283784304, agent episode reward: [2.17, 2.17, -108.08902546599373, -111.24310291243668], time: 57.103
steps: 49975, episodes: 2000, mean episode variance: 2.8992244056463243, agent episode variance: [0.4106402903795242, 0.41106793689727783, 0.8527973072528839, 1.2247188711166381], time: 57.104
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.08256755891393443, variance: 1.6829519271850586, lamda: 2.973677396774292
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.09057216956967214, variance: 1.6847046613693237, lamda: 2.9734156131744385
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.261037586099557, variance: 3.4950709342956543, lamda: 2.2695651054382324
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -4.56587186406722, variance: 5.019339561462402, lamda: 1.9757121801376343

...Finished total of 2001 episodes with the fixed policy.
