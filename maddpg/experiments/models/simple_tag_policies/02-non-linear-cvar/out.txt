WARNING: Logging before flag parsing goes to stderr.
W0826 23:43:51.359112 4613367232 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0826 23:43:51.359334 4613367232 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-26 23:43:51.359696: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0826 23:43:51.363463 4613367232 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0826 23:43:51.365496 4613367232 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0826 23:43:51.365612 4613367232 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0826 23:43:51.365685 4613367232 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0826 23:43:51.813207 4613367232 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0826 23:43:51.988442 4613367232 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0826 23:43:51.995428 4613367232 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0826 23:43:52.474949 4613367232 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation True
adversary agent:  28.72614097595215
adversary agent:  28.72614097595215
good agent:  -0.0656009316444397
good agent:  -0.0656009316444397
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -12.476390476955448, agent episode reward: [3.41, 3.41, -10.258960270313105, -9.037430206642345], time: 41.407
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 41.408
steps: 24975, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0, 0.0], time: 41.408
Running avgs for agent 0: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -17.433591091803816, agent episode reward: [5.02, 5.02, -11.140177367917016, -16.333413723886807], time: 79.551
steps: 49975, episodes: 2000, mean episode variance: 0.6042413669675588, agent episode variance: [0.0003559833467006683, 0.0, 0.21996025803685187, 0.3839251255840063], time: 79.551
steps: 49975, episodes: 2000, mean episode cvar: 12.651781223541125, agent episode cvar: [6.347762486219406, 6.233790204048157, 0.03311710968799889, 0.0371114235855639], time: 79.552
Running avgs for agent 0: q_loss: 40.07218551635742, u_loss: 268.7655944824219, p_loss: -5.025396347045898, mean_rew: 0.14836545850409835, variance: 0.0014589481422158537, cvar: 26.01542091369629, v: 2.613095998764038, mean_q: 4.950725555419922, std_q: 1.036992073059082, lamda: 1.0050873756408691
Running avgs for agent 1: q_loss: 19.296920776367188, u_loss: 260.16265869140625, p_loss: -4.971190452575684, mean_rew: 0.14852555071721313, variance: 0.0, cvar: 25.548320770263672, v: 2.613095998764038, mean_q: 4.905872821807861, std_q: 0.9979727268218994, lamda: 1.0037777423858643
Running avgs for agent 2: q_loss: 0.8216874003410339, u_loss: 120.15978240966797, p_loss: 0.9328115582466125, mean_rew: -0.44442324401914707, variance: 0.9014764673641471, cvar: 0.13572585582733154, v: 0.0891936719417572, mean_q: -1.0152530670166016, std_q: 1.326918125152588, lamda: 1.0001481771469116
Running avgs for agent 3: q_loss: 1.0619276762008667, u_loss: 143.1204833984375, p_loss: 0.9489768743515015, mean_rew: -0.5911587855788129, variance: 1.5734636294426487, cvar: 0.1520959883928299, v: 0.10977209359407425, mean_q: -1.0236520767211914, std_q: 1.9435936212539673, lamda: 0.9996566772460938

steps: 74975, episodes: 3000, mean episode reward: 5.7021351560808435, agent episode reward: [6.12, 6.12, -3.3821660897758488, -3.1556987541433084], time: 74.294
steps: 74975, episodes: 3000, mean episode variance: 1.735031215235591, agent episode variance: [0.0, 0.0, 0.6454635767191649, 1.0895676385164261], time: 74.295
steps: 74975, episodes: 3000, mean episode cvar: 13.36750554612279, agent episode cvar: [6.7189104385375975, 6.6808889331817625, -0.01838121649622917, -0.013912609100341797], time: 74.295
Running avgs for agent 0: q_loss: 13.488556861877441, u_loss: 815.333740234375, p_loss: -8.042271614074707, mean_rew: 0.1862890625, variance: 0.0, cvar: 26.875642776489258, v: 5.9084248542785645, mean_q: 7.97123384475708, std_q: 1.3736178874969482, lamda: 1.017361044883728
Running avgs for agent 1: q_loss: 17.634685516357422, u_loss: 791.986083984375, p_loss: -7.999729633331299, mean_rew: 0.182421875, variance: 0.0, cvar: 26.723556518554688, v: 5.9084248542785645, mean_q: 7.940993785858154, std_q: 1.5014162063598633, lamda: 1.0137906074523926
Running avgs for agent 2: q_loss: 0.8929650783538818, u_loss: 205.1055145263672, p_loss: 0.9565345048904419, mean_rew: -0.37217221399183975, variance: 2.5818543068766595, cvar: -0.07352486252784729, v: -0.14474962651729584, mean_q: -1.1226184368133545, std_q: 2.5917515754699707, lamda: 1.0012441873550415
Running avgs for agent 3: q_loss: 1.0715241432189941, u_loss: 230.6544189453125, p_loss: 1.0469346046447754, mean_rew: -0.4403318412401751, variance: 4.358270645141602, cvar: -0.055650439113378525, v: -0.1159149557352066, mean_q: -1.2238855361938477, std_q: 3.3536345958709717, lamda: 1.0001094341278076

steps: 99975, episodes: 4000, mean episode reward: 5.518875814515157, agent episode reward: [6.07, 6.07, -3.0168485798709814, -3.6042756056138625], time: 73.583
steps: 99975, episodes: 4000, mean episode variance: 2.9770925827026367, agent episode variance: [0.0, 0.0, 1.2587062106132507, 1.718386372089386], time: 73.584
steps: 99975, episodes: 4000, mean episode cvar: 13.773023719571531, agent episode cvar: [6.920483160018921, 6.906598289489746, -0.029347352296113967, -0.024710377641022206], time: 73.584
Running avgs for agent 0: q_loss: 17.425785064697266, u_loss: 1445.29931640625, p_loss: -10.780426025390625, mean_rew: 0.2051953125, variance: 0.0, cvar: 27.68193244934082, v: 9.082537651062012, mean_q: 10.705724716186523, std_q: 2.110945463180542, lamda: 1.033743143081665
Running avgs for agent 1: q_loss: 21.072818756103516, u_loss: 1371.626953125, p_loss: -10.524182319641113, mean_rew: 0.2003515625, variance: 0.0, cvar: 27.626394271850586, v: 9.082537651062012, mean_q: 10.463370323181152, std_q: 2.64292573928833, lamda: 1.0316601991653442
Running avgs for agent 2: q_loss: 0.9781683087348938, u_loss: 319.4716796875, p_loss: 0.9764881134033203, mean_rew: -0.2985163952897493, variance: 5.034825325012207, cvar: -0.11738941073417664, v: -0.18574658036231995, mean_q: -1.1813613176345825, std_q: 3.258582592010498, lamda: 1.004408597946167
Running avgs for agent 3: q_loss: 1.0988819599151611, u_loss: 381.6283874511719, p_loss: 1.0744874477386475, mean_rew: -0.34991440324861184, variance: 6.873546123504639, cvar: -0.09884150326251984, v: -0.13874493539333344, mean_q: -1.2683122158050537, std_q: 4.126064300537109, lamda: 1.0010043382644653

steps: 124975, episodes: 5000, mean episode reward: 3.709498374132216, agent episode reward: [4.21, 4.21, -2.3062821578704695, -2.4042194679973146], time: 71.91
steps: 124975, episodes: 5000, mean episode variance: 3.838773794174194, agent episode variance: [0.0, 0.0, 1.6870012805461883, 2.151772513628006], time: 71.91
steps: 124975, episodes: 5000, mean episode cvar: 13.878767769135536, agent episode cvar: [6.962212530136108, 6.997867021560669, -0.056283064365386964, -0.02502871819585562], time: 71.911
Running avgs for agent 0: q_loss: 14.325477600097656, u_loss: 1806.07177734375, p_loss: -13.086970329284668, mean_rew: 0.196953125, variance: 0.0, cvar: 27.848852157592773, v: 12.120458602905273, mean_q: 13.002965927124023, std_q: 2.8501338958740234, lamda: 1.0518922805786133
Running avgs for agent 1: q_loss: 16.342830657958984, u_loss: 1874.146484375, p_loss: -12.774985313415527, mean_rew: 0.2078515625, variance: 0.0, cvar: 27.99146842956543, v: 12.120458602905273, mean_q: 12.715567588806152, std_q: 3.277754306793213, lamda: 1.0519357919692993
Running avgs for agent 2: q_loss: 1.1090680360794067, u_loss: 492.3761291503906, p_loss: 1.104933261871338, mean_rew: -0.2576542413612069, variance: 6.748005390167236, cvar: -0.22513225674629211, v: -0.3380008339881897, mean_q: -1.3015207052230835, std_q: 3.541294813156128, lamda: 1.0162259340286255
Running avgs for agent 3: q_loss: 1.1545910835266113, u_loss: 570.8414916992188, p_loss: 0.9734741449356079, mean_rew: -0.29887672519551894, variance: 8.60708999633789, cvar: -0.10011487454175949, v: -0.13916070759296417, mean_q: -1.1604567766189575, std_q: 4.327949047088623, lamda: 1.0023822784423828

steps: 149975, episodes: 6000, mean episode reward: 3.4022195705893563, agent episode reward: [4.02, 4.02, -2.425430107878401, -2.212350321532242], time: 71.854
steps: 149975, episodes: 6000, mean episode variance: 4.357499117612838, agent episode variance: [0.0, 0.0, 2.0747251682281496, 2.2827739493846892], time: 71.855
steps: 149975, episodes: 6000, mean episode cvar: 13.878332836732268, agent episode cvar: [6.982093059539795, 6.991284521102905, -0.07222479301691055, -0.022819950893521308], time: 71.855
Running avgs for agent 0: q_loss: 10.337738037109375, u_loss: 2201.694091796875, p_loss: -14.931678771972656, mean_rew: 0.196640625, variance: 0.0, cvar: 27.92837142944336, v: 15.047926902770996, mean_q: 14.845433235168457, std_q: 3.520561695098877, lamda: 1.0714863538742065
Running avgs for agent 1: q_loss: 11.36573600769043, u_loss: 2263.430908203125, p_loss: -14.520539283752441, mean_rew: 0.193515625, variance: 0.0, cvar: 27.965139389038086, v: 15.047926902770996, mean_q: 14.470738410949707, std_q: 3.9187846183776855, lamda: 1.071974754333496
Running avgs for agent 2: q_loss: 1.020697832107544, u_loss: 548.5618286132812, p_loss: 1.232729434967041, mean_rew: -0.22518970326918497, variance: 8.298900604248047, cvar: -0.28889918327331543, v: -0.3914167582988739, mean_q: -1.4095537662506104, std_q: 3.5611696243286133, lamda: 1.0329400300979614
Running avgs for agent 3: q_loss: 1.1323528289794922, u_loss: 693.8453979492188, p_loss: 0.8834095001220703, mean_rew: -0.2637491106447723, variance: 9.131095886230469, cvar: -0.09127979725599289, v: -0.12212438881397247, mean_q: -1.0638688802719116, std_q: 4.448705196380615, lamda: 1.0035747289657593

steps: 174975, episodes: 7000, mean episode reward: 2.905480253980304, agent episode reward: [4.04, 4.04, -2.6252287895126605, -2.549290956507036], time: 72.145
steps: 174975, episodes: 7000, mean episode variance: 4.688215297460556, agent episode variance: [0.0, 0.0, 2.308859656572342, 2.379355640888214], time: 72.145
steps: 174975, episodes: 7000, mean episode cvar: 13.967938913583755, agent episode cvar: [7.004246988296509, 7.026238332748413, -0.04088385979831219, -0.021662547662854196], time: 72.146
Running avgs for agent 0: q_loss: 10.36257553100586, u_loss: 2450.62451171875, p_loss: -16.397903442382812, mean_rew: 0.18796875, variance: 0.0, cvar: 28.01698875427246, v: 17.886390686035156, mean_q: 16.30891990661621, std_q: 3.9861035346984863, lamda: 1.0909829139709473
Running avgs for agent 1: q_loss: 10.38510799407959, u_loss: 2663.385986328125, p_loss: -15.966056823730469, mean_rew: 0.1948046875, variance: 0.0, cvar: 28.10495376586914, v: 17.886137008666992, mean_q: 15.920452117919922, std_q: 4.396995544433594, lamda: 1.0912631750106812
Running avgs for agent 2: q_loss: 0.9983771443367004, u_loss: 494.2310791015625, p_loss: 1.0278972387313843, mean_rew: -0.20697589070942968, variance: 9.235438346862793, cvar: -0.16353543102741241, v: -0.1772853434085846, mean_q: -1.1872698068618774, std_q: 3.492269515991211, lamda: 1.03514564037323
Running avgs for agent 3: q_loss: 1.094416618347168, u_loss: 654.6842651367188, p_loss: 0.79207843542099, mean_rew: -0.2318177081846713, variance: 9.517422676086426, cvar: -0.08665019273757935, v: -0.11785420030355453, mean_q: -0.9518821835517883, std_q: 4.404757499694824, lamda: 1.0049549341201782

steps: 199975, episodes: 8000, mean episode reward: 3.1370988803652997, agent episode reward: [4.04, 4.04, -2.840832173112999, -2.102068946521701], time: 72.409
steps: 199975, episodes: 8000, mean episode variance: 4.477555737137794, agent episode variance: [0.0, 0.0, 2.1465325717926027, 2.331023165345192], time: 72.41
steps: 199975, episodes: 8000, mean episode cvar: 14.103543837528676, agent episode cvar: [7.041520565032959, 7.114240076065063, -0.03230337025970221, -0.01991343330964446], time: 72.41
Running avgs for agent 0: q_loss: 7.341493129730225, u_loss: 2765.839599609375, p_loss: -17.671812057495117, mean_rew: 0.1866796875, variance: 0.0, cvar: 28.16608238220215, v: 20.649250030517578, mean_q: 17.61354637145996, std_q: 4.19919490814209, lamda: 1.1078790426254272
Running avgs for agent 1: q_loss: 8.372215270996094, u_loss: 2773.821044921875, p_loss: -16.727218627929688, mean_rew: 0.185546875, variance: 0.0, cvar: 28.456960678100586, v: 20.573177337646484, mean_q: 16.68437957763672, std_q: 4.855097770690918, lamda: 1.1128418445587158
Running avgs for agent 2: q_loss: 1.0179849863052368, u_loss: 603.719970703125, p_loss: 0.9011309742927551, mean_rew: -0.1972839380458663, variance: 8.58613109588623, cvar: -0.12921346724033356, v: -0.1394398808479309, mean_q: -1.0435118675231934, std_q: 3.4116621017456055, lamda: 1.0357838869094849
Running avgs for agent 3: q_loss: 1.031438946723938, u_loss: 736.3760375976562, p_loss: 0.748647153377533, mean_rew: -0.2133335497001276, variance: 9.32409381866455, cvar: -0.07965373247861862, v: -0.10774891823530197, mean_q: -0.8964077830314636, std_q: 4.3841023445129395, lamda: 1.0068514347076416

steps: 224975, episodes: 9000, mean episode reward: 2.243541812793759, agent episode reward: [4.02, 4.02, -2.5086525629441723, -3.2878056242620692], time: 72.963
steps: 224975, episodes: 9000, mean episode variance: 4.43888512635231, agent episode variance: [0.0, 0.0, 2.1935714960098265, 2.2453136303424834], time: 72.963
steps: 224975, episodes: 9000, mean episode cvar: 14.127079134680331, agent episode cvar: [7.039470115661621, 7.1387952003479, -0.029394860804080963, -0.021791320525109768], time: 72.964
Running avgs for agent 0: q_loss: 5.175872325897217, u_loss: 3178.24609375, p_loss: -18.55514144897461, mean_rew: 0.1879296875, variance: 0.0, cvar: 28.157880783081055, v: 23.132869720458984, mean_q: 18.492238998413086, std_q: 4.293307781219482, lamda: 1.1244196891784668
Running avgs for agent 1: q_loss: 6.436178684234619, u_loss: 2903.7841796875, p_loss: -16.867843627929688, mean_rew: 0.182421875, variance: 0.0, cvar: 28.5551815032959, v: 22.352996826171875, mean_q: 16.823923110961914, std_q: 5.05612850189209, lamda: 1.1339497566223145
Running avgs for agent 2: q_loss: 1.07093346118927, u_loss: 590.1757202148438, p_loss: 0.8321651816368103, mean_rew: -0.18881528097474864, variance: 8.774286270141602, cvar: -0.11757944524288177, v: -0.13374124467372894, mean_q: -0.961694598197937, std_q: 3.4978973865509033, lamda: 1.036636233329773
Running avgs for agent 3: q_loss: 1.0050616264343262, u_loss: 829.7293090820312, p_loss: 0.7321972846984863, mean_rew: -0.2041053294662671, variance: 8.981254521369934, cvar: -0.08716528117656708, v: -0.12826213240623474, mean_q: -0.8780027627944946, std_q: 4.403936862945557, lamda: 1.0097733736038208

steps: 249975, episodes: 10000, mean episode reward: 1.8053491652332643, agent episode reward: [2.72, 2.72, -2.093972044238363, -1.5406787905283723], time: 72.589
steps: 249975, episodes: 10000, mean episode variance: 4.660892954826355, agent episode variance: [0.0, 0.0, 2.2531221573352815, 2.407770797491074], time: 72.589
steps: 249975, episodes: 10000, mean episode cvar: 14.224891357704998, agent episode cvar: [7.089491094589233, 7.185977737426758, -0.028027977734804154, -0.022549496576189994], time: 72.59
Running avgs for agent 0: q_loss: 4.865285873413086, u_loss: 3298.71875, p_loss: -18.966960906982422, mean_rew: 0.178515625, variance: 0.0, cvar: 28.35796546936035, v: 24.60736656188965, mean_q: 18.913738250732422, std_q: 4.332393646240234, lamda: 1.1419563293457031
Running avgs for agent 1: q_loss: 5.531430721282959, u_loss: 2830.56787109375, p_loss: -16.661846160888672, mean_rew: 0.1775390625, variance: 0.0, cvar: 28.74390983581543, v: 22.922718048095703, mean_q: 16.614160537719727, std_q: 5.196806907653809, lamda: 1.1553239822387695
Running avgs for agent 2: q_loss: 0.9992177486419678, u_loss: 754.670166015625, p_loss: 0.7186905741691589, mean_rew: -0.1752930364814145, variance: 9.012488629341126, cvar: -0.11211191117763519, v: -0.12908698618412018, mean_q: -0.8470100164413452, std_q: 3.4677929878234863, lamda: 1.0377285480499268
Running avgs for agent 3: q_loss: 0.9947497844696045, u_loss: 904.2472534179688, p_loss: 0.6910783648490906, mean_rew: -0.1916774619099571, variance: 9.631084442138672, cvar: -0.09019798040390015, v: -0.1264258772134781, mean_q: -0.8319024443626404, std_q: 4.278209209442139, lamda: 1.0136433839797974

steps: 274975, episodes: 11000, mean episode reward: 0.9776098195607514, agent episode reward: [2.27, 2.27, -2.153959658636286, -1.4084305218029627], time: 72.997
steps: 274975, episodes: 11000, mean episode variance: 4.596003338575363, agent episode variance: [0.0, 0.0, 2.317486016869545, 2.2785173217058183], time: 72.997
steps: 274975, episodes: 11000, mean episode cvar: 14.23082538792491, agent episode cvar: [7.08681960105896, 7.191604688644409, -0.024799814246594908, -0.022799087531864642], time: 72.998
Running avgs for agent 0: q_loss: 3.5304081439971924, u_loss: 3259.53173828125, p_loss: -19.365264892578125, mean_rew: 0.1692578125, variance: 0.0, cvar: 28.34727668762207, v: 25.129817962646484, mean_q: 19.304672241210938, std_q: 4.199132442474365, lamda: 1.159618616104126
Running avgs for agent 1: q_loss: 6.664738178253174, u_loss: 2609.33740234375, p_loss: -16.4423828125, mean_rew: 0.1691796875, variance: 0.0, cvar: 28.766420364379883, v: 23.00345230102539, mean_q: 16.382049560546875, std_q: 5.237772464752197, lamda: 1.1767719984054565
Running avgs for agent 2: q_loss: 0.95607590675354, u_loss: 752.6104736328125, p_loss: 0.616018533706665, mean_rew: -0.1684274755754149, variance: 9.269944190979004, cvar: -0.09919926524162292, v: -0.1274929940700531, mean_q: -0.7304127216339111, std_q: 3.095386266708374, lamda: 1.0392247438430786
Running avgs for agent 3: q_loss: 0.977961540222168, u_loss: 1060.073486328125, p_loss: 0.6649076342582703, mean_rew: -0.18100509636298506, variance: 9.114068984985352, cvar: -0.09119635075330734, v: -0.11648846417665482, mean_q: -0.7978724837303162, std_q: 4.205406665802002, lamda: 1.0169100761413574

steps: 299975, episodes: 12000, mean episode reward: -0.15986363244577906, agent episode reward: [1.74, 1.74, -2.3329712639724804, -1.3068923684732985], time: 73.276
steps: 299975, episodes: 12000, mean episode variance: 4.0664878544807435, agent episode variance: [0.0, 0.0, 2.1381019958257674, 1.9283858586549758], time: 73.276
steps: 299975, episodes: 12000, mean episode cvar: 14.2416529484801, agent episode cvar: [7.094256254196167, 7.194635204315185, -0.025227143347263337, -0.02201136668398976], time: 73.277
Running avgs for agent 0: q_loss: 3.5480170249938965, u_loss: 2983.680419921875, p_loss: -19.616275787353516, mean_rew: 0.162578125, variance: 0.0, cvar: 28.377023696899414, v: 25.292213439941406, mean_q: 19.557729721069336, std_q: 4.08736515045166, lamda: 1.178051233291626
Running avgs for agent 1: q_loss: 4.314818859100342, u_loss: 2441.26708984375, p_loss: -16.312946319580078, mean_rew: 0.165078125, variance: 0.0, cvar: 28.778541564941406, v: 23.165489196777344, mean_q: 16.248411178588867, std_q: 5.2783708572387695, lamda: 1.1985112428665161
Running avgs for agent 2: q_loss: 0.9534862041473389, u_loss: 865.4483032226562, p_loss: 0.5491997003555298, mean_rew: -0.16091806777803935, variance: 8.552408218383789, cvar: -0.10090856999158859, v: -0.12744779884815216, mean_q: -0.6600216627120972, std_q: 2.9027504920959473, lamda: 1.0414245128631592
Running avgs for agent 3: q_loss: 0.8939419984817505, u_loss: 1073.5467529296875, p_loss: 0.6056387424468994, mean_rew: -0.16218633310232503, variance: 7.713543434619903, cvar: -0.08804547041654587, v: -0.116355299949646, mean_q: -0.7422851324081421, std_q: 4.0582780838012695, lamda: 1.0229989290237427

steps: 324975, episodes: 13000, mean episode reward: 0.5985054498969894, agent episode reward: [2.07, 2.07, -2.1002138634291896, -1.4412806866738208], time: 72.927
steps: 324975, episodes: 13000, mean episode variance: 3.4182442277371883, agent episode variance: [0.0, 0.0, 1.7527947319447994, 1.6654494957923889], time: 72.928
steps: 324975, episodes: 13000, mean episode cvar: 14.2323440220505, agent episode cvar: [7.091676559448242, 7.185493200302124, -0.023182771060615778, -0.021642966639250517], time: 72.928
Running avgs for agent 0: q_loss: 3.472167730331421, u_loss: 2853.59326171875, p_loss: -19.857803344726562, mean_rew: 0.157578125, variance: 0.0, cvar: 28.36670684814453, v: 25.435638427734375, mean_q: 19.7989444732666, std_q: 3.9531753063201904, lamda: 1.1973916292190552
Running avgs for agent 1: q_loss: 4.315816402435303, u_loss: 2230.818359375, p_loss: -16.284912109375, mean_rew: 0.1534375, variance: 0.0, cvar: 28.741971969604492, v: 23.46746063232422, mean_q: 16.226369857788086, std_q: 5.288243770599365, lamda: 1.219085693359375
Running avgs for agent 2: q_loss: 0.8740956783294678, u_loss: 702.5147705078125, p_loss: 0.49237117171287537, mean_rew: -0.15719129489698486, variance: 7.011178927779198, cvar: -0.09273108094930649, v: -0.1136162132024765, mean_q: -0.5949582457542419, std_q: 2.7422327995300293, lamda: 1.042962908744812
Running avgs for agent 3: q_loss: 0.8519756197929382, u_loss: 906.068359375, p_loss: 0.5215588212013245, mean_rew: -0.1586587128167234, variance: 6.6617979831695555, cvar: -0.08657187223434448, v: -0.11589793860912323, mean_q: -0.65149986743927, std_q: 3.8136191368103027, lamda: 1.0277541875839233

steps: 349975, episodes: 14000, mean episode reward: -0.30974471799107256, agent episode reward: [1.59, 1.59, -2.607006432480817, -0.8827382855102553], time: 73.478
steps: 349975, episodes: 14000, mean episode variance: 3.2387858756780625, agent episode variance: [0.0, 0.0, 1.5824887144565583, 1.6562971612215043], time: 73.478
steps: 349975, episodes: 14000, mean episode cvar: 14.223622189074755, agent episode cvar: [7.103386962890625, 7.166027936935425, -0.02291405747830868, -0.022878653272986414], time: 73.478
Running avgs for agent 0: q_loss: 3.470507860183716, u_loss: 2737.083740234375, p_loss: -19.97389793395996, mean_rew: 0.1455859375, variance: 0.0, cvar: 28.41354751586914, v: 25.483335494995117, mean_q: 19.925935745239258, std_q: 3.8874502182006836, lamda: 1.2166682481765747
Running avgs for agent 1: q_loss: 5.1050567626953125, u_loss: 2126.7939453125, p_loss: -16.413597106933594, mean_rew: 0.148984375, variance: 0.0, cvar: 28.664112091064453, v: 23.691097259521484, mean_q: 16.3507137298584, std_q: 5.22377347946167, lamda: 1.2389788627624512
Running avgs for agent 2: q_loss: 0.8326485753059387, u_loss: 673.7713623046875, p_loss: 0.47213688492774963, mean_rew: -0.1500625412154452, variance: 6.329954857826233, cvar: -0.09165623784065247, v: -0.11428980529308319, mean_q: -0.5684150457382202, std_q: 2.642942428588867, lamda: 1.0447241067886353
Running avgs for agent 3: q_loss: 0.75839763879776, u_loss: 870.7591552734375, p_loss: 0.46521472930908203, mean_rew: -0.15019817022847112, variance: 6.625188644886017, cvar: -0.09151460975408554, v: -0.1139504536986351, mean_q: -0.5802364349365234, std_q: 3.691937208175659, lamda: 1.032928466796875

steps: 374975, episodes: 15000, mean episode reward: 0.1271493610194981, agent episode reward: [1.53, 1.53, -1.7998930137645155, -1.1329576252159863], time: 73.409
steps: 374975, episodes: 15000, mean episode variance: 2.9292760370969773, agent episode variance: [0.0, 0.0, 1.4943964136838912, 1.4348796234130858], time: 73.409
steps: 374975, episodes: 15000, mean episode cvar: 14.14394552282989, agent episode cvar: [7.097134534835815, 7.09202329826355, -0.02334503198415041, -0.021867278285324573], time: 73.41
Running avgs for agent 0: q_loss: 4.084023952484131, u_loss: 2759.5576171875, p_loss: -20.09803581237793, mean_rew: 0.1466796875, variance: 0.0, cvar: 28.388538360595703, v: 25.46790313720703, mean_q: 20.056076049804688, std_q: 3.771878957748413, lamda: 1.2336262464523315
Running avgs for agent 1: q_loss: 4.0978474617004395, u_loss: 2061.25146484375, p_loss: -16.59015655517578, mean_rew: 0.1435546875, variance: 0.0, cvar: 28.368091583251953, v: 23.886011123657227, mean_q: 16.547515869140625, std_q: 5.077725410461426, lamda: 1.25918710231781
Running avgs for agent 2: q_loss: 0.8134188055992126, u_loss: 870.6213989257812, p_loss: 0.46941256523132324, mean_rew: -0.14625255358074882, variance: 5.977585654735565, cvar: -0.09338012337684631, v: -0.10917700082063675, mean_q: -0.5659676790237427, std_q: 2.803607702255249, lamda: 1.0471571683883667
Running avgs for agent 3: q_loss: 0.7012719511985779, u_loss: 886.2573852539062, p_loss: 0.4202108085155487, mean_rew: -0.1452600185466423, variance: 5.739518493652343, cvar: -0.08746911585330963, v: -0.10418897867202759, mean_q: -0.5317378044128418, std_q: 3.5270633697509766, lamda: 1.0367566347122192

steps: 399975, episodes: 16000, mean episode reward: -0.21732371657262223, agent episode reward: [1.65, 1.65, -2.1001818278960256, -1.4171418886765974], time: 72.976
steps: 399975, episodes: 16000, mean episode variance: 2.831778184771538, agent episode variance: [0.0, 0.0, 1.391511892914772, 1.4402662918567657], time: 72.977
steps: 399975, episodes: 16000, mean episode cvar: 14.153747786555439, agent episode cvar: [7.09454548072815, 7.100787775039673, -0.021449316177517174, -0.020136153034865857], time: 72.978
Running avgs for agent 0: q_loss: 3.599138021469116, u_loss: 2599.785400390625, p_loss: -20.25607681274414, mean_rew: 0.1380859375, variance: 0.0, cvar: 28.3781795501709, v: 25.451330184936523, mean_q: 20.214717864990234, std_q: 3.6745307445526123, lamda: 1.250648021697998
Running avgs for agent 1: q_loss: 3.6265869140625, u_loss: 2014.6634521484375, p_loss: -16.978660583496094, mean_rew: 0.134140625, variance: 0.0, cvar: 28.40315055847168, v: 24.017234802246094, mean_q: 16.940229415893555, std_q: 4.9489359855651855, lamda: 1.2784374952316284
Running avgs for agent 2: q_loss: 0.8334867358207703, u_loss: 761.494140625, p_loss: 0.4413047432899475, mean_rew: -0.14169665601005077, variance: 5.566047571659088, cvar: -0.0857972651720047, v: -0.10544224828481674, mean_q: -0.5383515357971191, std_q: 2.7121028900146484, lamda: 1.0483922958374023
Running avgs for agent 3: q_loss: 0.687616229057312, u_loss: 845.9391479492188, p_loss: 0.3963008522987366, mean_rew: -0.13491138872438496, variance: 5.761065167427063, cvar: -0.0805446058511734, v: -0.10340093076229095, mean_q: -0.500511109828949, std_q: 3.4445371627807617, lamda: 1.040634036064148

steps: 424975, episodes: 17000, mean episode reward: -0.6952879217313138, agent episode reward: [1.21, 1.21, -1.8477644681228171, -1.2675234536084967], time: 73.042
steps: 424975, episodes: 17000, mean episode variance: 2.6121802642345426, agent episode variance: [0.0, 0.0, 1.391839094042778, 1.2203411701917648], time: 73.043
steps: 424975, episodes: 17000, mean episode cvar: 14.133086381152273, agent episode cvar: [7.0620163116455075, 7.11332943534851, -0.021754651956260203, -0.020504713885486126], time: 73.043
Running avgs for agent 0: q_loss: 3.0346572399139404, u_loss: 2589.69140625, p_loss: -20.37177276611328, mean_rew: 0.1376953125, variance: 0.0, cvar: 28.248065948486328, v: 25.401033401489258, mean_q: 20.335147857666016, std_q: 3.5510029792785645, lamda: 1.2686975002288818
Running avgs for agent 1: q_loss: 3.876814603805542, u_loss: 2006.334228515625, p_loss: -17.25991439819336, mean_rew: 0.1339453125, variance: 0.0, cvar: 28.45331573486328, v: 24.255767822265625, mean_q: 17.217313766479492, std_q: 4.9244890213012695, lamda: 1.2969706058502197
Running avgs for agent 2: q_loss: 0.8013924360275269, u_loss: 738.0428466796875, p_loss: 0.39829179644584656, mean_rew: -0.13976244395443344, variance: 5.567356376171112, cvar: -0.08701861649751663, v: -0.09915550798177719, mean_q: -0.4986017942428589, std_q: 2.62143611907959, lamda: 1.0495158433914185
Running avgs for agent 3: q_loss: 0.6678495407104492, u_loss: 912.603271484375, p_loss: 0.3711014986038208, mean_rew: -0.13075964007580218, variance: 4.881364680767059, cvar: -0.08201885968446732, v: -0.10408514738082886, mean_q: -0.4666644036769867, std_q: 3.20119309425354, lamda: 1.045544147491455

steps: 449975, episodes: 18000, mean episode reward: -0.3851342545520812, agent episode reward: [1.38, 1.38, -1.689255778769937, -1.4558784757821441], time: 73.454
steps: 449975, episodes: 18000, mean episode variance: 2.285963719248772, agent episode variance: [0.0, 0.0, 1.1885710135102272, 1.0973927057385444], time: 73.455
steps: 449975, episodes: 18000, mean episode cvar: 14.096984974585473, agent episode cvar: [7.051052597045898, 7.088993305206299, -0.020964222252368925, -0.0220967054143548], time: 73.455
Running avgs for agent 0: q_loss: 2.791351079940796, u_loss: 2363.25341796875, p_loss: -20.512601852416992, mean_rew: 0.132421875, variance: 0.0, cvar: 28.20421028137207, v: 25.414121627807617, mean_q: 20.477096557617188, std_q: 3.4409191608428955, lamda: 1.2842433452606201
Running avgs for agent 1: q_loss: 3.3463234901428223, u_loss: 1864.7139892578125, p_loss: -17.653871536254883, mean_rew: 0.1290234375, variance: 0.0, cvar: 28.355974197387695, v: 24.388755798339844, mean_q: 17.611223220825195, std_q: 4.7141618728637695, lamda: 1.3141882419586182
Running avgs for agent 2: q_loss: 0.743265688419342, u_loss: 572.6572265625, p_loss: 0.35422223806381226, mean_rew: -0.13375221909704998, variance: 4.754284054040909, cvar: -0.08385688811540604, v: -0.10260654985904694, mean_q: -0.45437926054000854, std_q: 2.392835855484009, lamda: 1.0506408214569092
Running avgs for agent 3: q_loss: 0.6277952790260315, u_loss: 914.8726196289062, p_loss: 0.3609188199043274, mean_rew: -0.1270264197388387, variance: 4.389570822954178, cvar: -0.08838681876659393, v: -0.10280130058526993, mean_q: -0.45507532358169556, std_q: 3.2731165885925293, lamda: 1.0498183965682983

steps: 474975, episodes: 19000, mean episode reward: -0.6133694983082216, agent episode reward: [1.05, 1.05, -1.3933346726480658, -1.3200348256601555], time: 73.121
steps: 474975, episodes: 19000, mean episode variance: 2.130369707226753, agent episode variance: [0.0, 0.0, 1.1619676898717881, 0.9684020173549652], time: 73.122
steps: 474975, episodes: 19000, mean episode cvar: 14.132450212802738, agent episode cvar: [7.0615050315856935, 7.1110657081604005, -0.02017068064957857, -0.01994984629377723], time: 73.122
Running avgs for agent 0: q_loss: 3.1751773357391357, u_loss: 2315.014892578125, p_loss: -20.745811462402344, mean_rew: 0.127734375, variance: 0.0, cvar: 28.24601936340332, v: 25.468074798583984, mean_q: 20.709245681762695, std_q: 3.3458619117736816, lamda: 1.298602819442749
Running avgs for agent 1: q_loss: 3.5567939281463623, u_loss: 1903.31884765625, p_loss: -18.026735305786133, mean_rew: 0.1267578125, variance: 0.0, cvar: 28.444263458251953, v: 24.604082107543945, mean_q: 17.97405433654785, std_q: 4.59266996383667, lamda: 1.3316293954849243
Running avgs for agent 2: q_loss: 0.6394059062004089, u_loss: 672.853759765625, p_loss: 0.32647308707237244, mean_rew: -0.1318284162675169, variance: 4.6478707594871524, cvar: -0.08068272471427917, v: -0.09800973534584045, mean_q: -0.4219675064086914, std_q: 2.2655625343322754, lamda: 1.0517163276672363
Running avgs for agent 3: q_loss: 0.6314705610275269, u_loss: 736.6338500976562, p_loss: 0.3486837148666382, mean_rew: -0.12615183013497402, variance: 3.873608069419861, cvar: -0.07979938387870789, v: -0.09762873500585556, mean_q: -0.43723687529563904, std_q: 3.146660566329956, lamda: 1.0527143478393555

steps: 499975, episodes: 20000, mean episode reward: -0.3273552674309523, agent episode reward: [0.97, 0.97, -1.2922651179874967, -0.9750901494434555], time: 72.926
steps: 499975, episodes: 20000, mean episode variance: 2.106255485057831, agent episode variance: [0.0, 0.0, 1.0688836833834647, 1.037371801674366], time: 72.927
steps: 499975, episodes: 20000, mean episode cvar: 14.138134750980884, agent episode cvar: [7.08026523399353, 7.099305456161499, -0.02096252277493477, -0.02047341639921069], time: 72.927
Running avgs for agent 0: q_loss: 3.4320430755615234, u_loss: 2129.526611328125, p_loss: -20.791484832763672, mean_rew: 0.1172265625, variance: 0.0, cvar: 28.321060180664062, v: 25.60528564453125, mean_q: 20.7666072845459, std_q: 3.3281514644622803, lamda: 1.3146302700042725
Running avgs for agent 1: q_loss: 3.4489781856536865, u_loss: 1857.712158203125, p_loss: -18.286436080932617, mean_rew: 0.123828125, variance: 0.0, cvar: 28.3972225189209, v: 24.686248779296875, mean_q: 18.245038986206055, std_q: 4.449377059936523, lamda: 1.350638508796692
Running avgs for agent 2: q_loss: 0.6422708034515381, u_loss: 553.3524780273438, p_loss: 0.3198120594024658, mean_rew: -0.12877196056276002, variance: 4.275534733533859, cvar: -0.08385009318590164, v: -0.10360600054264069, mean_q: -0.4094454050064087, std_q: 2.2302796840667725, lamda: 1.053810477256775
Running avgs for agent 3: q_loss: 0.6087599396705627, u_loss: 809.864013671875, p_loss: 0.34220752120018005, mean_rew: -0.12143262532878508, variance: 4.149487206697464, cvar: -0.0818936675786972, v: -0.10215438902378082, mean_q: -0.4298427700996399, std_q: 3.17496395111084, lamda: 1.056524395942688

steps: 524975, episodes: 21000, mean episode reward: 0.04048361749479661, agent episode reward: [1.09, 1.09, -1.0086288139730235, -1.13088756853218], time: 72.924
steps: 524975, episodes: 21000, mean episode variance: 2.0167893941402437, agent episode variance: [0.0, 0.0, 0.9338483555912972, 1.0829410385489464], time: 72.924
steps: 524975, episodes: 21000, mean episode cvar: 14.074591263726354, agent episode cvar: [7.0438780612945555, 7.072869718551636, -0.02135341804474592, -0.020803098075091838], time: 72.925
Running avgs for agent 0: q_loss: 2.572706699371338, u_loss: 2055.813720703125, p_loss: -20.81329917907715, mean_rew: 0.1138671875, variance: 0.0, cvar: 28.175512313842773, v: 25.498394012451172, mean_q: 20.79030418395996, std_q: 3.246138095855713, lamda: 1.3294556140899658
Running avgs for agent 1: q_loss: 3.0654726028442383, u_loss: 1801.4046630859375, p_loss: -18.47897720336914, mean_rew: 0.1199609375, variance: 0.0, cvar: 28.291481018066406, v: 24.65602684020996, mean_q: 18.447174072265625, std_q: 4.307349681854248, lamda: 1.3693621158599854
Running avgs for agent 2: q_loss: 0.6336753368377686, u_loss: 506.7430114746094, p_loss: 0.29866546392440796, mean_rew: -0.12183067720589504, variance: 3.735393422365189, cvar: -0.08541366457939148, v: -0.10907735675573349, mean_q: -0.37782710790634155, std_q: 2.021958351135254, lamda: 1.0572624206542969
Running avgs for agent 3: q_loss: 0.5828627347946167, u_loss: 872.9754028320312, p_loss: 0.31924185156822205, mean_rew: -0.11687843161489903, variance: 4.331764154195786, cvar: -0.08321239054203033, v: -0.09950471669435501, mean_q: -0.39741063117980957, std_q: 2.94781756401062, lamda: 1.0602535009384155

steps: 549975, episodes: 22000, mean episode reward: -1.0032090831606373, agent episode reward: [0.49, 0.49, -0.7745482132627102, -1.2086608698979269], time: 73.161
steps: 549975, episodes: 22000, mean episode variance: 1.8112935003489257, agent episode variance: [0.0, 0.0, 0.7814417212456465, 1.029851779103279], time: 73.161
steps: 549975, episodes: 22000, mean episode cvar: 14.079530266623944, agent episode cvar: [7.03983309173584, 7.081485683441162, -0.020980415973812342, -0.02080809257924557], time: 73.162
Running avgs for agent 0: q_loss: 2.845855951309204, u_loss: 1923.8397216796875, p_loss: -20.956172943115234, mean_rew: 0.1138671875, variance: 0.0, cvar: 28.159332275390625, v: 25.485321044921875, mean_q: 20.93000030517578, std_q: 3.16018009185791, lamda: 1.3429051637649536
Running avgs for agent 1: q_loss: 3.1547152996063232, u_loss: 1753.6185302734375, p_loss: -18.70957374572754, mean_rew: 0.114140625, variance: 0.0, cvar: 28.325942993164062, v: 24.765811920166016, mean_q: 18.675844192504883, std_q: 4.211724281311035, lamda: 1.3871148824691772
Running avgs for agent 2: q_loss: 0.5594044327735901, u_loss: 427.6576843261719, p_loss: 0.2720632255077362, mean_rew: -0.11572277493607896, variance: 3.125766884982586, cvar: -0.08392166346311569, v: -0.10199642926454544, mean_q: -0.3515268564224243, std_q: 1.993545413017273, lamda: 1.0600285530090332
Running avgs for agent 3: q_loss: 0.5496184825897217, u_loss: 704.9517211914062, p_loss: 0.3166801631450653, mean_rew: -0.11428267975753809, variance: 4.119407116413116, cvar: -0.08323236554861069, v: -0.0989503413438797, mean_q: -0.3945242166519165, std_q: 3.005070209503174, lamda: 1.0662044286727905

steps: 574975, episodes: 23000, mean episode reward: -1.058658090810011, agent episode reward: [0.89, 0.89, -1.0585778583339829, -1.7800802324760279], time: 73.142
steps: 574975, episodes: 23000, mean episode variance: 1.7344879639595747, agent episode variance: [0.0, 0.0, 0.688648475036025, 1.0458394889235496], time: 73.143
steps: 574975, episodes: 23000, mean episode cvar: 14.10521865598485, agent episode cvar: [7.072387289047241, 7.073774263381958, -0.02055018676072359, -0.02039270968362689], time: 73.143
Running avgs for agent 0: q_loss: 3.202130079269409, u_loss: 1938.3690185546875, p_loss: -21.081037521362305, mean_rew: 0.1099609375, variance: 0.0, cvar: 28.28955078125, v: 25.555728912353516, mean_q: 21.063392639160156, std_q: 3.1245293617248535, lamda: 1.3570351600646973
Running avgs for agent 1: q_loss: 2.855114698410034, u_loss: 1678.595947265625, p_loss: -18.82723045349121, mean_rew: 0.1092578125, variance: 0.0, cvar: 28.29509735107422, v: 24.876384735107422, mean_q: 18.790903091430664, std_q: 4.160538673400879, lamda: 1.404299259185791
Running avgs for agent 2: q_loss: 0.5499869585037231, u_loss: 379.5998229980469, p_loss: 0.25997015833854675, mean_rew: -0.11552776528018728, variance: 2.7545939001441, cvar: -0.08220074325799942, v: -0.10214138776063919, mean_q: -0.33662667870521545, std_q: 1.9029347896575928, lamda: 1.0630695819854736
Running avgs for agent 3: q_loss: 0.5558063983917236, u_loss: 644.4794921875, p_loss: 0.3009907305240631, mean_rew: -0.1097627955559192, variance: 4.183357955694198, cvar: -0.0815708339214325, v: -0.10079411417245865, mean_q: -0.3742891848087311, std_q: 2.930835485458374, lamda: 1.0704433917999268

steps: 599975, episodes: 24000, mean episode reward: -1.9759321821907947, agent episode reward: [0.43, 0.43, -1.097088992298322, -1.738843189892473], time: 73.199
steps: 599975, episodes: 24000, mean episode variance: 1.6046707395613193, agent episode variance: [0.0, 0.0, 0.47889876702427864, 1.1257719725370408], time: 73.199
steps: 599975, episodes: 24000, mean episode cvar: 14.109505651436747, agent episode cvar: [7.084570705413818, 7.067587158203125, -0.02074763422086835, -0.02190457795932889], time: 73.2
Running avgs for agent 0: q_loss: 3.480407238006592, u_loss: 1784.2393798828125, p_loss: -21.030858993530273, mean_rew: 0.10546875, variance: 0.0, cvar: 28.33828353881836, v: 25.593040466308594, mean_q: 21.011877059936523, std_q: 3.1418652534484863, lamda: 1.3731149435043335
Running avgs for agent 1: q_loss: 3.383420944213867, u_loss: 1585.419189453125, p_loss: -19.137388229370117, mean_rew: 0.10640625, variance: 0.0, cvar: 28.270349502563477, v: 24.956369400024414, mean_q: 19.095853805541992, std_q: 3.985872268676758, lamda: 1.4204782247543335
Running avgs for agent 2: q_loss: 0.5339463949203491, u_loss: 329.0584716796875, p_loss: 0.2483597993850708, mean_rew: -0.10941722445636289, variance: 1.9155950680971146, cvar: -0.08299054205417633, v: -0.09987908601760864, mean_q: -0.3165690302848816, std_q: 1.8673073053359985, lamda: 1.0660146474838257
Running avgs for agent 3: q_loss: 0.526357114315033, u_loss: 532.0292358398438, p_loss: 0.29629406332969666, mean_rew: -0.11044113532095644, variance: 4.503087890148163, cvar: -0.08761831372976303, v: -0.1081291064620018, mean_q: -0.36722156405448914, std_q: 2.8608498573303223, lamda: 1.0772221088409424

steps: 624975, episodes: 25000, mean episode reward: -1.5934581281317046, agent episode reward: [0.59, 0.59, -1.1506667916195816, -1.6227913365121232], time: 73.241
steps: 624975, episodes: 25000, mean episode variance: 1.5445987939685584, agent episode variance: [0.0, 0.0, 0.43569510100781916, 1.1089036929607392], time: 73.242
steps: 624975, episodes: 25000, mean episode cvar: 14.097364427916705, agent episode cvar: [7.058948616027832, 7.082214017868042, -0.02132550099492073, -0.022472704984247683], time: 73.242
Running avgs for agent 0: q_loss: 2.702052354812622, u_loss: 1769.393798828125, p_loss: -20.97334098815918, mean_rew: 0.104375, variance: 0.0, cvar: 28.23579216003418, v: 25.498233795166016, mean_q: 20.954425811767578, std_q: 3.132424831390381, lamda: 1.38796067237854
Running avgs for agent 1: q_loss: 3.01664137840271, u_loss: 1557.9146728515625, p_loss: -19.32767105102539, mean_rew: 0.10296875, variance: 0.0, cvar: 28.328855514526367, v: 25.00938606262207, mean_q: 19.30646324157715, std_q: 3.8962442874908447, lamda: 1.4356120824813843
Running avgs for agent 2: q_loss: 0.5119898915290833, u_loss: 329.4487609863281, p_loss: 0.2438926100730896, mean_rew: -0.10739109477844096, variance: 1.7427804040312767, cvar: -0.08530200272798538, v: -0.10505786538124084, mean_q: -0.3084329664707184, std_q: 1.7615567445755005, lamda: 1.0697993040084839
Running avgs for agent 3: q_loss: 0.5152819752693176, u_loss: 596.9521484375, p_loss: 0.29041752219200134, mean_rew: -0.11080189504349883, variance: 4.435614771842957, cvar: -0.08989082276821136, v: -0.10425776988267899, mean_q: -0.36091348528862, std_q: 2.856527805328369, lamda: 1.0841994285583496

steps: 649975, episodes: 26000, mean episode reward: -1.2373216365378334, agent episode reward: [0.56, 0.56, -0.8677038368605988, -1.4896177996772346], time: 73.17
steps: 649975, episodes: 26000, mean episode variance: 1.5752399192750455, agent episode variance: [0.0, 0.0, 0.420796645373106, 1.1544432739019395], time: 73.17
steps: 649975, episodes: 26000, mean episode cvar: 14.103610726557672, agent episode cvar: [7.053320154190064, 7.093482353210449, -0.020967383094131948, -0.022224397748708723], time: 73.171
Running avgs for agent 0: q_loss: 2.1107516288757324, u_loss: 1612.67578125, p_loss: -20.999479293823242, mean_rew: 0.0971875, variance: 0.0, cvar: 28.213281631469727, v: 25.45233917236328, mean_q: 20.97824478149414, std_q: 3.107943296432495, lamda: 1.4006296396255493
Running avgs for agent 1: q_loss: 3.495924234390259, u_loss: 1479.5611572265625, p_loss: -19.455406188964844, mean_rew: 0.095546875, variance: 0.0, cvar: 28.373929977416992, v: 25.142932891845703, mean_q: 19.42928123474121, std_q: 3.859912872314453, lamda: 1.4529057741165161
Running avgs for agent 2: q_loss: 0.5024829506874084, u_loss: 329.6422424316406, p_loss: 0.23719415068626404, mean_rew: -0.10630204194527752, variance: 1.683186581492424, cvar: -0.08386953920125961, v: -0.1020924523472786, mean_q: -0.30136314034461975, std_q: 1.773884892463684, lamda: 1.0734318494796753
Running avgs for agent 3: q_loss: 0.5050721764564514, u_loss: 547.7322387695312, p_loss: 0.283539742231369, mean_rew: -0.10589630171433019, variance: 4.617773095607758, cvar: -0.08889758586883545, v: -0.10386762022972107, mean_q: -0.3486706614494324, std_q: 2.7348334789276123, lamda: 1.0894256830215454

steps: 674975, episodes: 27000, mean episode reward: -0.7703077426349921, agent episode reward: [0.89, 0.89, -1.08033626402764, -1.469971478607352], time: 74.084
steps: 674975, episodes: 27000, mean episode variance: 1.4824604959338903, agent episode variance: [0.0, 0.0, 0.4192114495486021, 1.0632490463852882], time: 74.085
steps: 674975, episodes: 27000, mean episode cvar: 14.10845738933608, agent episode cvar: [7.062948934555053, 7.087763027191162, -0.021371553506702185, -0.020883018903434275], time: 74.086
Running avgs for agent 0: q_loss: 2.857318639755249, u_loss: 1624.9969482421875, p_loss: -20.925960540771484, mean_rew: 0.0990234375, variance: 0.0, cvar: 28.251794815063477, v: 25.479202270507812, mean_q: 20.90711212158203, std_q: 3.138375997543335, lamda: 1.413253903388977
Running avgs for agent 1: q_loss: 3.2423019409179688, u_loss: 1514.3905029296875, p_loss: -19.472402572631836, mean_rew: 0.097578125, variance: 0.0, cvar: 28.351051330566406, v: 25.171945571899414, mean_q: 19.446269989013672, std_q: 3.8272671699523926, lamda: 1.470086693763733
Running avgs for agent 2: q_loss: 0.45807743072509766, u_loss: 306.2052001953125, p_loss: 0.23330934345722198, mean_rew: -0.10232413740391991, variance: 1.6768457981944085, cvar: -0.08548621833324432, v: -0.10725811868906021, mean_q: -0.2971571683883667, std_q: 1.6874359846115112, lamda: 1.0779731273651123
Running avgs for agent 3: q_loss: 0.5288659930229187, u_loss: 754.0651245117188, p_loss: 0.28421294689178467, mean_rew: -0.10431200464692969, variance: 4.252996185541153, cvar: -0.08353207260370255, v: -0.10292970389127731, mean_q: -0.34791111946105957, std_q: 2.7980947494506836, lamda: 1.0988049507141113

steps: 699975, episodes: 28000, mean episode reward: -0.408314144386271, agent episode reward: [1.04, 1.04, -1.3712254883037445, -1.1170886560825266], time: 72.159
steps: 699975, episodes: 28000, mean episode variance: 1.6086788280904294, agent episode variance: [0.0, 0.0, 0.5044006479084492, 1.10427818018198], time: 72.159
steps: 699975, episodes: 28000, mean episode cvar: 14.096108164399862, agent episode cvar: [7.065714933395386, 7.074426506042481, -0.022386109460145236, -0.021647165577858687], time: 72.16
Running avgs for agent 0: q_loss: 2.4032576084136963, u_loss: 1587.3350830078125, p_loss: -20.946269989013672, mean_rew: 0.0955859375, variance: 0.0, cvar: 28.262859344482422, v: 25.494314193725586, mean_q: 20.924320220947266, std_q: 3.137397527694702, lamda: 1.426046371459961
Running avgs for agent 1: q_loss: 2.902160167694092, u_loss: 1451.8741455078125, p_loss: -19.541305541992188, mean_rew: 0.092109375, variance: 0.0, cvar: 28.297704696655273, v: 25.12734031677246, mean_q: 19.51780128479004, std_q: 3.7362611293792725, lamda: 1.4865930080413818
Running avgs for agent 2: q_loss: 0.44819384813308716, u_loss: 314.9798583984375, p_loss: 0.22135119140148163, mean_rew: -0.09980491617564792, variance: 2.0176025916337967, cvar: -0.08954443037509918, v: -0.1036781594157219, mean_q: -0.2807707190513611, std_q: 1.672961711883545, lamda: 1.084315299987793
Running avgs for agent 3: q_loss: 0.446082204580307, u_loss: 675.592529296875, p_loss: 0.28891730308532715, mean_rew: -0.10297349019530058, variance: 4.41711272072792, cvar: -0.08658865839242935, v: -0.10306193679571152, mean_q: -0.3523673713207245, std_q: 2.936450719833374, lamda: 1.107216238975525

steps: 724975, episodes: 29000, mean episode reward: -0.6101065251053877, agent episode reward: [0.41, 0.41, -0.6657267563338068, -0.7643797687715808], time: 72.49
steps: 724975, episodes: 29000, mean episode variance: 1.5457561389282346, agent episode variance: [0.0, 0.0, 0.5006184444501997, 1.045137694478035], time: 72.49
steps: 724975, episodes: 29000, mean episode cvar: 14.120800781134516, agent episode cvar: [7.083285312652588, 7.08010072517395, -0.021564570389688016, -0.02102068630233407], time: 72.491
Running avgs for agent 0: q_loss: 2.943293809890747, u_loss: 1550.99072265625, p_loss: -20.95427894592285, mean_rew: 0.0922265625, variance: 0.0, cvar: 28.333141326904297, v: 25.490737915039062, mean_q: 20.935270309448242, std_q: 3.1387901306152344, lamda: 1.4398239850997925
Running avgs for agent 1: q_loss: 3.0707602500915527, u_loss: 1430.875, p_loss: -19.738265991210938, mean_rew: 0.0941015625, variance: 0.0, cvar: 28.320404052734375, v: 25.186758041381836, mean_q: 19.71172332763672, std_q: 3.660590887069702, lamda: 1.501159906387329
Running avgs for agent 2: q_loss: 0.45799437165260315, u_loss: 302.5750732421875, p_loss: 0.2184564173221588, mean_rew: -0.09855616951613957, variance: 2.0024737778007986, cvar: -0.08625827729701996, v: -0.10088538378477097, mean_q: -0.27532097697257996, std_q: 1.5945616960525513, lamda: 1.0901738405227661
Running avgs for agent 3: q_loss: 0.43332639336586, u_loss: 695.24951171875, p_loss: 0.27277225255966187, mean_rew: -0.09932916898105634, variance: 4.18055077791214, cvar: -0.08408274501562119, v: -0.10461629927158356, mean_q: -0.33298107981681824, std_q: 2.734536647796631, lamda: 1.1153440475463867

steps: 749975, episodes: 30000, mean episode reward: -0.07610887108846018, agent episode reward: [0.95, 0.95, -0.9412111030145127, -1.0348977680739477], time: 73.089
steps: 749975, episodes: 30000, mean episode variance: 1.469022961884737, agent episode variance: [0.0, 0.0, 0.5121987926065922, 0.9568241692781448], time: 73.089
steps: 749975, episodes: 30000, mean episode cvar: 14.102094328556209, agent episode cvar: [7.059544937133789, 7.082816432952881, -0.020157100688666104, -0.020109940841794013], time: 73.09
Running avgs for agent 0: q_loss: 2.1990315914154053, u_loss: 1562.247802734375, p_loss: -20.95178985595703, mean_rew: 0.089296875, variance: 0.0, cvar: 28.238182067871094, v: 25.461870193481445, mean_q: 20.93626594543457, std_q: 3.097531318664551, lamda: 1.4543095827102661
Running avgs for agent 1: q_loss: 3.0019421577453613, u_loss: 1429.283447265625, p_loss: -19.718128204345703, mean_rew: 0.0916015625, variance: 0.0, cvar: 28.331266403198242, v: 25.29107093811035, mean_q: 19.693628311157227, std_q: 3.7163026332855225, lamda: 1.517868161201477
Running avgs for agent 2: q_loss: 0.44182536005973816, u_loss: 330.74346923828125, p_loss: 0.22093215584754944, mean_rew: -0.0961295358422831, variance: 2.0487951704263687, cvar: -0.080628402531147, v: -0.0978170782327652, mean_q: -0.2798365354537964, std_q: 1.5564945936203003, lamda: 1.0955696105957031
Running avgs for agent 3: q_loss: 0.4341726005077362, u_loss: 521.7222290039062, p_loss: 0.2656494677066803, mean_rew: -0.0988041962762172, variance: 3.8272966771125794, cvar: -0.08043976873159409, v: -0.09587007015943527, mean_q: -0.3249504864215851, std_q: 2.71435546875, lamda: 1.123865008354187

steps: 774975, episodes: 31000, mean episode reward: -0.2623319524139015, agent episode reward: [0.59, 0.59, -0.682999253652667, -0.7593326987612345], time: 73.507
steps: 774975, episodes: 31000, mean episode variance: 1.2786897175610066, agent episode variance: [0.0, 0.0, 0.5152868419885636, 0.763402875572443], time: 73.507
steps: 774975, episodes: 31000, mean episode cvar: 14.113531793564558, agent episode cvar: [7.0697417640686036, 7.084318216323853, -0.019876132886856794, -0.02065205394104123], time: 73.508
Running avgs for agent 0: q_loss: 2.6071176528930664, u_loss: 1511.6065673828125, p_loss: -20.91826629638672, mean_rew: 0.0884765625, variance: 0.0, cvar: 28.278968811035156, v: 25.462909698486328, mean_q: 20.902307510375977, std_q: 3.1222615242004395, lamda: 1.4673727750778198
Running avgs for agent 1: q_loss: 3.130093812942505, u_loss: 1382.7103271484375, p_loss: -19.908123016357422, mean_rew: 0.0865625, variance: 0.0, cvar: 28.33727264404297, v: 25.336050033569336, mean_q: 19.88671112060547, std_q: 3.6268227100372314, lamda: 1.535760521888733
Running avgs for agent 2: q_loss: 0.46139729022979736, u_loss: 288.68426513671875, p_loss: 0.2221403867006302, mean_rew: -0.09482839856642933, variance: 2.0611473679542542, cvar: -0.07950453460216522, v: -0.09640495479106903, mean_q: -0.27695074677467346, std_q: 1.6061855554580688, lamda: 1.099948763847351
Running avgs for agent 3: q_loss: 0.38951563835144043, u_loss: 623.4705200195312, p_loss: 0.2679937779903412, mean_rew: -0.09496097099301269, variance: 3.053611502289772, cvar: -0.08260821551084518, v: -0.10048368573188782, mean_q: -0.32854676246643066, std_q: 2.6829581260681152, lamda: 1.1324565410614014

steps: 799975, episodes: 32000, mean episode reward: 0.5720233651479263, agent episode reward: [1.6, 1.6, -1.6683972325331378, -0.9595794023189358], time: 73.345
steps: 799975, episodes: 32000, mean episode variance: 1.2747313369810582, agent episode variance: [0.0, 0.0, 0.44961893889307974, 0.8251123980879783], time: 73.345
steps: 799975, episodes: 32000, mean episode cvar: 14.11078371130675, agent episode cvar: [7.0697668800354005, 7.079721694946289, -0.018609950445592403, -0.020094913229346277], time: 73.346
Running avgs for agent 0: q_loss: 3.280729055404663, u_loss: 1596.3194580078125, p_loss: -20.869754791259766, mean_rew: 0.0859765625, variance: 0.0, cvar: 28.27906608581543, v: 25.37193489074707, mean_q: 20.83576202392578, std_q: 3.136220693588257, lamda: 1.4827663898468018
Running avgs for agent 1: q_loss: 3.144627332687378, u_loss: 1476.9599609375, p_loss: -19.95221519470215, mean_rew: 0.0846875, variance: 0.0, cvar: 28.318889617919922, v: 25.333349227905273, mean_q: 19.93746566772461, std_q: 3.5994834899902344, lamda: 1.552964687347412
Running avgs for agent 2: q_loss: 0.49854332208633423, u_loss: 312.8644104003906, p_loss: 0.2256212830543518, mean_rew: -0.0920483672750528, variance: 1.798475755572319, cvar: -0.07443980127573013, v: -0.09590312093496323, mean_q: -0.28162881731987, std_q: 1.6686162948608398, lamda: 1.105859637260437
Running avgs for agent 3: q_loss: 0.40181678533554077, u_loss: 614.88818359375, p_loss: 0.2598351538181305, mean_rew: -0.09259535087355333, variance: 3.3004495923519133, cvar: -0.0803796574473381, v: -0.09774323552846909, mean_q: -0.31732332706451416, std_q: 2.669229030609131, lamda: 1.140586018562317

steps: 824975, episodes: 33000, mean episode reward: -0.30403492821106876, agent episode reward: [0.66, 0.66, -0.7906283013081734, -0.8334066269028954], time: 73.447
steps: 824975, episodes: 33000, mean episode variance: 1.4391526677384971, agent episode variance: [0.0, 0.0, 0.46890349838882683, 0.9702491693496704], time: 73.447
steps: 824975, episodes: 33000, mean episode cvar: 14.114554803740234, agent episode cvar: [7.077717929840088, 7.07773759841919, -0.019655766144394876, -0.021244958374649285], time: 73.448
Running avgs for agent 0: q_loss: 2.742161512374878, u_loss: 1498.3380126953125, p_loss: -20.90221405029297, mean_rew: 0.0860546875, variance: 0.0, cvar: 28.310871124267578, v: 25.378271102905273, mean_q: 20.8785457611084, std_q: 3.1151251792907715, lamda: 1.4979465007781982
Running avgs for agent 1: q_loss: 3.55981707572937, u_loss: 1484.9449462890625, p_loss: -20.08311653137207, mean_rew: 0.086640625, variance: 0.0, cvar: 28.310949325561523, v: 25.315502166748047, mean_q: 20.04941177368164, std_q: 3.5290515422821045, lamda: 1.5700435638427734
Running avgs for agent 2: q_loss: 0.4476500153541565, u_loss: 312.7920227050781, p_loss: 0.2322247475385666, mean_rew: -0.08898523652031548, variance: 1.8756139935553073, cvar: -0.07862306386232376, v: -0.09637532383203506, mean_q: -0.28549233078956604, std_q: 1.7793740034103394, lamda: 1.1134265661239624
Running avgs for agent 3: q_loss: 0.42038968205451965, u_loss: 690.9026489257812, p_loss: 0.2520502805709839, mean_rew: -0.0903122891018353, variance: 3.8809966773986817, cvar: -0.0849798247218132, v: -0.09929683059453964, mean_q: -0.30688783526420593, std_q: 2.4920594692230225, lamda: 1.1484577655792236

steps: 849975, episodes: 34000, mean episode reward: -0.22637340402803618, agent episode reward: [0.78, 0.78, -0.9281474242790553, -0.858225979748981], time: 73.35
steps: 849975, episodes: 34000, mean episode variance: 1.589034583568573, agent episode variance: [0.0, 0.0, 0.5288214341402054, 1.0602131494283675], time: 73.351
steps: 849975, episodes: 34000, mean episode cvar: 14.094061536740512, agent episode cvar: [7.047806020736695, 7.087005718231201, -0.020668107599020005, -0.02008209462836385], time: 73.351
Running avgs for agent 0: q_loss: 3.072434425354004, u_loss: 1488.6689453125, p_loss: -20.859233856201172, mean_rew: 0.08296875, variance: 0.0, cvar: 28.19122314453125, v: 25.33470344543457, mean_q: 20.842615127563477, std_q: 3.0674984455108643, lamda: 1.5121753215789795
Running avgs for agent 1: q_loss: 3.324754238128662, u_loss: 1427.0400390625, p_loss: -20.278675079345703, mean_rew: 0.0855859375, variance: 0.0, cvar: 28.348024368286133, v: 25.33686065673828, mean_q: 20.263010025024414, std_q: 3.442568063735962, lamda: 1.5856695175170898
Running avgs for agent 2: q_loss: 0.44312310218811035, u_loss: 297.4550476074219, p_loss: 0.23203909397125244, mean_rew: -0.0904581090725392, variance: 2.1152857365608217, cvar: -0.08267243206501007, v: -0.09690968692302704, mean_q: -0.29023444652557373, std_q: 1.8653323650360107, lamda: 1.1218767166137695
Running avgs for agent 3: q_loss: 0.386332243680954, u_loss: 587.2886352539062, p_loss: 0.23827344179153442, mean_rew: -0.08937767665352803, variance: 4.24085259771347, cvar: -0.08032837510108948, v: -0.09852973371744156, mean_q: -0.29105913639068604, std_q: 2.348134994506836, lamda: 1.1551110744476318

steps: 874975, episodes: 35000, mean episode reward: -0.07513786879303105, agent episode reward: [0.79, 0.79, -0.8089709158071878, -0.8461669529858434], time: 73.377
steps: 874975, episodes: 35000, mean episode variance: 1.4441487557590007, agent episode variance: [0.0, 0.0, 0.5548664079606533, 0.8892823477983475], time: 73.378
steps: 874975, episodes: 35000, mean episode cvar: 14.10955540766567, agent episode cvar: [7.07306088256836, 7.077436311721802, -0.02024375358596444, -0.020698033038526773], time: 73.378
Running avgs for agent 0: q_loss: 3.100788116455078, u_loss: 1454.86328125, p_loss: -21.02663803100586, mean_rew: 0.0837890625, variance: 0.0, cvar: 28.2922420501709, v: 25.316753387451172, mean_q: 21.009977340698242, std_q: 3.0354344844818115, lamda: 1.5284875631332397
Running avgs for agent 1: q_loss: 3.039602041244507, u_loss: 1361.810302734375, p_loss: -20.220224380493164, mean_rew: 0.081484375, variance: 0.0, cvar: 28.30974578857422, v: 25.356216430664062, mean_q: 20.200891494750977, std_q: 3.453809976577759, lamda: 1.6014949083328247
Running avgs for agent 2: q_loss: 0.45235559344291687, u_loss: 305.4038391113281, p_loss: 0.22747617959976196, mean_rew: -0.08946350815999536, variance: 2.2194656318426134, cvar: -0.0809750109910965, v: -0.09511981904506683, mean_q: -0.2806621789932251, std_q: 1.801256537437439, lamda: 1.1290830373764038
Running avgs for agent 3: q_loss: 0.36446264386177063, u_loss: 543.1704711914062, p_loss: 0.23772670328617096, mean_rew: -0.08641107851205565, variance: 3.55712939119339, cvar: -0.08279213309288025, v: -0.09914077818393707, mean_q: -0.28988704085350037, std_q: 2.340350866317749, lamda: 1.1651805639266968

steps: 899975, episodes: 36000, mean episode reward: -0.3233122242812217, agent episode reward: [0.6, 0.6, -0.6420926942028174, -0.8812195300784043], time: 73.561
steps: 899975, episodes: 36000, mean episode variance: 1.3142493994235993, agent episode variance: [0.0, 0.0, 0.576368943542242, 0.7378804558813572], time: 73.561
steps: 899975, episodes: 36000, mean episode cvar: 14.07173608179018, agent episode cvar: [7.051244707107544, 7.061413297653198, -0.020137022130191327, -0.020784900840371847], time: 73.562
Running avgs for agent 0: q_loss: 1.935610294342041, u_loss: 1438.5164794921875, p_loss: -20.972747802734375, mean_rew: 0.0797265625, variance: 0.0, cvar: 28.204980850219727, v: 25.295866012573242, mean_q: 20.9545955657959, std_q: 3.0479471683502197, lamda: 1.542040228843689
Running avgs for agent 1: q_loss: 2.1982462406158447, u_loss: 1355.3106689453125, p_loss: -20.4329891204834, mean_rew: 0.0791015625, variance: 0.0, cvar: 28.24565315246582, v: 25.323898315429688, mean_q: 20.410913467407227, std_q: 3.3408043384552, lamda: 1.6147407293319702
Running avgs for agent 2: q_loss: 0.40987536311149597, u_loss: 340.70318603515625, p_loss: 0.22553518414497375, mean_rew: -0.08539760104183429, variance: 2.305475774168968, cvar: -0.08054808527231216, v: -0.09579561650753021, mean_q: -0.2744509279727936, std_q: 1.7857716083526611, lamda: 1.1354104280471802
Running avgs for agent 3: q_loss: 0.38807693123817444, u_loss: 560.1693115234375, p_loss: 0.24294067919254303, mean_rew: -0.08699093358479078, variance: 2.951521823525429, cvar: -0.08313960582017899, v: -0.09921574592590332, mean_q: -0.292337030172348, std_q: 2.383111000061035, lamda: 1.173108696937561

steps: 924975, episodes: 37000, mean episode reward: 0.08967357735713259, agent episode reward: [1.23, 1.23, -1.4766796505581328, -0.8936467720847344], time: 73.574
steps: 924975, episodes: 37000, mean episode variance: 1.182811486274004, agent episode variance: [0.0, 0.0, 0.636716882199049, 0.546094604074955], time: 73.574
steps: 924975, episodes: 37000, mean episode cvar: 14.117149560756982, agent episode cvar: [7.06174130821228, 7.094362743377686, -0.01925256860256195, -0.019701922230422497], time: 73.575
Running avgs for agent 0: q_loss: 2.7681963443756104, u_loss: 1345.567138671875, p_loss: -21.014421463012695, mean_rew: 0.0791796875, variance: 0.0, cvar: 28.246965408325195, v: 25.289342880249023, mean_q: 20.989093780517578, std_q: 3.033982276916504, lamda: 1.5569003820419312
Running avgs for agent 1: q_loss: 2.8092777729034424, u_loss: 1360.278076171875, p_loss: -20.480831146240234, mean_rew: 0.078359375, variance: 0.0, cvar: 28.37744903564453, v: 25.435354232788086, mean_q: 20.46280288696289, std_q: 3.361654281616211, lamda: 1.6276832818984985
Running avgs for agent 2: q_loss: 0.4084011912345886, u_loss: 332.2892150878906, p_loss: 0.2202574759721756, mean_rew: -0.08541769129993171, variance: 2.546867528796196, cvar: -0.07701026648283005, v: -0.09658415615558624, mean_q: -0.27273809909820557, std_q: 1.74387788772583, lamda: 1.146360158920288
Running avgs for agent 3: q_loss: 0.3531132936477661, u_loss: 619.4212646484375, p_loss: 0.23124267160892487, mean_rew: -0.08532906517628722, variance: 2.18437841629982, cvar: -0.07880769670009613, v: -0.09473968297243118, mean_q: -0.2833530604839325, std_q: 2.288456916809082, lamda: 1.179182529449463

steps: 949975, episodes: 38000, mean episode reward: -0.23205370371670367, agent episode reward: [0.88, 0.88, -0.9551365489671554, -1.0369171547495482], time: 73.829
steps: 949975, episodes: 38000, mean episode variance: 1.1851943972706795, agent episode variance: [0.0, 0.0, 0.702819149017334, 0.4823752482533455], time: 73.83
steps: 949975, episodes: 38000, mean episode cvar: 14.114421113025397, agent episode cvar: [7.065126497268677, 7.090987503051758, -0.02046843535080552, -0.021224451944231986], time: 73.83
Running avgs for agent 0: q_loss: 2.019909381866455, u_loss: 1428.0052490234375, p_loss: -21.090219497680664, mean_rew: 0.0779296875, variance: 0.0, cvar: 28.26050567626953, v: 25.29194450378418, mean_q: 21.069087982177734, std_q: 3.018028974533081, lamda: 1.571333885192871
Running avgs for agent 1: q_loss: 2.5493359565734863, u_loss: 1352.0347900390625, p_loss: -20.499284744262695, mean_rew: 0.0755078125, variance: 0.0, cvar: 28.363948822021484, v: 25.407262802124023, mean_q: 20.481613159179688, std_q: 3.325040340423584, lamda: 1.642762541770935
Running avgs for agent 2: q_loss: 0.4465416967868805, u_loss: 306.52130126953125, p_loss: 0.21883121132850647, mean_rew: -0.08537291574088991, variance: 2.811276596069336, cvar: -0.08187374472618103, v: -0.09407003968954086, mean_q: -0.269504576921463, std_q: 1.731109380722046, lamda: 1.1580435037612915
Running avgs for agent 3: q_loss: 0.34696462750434875, u_loss: 580.72998046875, p_loss: 0.23831292986869812, mean_rew: -0.08426132883402451, variance: 1.929500993013382, cvar: -0.08489780128002167, v: -0.09931985288858414, mean_q: -0.2878900170326233, std_q: 2.3485512733459473, lamda: 1.1895467042922974

steps: 974975, episodes: 39000, mean episode reward: -0.3483699675860117, agent episode reward: [0.59, 0.59, -0.7628073011204801, -0.7655626664655316], time: 73.651
steps: 974975, episodes: 39000, mean episode variance: 1.1292377743646502, agent episode variance: [0.0, 0.0, 0.6948779710903764, 0.43435980327427387], time: 73.651
steps: 974975, episodes: 39000, mean episode cvar: 14.128131933704019, agent episode cvar: [7.084894655227661, 7.084334760665894, -0.01976047733053565, -0.021337004859000443], time: 73.652
Running avgs for agent 0: q_loss: 2.5921130180358887, u_loss: 1315.6031494140625, p_loss: -21.086894989013672, mean_rew: 0.0744921875, variance: 0.0, cvar: 28.33957862854004, v: 25.349027633666992, mean_q: 21.06275749206543, std_q: 3.0479421615600586, lamda: 1.585186243057251
Running avgs for agent 1: q_loss: 2.3640596866607666, u_loss: 1342.6822509765625, p_loss: -20.480758666992188, mean_rew: 0.0767578125, variance: 0.0, cvar: 28.337337493896484, v: 25.407325744628906, mean_q: 20.45922088623047, std_q: 3.3132755756378174, lamda: 1.6594775915145874
Running avgs for agent 2: q_loss: 0.3894979655742645, u_loss: 283.8787536621094, p_loss: 0.20993897318840027, mean_rew: -0.08198564779056848, variance: 2.7795118843615056, cvar: -0.07904191315174103, v: -0.09928684681653976, mean_q: -0.25684329867362976, std_q: 1.5592355728149414, lamda: 1.1691516637802124
Running avgs for agent 3: q_loss: 0.3489893674850464, u_loss: 603.5958862304688, p_loss: 0.23906414210796356, mean_rew: -0.08407887258947636, variance: 1.7374392130970955, cvar: -0.08534802496433258, v: -0.10141973197460175, mean_q: -0.2895859479904175, std_q: 2.3901071548461914, lamda: 1.2022112607955933

steps: 999975, episodes: 40000, mean episode reward: -0.32094214753195066, agent episode reward: [0.48, 0.48, -0.6871705462764969, -0.5937716012554538], time: 73.751
steps: 999975, episodes: 40000, mean episode variance: 1.0256504604816437, agent episode variance: [0.0, 0.0, 0.5927154899835586, 0.43293497049808505], time: 73.752
steps: 999975, episodes: 40000, mean episode cvar: 14.10104951415956, agent episode cvar: [7.0761897735595705, 7.0654614028930665, -0.01925093976408243, -0.021350722528994084], time: 73.752
Running avgs for agent 0: q_loss: 4.038175106048584, u_loss: 1421.231689453125, p_loss: -21.096641540527344, mean_rew: 0.0792578125, variance: 0.0, cvar: 28.304758071899414, v: 25.275104522705078, mean_q: 21.07268524169922, std_q: 2.9994826316833496, lamda: 1.6025822162628174
Running avgs for agent 1: q_loss: 2.5534183979034424, u_loss: 1295.19921875, p_loss: -20.6129093170166, mean_rew: 0.0737890625, variance: 0.0, cvar: 28.2618465423584, v: 25.43065071105957, mean_q: 20.588863372802734, std_q: 3.221466302871704, lamda: 1.6746996641159058
Running avgs for agent 2: q_loss: 0.41063469648361206, u_loss: 330.3688049316406, p_loss: 0.2084888517856598, mean_rew: -0.08060003302831643, variance: 2.3708619599342344, cvar: -0.07700375467538834, v: -0.0939459279179573, mean_q: -0.2573140859603882, std_q: 1.58464777469635, lamda: 1.1799147129058838
Running avgs for agent 3: q_loss: 0.3271898925304413, u_loss: 473.2310485839844, p_loss: 0.2375781536102295, mean_rew: -0.08245728795771824, variance: 1.7317398819923402, cvar: -0.08540288358926773, v: -0.1039067953824997, mean_q: -0.28579357266426086, std_q: 2.228393077850342, lamda: 1.219820499420166

steps: 1024975, episodes: 41000, mean episode reward: -0.16761023007258305, agent episode reward: [0.56, 0.56, -0.7621964362158906, -0.5254137938566924], time: 74.161
steps: 1024975, episodes: 41000, mean episode variance: 0.891963449433446, agent episode variance: [0.0, 0.0, 0.5009472491592168, 0.39101620027422906], time: 74.161
steps: 1024975, episodes: 41000, mean episode cvar: 14.109712328612805, agent episode cvar: [7.079382036209107, 7.069901573181152, -0.018994640175253152, -0.020576640602201223], time: 74.162
Running avgs for agent 0: q_loss: 2.679373264312744, u_loss: 1296.7344970703125, p_loss: -21.057334899902344, mean_rew: 0.0721484375, variance: 0.0, cvar: 28.317527770996094, v: 25.26351547241211, mean_q: 21.027496337890625, std_q: 3.048173427581787, lamda: 1.6196341514587402
Running avgs for agent 1: q_loss: 3.1728861331939697, u_loss: 1355.0074462890625, p_loss: -20.97625160217285, mean_rew: 0.07671875, variance: 0.0, cvar: 28.27960777282715, v: 25.517230987548828, mean_q: 20.95749282836914, std_q: 3.0841665267944336, lamda: 1.6881309747695923
Running avgs for agent 2: q_loss: 0.3973377048969269, u_loss: 299.1138000488281, p_loss: 0.2008141428232193, mean_rew: -0.07581980540757706, variance: 2.0037889966368674, cvar: -0.07597856223583221, v: -0.09091221541166306, mean_q: -0.24433495104312897, std_q: 1.5148637294769287, lamda: 1.1869776248931885
Running avgs for agent 3: q_loss: 0.32857197523117065, u_loss: 446.15380859375, p_loss: 0.23141664266586304, mean_rew: -0.07529379869846935, variance: 1.5640648010969163, cvar: -0.08230656385421753, v: -0.09533914178609848, mean_q: -0.2777178883552551, std_q: 2.383655071258545, lamda: 1.2321909666061401

steps: 1049975, episodes: 42000, mean episode reward: -0.4994198757964981, agent episode reward: [0.57, 0.57, -0.9281332586940112, -0.711286617102487], time: 74.364
steps: 1049975, episodes: 42000, mean episode variance: 0.7666147442907095, agent episode variance: [0.0, 0.0, 0.4336512825787067, 0.33296346171200275], time: 74.365
steps: 1049975, episodes: 42000, mean episode cvar: 14.112846685167401, agent episode cvar: [7.0823084812164305, 7.0678619918823244, -0.018084815289825203, -0.019238972641527654], time: 74.365
Running avgs for agent 0: q_loss: 2.60433292388916, u_loss: 1218.751708984375, p_loss: -20.894023895263672, mean_rew: 0.0699609375, variance: 0.0, cvar: 28.329235076904297, v: 25.30254364013672, mean_q: 20.868886947631836, std_q: 3.1390819549560547, lamda: 1.6367812156677246
Running avgs for agent 1: q_loss: 2.4520461559295654, u_loss: 1244.9720458984375, p_loss: -21.075815200805664, mean_rew: 0.070078125, variance: 0.0, cvar: 28.27145004272461, v: 25.624914169311523, mean_q: 21.063121795654297, std_q: 3.0464792251586914, lamda: 1.7040152549743652
Running avgs for agent 2: q_loss: 0.35357752442359924, u_loss: 166.36050415039062, p_loss: 0.1748380810022354, mean_rew: -0.06733495155446624, variance: 1.7346051303148269, cvar: -0.07233926653862, v: -0.09352325648069382, mean_q: -0.20823264122009277, std_q: 1.152853012084961, lamda: 1.1989916563034058
Running avgs for agent 3: q_loss: 0.25510573387145996, u_loss: 196.18663024902344, p_loss: 0.1544366180896759, mean_rew: -0.05652247635737073, variance: 1.331853846848011, cvar: -0.0769558846950531, v: -0.09579306840896606, mean_q: -0.17830637097358704, std_q: 0.919310450553894, lamda: 1.2419962882995605

steps: 1074975, episodes: 43000, mean episode reward: -0.5512928029338541, agent episode reward: [0.29, 0.29, -0.5644040427816881, -0.5668887601521662], time: 73.824
steps: 1074975, episodes: 43000, mean episode variance: 0.6819775283783674, agent episode variance: [0.0, 0.0, 0.4152898304313421, 0.2666876979470253], time: 73.824
steps: 1074975, episodes: 43000, mean episode cvar: 14.116087814781816, agent episode cvar: [7.103262540817261, 7.05042707824707, -0.019182016260921954, -0.018419788021594285], time: 73.825
Running avgs for agent 0: q_loss: 2.5891385078430176, u_loss: 1090.2269287109375, p_loss: -20.94693946838379, mean_rew: 0.0664453125, variance: 0.0, cvar: 28.41305160522461, v: 25.33124542236328, mean_q: 20.923295974731445, std_q: 3.1550519466400146, lamda: 1.653071641921997
Running avgs for agent 1: q_loss: 2.5868442058563232, u_loss: 1164.389892578125, p_loss: -21.275426864624023, mean_rew: 0.063828125, variance: 0.0, cvar: 28.20170783996582, v: 25.647451400756836, mean_q: 21.256208419799805, std_q: 2.9558842182159424, lamda: 1.7178925275802612
Running avgs for agent 2: q_loss: 0.3148258626461029, u_loss: 139.368408203125, p_loss: 0.1622045338153839, mean_rew: -0.061421012502856344, variance: 1.6611593217253684, cvar: -0.07672806084156036, v: -0.09279222786426544, mean_q: -0.18876831233501434, std_q: 0.7982890009880066, lamda: 1.2140028476715088
Running avgs for agent 3: q_loss: 0.25468650460243225, u_loss: 121.3582992553711, p_loss: 0.14445754885673523, mean_rew: -0.05480871669745462, variance: 1.0667507917881012, cvar: -0.07367915660142899, v: -0.09610631316900253, mean_q: -0.16583257913589478, std_q: 0.7227928638458252, lamda: 1.2585294246673584

steps: 1099975, episodes: 44000, mean episode reward: -0.34378420483459105, agent episode reward: [0.35, 0.35, -0.44480705575511226, -0.5989771490794787], time: 74.468
steps: 1099975, episodes: 44000, mean episode variance: 0.5524132173582912, agent episode variance: [0.0, 0.0, 0.3957638883143663, 0.1566493290439248], time: 74.469
steps: 1099975, episodes: 44000, mean episode cvar: 14.117298158630728, agent episode cvar: [7.102282209396362, 7.054145578384399, -0.018831304971128702, -0.020298324178904294], time: 74.469
Running avgs for agent 0: q_loss: 3.1029696464538574, u_loss: 1006.0452270507812, p_loss: -20.78758430480957, mean_rew: 0.05921875, variance: 0.0, cvar: 28.409128189086914, v: 25.394180297851562, mean_q: 20.763580322265625, std_q: 3.2447681427001953, lamda: 1.6714881658554077
Running avgs for agent 1: q_loss: 1.9590661525726318, u_loss: 1092.1429443359375, p_loss: -21.52640151977539, mean_rew: 0.059140625, variance: 0.0, cvar: 28.216581344604492, v: 25.786561965942383, mean_q: 21.506689071655273, std_q: 2.8960182666778564, lamda: 1.7304930686950684
Running avgs for agent 2: q_loss: 0.28026628494262695, u_loss: 116.73395538330078, p_loss: 0.15191055834293365, mean_rew: -0.05469865823140617, variance: 1.5830555532574653, cvar: -0.07532522082328796, v: -0.08917072415351868, mean_q: -0.17780184745788574, std_q: 0.7526163458824158, lamda: 1.2245513200759888
Running avgs for agent 3: q_loss: 0.2268017828464508, u_loss: 98.99730682373047, p_loss: 0.14176921546459198, mean_rew: -0.053276282600859017, variance: 0.6265973161756992, cvar: -0.08119329810142517, v: -0.095377616584301, mean_q: -0.16262607276439667, std_q: 0.7116460204124451, lamda: 1.2694270610809326

steps: 1124975, episodes: 45000, mean episode reward: -0.5018366931410884, agent episode reward: [0.32, 0.32, -0.5845221639443916, -0.5573145291966969], time: 74.273
steps: 1124975, episodes: 45000, mean episode variance: 0.45560965695697814, agent episode variance: [0.0, 0.0, 0.3598538772165775, 0.09575577974040062], time: 74.274
steps: 1124975, episodes: 45000, mean episode cvar: 14.103951485976577, agent episode cvar: [7.088794639587403, 7.054186925888062, -0.01864474369958043, -0.020385335799306632], time: 74.274
Running avgs for agent 0: q_loss: 2.1979193687438965, u_loss: 938.33935546875, p_loss: -20.73108673095703, mean_rew: 0.0540234375, variance: 0.0, cvar: 28.355178833007812, v: 25.366943359375, mean_q: 20.709741592407227, std_q: 3.2694122791290283, lamda: 1.6884756088256836
Running avgs for agent 1: q_loss: 2.260388135910034, u_loss: 1048.737060546875, p_loss: -21.820331573486328, mean_rew: 0.0521484375, variance: 0.0, cvar: 28.216747283935547, v: 25.87425422668457, mean_q: 21.799367904663086, std_q: 2.766268014907837, lamda: 1.7439078092575073
Running avgs for agent 2: q_loss: 0.26605838537216187, u_loss: 98.62396240234375, p_loss: 0.14482297003269196, mean_rew: -0.05527170187886751, variance: 1.43941550886631, cvar: -0.07457897067070007, v: -0.08917316794395447, mean_q: -0.1676851063966751, std_q: 0.6945828795433044, lamda: 1.235085129737854
Running avgs for agent 3: q_loss: 0.215678408741951, u_loss: 137.0098419189453, p_loss: 0.14413082599639893, mean_rew: -0.051147432785347155, variance: 0.38302311896160246, cvar: -0.08154133707284927, v: -0.096328504383564, mean_q: -0.16317877173423767, std_q: 0.7328442931175232, lamda: 1.2785708904266357

steps: 1149975, episodes: 46000, mean episode reward: -0.4702257326620119, agent episode reward: [0.51, 0.51, -0.560029336084654, -0.9301963965773579], time: 74.401
steps: 1149975, episodes: 46000, mean episode variance: 0.40580753243528306, agent episode variance: [0.0, 0.0, 0.35260619907081125, 0.053201333364471796], time: 74.401
steps: 1149975, episodes: 46000, mean episode cvar: 14.131183485556393, agent episode cvar: [7.108688318252564, 7.061359636306762, -0.018679952580481766, -0.020184516422450543], time: 74.402
Running avgs for agent 0: q_loss: 3.5618979930877686, u_loss: 820.4852294921875, p_loss: -20.73055076599121, mean_rew: 0.048203125, variance: 0.0, cvar: 28.43475341796875, v: 25.415760040283203, mean_q: 20.713136672973633, std_q: 3.30435848236084, lamda: 1.7044639587402344
Running avgs for agent 1: q_loss: 2.2977848052978516, u_loss: 989.572509765625, p_loss: -22.0218448638916, mean_rew: 0.0495703125, variance: 0.0, cvar: 28.245437622070312, v: 25.95954704284668, mean_q: 22.003406524658203, std_q: 2.692715883255005, lamda: 1.7575559616088867
Running avgs for agent 2: q_loss: 0.25856077671051025, u_loss: 108.63481903076172, p_loss: 0.14531701803207397, mean_rew: -0.05512773395039029, variance: 1.410424796283245, cvar: -0.07471980899572372, v: -0.08947832137346268, mean_q: -0.169941708445549, std_q: 0.7216435074806213, lamda: 1.245333194732666
Running avgs for agent 3: q_loss: 0.20938363671302795, u_loss: 169.14337158203125, p_loss: 0.14251315593719482, mean_rew: -0.04932408740900162, variance: 0.21280533345788719, cvar: -0.08073806017637253, v: -0.0955834910273552, mean_q: -0.15913031995296478, std_q: 0.679975688457489, lamda: 1.2911723852157593

steps: 1174975, episodes: 47000, mean episode reward: -0.48894925850630455, agent episode reward: [0.36, 0.36, -0.5038894188588496, -0.705059839647455], time: 84.89
steps: 1174975, episodes: 47000, mean episode variance: 0.3862278953064233, agent episode variance: [0.0, 0.0, 0.3420482514500618, 0.04417964385636151], time: 84.89
steps: 1174975, episodes: 47000, mean episode cvar: 14.130532347880305, agent episode cvar: [7.098647586822509, 7.069114810943604, -0.018013471007347105, -0.019216578878462315], time: 84.891
Running avgs for agent 0: q_loss: 2.355252504348755, u_loss: 786.0595092773438, p_loss: -20.795738220214844, mean_rew: 0.0451171875, variance: 0.0, cvar: 28.394590377807617, v: 25.43048858642578, mean_q: 20.777820587158203, std_q: 3.2714250087738037, lamda: 1.722469449043274
Running avgs for agent 1: q_loss: 2.439671277999878, u_loss: 919.8480224609375, p_loss: -22.259471893310547, mean_rew: 0.0451953125, variance: 0.0, cvar: 28.276458740234375, v: 26.055326461791992, mean_q: 22.244401931762695, std_q: 2.608151912689209, lamda: 1.7700954675674438
Running avgs for agent 2: q_loss: 0.21750909090042114, u_loss: 83.93327331542969, p_loss: 0.14083774387836456, mean_rew: -0.04798644535310172, variance: 1.3681930303573608, cvar: -0.07205387949943542, v: -0.0891091451048851, mean_q: -0.16273006796836853, std_q: 0.6583041548728943, lamda: 1.2558499574661255
Running avgs for agent 3: q_loss: 0.19262348115444183, u_loss: 80.68547058105469, p_loss: 0.13871654868125916, mean_rew: -0.04711854357626708, variance: 0.17671857542544603, cvar: -0.07686632126569748, v: -0.09302676469087601, mean_q: -0.1542845070362091, std_q: 0.6341649293899536, lamda: 1.3012725114822388

steps: 1199975, episodes: 48000, mean episode reward: -0.5312604998367, agent episode reward: [0.46, 0.46, -0.7787359667196185, -0.6725245331170813], time: 77.427
steps: 1199975, episodes: 48000, mean episode variance: 0.35774826755933464, agent episode variance: [0.0, 0.0, 0.32482831066846846, 0.03291995689086616], time: 77.427
steps: 1199975, episodes: 48000, mean episode cvar: 14.115067462839185, agent episode cvar: [7.085221855163574, 7.067279613494873, -0.01815453091263771, -0.019279474906623364], time: 77.428
Running avgs for agent 0: q_loss: 2.488945245742798, u_loss: 784.0159301757812, p_loss: -20.825820922851562, mean_rew: 0.04296875, variance: 0.0, cvar: 28.34088897705078, v: 25.41640853881836, mean_q: 20.807601928710938, std_q: 3.240119218826294, lamda: 1.7394368648529053
Running avgs for agent 1: q_loss: 1.9855703115463257, u_loss: 909.822265625, p_loss: -22.496829986572266, mean_rew: 0.042578125, variance: 0.0, cvar: 28.26911735534668, v: 26.156917572021484, mean_q: 22.47320556640625, std_q: 2.502293586730957, lamda: 1.7830034494400024
Running avgs for agent 2: q_loss: 0.2263680398464203, u_loss: 83.84501647949219, p_loss: 0.1420709788799286, mean_rew: -0.04940022873511997, variance: 1.2993131875991821, cvar: -0.07261811941862106, v: -0.09235993772745132, mean_q: -0.16384179890155792, std_q: 0.6481202244758606, lamda: 1.268476128578186
Running avgs for agent 3: q_loss: 0.18511909246444702, u_loss: 92.39482116699219, p_loss: 0.1443840116262436, mean_rew: -0.047333021270689744, variance: 0.13167982756346464, cvar: -0.07711789757013321, v: -0.09439509361982346, mean_q: -0.160401850938797, std_q: 0.6583924293518066, lamda: 1.3114004135131836

steps: 1224975, episodes: 49000, mean episode reward: -0.04553462144684083, agent episode reward: [0.6, 0.6, -0.6401223798653006, -0.6054122415815403], time: 71.421
steps: 1224975, episodes: 49000, mean episode variance: 0.30433227052167056, agent episode variance: [0.0, 0.0, 0.3034479803591967, 0.0008842901624739171], time: 71.422
steps: 1224975, episodes: 49000, mean episode cvar: 14.125987269487233, agent episode cvar: [7.087905237197876, 7.075671005249023, -0.01895393608510494, -0.018635036874562503], time: 71.422
Running avgs for agent 0: q_loss: 2.968790054321289, u_loss: 727.5176391601562, p_loss: -20.90496063232422, mean_rew: 0.036484375, variance: 0.0, cvar: 28.351621627807617, v: 25.41602897644043, mean_q: 20.886749267578125, std_q: 3.199453115463257, lamda: 1.7573294639587402
Running avgs for agent 1: q_loss: 2.326406717300415, u_loss: 861.8668823242188, p_loss: -22.59764862060547, mean_rew: 0.0402734375, variance: 0.0, cvar: 28.302682876586914, v: 26.18155288696289, mean_q: 22.573951721191406, std_q: 2.44254207611084, lamda: 1.7953020334243774
Running avgs for agent 2: q_loss: 0.2168881744146347, u_loss: 77.55738830566406, p_loss: 0.138380765914917, mean_rew: -0.04634103813428439, variance: 1.2137919214367867, cvar: -0.07581574469804764, v: -0.08942678570747375, mean_q: -0.15642349421977997, std_q: 0.615016758441925, lamda: 1.2802354097366333
Running avgs for agent 3: q_loss: 0.15657544136047363, u_loss: 63.374061584472656, p_loss: 0.14364764094352722, mean_rew: -0.04283032646733527, variance: 0.0035371606498956682, cvar: -0.0745401456952095, v: -0.08997422456741333, mean_q: -0.15876241028308868, std_q: 0.6125345230102539, lamda: 1.3214948177337646

steps: 1249975, episodes: 50000, mean episode reward: -0.1137234007017793, agent episode reward: [0.61, 0.61, -0.6806100020719332, -0.6531133986298462], time: 71.086
steps: 1249975, episodes: 50000, mean episode variance: 0.27337598968483506, agent episode variance: [0.0, 0.0, 0.27229323372989894, 0.0010827559549361468], time: 71.087
steps: 1249975, episodes: 50000, mean episode cvar: 14.129767174892127, agent episode cvar: [7.101045833587646, 7.064584840774536, -0.017809534318745136, -0.018053965151309966], time: 71.087
Running avgs for agent 0: q_loss: 2.558938503265381, u_loss: 700.344482421875, p_loss: -21.012638092041016, mean_rew: 0.03640625, variance: 0.0, cvar: 28.404184341430664, v: 25.491710662841797, mean_q: 20.99680519104004, std_q: 3.1862294673919678, lamda: 1.773288607597351
Running avgs for agent 1: q_loss: 2.1894876956939697, u_loss: 817.2737426757812, p_loss: -22.831745147705078, mean_rew: 0.0376953125, variance: 0.0, cvar: 28.258338928222656, v: 26.122962951660156, mean_q: 22.804746627807617, std_q: 2.313241958618164, lamda: 1.808793544769287
Running avgs for agent 2: q_loss: 0.20924130082130432, u_loss: 95.18550109863281, p_loss: 0.14073924720287323, mean_rew: -0.04661171590975823, variance: 1.0891729349195958, cvar: -0.07123814523220062, v: -0.08771307021379471, mean_q: -0.15909762680530548, std_q: 0.672442615032196, lamda: 1.2929365634918213
Running avgs for agent 3: q_loss: 0.1560540497303009, u_loss: 65.2270736694336, p_loss: 0.14392736554145813, mean_rew: -0.04103764711367246, variance: 0.004331023819744587, cvar: -0.07221586257219315, v: -0.08922825753688812, mean_q: -0.16020718216896057, std_q: 0.6059495210647583, lamda: 1.33048677444458

steps: 1274975, episodes: 51000, mean episode reward: -0.11775890218986637, agent episode reward: [0.6, 0.6, -0.6474516907279608, -0.6703072114619055], time: 71.17
steps: 1274975, episodes: 51000, mean episode variance: 0.2578216669037938, agent episode variance: [0.0, 0.0, 0.2572569800615311, 0.0005646868422627449], time: 71.17
steps: 1274975, episodes: 51000, mean episode cvar: 14.11245664313063, agent episode cvar: [7.083953834533691, 7.065690788269043, -0.0190341040417552, -0.018153875630348922], time: 71.171
Running avgs for agent 0: q_loss: 2.167015552520752, u_loss: 748.1268920898438, p_loss: -21.141355514526367, mean_rew: 0.03609375, variance: 0.0, cvar: 28.335817337036133, v: 25.49129295349121, mean_q: 21.128189086914062, std_q: 3.1125521659851074, lamda: 1.7888469696044922
Running avgs for agent 1: q_loss: 2.381014347076416, u_loss: 873.259521484375, p_loss: -22.998674392700195, mean_rew: 0.036953125, variance: 0.0, cvar: 28.26276206970215, v: 26.120586395263672, mean_q: 22.977027893066406, std_q: 2.2242939472198486, lamda: 1.8210662603378296
Running avgs for agent 2: q_loss: 0.21176676452159882, u_loss: 84.841064453125, p_loss: 0.14107654988765717, mean_rew: -0.04424825583490772, variance: 1.0290279202461243, cvar: -0.07613641023635864, v: -0.08706555515527725, mean_q: -0.1553739309310913, std_q: 0.6557425856590271, lamda: 1.3009446859359741
Running avgs for agent 3: q_loss: 0.14788471162319183, u_loss: 59.46064376831055, p_loss: 0.14256343245506287, mean_rew: -0.04047404960225739, variance: 0.0022587473690509796, cvar: -0.07261551171541214, v: -0.09113489836454391, mean_q: -0.15851305425167084, std_q: 0.5910730957984924, lamda: 1.3448503017425537

steps: 1299975, episodes: 52000, mean episode reward: -0.24995987325351163, agent episode reward: [0.55, 0.55, -0.515707700279239, -0.8342521729742728], time: 71.538
steps: 1299975, episodes: 52000, mean episode variance: 0.25409363032132387, agent episode variance: [0.0, 0.0, 0.25284018921852114, 0.0012534411028027534], time: 71.538
steps: 1299975, episodes: 52000, mean episode cvar: 14.11061460531503, agent episode cvar: [7.071855405807495, 7.0736435050964355, -0.01869019777700305, -0.016194107811897993], time: 71.539
Running avgs for agent 0: q_loss: 2.079406261444092, u_loss: 725.7681274414062, p_loss: -21.212427139282227, mean_rew: 0.0350390625, variance: 0.0, cvar: 28.28742218017578, v: 25.474258422851562, mean_q: 21.202342987060547, std_q: 3.063236951828003, lamda: 1.8040515184402466
Running avgs for agent 1: q_loss: 2.2788150310516357, u_loss: 822.3563842773438, p_loss: -23.07135772705078, mean_rew: 0.03421875, variance: 0.0, cvar: 28.294574737548828, v: 26.110307693481445, mean_q: 23.05299186706543, std_q: 2.187145233154297, lamda: 1.8331819772720337
Running avgs for agent 2: q_loss: 0.21462732553482056, u_loss: 86.83219909667969, p_loss: 0.135991632938385, mean_rew: -0.0426945773583467, variance: 1.0113607568740846, cvar: -0.0747607871890068, v: -0.08800897747278214, mean_q: -0.15164077281951904, std_q: 0.641831636428833, lamda: 1.3116509914398193
Running avgs for agent 3: q_loss: 0.16184835135936737, u_loss: 66.74812316894531, p_loss: 0.13850176334381104, mean_rew: -0.039737204346363866, variance: 0.005013764411211014, cvar: -0.06477642804384232, v: -0.08628817647695541, mean_q: -0.1533059924840927, std_q: 0.6040624976158142, lamda: 1.3649829626083374

steps: 1324975, episodes: 53000, mean episode reward: -0.43635055615070484, agent episode reward: [0.47, 0.47, -0.8432412221095901, -0.5331093340411146], time: 70.991
steps: 1324975, episodes: 53000, mean episode variance: 0.2328988206461072, agent episode variance: [0.0, 0.0, 0.23199407947063447, 0.0009047411754727363], time: 70.991
steps: 1324975, episodes: 53000, mean episode cvar: 14.136047441009431, agent episode cvar: [7.094466678619384, 7.078144260406495, -0.018970362085849048, -0.017593135930597782], time: 70.992
Running avgs for agent 0: q_loss: 2.5534963607788086, u_loss: 646.5750122070312, p_loss: -21.34133529663086, mean_rew: 0.0323828125, variance: 0.0, cvar: 28.377866744995117, v: 25.538663864135742, mean_q: 21.336301803588867, std_q: 3.0408387184143066, lamda: 1.819962978363037
Running avgs for agent 1: q_loss: 1.9488189220428467, u_loss: 808.0780029296875, p_loss: -22.877511978149414, mean_rew: 0.032109375, variance: 0.0, cvar: 28.312578201293945, v: 26.07135772705078, mean_q: 22.861312866210938, std_q: 2.2554752826690674, lamda: 1.8481096029281616
Running avgs for agent 2: q_loss: 0.20317190885543823, u_loss: 81.37942504882812, p_loss: 0.13622888922691345, mean_rew: -0.04026527569539127, variance: 0.9279763178825379, cvar: -0.07588144391775131, v: -0.08957325667142868, mean_q: -0.14947755634784698, std_q: 0.6127375364303589, lamda: 1.3224414587020874
Running avgs for agent 3: q_loss: 0.1368895322084427, u_loss: 64.5608139038086, p_loss: 0.12953273952007294, mean_rew: -0.037985065807735516, variance: 0.0036189647018909452, cvar: -0.07037254422903061, v: -0.08576036989688873, mean_q: -0.14297308027744293, std_q: 0.5817869901657104, lamda: 1.3783074617385864

steps: 1349975, episodes: 54000, mean episode reward: -0.5235453216515, agent episode reward: [0.48, 0.48, -0.8324698488311181, -0.6510754728203819], time: 71.268
steps: 1349975, episodes: 54000, mean episode variance: 0.24802648884057998, agent episode variance: [0.0, 0.0, 0.24712529924511908, 0.0009011895954608917], time: 71.269
steps: 1349975, episodes: 54000, mean episode cvar: 14.129500214438885, agent episode cvar: [7.087343090057373, 7.078935056686402, -0.018047911319881677, -0.018730020985007288], time: 71.269
Running avgs for agent 0: q_loss: 2.537625789642334, u_loss: 683.3884887695312, p_loss: -21.36313819885254, mean_rew: 0.033203125, variance: 0.0, cvar: 28.34937286376953, v: 25.579057693481445, mean_q: 21.35603141784668, std_q: 3.0352671146392822, lamda: 1.8347275257110596
Running avgs for agent 1: q_loss: 2.8563642501831055, u_loss: 710.8272705078125, p_loss: -22.83903694152832, mean_rew: 0.0289453125, variance: 0.0, cvar: 28.31574058532715, v: 26.037612915039062, mean_q: 22.82133674621582, std_q: 2.2618296146392822, lamda: 1.8613786697387695
Running avgs for agent 2: q_loss: 0.1996273696422577, u_loss: 85.60922241210938, p_loss: 0.13454535603523254, mean_rew: -0.040107650278366, variance: 0.9885011969804763, cvar: -0.07219164818525314, v: -0.08804459869861603, mean_q: -0.1469012349843979, std_q: 0.6106931567192078, lamda: 1.3334741592407227
Running avgs for agent 3: q_loss: 0.14360171556472778, u_loss: 55.12289047241211, p_loss: 0.1303824782371521, mean_rew: -0.038222341204770735, variance: 0.0036047583818435668, cvar: -0.07492008805274963, v: -0.08849630504846573, mean_q: -0.14273600280284882, std_q: 0.5609716773033142, lamda: 1.3881831169128418

steps: 1374975, episodes: 55000, mean episode reward: -0.3067591221707882, agent episode reward: [0.39, 0.39, -0.583817300568475, -0.502941821602313], time: 71.239
steps: 1374975, episodes: 55000, mean episode variance: 0.22933020504936577, agent episode variance: [0.0, 0.0, 0.2285634189322591, 0.0007667861171066761], time: 71.239
steps: 1374975, episodes: 55000, mean episode cvar: 14.113099364783615, agent episode cvar: [7.081220495223999, 7.0669831447601315, -0.016892312366515397, -0.01821196283400059], time: 71.24
Running avgs for agent 0: q_loss: 2.072585344314575, u_loss: 603.2421264648438, p_loss: -21.396249771118164, mean_rew: 0.0290625, variance: 0.0, cvar: 28.32488250732422, v: 25.552230834960938, mean_q: 21.38429832458496, std_q: 3.016587972640991, lamda: 1.8512158393859863
Running avgs for agent 1: q_loss: 1.8226889371871948, u_loss: 683.8117065429688, p_loss: -22.66375160217285, mean_rew: 0.0290625, variance: 0.0, cvar: 28.267932891845703, v: 25.961685180664062, mean_q: 22.641016006469727, std_q: 2.3226099014282227, lamda: 1.8772504329681396
Running avgs for agent 2: q_loss: 0.2008471041917801, u_loss: 85.3189697265625, p_loss: 0.13813403248786926, mean_rew: -0.03934735346078364, variance: 0.9142536757290364, cvar: -0.06756924837827682, v: -0.09367378056049347, mean_q: -0.15127745270729065, std_q: 0.6017870903015137, lamda: 1.3518798351287842
Running avgs for agent 3: q_loss: 0.12684088945388794, u_loss: 50.47310256958008, p_loss: 0.13100984692573547, mean_rew: -0.03831702088541034, variance: 0.0030671444684267045, cvar: -0.07284785807132721, v: -0.08839622139930725, mean_q: -0.1439734846353531, std_q: 0.5637121796607971, lamda: 1.3970082998275757

steps: 1399975, episodes: 56000, mean episode reward: -0.35351469520328455, agent episode reward: [0.39, 0.39, -0.7211833791025695, -0.41233131610071505], time: 71.25
steps: 1399975, episodes: 56000, mean episode variance: 0.20902998200803996, agent episode variance: [0.0, 0.0, 0.20679391089826824, 0.0022360711097717287], time: 71.25
steps: 1399975, episodes: 56000, mean episode cvar: 14.122458105802536, agent episode cvar: [7.078101034164429, 7.080089973449707, -0.01780303614959121, -0.017929865662008524], time: 71.251
Running avgs for agent 0: q_loss: 3.239389657974243, u_loss: 627.1919555664062, p_loss: -21.51204490661621, mean_rew: 0.028203125, variance: 0.0, cvar: 28.31240463256836, v: 25.47657012939453, mean_q: 21.500595092773438, std_q: 2.944943904876709, lamda: 1.8688417673110962
Running avgs for agent 1: q_loss: 1.970132827758789, u_loss: 681.3636474609375, p_loss: -22.625192642211914, mean_rew: 0.0294140625, variance: 0.0, cvar: 28.32036018371582, v: 26.0372257232666, mean_q: 22.60430908203125, std_q: 2.3669493198394775, lamda: 1.8908272981643677
Running avgs for agent 2: q_loss: 0.181978240609169, u_loss: 66.94404602050781, p_loss: 0.13260257244110107, mean_rew: -0.03769659325269431, variance: 0.8271756435930729, cvar: -0.07121214270591736, v: -0.08781690895557404, mean_q: -0.14520758390426636, std_q: 0.5864666700363159, lamda: 1.3675366640090942
Running avgs for agent 3: q_loss: 0.14145293831825256, u_loss: 56.20374298095703, p_loss: 0.1326720416545868, mean_rew: -0.03809565440469716, variance: 0.008944284439086915, cvar: -0.0717194676399231, v: -0.08938080072402954, mean_q: -0.14336852729320526, std_q: 0.5485957264900208, lamda: 1.408726453781128

steps: 1424975, episodes: 57000, mean episode reward: -0.07670685851039524, agent episode reward: [0.53, 0.53, -0.5678247846779814, -0.568882073832414], time: 71.188
steps: 1424975, episodes: 57000, mean episode variance: 0.21651719012204557, agent episode variance: [0.0, 0.0, 0.20947990257665514, 0.007037287545390427], time: 71.189
steps: 1424975, episodes: 57000, mean episode cvar: 14.117207310240715, agent episode cvar: [7.081115074157715, 7.071766044616699, -0.01800790028274059, -0.017665908250957728], time: 71.19
Running avgs for agent 0: q_loss: 2.0882763862609863, u_loss: 595.5593872070312, p_loss: -21.69864845275879, mean_rew: 0.026171875, variance: 0.0, cvar: 28.324459075927734, v: 25.567262649536133, mean_q: 21.686119079589844, std_q: 2.8775298595428467, lamda: 1.8855327367782593
Running avgs for agent 1: q_loss: 1.8132301568984985, u_loss: 694.7341918945312, p_loss: -22.480573654174805, mean_rew: 0.0275390625, variance: 0.0, cvar: 28.287063598632812, v: 26.065649032592773, mean_q: 22.462610244750977, std_q: 2.4202022552490234, lamda: 1.9039270877838135
Running avgs for agent 2: q_loss: 0.183634951710701, u_loss: 66.67937469482422, p_loss: 0.13257166743278503, mean_rew: -0.03702352830591367, variance: 0.8379196103066205, cvar: -0.07203160226345062, v: -0.08717921376228333, mean_q: -0.14370271563529968, std_q: 0.555612325668335, lamda: 1.3780587911605835
Running avgs for agent 3: q_loss: 0.12477404624223709, u_loss: 52.566558837890625, p_loss: 0.1303941011428833, mean_rew: -0.035744255800037476, variance: 0.028149150181561708, cvar: -0.07066363841295242, v: -0.09007998555898666, mean_q: -0.14214970171451569, std_q: 0.5241107940673828, lamda: 1.4243769645690918

steps: 1449975, episodes: 58000, mean episode reward: 0.00900546146911411, agent episode reward: [0.55, 0.55, -0.6953780972007315, -0.39561644133015444], time: 71.115
steps: 1449975, episodes: 58000, mean episode variance: 0.2041807820070535, agent episode variance: [0.0, 0.0, 0.20161065634340047, 0.002570125663653016], time: 71.116
steps: 1449975, episodes: 58000, mean episode cvar: 14.141174709394575, agent episode cvar: [7.089142068862915, 7.087215957641601, -0.017714368656277655, -0.01746894845366478], time: 71.116
Running avgs for agent 0: q_loss: 2.6277315616607666, u_loss: 606.0418701171875, p_loss: -21.68963050842285, mean_rew: 0.0265625, variance: 0.0, cvar: 28.356569290161133, v: 25.558422088623047, mean_q: 21.679452896118164, std_q: 2.8560733795166016, lamda: 1.9010885953903198
Running avgs for agent 1: q_loss: 1.940778374671936, u_loss: 677.1085205078125, p_loss: -22.3554744720459, mean_rew: 0.0255078125, variance: 0.0, cvar: 28.34886360168457, v: 26.065351486206055, mean_q: 22.338531494140625, std_q: 2.4968950748443604, lamda: 1.918615460395813
Running avgs for agent 2: q_loss: 0.16351671516895294, u_loss: 64.07294464111328, p_loss: 0.1315084546804428, mean_rew: -0.034233773802803395, variance: 0.8064426253736019, cvar: -0.070857472717762, v: -0.08700951188802719, mean_q: -0.1424548625946045, std_q: 0.5471820831298828, lamda: 1.3932886123657227
Running avgs for agent 3: q_loss: 0.12969408929347992, u_loss: 57.54846954345703, p_loss: 0.12928172945976257, mean_rew: -0.03684475069029315, variance: 0.010280502654612065, cvar: -0.0698757916688919, v: -0.08602426201105118, mean_q: -0.14272204041481018, std_q: 0.5540609359741211, lamda: 1.436572790145874

steps: 1474975, episodes: 59000, mean episode reward: -0.05217711477264775, agent episode reward: [0.66, 0.66, -0.7626205149239074, -0.6095565998487402], time: 71.176
steps: 1474975, episodes: 59000, mean episode variance: 0.2144849048294127, agent episode variance: [0.0, 0.0, 0.21006193550676108, 0.004422969322651625], time: 71.177
steps: 1474975, episodes: 59000, mean episode cvar: 14.117108773378655, agent episode cvar: [7.062658246994019, 7.088285955429077, -0.016891386412084104, -0.016944042632356285], time: 71.177
Running avgs for agent 0: q_loss: 2.6418843269348145, u_loss: 588.6934814453125, p_loss: -21.846134185791016, mean_rew: 0.0266015625, variance: 0.0, cvar: 28.250633239746094, v: 25.463958740234375, mean_q: 21.836463928222656, std_q: 2.7356717586517334, lamda: 1.9181090593338013
Running avgs for agent 1: q_loss: 2.2143800258636475, u_loss: 597.3980712890625, p_loss: -22.224592208862305, mean_rew: 0.02625, variance: 0.0, cvar: 28.3531436920166, v: 26.115459442138672, mean_q: 22.20994758605957, std_q: 2.5673012733459473, lamda: 1.933777928352356
Running avgs for agent 2: q_loss: 0.1777307689189911, u_loss: 77.19429016113281, p_loss: 0.13304154574871063, mean_rew: -0.03444570597031191, variance: 0.8402477420270443, cvar: -0.06756555289030075, v: -0.08791819214820862, mean_q: -0.1434355229139328, std_q: 0.5892404317855835, lamda: 1.4089415073394775
Running avgs for agent 3: q_loss: 0.12383260577917099, u_loss: 51.63957214355469, p_loss: 0.12967798113822937, mean_rew: -0.03515455220793238, variance: 0.0176918772906065, cvar: -0.0677761659026146, v: -0.08614883571863174, mean_q: -0.1408478021621704, std_q: 0.5336742401123047, lamda: 1.45059335231781

steps: 1499975, episodes: 60000, mean episode reward: -0.4954560495708778, agent episode reward: [0.28, 0.28, -0.6194160777374473, -0.4360399718334304], time: 71.311
steps: 1499975, episodes: 60000, mean episode variance: 0.23821188204549254, agent episode variance: [0.0, 0.0, 0.23313802129030228, 0.0050738607551902536], time: 71.312
steps: 1499975, episodes: 60000, mean episode cvar: 14.125142856221647, agent episode cvar: [7.082629955291748, 7.076675214767456, -0.017646077018231152, -0.016516236819326877], time: 71.312
Running avgs for agent 0: q_loss: 2.228595733642578, u_loss: 579.9631958007812, p_loss: -21.939361572265625, mean_rew: 0.0252734375, variance: 0.0, cvar: 28.33051872253418, v: 25.52142906188965, mean_q: 21.933324813842773, std_q: 2.7293436527252197, lamda: 1.9339975118637085
Running avgs for agent 1: q_loss: 1.7541981935501099, u_loss: 658.4409790039062, p_loss: -22.13815689086914, mean_rew: 0.0259765625, variance: 0.0, cvar: 28.30670166015625, v: 26.00298309326172, mean_q: 22.125349044799805, std_q: 2.588597059249878, lamda: 1.9484517574310303
Running avgs for agent 2: q_loss: 0.16321641206741333, u_loss: 63.07011032104492, p_loss: 0.12713013589382172, mean_rew: -0.03286931056393138, variance: 0.9325520851612091, cvar: -0.07058430463075638, v: -0.0859188586473465, mean_q: -0.1382346749305725, std_q: 0.5338112711906433, lamda: 1.4248627424240112
Running avgs for agent 3: q_loss: 0.13845887780189514, u_loss: 44.224952697753906, p_loss: 0.12960189580917358, mean_rew: -0.034526008356445516, variance: 0.020295443020761014, cvar: -0.06606494635343552, v: -0.08473020046949387, mean_q: -0.1418333798646927, std_q: 0.526506245136261, lamda: 1.4632418155670166

steps: 1524975, episodes: 61000, mean episode reward: -0.34227616297849894, agent episode reward: [0.39, 0.39, -0.6236883513549624, -0.49858781162353655], time: 71.145
steps: 1524975, episodes: 61000, mean episode variance: 0.2615372750610113, agent episode variance: [0.0, 0.0, 0.25391498604416846, 0.007622289016842842], time: 71.146
steps: 1524975, episodes: 61000, mean episode cvar: 14.108589733902365, agent episode cvar: [7.058283798217773, 7.086269510269165, -0.017783155113458633, -0.018180419471114873], time: 71.147
Running avgs for agent 0: q_loss: 2.915487289428711, u_loss: 640.1259155273438, p_loss: -22.110427856445312, mean_rew: 0.025625, variance: 0.0, cvar: 28.233137130737305, v: 25.51665496826172, mean_q: 22.103778839111328, std_q: 2.6015279293060303, lamda: 1.95009446144104
Running avgs for agent 1: q_loss: 2.82924747467041, u_loss: 585.1680908203125, p_loss: -22.121471405029297, mean_rew: 0.0252734375, variance: 0.0, cvar: 28.345077514648438, v: 25.976600646972656, mean_q: 22.108570098876953, std_q: 2.588618278503418, lamda: 1.9621626138687134
Running avgs for agent 2: q_loss: 0.18185898661613464, u_loss: 81.4540786743164, p_loss: 0.12969492375850677, mean_rew: -0.03318424795360057, variance: 1.0156599441766738, cvar: -0.07113262265920639, v: -0.08465518802404404, mean_q: -0.13966593146324158, std_q: 0.5757912397384644, lamda: 1.436187505722046
Running avgs for agent 3: q_loss: 0.12021803855895996, u_loss: 37.81829071044922, p_loss: 0.13126149773597717, mean_rew: -0.03438521409678326, variance: 0.03048915606737137, cvar: -0.0727216824889183, v: -0.08669336140155792, mean_q: -0.14241552352905273, std_q: 0.5262468457221985, lamda: 1.4776231050491333

steps: 1549975, episodes: 62000, mean episode reward: 0.16671114584857616, agent episode reward: [0.69, 0.69, -0.76244196974391, -0.45084688440751386], time: 71.197
steps: 1549975, episodes: 62000, mean episode variance: 0.2977558875894174, agent episode variance: [0.0, 0.0, 0.28943204195797445, 0.008323845631442964], time: 71.197
steps: 1549975, episodes: 62000, mean episode cvar: 14.105693961881101, agent episode cvar: [7.0639927845001225, 7.075706041336059, -0.016963948599994182, -0.017040915355086325], time: 71.198
Running avgs for agent 0: q_loss: 3.7608251571655273, u_loss: 656.9364013671875, p_loss: -22.12571907043457, mean_rew: 0.0258203125, variance: 0.0, cvar: 28.255970001220703, v: 25.4749698638916, mean_q: 22.11406707763672, std_q: 2.5680627822875977, lamda: 1.9672529697418213
Running avgs for agent 1: q_loss: 2.094886302947998, u_loss: 584.708740234375, p_loss: -22.06598663330078, mean_rew: 0.0240625, variance: 0.0, cvar: 28.302824020385742, v: 25.813074111938477, mean_q: 22.055875778198242, std_q: 2.587238311767578, lamda: 1.9776886701583862
Running avgs for agent 2: q_loss: 0.17861592769622803, u_loss: 68.92064666748047, p_loss: 0.12861588597297668, mean_rew: -0.03348194216915481, variance: 1.1577281951904297, cvar: -0.06785579770803452, v: -0.08510066568851471, mean_q: -0.13859553635120392, std_q: 0.5628997087478638, lamda: 1.4530903100967407
Running avgs for agent 3: q_loss: 0.11521702259778976, u_loss: 40.61463928222656, p_loss: 0.12238750606775284, mean_rew: -0.03314886567562091, variance: 0.033295382525771855, cvar: -0.06816366314888, v: -0.08120288699865341, mean_q: -0.13457955420017242, std_q: 0.5175668001174927, lamda: 1.4855756759643555

steps: 1574975, episodes: 63000, mean episode reward: 0.20915143830593047, agent episode reward: [0.93, 0.93, -0.7320567830902113, -0.9187917786038581], time: 71.141
steps: 1574975, episodes: 63000, mean episode variance: 0.32243884293176234, agent episode variance: [0.0, 0.0, 0.30402853319048884, 0.01841030974127352], time: 71.142
steps: 1574975, episodes: 63000, mean episode cvar: 14.131417545810342, agent episode cvar: [7.085615253448486, 7.080014568328857, -0.01676097681745887, -0.017451299149543048], time: 71.143
Running avgs for agent 0: q_loss: 4.5482306480407715, u_loss: 617.5513916015625, p_loss: -22.299304962158203, mean_rew: 0.023984375, variance: 0.0, cvar: 28.34246063232422, v: 25.49987030029297, mean_q: 22.279605865478516, std_q: 2.5018088817596436, lamda: 1.9858170747756958
Running avgs for agent 1: q_loss: 2.637561082839966, u_loss: 625.5894165039062, p_loss: -22.096242904663086, mean_rew: 0.0258984375, variance: 0.0, cvar: 28.320058822631836, v: 25.6995906829834, mean_q: 22.088491439819336, std_q: 2.5555262565612793, lamda: 1.98992919921875
Running avgs for agent 2: q_loss: 0.1672263741493225, u_loss: 66.65298461914062, p_loss: 0.13161416351795197, mean_rew: -0.03303911844540723, variance: 1.2161141327619553, cvar: -0.06704390048980713, v: -0.09099884331226349, mean_q: -0.14107537269592285, std_q: 0.5474197864532471, lamda: 1.4767875671386719
Running avgs for agent 3: q_loss: 0.10801982134580612, u_loss: 29.359012603759766, p_loss: 0.12299498915672302, mean_rew: -0.03139288770118963, variance: 0.07364123896509409, cvar: -0.06980519741773605, v: -0.08891576528549194, mean_q: -0.13358695805072784, std_q: 0.44771093130111694, lamda: 1.5034880638122559

steps: 1599975, episodes: 64000, mean episode reward: 0.3407603238516732, agent episode reward: [0.78, 0.78, -0.562057548750463, -0.6571821273978639], time: 71.725
steps: 1599975, episodes: 64000, mean episode variance: 0.33351302581559866, agent episode variance: [0.0, 0.0, 0.309253639832139, 0.024259385983459652], time: 71.726
steps: 1599975, episodes: 64000, mean episode cvar: 14.139503611333668, agent episode cvar: [7.105084321975708, 7.070117547988891, -0.018458384629338978, -0.0172398740015924], time: 71.726
Running avgs for agent 0: q_loss: 4.830812931060791, u_loss: 559.4408569335938, p_loss: -22.31902313232422, mean_rew: 0.024453125, variance: 0.0, cvar: 28.42033576965332, v: 25.519556045532227, mean_q: 22.294567108154297, std_q: 2.5281529426574707, lamda: 2.0039477348327637
Running avgs for agent 1: q_loss: 2.172093391418457, u_loss: 605.1890258789062, p_loss: -22.101274490356445, mean_rew: 0.0245703125, variance: 0.0, cvar: 28.28046989440918, v: 25.488679885864258, mean_q: 22.089128494262695, std_q: 2.5179073810577393, lamda: 2.00433611869812
Running avgs for agent 2: q_loss: 0.15324118733406067, u_loss: 67.33094024658203, p_loss: 0.12440481036901474, mean_rew: -0.03139013155671557, variance: 1.237014651298523, cvar: -0.07383354008197784, v: -0.08712191879749298, mean_q: -0.13435663282871246, std_q: 0.5085496306419373, lamda: 1.4886178970336914
Running avgs for agent 3: q_loss: 0.11317308247089386, u_loss: 33.991634368896484, p_loss: 0.11973381042480469, mean_rew: -0.031156042437186476, variance: 0.09703754393383861, cvar: -0.06895949691534042, v: -0.08417636156082153, mean_q: -0.12965832650661469, std_q: 0.44763240218162537, lamda: 1.5191316604614258

steps: 1624975, episodes: 65000, mean episode reward: 0.08558662386248145, agent episode reward: [0.85, 0.85, -0.7165335187823202, -0.8978798573551984], time: 71.21
steps: 1624975, episodes: 65000, mean episode variance: 0.3595383646413684, agent episode variance: [0.0, 0.0, 0.32547817707061766, 0.034060187570750716], time: 71.21
steps: 1624975, episodes: 65000, mean episode cvar: 14.156748405696824, agent episode cvar: [7.1183471069335935, 7.072338682174682, -0.017618452476337552, -0.016318930935114623], time: 71.211
Running avgs for agent 0: q_loss: 3.2747135162353516, u_loss: 653.6458740234375, p_loss: -22.066295623779297, mean_rew: 0.024765625, variance: 0.0, cvar: 28.473388671875, v: 25.52561378479004, mean_q: 22.04206657409668, std_q: 2.612541437149048, lamda: 2.023252010345459
Running avgs for agent 1: q_loss: 3.50173282623291, u_loss: 627.9048461914062, p_loss: -21.947402954101562, mean_rew: 0.02578125, variance: 0.0, cvar: 28.289356231689453, v: 25.170761108398438, mean_q: 21.935688018798828, std_q: 2.5423452854156494, lamda: 2.0196828842163086
Running avgs for agent 2: q_loss: 0.15015815198421478, u_loss: 53.09549331665039, p_loss: 0.11784262210130692, mean_rew: -0.030556715500736416, variance: 1.3019126653671265, cvar: -0.070473812520504, v: -0.08371101319789886, mean_q: -0.1274053305387497, std_q: 0.4796525835990906, lamda: 1.500034213066101
Running avgs for agent 3: q_loss: 0.1055222824215889, u_loss: 35.360252380371094, p_loss: 0.11809947341680527, mean_rew: -0.030026925983068543, variance: 0.13624075028300286, cvar: -0.06527572870254517, v: -0.08555467426776886, mean_q: -0.127140074968338, std_q: 0.4350737929344177, lamda: 1.5329846143722534

steps: 1649975, episodes: 66000, mean episode reward: 0.04383534863964304, agent episode reward: [0.69, 0.69, -0.8093322234644414, -0.5268324278959154], time: 72.197
steps: 1649975, episodes: 66000, mean episode variance: 0.3855852400446311, agent episode variance: [0.0, 0.0, 0.3418121054172516, 0.04377313462737948], time: 72.198
steps: 1649975, episodes: 66000, mean episode cvar: 14.148818081315607, agent episode cvar: [7.0936674308776855, 7.089966651916504, -0.017648838967084884, -0.017167162511497736], time: 72.198
Running avgs for agent 0: q_loss: 3.2717936038970947, u_loss: 588.3729858398438, p_loss: -21.673418045043945, mean_rew: 0.0245703125, variance: 0.0, cvar: 28.374670028686523, v: 25.443248748779297, mean_q: 21.651540756225586, std_q: 2.7190101146698, lamda: 2.0414133071899414
Running avgs for agent 1: q_loss: 5.272035121917725, u_loss: 620.41357421875, p_loss: -21.88095474243164, mean_rew: 0.0256640625, variance: 0.0, cvar: 28.359867095947266, v: 24.67840576171875, mean_q: 21.864303588867188, std_q: 2.566019296646118, lamda: 2.03757905960083
Running avgs for agent 2: q_loss: 0.15834078192710876, u_loss: 65.890625, p_loss: 0.1211017444729805, mean_rew: -0.030392054097710734, variance: 1.3672484159469604, cvar: -0.07059535384178162, v: -0.08856291323900223, mean_q: -0.12886559963226318, std_q: 0.47989097237586975, lamda: 1.5124448537826538
Running avgs for agent 3: q_loss: 0.12806765735149384, u_loss: 48.417850494384766, p_loss: 0.12115016579627991, mean_rew: -0.030060413853470882, variance: 0.17509253850951792, cvar: -0.0686686560511589, v: -0.0909586027264595, mean_q: -0.12907291948795319, std_q: 0.4366563856601715, lamda: 1.5478622913360596

steps: 1674975, episodes: 67000, mean episode reward: -0.23919059871887668, agent episode reward: [0.45, 0.45, -0.5021191650760568, -0.6370714336428199], time: 71.656
steps: 1674975, episodes: 67000, mean episode variance: 0.37210753054916856, agent episode variance: [0.0, 0.0, 0.3327049760818481, 0.039402554467320446], time: 71.656
steps: 1674975, episodes: 67000, mean episode cvar: 14.162338857788592, agent episode cvar: [7.100817363739013, 7.094642395019531, -0.016008503410965203, -0.01711239755898714], time: 71.657
Running avgs for agent 0: q_loss: 2.9028921127319336, u_loss: 562.5726318359375, p_loss: -21.678651809692383, mean_rew: 0.0249609375, variance: 0.0, cvar: 28.403268814086914, v: 25.442684173583984, mean_q: 21.65962028503418, std_q: 2.7347214221954346, lamda: 2.0596396923065186
Running avgs for agent 1: q_loss: 4.022295951843262, u_loss: 671.7846069335938, p_loss: -21.819082260131836, mean_rew: 0.024921875, variance: 0.0, cvar: 28.378570556640625, v: 24.30483627319336, mean_q: 21.79222869873047, std_q: 2.596404790878296, lamda: 2.0567309856414795
Running avgs for agent 2: q_loss: 0.14970791339874268, u_loss: 59.228294372558594, p_loss: 0.12199835479259491, mean_rew: -0.029661695763379767, variance: 1.3308199043273925, cvar: -0.06403401494026184, v: -0.08671827614307404, mean_q: -0.1297788769006729, std_q: 0.4701541066169739, lamda: 1.5289738178253174
Running avgs for agent 3: q_loss: 0.11364280432462692, u_loss: 41.34313201904297, p_loss: 0.11662457883358002, mean_rew: -0.027313838168715243, variance: 0.15761021786928178, cvar: -0.06844959408044815, v: -0.08667168021202087, mean_q: -0.12386315315961838, std_q: 0.4078448414802551, lamda: 1.5651674270629883

steps: 1699975, episodes: 68000, mean episode reward: -0.14350855536507356, agent episode reward: [0.59, 0.59, -0.7048364588261772, -0.6186720965388964], time: 71.449
steps: 1699975, episodes: 68000, mean episode variance: 0.36281024884805085, agent episode variance: [0.0, 0.0, 0.3097047775685787, 0.05310547127947211], time: 71.45
steps: 1699975, episodes: 68000, mean episode cvar: 14.170521080750971, agent episode cvar: [7.111045358657837, 7.09362393951416, -0.017557772651314734, -0.016590444769710302], time: 71.451
Running avgs for agent 0: q_loss: 3.8061530590057373, u_loss: 556.7050170898438, p_loss: -21.528547286987305, mean_rew: 0.0246484375, variance: 0.0, cvar: 28.444181442260742, v: 25.495389938354492, mean_q: 21.511314392089844, std_q: 2.8199472427368164, lamda: 2.0785365104675293
Running avgs for agent 1: q_loss: 6.220518112182617, u_loss: 647.2053833007812, p_loss: -21.629438400268555, mean_rew: 0.0258203125, variance: 0.0, cvar: 28.374496459960938, v: 23.959463119506836, mean_q: 21.602434158325195, std_q: 2.665926456451416, lamda: 2.0753653049468994
Running avgs for agent 2: q_loss: 0.16530553996562958, u_loss: 54.54845428466797, p_loss: 0.12204595655202866, mean_rew: -0.029680771085330973, variance: 1.238819110274315, cvar: -0.07023108750581741, v: -0.08568046241998672, mean_q: -0.12916691601276398, std_q: 0.48434093594551086, lamda: 1.5465283393859863
Running avgs for agent 3: q_loss: 0.114862821996212, u_loss: 47.26324462890625, p_loss: 0.11593888700008392, mean_rew: -0.028503193877448756, variance: 0.21242188511788845, cvar: -0.06636178493499756, v: -0.08372610807418823, mean_q: -0.12363477796316147, std_q: 0.4245114326477051, lamda: 1.5774282217025757

steps: 1724975, episodes: 69000, mean episode reward: 0.0518039773692539, agent episode reward: [0.66, 0.66, -0.7562606484179398, -0.5119353742128063], time: 71.471
steps: 1724975, episodes: 69000, mean episode variance: 0.36357750232424585, agent episode variance: [0.0, 0.0, 0.2884738380014896, 0.07510366432275623], time: 71.471
steps: 1724975, episodes: 69000, mean episode cvar: 14.178837590079755, agent episode cvar: [7.102739818572998, 7.111578319549561, -0.01817596497386694, -0.017304583068937063], time: 71.472
Running avgs for agent 0: q_loss: 3.5450599193573, u_loss: 536.8287963867188, p_loss: -21.532939910888672, mean_rew: 0.0250390625, variance: 0.0, cvar: 28.410959243774414, v: 25.4669132232666, mean_q: 21.518484115600586, std_q: 2.798969268798828, lamda: 2.095939874649048
Running avgs for agent 1: q_loss: 7.438615322113037, u_loss: 617.9054565429688, p_loss: -21.578487396240234, mean_rew: 0.0256640625, variance: 0.0, cvar: 28.446311950683594, v: 23.677988052368164, mean_q: 21.552335739135742, std_q: 2.7080330848693848, lamda: 2.0944769382476807
Running avgs for agent 2: q_loss: 0.1551702916622162, u_loss: 54.72744369506836, p_loss: 0.12097538262605667, mean_rew: -0.030693311284134252, variance: 1.1538953520059585, cvar: -0.07270386070013046, v: -0.08591275662183762, mean_q: -0.12909933924674988, std_q: 0.4763964116573334, lamda: 1.5571682453155518
Running avgs for agent 3: q_loss: 0.10947652906179428, u_loss: 40.6069221496582, p_loss: 0.1127970814704895, mean_rew: -0.026994310536170085, variance: 0.3004146572910249, cvar: -0.06921833753585815, v: -0.08132793009281158, mean_q: -0.1208784431219101, std_q: 0.4244091808795929, lamda: 1.591662883758545

steps: 1749975, episodes: 70000, mean episode reward: 0.12062496748105407, agent episode reward: [0.59, 0.59, -0.5895737239959666, -0.4698013085229794], time: 71.945
steps: 1749975, episodes: 70000, mean episode variance: 0.39182667015492917, agent episode variance: [0.0, 0.0, 0.27662327465415, 0.11520339550077915], time: 71.946
steps: 1749975, episodes: 70000, mean episode cvar: 14.163182591075078, agent episode cvar: [7.086103084564209, 7.111178646087646, -0.017778242472559214, -0.016320897104218603], time: 71.946
Running avgs for agent 0: q_loss: 3.2889230251312256, u_loss: 553.1734008789062, p_loss: -21.68315315246582, mean_rew: 0.0240625, variance: 0.0, cvar: 28.344411849975586, v: 25.37977409362793, mean_q: 21.67106056213379, std_q: 2.7208101749420166, lamda: 2.112611770629883
Running avgs for agent 1: q_loss: 5.967220783233643, u_loss: 580.6983032226562, p_loss: -21.39130210876465, mean_rew: 0.023359375, variance: 0.0, cvar: 28.44471549987793, v: 23.466527938842773, mean_q: 21.36394500732422, std_q: 2.7899155616760254, lamda: 2.1139135360717773
Running avgs for agent 2: q_loss: 0.1419113129377365, u_loss: 46.205074310302734, p_loss: 0.118138886988163, mean_rew: -0.028611470103245537, variance: 1.1064931154251099, cvar: -0.0711129754781723, v: -0.08519023656845093, mean_q: -0.12631306052207947, std_q: 0.43808338046073914, lamda: 1.5681769847869873
Running avgs for agent 3: q_loss: 0.11988244950771332, u_loss: 56.648372650146484, p_loss: 0.12223450839519501, mean_rew: -0.02689532930501386, variance: 0.4608135820031166, cvar: -0.06528358161449432, v: -0.08914609998464584, mean_q: -0.12823906540870667, std_q: 0.44682586193084717, lamda: 1.6087679862976074

steps: 1774975, episodes: 71000, mean episode reward: 0.1974921397474948, agent episode reward: [0.78, 0.78, -0.5333229752868419, -0.8291848849656632], time: 71.718
steps: 1774975, episodes: 71000, mean episode variance: 0.390216024979949, agent episode variance: [0.0, 0.0, 0.25719451951980593, 0.1330215054601431], time: 71.718
steps: 1774975, episodes: 71000, mean episode cvar: 14.140171232467518, agent episode cvar: [7.075513824462891, 7.098390430450439, -0.01625783368386328, -0.01747518876194954], time: 71.719
Running avgs for agent 0: q_loss: 3.849738836288452, u_loss: 545.7852783203125, p_loss: -21.71457862854004, mean_rew: 0.0246484375, variance: 0.0, cvar: 28.30205535888672, v: 25.376300811767578, mean_q: 21.702566146850586, std_q: 2.6352734565734863, lamda: 2.1287612915039062
Running avgs for agent 1: q_loss: 9.438159942626953, u_loss: 642.7157592773438, p_loss: -21.101083755493164, mean_rew: 0.0240234375, variance: 0.0, cvar: 28.39356231689453, v: 23.235523223876953, mean_q: 21.070547103881836, std_q: 2.858870029449463, lamda: 2.1330599784851074
Running avgs for agent 2: q_loss: 0.14766377210617065, u_loss: 47.658329010009766, p_loss: 0.12003243714570999, mean_rew: -0.028716311852246088, variance: 1.028778076171875, cvar: -0.06503133475780487, v: -0.0857505202293396, mean_q: -0.12804093956947327, std_q: 0.4530929625034332, lamda: 1.5807641744613647
Running avgs for agent 3: q_loss: 0.13989676535129547, u_loss: 48.158721923828125, p_loss: 0.11618432402610779, mean_rew: -0.026566585554321242, variance: 0.5320860218405724, cvar: -0.0699007585644722, v: -0.08730972558259964, mean_q: -0.12278185784816742, std_q: 0.4239446222782135, lamda: 1.6255149841308594

steps: 1799975, episodes: 72000, mean episode reward: 0.19628045292559035, agent episode reward: [1.15, 1.15, -0.8214394081908295, -1.28228013888358], time: 71.746
steps: 1799975, episodes: 72000, mean episode variance: 0.45018951765447857, agent episode variance: [0.0, 0.0, 0.2752988261282444, 0.17489069152623415], time: 71.747
steps: 1799975, episodes: 72000, mean episode cvar: 14.148401500299572, agent episode cvar: [7.077100870132447, 7.1054205875396725, -0.016975867558270692, -0.017144089814275502], time: 71.747
Running avgs for agent 0: q_loss: 5.404112815856934, u_loss: 610.3990478515625, p_loss: -22.194398880004883, mean_rew: 0.02515625, variance: 0.0, cvar: 28.30840301513672, v: 25.27751922607422, mean_q: 22.183320999145508, std_q: 2.429917812347412, lamda: 2.145019054412842
Running avgs for agent 1: q_loss: 6.706015110015869, u_loss: 600.5941162109375, p_loss: -21.218385696411133, mean_rew: 0.02296875, variance: 0.0, cvar: 28.421682357788086, v: 23.3237247467041, mean_q: 21.181636810302734, std_q: 2.8466503620147705, lamda: 2.15152645111084
Running avgs for agent 2: q_loss: 0.13657262921333313, u_loss: 52.635414123535156, p_loss: 0.12103678286075592, mean_rew: -0.02828590817293327, variance: 1.1011953045129776, cvar: -0.06790347397327423, v: -0.08933831751346588, mean_q: -0.12863312661647797, std_q: 0.45010340213775635, lamda: 1.600244164466858
Running avgs for agent 3: q_loss: 0.1401543915271759, u_loss: 51.49724578857422, p_loss: 0.11693032830953598, mean_rew: -0.027899986385076486, variance: 0.6995627661049366, cvar: -0.06857636570930481, v: -0.08775945752859116, mean_q: -0.12266883999109268, std_q: 0.4116072952747345, lamda: 1.6377753019332886

steps: 1824975, episodes: 73000, mean episode reward: 0.9485892250297914, agent episode reward: [1.9, 1.9, -1.358773614152547, -1.4926371608176618], time: 71.89
steps: 1824975, episodes: 73000, mean episode variance: 0.5020618878155947, agent episode variance: [0.0, 0.0, 0.29215823630988597, 0.2099036515057087], time: 71.891
steps: 1824975, episodes: 73000, mean episode cvar: 14.13018293569237, agent episode cvar: [7.089056182861328, 7.07567896270752, -0.016683261707425116, -0.0178689481690526], time: 71.891
Running avgs for agent 0: q_loss: 4.27674674987793, u_loss: 653.7683715820312, p_loss: -22.220388412475586, mean_rew: 0.0235546875, variance: 0.0, cvar: 28.356224060058594, v: 25.308202743530273, mean_q: 22.213624954223633, std_q: 2.442002058029175, lamda: 2.1610071659088135
Running avgs for agent 1: q_loss: 6.953611850738525, u_loss: 677.6572265625, p_loss: -21.09073829650879, mean_rew: 0.0251171875, variance: 0.0, cvar: 28.302717208862305, v: 23.354942321777344, mean_q: 21.042341232299805, std_q: 2.8412578105926514, lamda: 2.171154737472534
Running avgs for agent 2: q_loss: 0.15100060403347015, u_loss: 57.24198532104492, p_loss: 0.12315551936626434, mean_rew: -0.02917744867183832, variance: 1.1686329452395439, cvar: -0.06673304736614227, v: -0.08758033812046051, mean_q: -0.130090594291687, std_q: 0.4578479528427124, lamda: 1.6188533306121826
Running avgs for agent 3: q_loss: 0.1403985470533371, u_loss: 43.949100494384766, p_loss: 0.11782234162092209, mean_rew: -0.027224915979108102, variance: 0.8396146060228348, cvar: -0.07147578150033951, v: -0.0872228592634201, mean_q: -0.12396731972694397, std_q: 0.40447139739990234, lamda: 1.6497735977172852

steps: 1849975, episodes: 74000, mean episode reward: -0.30498845173456457, agent episode reward: [0.95, 0.95, -0.7781042681331073, -1.4268841836014572], time: 71.706
steps: 1849975, episodes: 74000, mean episode variance: 0.5382742191702128, agent episode variance: [0.0, 0.0, 0.3139115878045559, 0.22436263136565684], time: 71.707
steps: 1849975, episodes: 74000, mean episode cvar: 14.112931888408959, agent episode cvar: [7.070834161758423, 7.07738657951355, -0.01745753265172243, -0.017831320211291312], time: 71.707
Running avgs for agent 0: q_loss: 3.654259204864502, u_loss: 659.8592529296875, p_loss: -22.26371955871582, mean_rew: 0.0255078125, variance: 0.0, cvar: 28.28333854675293, v: 25.29993438720703, mean_q: 22.240589141845703, std_q: 2.364485740661621, lamda: 2.1754848957061768
Running avgs for agent 1: q_loss: 5.250665187835693, u_loss: 665.4392700195312, p_loss: -21.216983795166016, mean_rew: 0.0250390625, variance: 0.0, cvar: 28.309547424316406, v: 23.361249923706055, mean_q: 21.159765243530273, std_q: 2.8019542694091797, lamda: 2.1909902095794678
Running avgs for agent 2: q_loss: 0.1717178076505661, u_loss: 57.83271789550781, p_loss: 0.12451761960983276, mean_rew: -0.029626820803428443, variance: 1.2556464672088623, cvar: -0.06983012706041336, v: -0.08682600408792496, mean_q: -0.13094085454940796, std_q: 0.46206793189048767, lamda: 1.6310138702392578
Running avgs for agent 3: q_loss: 0.1499325931072235, u_loss: 46.19585037231445, p_loss: 0.11423418670892715, mean_rew: -0.0282803136142021, variance: 0.8974505254626274, cvar: -0.07132527977228165, v: -0.08611010760068893, mean_q: -0.12167587131261826, std_q: 0.4086258113384247, lamda: 1.6626293659210205

steps: 1874975, episodes: 75000, mean episode reward: 0.22290043409274302, agent episode reward: [0.94, 0.94, -0.735388500076045, -0.9217110658312121], time: 72.076
steps: 1874975, episodes: 75000, mean episode variance: 0.5751335259377957, agent episode variance: [0.0, 0.0, 0.3333887759447098, 0.24174474999308587], time: 72.076
steps: 1874975, episodes: 75000, mean episode cvar: 14.09480657717213, agent episode cvar: [7.052433624267578, 7.077081298828125, -0.0174424184076488, -0.017265927515923978], time: 72.077
Running avgs for agent 0: q_loss: 3.70125675201416, u_loss: 665.0083618164062, p_loss: -22.815486907958984, mean_rew: 0.0255859375, variance: 0.0, cvar: 28.209733963012695, v: 25.177663803100586, mean_q: 22.806106567382812, std_q: 2.1180453300476074, lamda: 2.1896212100982666
Running avgs for agent 1: q_loss: 7.245710849761963, u_loss: 749.3579711914062, p_loss: -21.241060256958008, mean_rew: 0.0257421875, variance: 0.0, cvar: 28.308324813842773, v: 23.452054977416992, mean_q: 21.180679321289062, std_q: 2.7991130352020264, lamda: 2.2105448246002197
Running avgs for agent 2: q_loss: 0.15939587354660034, u_loss: 60.55625915527344, p_loss: 0.12150819599628448, mean_rew: -0.027837885043291356, variance: 1.3335551037788391, cvar: -0.06976967304944992, v: -0.08580143749713898, mean_q: -0.12728676199913025, std_q: 0.4425174593925476, lamda: 1.6416511535644531
Running avgs for agent 3: q_loss: 0.15569114685058594, u_loss: 49.16298294067383, p_loss: 0.11524742841720581, mean_rew: -0.029491999018346024, variance: 0.9669789999723435, cvar: -0.0690637081861496, v: -0.08650150895118713, mean_q: -0.12234756350517273, std_q: 0.42167821526527405, lamda: 1.6737078428268433

steps: 1899975, episodes: 76000, mean episode reward: -0.46276303670849456, agent episode reward: [0.64, 0.64, -0.8224686672799401, -0.9202943694285546], time: 71.886
steps: 1899975, episodes: 76000, mean episode variance: 0.5920791399180889, agent episode variance: [0.0, 0.0, 0.3462161656022072, 0.24586297431588172], time: 71.886
steps: 1899975, episodes: 76000, mean episode cvar: 14.124768848381937, agent episode cvar: [7.060173530578613, 7.099362449645996, -0.017144102986902, -0.017623028855770825], time: 71.887
Running avgs for agent 0: q_loss: 3.3383519649505615, u_loss: 689.4512329101562, p_loss: -23.102651596069336, mean_rew: 0.024921875, variance: 0.0, cvar: 28.24069595336914, v: 25.156587600708008, mean_q: 23.098037719726562, std_q: 2.021507978439331, lamda: 2.2014002799987793
Running avgs for agent 1: q_loss: 9.261655807495117, u_loss: 780.0397338867188, p_loss: -21.367141723632812, mean_rew: 0.0262890625, variance: 0.0, cvar: 28.397451400756836, v: 23.382089614868164, mean_q: 21.30724334716797, std_q: 2.7732534408569336, lamda: 2.229499101638794
Running avgs for agent 2: q_loss: 0.17939481139183044, u_loss: 64.74423217773438, p_loss: 0.12303220480680466, mean_rew: -0.029062439094801766, variance: 1.3848646624088288, cvar: -0.06857641786336899, v: -0.0841381847858429, mean_q: -0.12855835258960724, std_q: 0.48816779255867004, lamda: 1.6580889225006104
Running avgs for agent 3: q_loss: 0.15402734279632568, u_loss: 59.45463943481445, p_loss: 0.11544974148273468, mean_rew: -0.029656444666264523, variance: 0.9834518972635269, cvar: -0.07049211114645004, v: -0.08557116985321045, mean_q: -0.1237354576587677, std_q: 0.4568087160587311, lamda: 1.687117099761963

steps: 1924975, episodes: 77000, mean episode reward: -0.009932655953883297, agent episode reward: [0.99, 0.99, -1.0094366019441297, -0.9804960540097536], time: 71.941
steps: 1924975, episodes: 77000, mean episode variance: 0.6247516918666661, agent episode variance: [0.0, 0.0, 0.3592104237675667, 0.2655412680990994], time: 71.942
steps: 1924975, episodes: 77000, mean episode cvar: 14.14366615234688, agent episode cvar: [7.080696838378906, 7.098085466384887, -0.017391211364418267, -0.017724941052496432], time: 71.942
Running avgs for agent 0: q_loss: 3.9243576526641846, u_loss: 688.110107421875, p_loss: -23.280410766601562, mean_rew: 0.02390625, variance: 0.0, cvar: 28.32278823852539, v: 25.187793731689453, mean_q: 23.27382469177246, std_q: 1.971765398979187, lamda: 2.2139828205108643
Running avgs for agent 1: q_loss: 6.865991115570068, u_loss: 705.2047119140625, p_loss: -21.462968826293945, mean_rew: 0.0252734375, variance: 0.0, cvar: 28.392343521118164, v: 23.375455856323242, mean_q: 21.402097702026367, std_q: 2.7362148761749268, lamda: 2.248903751373291
Running avgs for agent 2: q_loss: 0.17574112117290497, u_loss: 66.84172058105469, p_loss: 0.1253938227891922, mean_rew: -0.029287408666842124, variance: 1.4368416950702667, cvar: -0.06956484913825989, v: -0.08706626296043396, mean_q: -0.1314629316329956, std_q: 0.4977133870124817, lamda: 1.6715853214263916
Running avgs for agent 3: q_loss: 0.1536533236503601, u_loss: 52.442073822021484, p_loss: 0.11687551438808441, mean_rew: -0.028814584762403332, variance: 1.0621650723963976, cvar: -0.07089976221323013, v: -0.08829760551452637, mean_q: -0.12465695291757584, std_q: 0.42058295011520386, lamda: 1.7006598711013794

steps: 1949975, episodes: 78000, mean episode reward: -0.5015678104654643, agent episode reward: [0.41, 0.41, -0.7374347902262204, -0.584133020239244], time: 71.694
steps: 1949975, episodes: 78000, mean episode variance: 0.632018173456192, agent episode variance: [0.0, 0.0, 0.3789663488864899, 0.25305182456970216], time: 71.694
steps: 1949975, episodes: 78000, mean episode cvar: 14.145102279331535, agent episode cvar: [7.075776699066162, 7.103727632522583, -0.016865730542689562, -0.017536321714520454], time: 71.695
Running avgs for agent 0: q_loss: 5.0767598152160645, u_loss: 669.5536499023438, p_loss: -23.410749435424805, mean_rew: 0.0237890625, variance: 0.0, cvar: 28.3031063079834, v: 25.124574661254883, mean_q: 23.403629302978516, std_q: 1.8832813501358032, lamda: 2.2300243377685547
Running avgs for agent 1: q_loss: 7.939187526702881, u_loss: 674.48486328125, p_loss: -21.441564559936523, mean_rew: 0.0251171875, variance: 0.0, cvar: 28.41490936279297, v: 23.294950485229492, mean_q: 21.386035919189453, std_q: 2.7446842193603516, lamda: 2.268090009689331
Running avgs for agent 2: q_loss: 0.16986776888370514, u_loss: 59.584354400634766, p_loss: 0.11993130296468735, mean_rew: -0.02875947592177279, variance: 1.5158653259277344, cvar: -0.06746292114257812, v: -0.08524558693170547, mean_q: -0.1259673833847046, std_q: 0.4754681885242462, lamda: 1.6884973049163818
Running avgs for agent 3: q_loss: 0.13975058495998383, u_loss: 45.203407287597656, p_loss: 0.11716798692941666, mean_rew: -0.028428962175744782, variance: 1.0122072982788086, cvar: -0.07014527916908264, v: -0.0868692621588707, mean_q: -0.12451370060443878, std_q: 0.42327404022216797, lamda: 1.7148774862289429

steps: 1974975, episodes: 79000, mean episode reward: 0.14287777644454916, agent episode reward: [0.84, 0.84, -0.5921859142690695, -0.9449363092863814], time: 71.824
steps: 1974975, episodes: 79000, mean episode variance: 0.6274315780252219, agent episode variance: [0.0, 0.0, 0.3898613122403622, 0.23757026578485965], time: 71.824
steps: 1974975, episodes: 79000, mean episode cvar: 14.137385356102138, agent episode cvar: [7.064467697143555, 7.108095886230469, -0.017479857716709374, -0.017698369555175304], time: 71.825
Running avgs for agent 0: q_loss: 3.840118646621704, u_loss: 715.0615234375, p_loss: -23.379911422729492, mean_rew: 0.0233984375, variance: 0.0, cvar: 28.257871627807617, v: 25.056737899780273, mean_q: 23.36399269104004, std_q: 1.9106366634368896, lamda: 2.2450292110443115
Running avgs for agent 1: q_loss: 6.899425983428955, u_loss: 668.2140502929688, p_loss: -21.51310157775879, mean_rew: 0.0249609375, variance: 0.0, cvar: 28.432382583618164, v: 23.342214584350586, mean_q: 21.456777572631836, std_q: 2.711538553237915, lamda: 2.286614418029785
Running avgs for agent 2: q_loss: 0.15449661016464233, u_loss: 54.148921966552734, p_loss: 0.1178971379995346, mean_rew: -0.028364212679541503, variance: 1.5594452619552612, cvar: -0.06991943717002869, v: -0.08517463505268097, mean_q: -0.12395820021629333, std_q: 0.4286849796772003, lamda: 1.698908805847168
Running avgs for agent 3: q_loss: 0.14768920838832855, u_loss: 61.591922760009766, p_loss: 0.11911571025848389, mean_rew: -0.028862882241224675, variance: 0.9502810631394386, cvar: -0.07079347968101501, v: -0.08544553071260452, mean_q: -0.12654806673526764, std_q: 0.44944170117378235, lamda: 1.7280961275100708

steps: 1999975, episodes: 80000, mean episode reward: -0.0505892073737669, agent episode reward: [0.51, 0.51, -0.5894651594960549, -0.48112404787771196], time: 71.815
steps: 1999975, episodes: 80000, mean episode variance: 0.6364936810731888, agent episode variance: [0.0, 0.0, 0.37943633329868315, 0.2570573477745056], time: 71.816
steps: 1999975, episodes: 80000, mean episode cvar: 14.196517513044178, agent episode cvar: [7.10107869720459, 7.130806276321411, -0.01745101312175393, -0.01791644736006856], time: 71.816
Running avgs for agent 0: q_loss: 5.61821174621582, u_loss: 707.431640625, p_loss: -23.358177185058594, mean_rew: 0.02546875, variance: 0.0, cvar: 28.404314041137695, v: 25.01506233215332, mean_q: 23.32500457763672, std_q: 1.951522707939148, lamda: 2.2614383697509766
Running avgs for agent 1: q_loss: 7.020693302154541, u_loss: 615.0399169921875, p_loss: -21.595420837402344, mean_rew: 0.0239453125, variance: 0.0, cvar: 28.523223876953125, v: 23.439651489257812, mean_q: 21.544164657592773, std_q: 2.709190845489502, lamda: 2.303854465484619
Running avgs for agent 2: q_loss: 0.15984958410263062, u_loss: 61.668312072753906, p_loss: 0.11743248254060745, mean_rew: -0.028626026062145895, variance: 1.5177453331947326, cvar: -0.06980405002832413, v: -0.08295823633670807, mean_q: -0.12428193539381027, std_q: 0.45222559571266174, lamda: 1.7089565992355347
Running avgs for agent 3: q_loss: 0.1361866146326065, u_loss: 48.73794937133789, p_loss: 0.11969249695539474, mean_rew: -0.027465672190713476, variance: 1.0282293910980225, cvar: -0.07166579365730286, v: -0.0876401960849762, mean_q: -0.12773029506206512, std_q: 0.4358426332473755, lamda: 1.741023302078247/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)


...Finished total of 80001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -0.22990295985656964, agent episode reward: [0.86, 0.86, -1.0211976538543572, -0.9287053060022121], time: 47.327
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 47.328
steps: 24975, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0, 0.0], time: 47.328
Running avgs for agent 0: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, u_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -0.43300448933389135, agent episode reward: [0.6, 0.6, -0.8930490255059161, -0.7399554638279751], time: 61.109
steps: 49975, episodes: 2000, mean episode variance: 0.6852959229536355, agent episode variance: [0.0, 0.0, 0.6616950271129608, 0.023600895840674638], time: 61.109
steps: 49975, episodes: 2000, mean episode cvar: 12.746320023793727, agent episode cvar: [5.829612951278687, 6.953520050048828, -0.020156661488115787, -0.0166563160456717], time: 61.11
Running avgs for agent 0: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: 0.03205846567622951, variance: 0.0, cvar: 23.891857147216797, v: 23.478837966918945, mean_q: 22.7132511138916, std_q: 0.5596036314964294, lamda: 2.2697913646698
Running avgs for agent 1: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: 0.03281890368852459, variance: 0.0, cvar: 28.498035430908203, v: 23.548051834106445, mean_q: 22.43111228942871, std_q: 2.3562679290771484, lamda: 2.312778949737549
Running avgs for agent 2: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.03971335304126498, variance: 2.711864709854126, cvar: -0.08260927349328995, v: -0.09096905589103699, mean_q: -0.1468064934015274, std_q: 0.46913886070251465, lamda: 1.7141379117965698
Running avgs for agent 3: q_loss: 0.0, u_loss: 0.0, p_loss: 0.0, mean_rew: -0.03748698042536302, variance: 0.09672498295358459, cvar: -0.06826359778642654, v: -0.07432393729686737, mean_q: -0.11698751151561737, std_q: 0.48591625690460205, lamda: 1.7487753629684448

...Finished total of 2001 episodes with the fixed policy.
