WARNING: Logging before flag parsing goes to stderr.
W0826 22:10:03.743330 4557272512 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0826 22:10:03.743541 4557272512 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-26 22:10:03.743902: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0826 22:10:03.746495 4557272512 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0826 22:10:03.748464 4557272512 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0826 22:10:03.748578 4557272512 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0826 22:10:03.748648 4557272512 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0826 22:10:04.104247 4557272512 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0826 22:10:04.237525 4557272512 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0826 22:10:04.245232 4557272512 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0826 22:10:04.685271 4557272512 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation False
adversary agent:  28.72614097595215
adversary agent:  28.72614097595215
good agent:  -0.0656009316444397
good agent:  -0.0656009316444397
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -16.96266047467842, agent episode reward: [3.22, 3.22, -8.266622415779285, -15.136038058899135], time: 39.853
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 39.854
steps: 24975, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0, 0.0], time: 39.854
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -9.313076101725764, agent episode reward: [5.31, 5.31, -9.950468515443575, -9.982607586282189], time: 70.01
steps: 49975, episodes: 2000, mean episode variance: 0.2914575292468071, agent episode variance: [0.000331050843000412, 0.00037032461166381834, 0.12620274157077074, 0.16455341222137213], time: 70.01
steps: 49975, episodes: 2000, mean episode cvar: 12.600877527476289, agent episode cvar: [6.271817571640015, 6.269152195692063, 0.029116484203375877, 0.030791275940835475], time: 70.011
Running avgs for agent 0: q_loss: 28.524497985839844, p_loss: -4.993574619293213, mean_rew: 0.14072105532786885, variance: 0.0013567657500016884, cvar: 25.70417022705078, v: 2.613095998764038, mean_q: 4.92098331451416, std_q: 1.0024887323379517, lamda: 1.0051769018173218
Running avgs for agent 1: q_loss: 23.849082946777344, p_loss: -4.987241744995117, mean_rew: 0.1426021388319672, variance: 0.0015177238182943376, cvar: 25.693246841430664, v: 2.613095998764038, mean_q: 4.917862892150879, std_q: 1.0928101539611816, lamda: 1.0048271417617798
Running avgs for agent 2: q_loss: 0.8069111704826355, p_loss: 0.9074024558067322, mean_rew: -0.42928363931835545, variance: 0.51722435069988, cvar: 0.11932985484600067, v: 0.08343948423862457, mean_q: -0.9952083230018616, std_q: 1.486440658569336, lamda: 1.0004647970199585
Running avgs for agent 3: q_loss: 1.0127791166305542, p_loss: 1.070233941078186, mean_rew: -0.590788814650564, variance: 0.6743992304154596, cvar: 0.12619374692440033, v: 0.07136579602956772, mean_q: -1.1850252151489258, std_q: 1.750903844833374, lamda: 1.0000079870224

steps: 74975, episodes: 3000, mean episode reward: 5.571254380785896, agent episode reward: [6.19, 6.19, -3.1547678590056605, -3.653977760208445], time: 66.486
steps: 74975, episodes: 3000, mean episode variance: 0.4703501985054463, agent episode variance: [0.0, 0.0, 0.21484628215432167, 0.2555039163511246], time: 66.487
steps: 74975, episodes: 3000, mean episode cvar: 13.487159330032766, agent episode cvar: [6.690978761672974, 6.827833276748657, -0.0142363427169621, -0.017416365671902894], time: 66.487
Running avgs for agent 0: q_loss: 11.343300819396973, p_loss: -8.037419319152832, mean_rew: 0.1891796875, variance: 0.0, cvar: 26.763914108276367, v: 5.9084248542785645, mean_q: 7.9655938148498535, std_q: 1.3683840036392212, lamda: 1.012141227722168
Running avgs for agent 1: q_loss: 17.229188919067383, p_loss: -8.015263557434082, mean_rew: 0.185859375, variance: 0.0, cvar: 27.31133270263672, v: 5.9084248542785645, mean_q: 7.961958885192871, std_q: 1.7652822732925415, lamda: 1.0180519819259644
Running avgs for agent 2: q_loss: 0.8921859264373779, p_loss: 0.7401246428489685, mean_rew: -0.3174485615555252, variance: 0.8593851286172867, cvar: -0.05694537237286568, v: -0.10971854627132416, mean_q: -0.9044708013534546, std_q: 2.349046468734741, lamda: 1.000931978225708
Running avgs for agent 3: q_loss: 1.0516133308410645, p_loss: 1.1107797622680664, mean_rew: -0.4386735060189098, variance: 1.0220156654044985, cvar: -0.06966546922922134, v: -0.15565460920333862, mean_q: -1.3333030939102173, std_q: 2.9910714626312256, lamda: 1.0011601448059082

steps: 99975, episodes: 4000, mean episode reward: 3.950974546536575, agent episode reward: [5.05, 5.05, -3.8779521165599373, -2.2710733369034877], time: 67.607
steps: 99975, episodes: 4000, mean episode variance: 0.5691689473465085, agent episode variance: [0.0, 0.0, 0.2569386034011841, 0.3122303439453244], time: 67.607
steps: 99975, episodes: 4000, mean episode cvar: 13.748605731140822, agent episode cvar: [6.864332624435425, 6.9326732616424565, -0.021473976258188487, -0.0269261786788702], time: 67.608
Running avgs for agent 0: q_loss: 13.612646102905273, p_loss: -10.772324562072754, mean_rew: 0.194453125, variance: 0.0, cvar: 27.457332611083984, v: 9.082537651062012, mean_q: 10.710074424743652, std_q: 2.052915573120117, lamda: 1.0269192457199097
Running avgs for agent 1: q_loss: 18.10604476928711, p_loss: -10.606706619262695, mean_rew: 0.192421875, variance: 0.0, cvar: 27.73069190979004, v: 9.082537651062012, mean_q: 10.531413078308105, std_q: 2.590749502182007, lamda: 1.0352256298065186
Running avgs for agent 2: q_loss: 1.0202142000198364, p_loss: 0.7650279402732849, mean_rew: -0.2664717273542276, variance: 1.0277544136047363, cvar: -0.08589590340852737, v: -0.11407160013914108, mean_q: -0.9280869364738464, std_q: 2.758218765258789, lamda: 1.0014618635177612
Running avgs for agent 3: q_loss: 1.0748722553253174, p_loss: 1.1029794216156006, mean_rew: -0.34026519318784054, variance: 1.2489213757812976, cvar: -0.10770471394062042, v: -0.19176675379276276, mean_q: -1.3438512086868286, std_q: 3.507336378097534, lamda: 1.0037707090377808

steps: 124975, episodes: 5000, mean episode reward: 4.514148691307239, agent episode reward: [5.29, 5.29, -3.3347112933472096, -2.7311400153455514], time: 68.299
steps: 124975, episodes: 5000, mean episode variance: 0.5661622552946209, agent episode variance: [0.0, 0.0, 0.2865334416106343, 0.2796288136839867], time: 68.3
steps: 124975, episodes: 5000, mean episode cvar: 13.81858413632959, agent episode cvar: [6.873720294952393, 6.9978671054840085, -0.026009457141160965, -0.02699380696564913], time: 68.3
Running avgs for agent 0: q_loss: 10.518129348754883, p_loss: -13.193941116333008, mean_rew: 0.19875, variance: 0.0, cvar: 27.49488067626953, v: 12.120458602905273, mean_q: 13.130672454833984, std_q: 2.468468189239502, lamda: 1.0443674325942993
Running avgs for agent 1: q_loss: 11.8028564453125, p_loss: -12.843323707580566, mean_rew: 0.2005859375, variance: 0.0, cvar: 27.99146842956543, v: 12.120458602905273, mean_q: 12.789097785949707, std_q: 3.1809773445129395, lamda: 1.0531233549118042
Running avgs for agent 2: q_loss: 1.0737224817276, p_loss: 0.7406116724014282, mean_rew: -0.24263404033519953, variance: 1.1461337664425373, cvar: -0.1040378287434578, v: -0.12337402999401093, mean_q: -0.8928870558738708, std_q: 2.9743661880493164, lamda: 1.0020323991775513
Running avgs for agent 3: q_loss: 1.0138564109802246, p_loss: 1.0049083232879639, mean_rew: -0.2847077648055748, variance: 1.1185152547359467, cvar: -0.10797523707151413, v: -0.18034231662750244, mean_q: -1.225585699081421, std_q: 3.5744550228118896, lamda: 1.007887840270996

steps: 149975, episodes: 6000, mean episode reward: 3.259877985918049, agent episode reward: [4.13, 4.13, -2.885007822742285, -2.115114191339666], time: 67.928
steps: 149975, episodes: 6000, mean episode variance: 0.5426574124339968, agent episode variance: [0.0, 0.019422953009605406, 0.2774387977346778, 0.2457956616897136], time: 67.929
steps: 149975, episodes: 6000, mean episode cvar: 13.834115919936448, agent episode cvar: [6.863096513748169, 7.016536252975464, -0.026255242738872767, -0.01926160404831171], time: 67.929
Running avgs for agent 0: q_loss: 8.906438827514648, p_loss: -15.268284797668457, mean_rew: 0.198515625, variance: 0.0, cvar: 27.4523868560791, v: 15.047926902770996, mean_q: 15.195873260498047, std_q: 2.927769184112549, lamda: 1.0600048303604126
Running avgs for agent 1: q_loss: 11.3941011428833, p_loss: -14.871818542480469, mean_rew: 0.203515625, variance: 0.07769181203842163, cvar: 28.066144943237305, v: 15.047926902770996, mean_q: 14.826986312866211, std_q: 3.527339458465576, lamda: 1.0709037780761719
Running avgs for agent 2: q_loss: 1.0698858499526978, p_loss: 0.6685895919799805, mean_rew: -0.21727999357890268, variance: 1.1097551909387111, cvar: -0.10502096265554428, v: -0.12722046673297882, mean_q: -0.7964920401573181, std_q: 2.9551122188568115, lamda: 1.0030606985092163
Running avgs for agent 3: q_loss: 0.9333296418190002, p_loss: 0.9506999850273132, mean_rew: -0.25415204028656085, variance: 0.9831826467588544, cvar: -0.07704641669988632, v: -0.1560511738061905, mean_q: -1.1527535915374756, std_q: 3.6760165691375732, lamda: 1.011642575263977

steps: 174975, episodes: 7000, mean episode reward: 3.323214979201354, agent episode reward: [4.15, 4.15, -2.768338408010135, -2.2084466127885105], time: 67.985
steps: 174975, episodes: 7000, mean episode variance: 0.6521495208889246, agent episode variance: [0.018031940340995787, 0.057720274329185486, 0.26654708935320376, 0.30985021686553954], time: 67.986
steps: 174975, episodes: 7000, mean episode cvar: 13.876806234292687, agent episode cvar: [6.952065767288208, 6.988081241607666, -0.024099795185029506, -0.03924097941815853], time: 67.986
Running avgs for agent 0: q_loss: 13.183531761169434, p_loss: -17.07455062866211, mean_rew: 0.1935546875, variance: 0.07212776136398315, cvar: 27.80826187133789, v: 17.886390686035156, mean_q: 17.017436981201172, std_q: 3.3350939750671387, lamda: 1.077455759048462
Running avgs for agent 1: q_loss: 9.2439603805542, p_loss: -16.49113655090332, mean_rew: 0.188984375, variance: 0.23088109731674195, cvar: 27.95232391357422, v: 17.886390686035156, mean_q: 16.442663192749023, std_q: 3.871934652328491, lamda: 1.089648723602295
Running avgs for agent 2: q_loss: 1.0575159788131714, p_loss: 0.6164372563362122, mean_rew: -0.20181088542675815, variance: 1.066188357412815, cvar: -0.09639918804168701, v: -0.12309646606445312, mean_q: -0.7348973155021667, std_q: 2.8744711875915527, lamda: 1.0046883821487427
Running avgs for agent 3: q_loss: 0.9740754961967468, p_loss: 0.97150719165802, mean_rew: -0.22608962334210161, variance: 1.2394008674621582, cvar: -0.15696392953395844, v: -0.23272988200187683, mean_q: -1.1522939205169678, std_q: 3.4473490715026855, lamda: 1.0274196863174438

steps: 199975, episodes: 8000, mean episode reward: 2.2937243687767017, agent episode reward: [3.38, 3.38, -2.142734799765084, -2.3235408314582138], time: 68.501
steps: 199975, episodes: 8000, mean episode variance: 1.0024288329929114, agent episode variance: [0.0, 0.06313525819778443, 0.25804756455123423, 0.6812460102438926], time: 68.501
steps: 199975, episodes: 8000, mean episode cvar: 13.868231766305865, agent episode cvar: [6.952315843582153, 6.999917457580566, -0.025460633404552936, -0.05854090145230293], time: 68.501
Running avgs for agent 0: q_loss: 6.275140762329102, p_loss: -18.732999801635742, mean_rew: 0.18328125, variance: 0.0, cvar: 27.809261322021484, v: 20.653553009033203, mean_q: 18.69327735900879, std_q: 3.481752395629883, lamda: 1.09336519241333
Running avgs for agent 1: q_loss: 6.1055803298950195, p_loss: -18.053382873535156, mean_rew: 0.18640625, variance: 0.2525410327911377, cvar: 27.999671936035156, v: 20.65020179748535, mean_q: 18.006715774536133, std_q: 3.9690611362457275, lamda: 1.1038917303085327
Running avgs for agent 2: q_loss: 0.9976093173027039, p_loss: 0.5576193332672119, mean_rew: -0.1847657255700658, variance: 1.032190258204937, cvar: -0.10184253007173538, v: -0.1251833736896515, mean_q: -0.6736716032028198, std_q: 2.8300578594207764, lamda: 1.006739854812622
Running avgs for agent 3: q_loss: 2.0399951934814453, p_loss: 1.5656157732009888, mean_rew: -0.21218752981335592, variance: 2.7249840409755706, cvar: -0.23416361212730408, v: -0.5440212488174438, mean_q: -1.7471758127212524, std_q: 3.5457749366760254, lamda: 1.0548429489135742

steps: 224975, episodes: 9000, mean episode reward: -0.12819466089465267, agent episode reward: [2.31, 2.31, -1.9269090610569692, -2.8212855998376836], time: 68.622
steps: 224975, episodes: 9000, mean episode variance: 1.6950243144407868, agent episode variance: [0.0696841413974762, 0.2131679707467556, 0.26445294788479806, 1.147719254411757], time: 68.623
steps: 224975, episodes: 9000, mean episode cvar: 13.973692699179054, agent episode cvar: [7.036197708129883, 7.098008314132691, -0.024187345638871195, -0.13632597744464875], time: 68.623
Running avgs for agent 0: q_loss: 5.14834451675415, p_loss: -19.766164779663086, mean_rew: 0.181640625, variance: 0.2787365655899048, cvar: 28.144790649414062, v: 23.225101470947266, mean_q: 19.731470108032227, std_q: 3.715428113937378, lamda: 1.1064469814300537
Running avgs for agent 1: q_loss: 5.585386276245117, p_loss: -18.887985229492188, mean_rew: 0.18203125, variance: 0.8526718829870223, cvar: 28.392030715942383, v: 23.16176986694336, mean_q: 18.827234268188477, std_q: 4.240756034851074, lamda: 1.1207988262176514
Running avgs for agent 2: q_loss: 0.9732311964035034, p_loss: 0.49563947319984436, mean_rew: -0.17469782391872984, variance: 1.0578117915391922, cvar: -0.09674938768148422, v: -0.1255003958940506, mean_q: -0.609284520149231, std_q: 2.7545976638793945, lamda: 1.008197546005249
Running avgs for agent 3: q_loss: 2.0699644088745117, p_loss: 2.153542995452881, mean_rew: -0.2003021110490544, variance: 4.590877017647028, cvar: -0.5453039407730103, v: -0.8785383105278015, mean_q: -2.383826732635498, std_q: 3.9110190868377686, lamda: 1.0857919454574585

steps: 249975, episodes: 10000, mean episode reward: 1.2054273464747376, agent episode reward: [2.31, 2.31, -1.7247650239213372, -1.6898076296039253], time: 68.791
steps: 249975, episodes: 10000, mean episode variance: 2.909556093946099, agent episode variance: [1.2195165432691575, 0.7931113401353359, 0.2452791820168495, 0.6516490285247565], time: 68.792
steps: 249975, episodes: 10000, mean episode cvar: 14.091256889477371, agent episode cvar: [7.099186744689941, 7.124147216796875, -0.023971233412623406, -0.10810583859682082], time: 68.792
Running avgs for agent 0: q_loss: 6.040681838989258, p_loss: -19.847444534301758, mean_rew: 0.172578125, variance: 4.87806617307663, cvar: 28.396745681762695, v: 24.58483123779297, mean_q: 19.817644119262695, std_q: 3.8522162437438965, lamda: 1.1233052015304565
Running avgs for agent 1: q_loss: 3.9413936138153076, p_loss: -18.84735107421875, mean_rew: 0.168359375, variance: 3.1724453605413436, cvar: 28.49658966064453, v: 24.440181732177734, mean_q: 18.78460121154785, std_q: 4.3878045082092285, lamda: 1.139009714126587
Running avgs for agent 2: q_loss: 0.9354199767112732, p_loss: 0.4670700430870056, mean_rew: -0.1679562589071485, variance: 0.981116728067398, cvar: -0.09588493406772614, v: -0.13045449554920197, mean_q: -0.5751516222953796, std_q: 2.620110273361206, lamda: 1.010318398475647
Running avgs for agent 3: q_loss: 0.8880179524421692, p_loss: 1.7571711540222168, mean_rew: -0.18202285971713766, variance: 2.606596114099026, cvar: -0.43242335319519043, v: -0.6941828727722168, mean_q: -1.967373251914978, std_q: 3.6415536403656006, lamda: 1.097885012626648

steps: 274975, episodes: 11000, mean episode reward: 0.744764623747556, agent episode reward: [1.86, 1.86, -1.8614202963057678, -1.1138150799466762], time: 68.595
steps: 274975, episodes: 11000, mean episode variance: 2.349600008485839, agent episode variance: [0.9328014851808548, 0.8072227013111114, 0.2184120990615338, 0.3911637229323387], time: 68.595
steps: 274975, episodes: 11000, mean episode cvar: 14.171208370093256, agent episode cvar: [7.113192884445191, 7.139711563110351, -0.024843841951340435, -0.05685223551094532], time: 68.596
Running avgs for agent 0: q_loss: 5.165185928344727, p_loss: -19.60636329650879, mean_rew: 0.16453125, variance: 3.7312059407234193, cvar: 28.45277214050293, v: 24.71207046508789, mean_q: 19.57434844970703, std_q: 3.9128358364105225, lamda: 1.1430675983428955
Running avgs for agent 1: q_loss: 4.456236362457275, p_loss: -18.737878799438477, mean_rew: 0.1628125, variance: 3.2288908052444456, cvar: 28.558847427368164, v: 24.688846588134766, mean_q: 18.69019889831543, std_q: 4.4252142906188965, lamda: 1.1566991806030273
Running avgs for agent 2: q_loss: 0.8240743279457092, p_loss: 0.42843320965766907, mean_rew: -0.15610039641793927, variance: 0.8736483962461352, cvar: -0.09937536716461182, v: -0.12532295286655426, mean_q: -0.523024320602417, std_q: 2.3651201725006104, lamda: 1.0132817029953003
Running avgs for agent 3: q_loss: 0.9964849948883057, p_loss: 1.3045766353607178, mean_rew: -0.16994682423917984, variance: 1.5646548917293548, cvar: -0.22740891575813293, v: -0.4675828218460083, mean_q: -1.4793158769607544, std_q: 3.3971903324127197, lamda: 1.104162335395813

steps: 299975, episodes: 12000, mean episode reward: 0.7449464741669699, agent episode reward: [2.13, 2.13, -1.7777856491243478, -1.737267876708682], time: 76.176
steps: 299975, episodes: 12000, mean episode variance: 2.430851077321917, agent episode variance: [0.6546236766576767, 1.0920531443357469, 0.2186848872192204, 0.46548936910927297], time: 76.177
steps: 299975, episodes: 12000, mean episode cvar: 14.191132479336114, agent episode cvar: [7.125798801422119, 7.144732778549194, -0.023031705129891635, -0.05636739550530911], time: 76.177
Running avgs for agent 0: q_loss: 4.392998218536377, p_loss: -19.32600212097168, mean_rew: 0.151953125, variance: 2.618494706630707, cvar: 28.50319480895996, v: 24.59294319152832, mean_q: 19.289358139038086, std_q: 3.9949541091918945, lamda: 1.1635583639144897
Running avgs for agent 1: q_loss: 4.377820014953613, p_loss: -18.4382266998291, mean_rew: 0.1554296875, variance: 4.368212577342987, cvar: 28.57893180847168, v: 24.67516326904297, mean_q: 18.373136520385742, std_q: 4.525160789489746, lamda: 1.178423285484314
Running avgs for agent 2: q_loss: 0.8220328688621521, p_loss: 0.43366754055023193, mean_rew: -0.15056199272171153, variance: 0.8747395488768815, cvar: -0.09212682396173477, v: -0.12679553031921387, mean_q: -0.5223451256752014, std_q: 2.3365840911865234, lamda: 1.0160695314407349
Running avgs for agent 3: q_loss: 1.4849556684494019, p_loss: 1.3422719240188599, mean_rew: -0.16320033314473822, variance: 1.8619574764370919, cvar: -0.22546957433223724, v: -0.47480565309524536, mean_q: -1.4992715120315552, std_q: 3.4560651779174805, lamda: 1.122125267982483

steps: 324975, episodes: 13000, mean episode reward: -0.489065810658139, agent episode reward: [1.4, 1.4, -1.439108094773522, -1.849957715884617], time: 73.392
steps: 324975, episodes: 13000, mean episode variance: 2.9229426880963145, agent episode variance: [0.33057219618558886, 1.0975319356918336, 0.20016346176341177, 1.2946750944554806], time: 73.392
steps: 324975, episodes: 13000, mean episode cvar: 14.397954011630267, agent episode cvar: [7.118999439239502, 7.154074970245361, -0.022684276569634676, 0.1475638787150383], time: 73.393
Running avgs for agent 0: q_loss: 4.100764751434326, p_loss: -19.26262855529785, mean_rew: 0.146015625, variance: 1.3222887847423554, cvar: 28.475997924804688, v: 24.612035751342773, mean_q: 19.22861671447754, std_q: 4.004117488861084, lamda: 1.1809452772140503
Running avgs for agent 1: q_loss: 3.9004828929901123, p_loss: -17.98005485534668, mean_rew: 0.14953125, variance: 4.390127742767334, cvar: 28.616300582885742, v: 24.537012100219727, mean_q: 17.918659210205078, std_q: 4.696785926818848, lamda: 1.200081706047058
Running avgs for agent 2: q_loss: 0.7524821758270264, p_loss: 0.4063311815261841, mean_rew: -0.13781699340698866, variance: 0.8006538470536471, cvar: -0.09073710441589355, v: -0.11873455345630646, mean_q: -0.4933929443359375, std_q: 2.2866110801696777, lamda: 1.020336627960205
Running avgs for agent 3: q_loss: 20.961692810058594, p_loss: 2.044301748275757, mean_rew: -0.1558231916264238, variance: 5.178700377821922, cvar: 0.5902554988861084, v: -0.8615401387214661, mean_q: -2.1845479011535645, std_q: 3.577946662902832, lamda: 1.1444224119186401

steps: 349975, episodes: 14000, mean episode reward: -5.48164429479083, agent episode reward: [1.46, 1.46, -1.6687675152273458, -6.732876779563485], time: 72.415
steps: 349975, episodes: 14000, mean episode variance: 3.2627375759258865, agent episode variance: [0.26628188461065294, 1.2646745714843273, 0.2069707807675004, 1.524810339063406], time: 72.415
steps: 349975, episodes: 14000, mean episode cvar: 14.734767927926034, agent episode cvar: [7.102626800537109, 7.165706100463868, -0.022421031404286623, 0.4888560583293438], time: 72.416
Running avgs for agent 0: q_loss: 3.5369365215301514, p_loss: -19.287874221801758, mean_rew: 0.1396875, variance: 1.0651275384426118, cvar: 28.410507202148438, v: 24.802371978759766, mean_q: 19.254791259765625, std_q: 3.970902442932129, lamda: 1.1997027397155762
Running avgs for agent 1: q_loss: 5.127476692199707, p_loss: -17.829832077026367, mean_rew: 0.1382421875, variance: 5.058698285937309, cvar: 28.662824630737305, v: 24.48712730407715, mean_q: 17.75935173034668, std_q: 4.759248733520508, lamda: 1.2230536937713623
Running avgs for agent 2: q_loss: 0.760260820388794, p_loss: 0.3712124526500702, mean_rew: -0.13450843313628258, variance: 0.8278831230700016, cvar: -0.0896841287612915, v: -0.11359307914972305, mean_q: -0.4637540876865387, std_q: 2.327514171600342, lamda: 1.0229847431182861
Running avgs for agent 3: q_loss: 65.82315826416016, p_loss: 2.234083890914917, mean_rew: -0.1567327377884637, variance: 6.099241356253624, cvar: 1.955424189567566, v: -0.9118646383285522, mean_q: -2.3947980403900146, std_q: 3.868295192718506, lamda: 1.1736277341842651

steps: 374975, episodes: 15000, mean episode reward: -3.504945970452011, agent episode reward: [1.12, 1.12, -1.2185580667311784, -4.526387903720833], time: 69.052
steps: 374975, episodes: 15000, mean episode variance: 3.459065603837371, agent episode variance: [0.5235618677139282, 0.4937019602060318, 0.19306792470812797, 2.2487338512092827], time: 69.052
steps: 374975, episodes: 15000, mean episode cvar: 14.203674528591336, agent episode cvar: [7.1202153301239015, 7.12468662071228, -0.02253140062838793, -0.018696021616458895], time: 69.053
Running avgs for agent 0: q_loss: 4.233233451843262, p_loss: -19.141061782836914, mean_rew: 0.1313671875, variance: 2.094247470855713, cvar: 28.48086166381836, v: 24.880496978759766, mean_q: 19.106178283691406, std_q: 4.053340435028076, lamda: 1.2197915315628052
Running avgs for agent 1: q_loss: 3.4766509532928467, p_loss: -17.785390853881836, mean_rew: 0.136015625, variance: 1.9748078408241272, cvar: 28.498746871948242, v: 24.55393409729004, mean_q: 17.73665428161621, std_q: 4.728978633880615, lamda: 1.2434600591659546
Running avgs for agent 2: q_loss: 0.7234158515930176, p_loss: 0.32101407647132874, mean_rew: -0.13179480503611368, variance: 0.7722716988325119, cvar: -0.09012560546398163, v: -0.1121489405632019, mean_q: -0.41581493616104126, std_q: 2.1785264015197754, lamda: 1.0270248651504517
Running avgs for agent 3: q_loss: 14.856511116027832, p_loss: 2.8213818073272705, mean_rew: -0.16237626333807167, variance: 8.99493540483713, cvar: -0.07478410005569458, v: -1.1592425107955933, mean_q: -2.962949514389038, std_q: 3.6613450050354004, lamda: 1.198993444442749

steps: 399975, episodes: 16000, mean episode reward: -0.011963763127646132, agent episode reward: [1.64, 1.64, -1.1861496775572251, -2.105814085570421], time: 65.076
steps: 399975, episodes: 16000, mean episode variance: 1.7393868066705764, agent episode variance: [0.08670974034070969, 0.18120548099279404, 0.16591887084022164, 1.305552714496851], time: 65.076
steps: 399975, episodes: 16000, mean episode cvar: 14.409129775382578, agent episode cvar: [7.108983476638794, 7.123709192276001, -0.021825416915118693, 0.19826252338290215], time: 65.076
Running avgs for agent 0: q_loss: 3.5654375553131104, p_loss: -19.335020065307617, mean_rew: 0.1254296875, variance: 0.34683896136283876, cvar: 28.43593406677246, v: 24.987445831298828, mean_q: 19.305593490600586, std_q: 3.98443603515625, lamda: 1.2391753196716309
Running avgs for agent 1: q_loss: 3.0171313285827637, p_loss: -17.949926376342773, mean_rew: 0.12953125, variance: 0.7248219239711762, cvar: 28.49483871459961, v: 24.668548583984375, mean_q: 17.909465789794922, std_q: 4.67588472366333, lamda: 1.26350998878479
Running avgs for agent 2: q_loss: 0.6467951536178589, p_loss: 0.2838411331176758, mean_rew: -0.12393766564949997, variance: 0.6636754833608866, cvar: -0.08730166405439377, v: -0.10712502896785736, mean_q: -0.3697837293148041, std_q: 2.019911527633667, lamda: 1.0302571058273315
Running avgs for agent 3: q_loss: 29.88420295715332, p_loss: 2.0828936100006104, mean_rew: -0.15989088597554116, variance: 5.222210857987404, cvar: 0.7930501103401184, v: -1.077486276626587, mean_q: -2.197456121444702, std_q: 3.2800679206848145, lamda: 1.2202404737472534

steps: 424975, episodes: 17000, mean episode reward: -2.362834785888652, agent episode reward: [2.09, 2.09, -1.1714119200741433, -5.371422865814508], time: 64.862
steps: 424975, episodes: 17000, mean episode variance: 1.2973944322876632, agent episode variance: [0.08476720941066741, 0.5418468109071255, 0.1574384797923267, 0.5133419321775436], time: 64.863
steps: 424975, episodes: 17000, mean episode cvar: 14.094323771793395, agent episode cvar: [7.058007776260376, 7.1008265018463135, -0.02203048212453723, -0.042480024188756946], time: 64.863
Running avgs for agent 0: q_loss: 3.763751268386841, p_loss: -19.657672882080078, mean_rew: 0.125, variance: 0.33906883764266965, cvar: 28.232030868530273, v: 25.026840209960938, mean_q: 19.62851905822754, std_q: 3.7450273036956787, lamda: 1.2580727338790894
Running avgs for agent 1: q_loss: 4.2881035804748535, p_loss: -17.991004943847656, mean_rew: 0.1259375, variance: 2.167387243628502, cvar: 28.40330696105957, v: 24.768463134765625, mean_q: 17.95824432373047, std_q: 4.621049880981445, lamda: 1.2841116189956665
Running avgs for agent 2: q_loss: 0.6078162789344788, p_loss: 0.2686612606048584, mean_rew: -0.12180187525976306, variance: 0.6297539191693068, cvar: -0.0881219357252121, v: -0.10481821745634079, mean_q: -0.3529245853424072, std_q: 1.888639211654663, lamda: 1.0334887504577637
Running avgs for agent 3: q_loss: 1.7325488328933716, p_loss: 1.9465597867965698, mean_rew: -0.15986845957384935, variance: 2.0533677287101746, cvar: -0.16992010176181793, v: -0.9616061449050903, mean_q: -2.0785586833953857, std_q: 3.1399357318878174, lamda: 1.238738775253296

steps: 449975, episodes: 18000, mean episode reward: -5.108248951838261, agent episode reward: [1.73, 1.73, -0.48662504279220126, -8.08162390904606], time: 64.428
steps: 449975, episodes: 18000, mean episode variance: 0.9637593958750367, agent episode variance: [0.0838116774559021, 0.3065584508180618, 0.15090750620514154, 0.42248176139593124], time: 64.428
steps: 449975, episodes: 18000, mean episode cvar: 14.133937353134156, agent episode cvar: [7.0415516929626465, 7.087644256591797, -0.020363070130348205, 0.02510447371006012], time: 64.429
Running avgs for agent 0: q_loss: 3.417820453643799, p_loss: -19.987443923950195, mean_rew: 0.1209375, variance: 0.3352467098236084, cvar: 28.16620635986328, v: 25.09583282470703, mean_q: 19.964393615722656, std_q: 3.5984888076782227, lamda: 1.2730745077133179
Running avgs for agent 1: q_loss: 3.733093738555908, p_loss: -18.124820709228516, mean_rew: 0.1228125, variance: 1.2262338032722473, cvar: 28.350576400756836, v: 24.801883697509766, mean_q: 18.099803924560547, std_q: 4.576910972595215, lamda: 1.3041599988937378
Running avgs for agent 2: q_loss: 0.587920069694519, p_loss: 0.25528326630592346, mean_rew: -0.11755697327909771, variance: 0.6036300248205662, cvar: -0.08145228773355484, v: -0.10098740458488464, mean_q: -0.33505865931510925, std_q: 1.839072346687317, lamda: 1.0369802713394165
Running avgs for agent 3: q_loss: 5.751928806304932, p_loss: 1.6540181636810303, mean_rew: -0.16305049967135546, variance: 1.689927045583725, cvar: 0.10041790455579758, v: -0.8910230994224548, mean_q: -1.7708581686019897, std_q: 2.8275821208953857, lamda: 1.2571202516555786

steps: 474975, episodes: 19000, mean episode reward: -6.43118958535897, agent episode reward: [1.69, 1.69, -0.6251243302085968, -9.186065255150371], time: 64.836
steps: 474975, episodes: 19000, mean episode variance: 0.891130240894854, agent episode variance: [0.05389766025543213, 0.2516501225233078, 0.14115608712285757, 0.44442637099325655], time: 64.837
steps: 474975, episodes: 19000, mean episode cvar: 14.374608755167573, agent episode cvar: [7.03581822013855, 7.112771333694458, -0.020107165101915597, 0.24612636643648148], time: 64.837
Running avgs for agent 0: q_loss: 3.6129441261291504, p_loss: -20.488832473754883, mean_rew: 0.11953125, variance: 0.2155906410217285, cvar: 28.143274307250977, v: 25.196208953857422, mean_q: 20.46782875061035, std_q: 3.3703062534332275, lamda: 1.2880901098251343
Running avgs for agent 1: q_loss: 5.003737926483154, p_loss: -18.482839584350586, mean_rew: 0.12296875, variance: 1.0066004900932313, cvar: 28.451086044311523, v: 24.97056770324707, mean_q: 18.460453033447266, std_q: 4.4740214347839355, lamda: 1.3242346048355103
Running avgs for agent 2: q_loss: 0.5324252843856812, p_loss: 0.24496610462665558, mean_rew: -0.11150795694761106, variance: 0.5646243484914303, cvar: -0.08042866736650467, v: -0.09845224022865295, mean_q: -0.31580492854118347, std_q: 1.7772424221038818, lamda: 1.0398552417755127
Running avgs for agent 3: q_loss: 34.31494903564453, p_loss: 1.543886423110962, mean_rew: -0.17459186227930865, variance: 1.7777054839730262, cvar: 0.9845055341720581, v: -0.8145091533660889, mean_q: -1.6466375589370728, std_q: 2.574258327484131, lamda: 1.279745101928711

steps: 499975, episodes: 20000, mean episode reward: -12.099040505043833, agent episode reward: [1.52, 1.52, -0.46041370422296557, -14.678626800820869], time: 64.824
steps: 499975, episodes: 20000, mean episode variance: 1.0160995476171375, agent episode variance: [0.01226727294921875, 0.4045565579533577, 0.1256549561545253, 0.4736207605600357], time: 64.825
steps: 499975, episodes: 20000, mean episode cvar: 14.405336287997663, agent episode cvar: [7.026231155395508, 7.096224178314209, -0.021311207689344882, 0.3041921619772911], time: 64.825
Running avgs for agent 0: q_loss: 2.5366809368133545, p_loss: -20.797584533691406, mean_rew: 0.113125, variance: 0.049069091796875, cvar: 28.104923248291016, v: 25.360414505004883, mean_q: 20.779287338256836, std_q: 3.2288870811462402, lamda: 1.2996387481689453
Running avgs for agent 1: q_loss: 5.7621378898620605, p_loss: -18.903078079223633, mean_rew: 0.1196875, variance: 1.6182262318134308, cvar: 28.38489532470703, v: 25.053190231323242, mean_q: 18.87958526611328, std_q: 4.2688703536987305, lamda: 1.344283103942871
Running avgs for agent 2: q_loss: 0.47541704773902893, p_loss: 0.23089784383773804, mean_rew: -0.10489515648451103, variance: 0.5026198246181012, cvar: -0.08524482697248459, v: -0.09692378342151642, mean_q: -0.29872825741767883, std_q: 1.7611726522445679, lamda: 1.0428366661071777
Running avgs for agent 3: q_loss: 47.13362121582031, p_loss: 1.7016650438308716, mean_rew: -0.19287576040481627, variance: 1.8944830422401429, cvar: 1.216768503189087, v: -0.8348586559295654, mean_q: -1.8103193044662476, std_q: 2.4800970554351807, lamda: 1.3033674955368042

steps: 524975, episodes: 21000, mean episode reward: -13.301493992428133, agent episode reward: [1.8, 1.8, -1.0024492642744054, -15.899044728153727], time: 64.399
steps: 524975, episodes: 21000, mean episode variance: 1.0652135503366589, agent episode variance: [0.22237525987625123, 0.18552188622951507, 0.1262844446375966, 0.5310319595932961], time: 64.4
steps: 524975, episodes: 21000, mean episode cvar: 14.208332733113318, agent episode cvar: [7.046271015167236, 7.084878999710083, -0.020046103995293378, 0.09722882223129273], time: 64.4
Running avgs for agent 0: q_loss: 4.022720813751221, p_loss: -21.07805824279785, mean_rew: 0.113359375, variance: 0.8895010395050049, cvar: 28.185083389282227, v: 25.431957244873047, mean_q: 21.064451217651367, std_q: 3.087491273880005, lamda: 1.3129420280456543
Running avgs for agent 1: q_loss: 5.027357578277588, p_loss: -19.314613342285156, mean_rew: 0.1137109375, variance: 0.7420875449180603, cvar: 28.339515686035156, v: 25.037551879882812, mean_q: 19.27328109741211, std_q: 4.0446391105651855, lamda: 1.364195466041565
Running avgs for agent 2: q_loss: 0.4720909595489502, p_loss: 0.22289782762527466, mean_rew: -0.10188361445521453, variance: 0.5051377785503864, cvar: -0.08018441498279572, v: -0.09422927349805832, mean_q: -0.2858675420284271, std_q: 1.7256425619125366, lamda: 1.045122504234314
Running avgs for agent 3: q_loss: 13.77291488647461, p_loss: 2.0115461349487305, mean_rew: -0.20855411396419027, variance: 2.1241278383731843, cvar: 0.3889152705669403, v: -0.9864627718925476, mean_q: -2.144561529159546, std_q: 2.4297969341278076, lamda: 1.3254270553588867

steps: 549975, episodes: 22000, mean episode reward: -12.339656220314374, agent episode reward: [1.71, 1.71, -1.054672485709606, -14.704983734604772], time: 64.912
steps: 549975, episodes: 22000, mean episode variance: 0.9470497215986252, agent episode variance: [0.04513438868522644, 0.06827912199497223, 0.10735161136090755, 0.726284599557519], time: 64.913
steps: 549975, episodes: 22000, mean episode cvar: 14.504815898649394, agent episode cvar: [7.035496215820313, 7.060544361114502, -0.02059856904298067, 0.4293738907575607], time: 64.913
Running avgs for agent 0: q_loss: 3.2942397594451904, p_loss: -21.329675674438477, mean_rew: 0.111171875, variance: 0.18053755474090577, cvar: 28.141984939575195, v: 25.37755012512207, mean_q: 21.319231033325195, std_q: 2.9503931999206543, lamda: 1.3293575048446655
Running avgs for agent 1: q_loss: 3.468954086303711, p_loss: -19.6723575592041, mean_rew: 0.112578125, variance: 0.2731164879798889, cvar: 28.242177963256836, v: 24.985702514648438, mean_q: 19.638885498046875, std_q: 3.8372275829315186, lamda: 1.3816231489181519
Running avgs for agent 2: q_loss: 0.4389137923717499, p_loss: 0.21517916023731232, mean_rew: -0.09929366412624359, variance: 0.4294064454436302, cvar: -0.08239427953958511, v: -0.09906566143035889, mean_q: -0.27536773681640625, std_q: 1.619083046913147, lamda: 1.0483129024505615
Running avgs for agent 3: q_loss: 53.7921028137207, p_loss: 2.9591503143310547, mean_rew: -0.22620857564587374, variance: 2.905138398230076, cvar: 1.717495322227478, v: -1.6563334465026855, mean_q: -3.139094829559326, std_q: 2.7608134746551514, lamda: 1.3427683115005493

steps: 574975, episodes: 23000, mean episode reward: -4.957546056190229, agent episode reward: [1.52, 1.52, -0.7104734851762119, -7.287072571014017], time: 66.112
steps: 574975, episodes: 23000, mean episode variance: 1.7621131536122412, agent episode variance: [0.1299521918296814, 0.11692231658101082, 0.12267143238149583, 1.392567212820053], time: 66.112
steps: 574975, episodes: 23000, mean episode cvar: 14.79539127895236, agent episode cvar: [7.044975727081299, 7.0794022827148435, -0.02058028307557106, 0.6915935522317886], time: 66.113
Running avgs for agent 0: q_loss: 3.4626247882843018, p_loss: -21.476659774780273, mean_rew: 0.1098046875, variance: 0.5198087673187256, cvar: 28.179903030395508, v: 25.38783836364746, mean_q: 21.469722747802734, std_q: 2.8757662773132324, lamda: 1.3435418605804443
Running avgs for agent 1: q_loss: 3.9616165161132812, p_loss: -20.057680130004883, mean_rew: 0.11296875, variance: 0.4676892663240433, cvar: 28.317609786987305, v: 25.06915283203125, mean_q: 20.0284481048584, std_q: 3.68438982963562, lamda: 1.396139144897461
Running avgs for agent 2: q_loss: 0.43569043278694153, p_loss: 0.2076251357793808, mean_rew: -0.09585230591037401, variance: 0.4906857295259833, cvar: -0.08232113718986511, v: -0.09545057266950607, mean_q: -0.2663029730319977, std_q: 1.6025502681732178, lamda: 1.053011178970337
Running avgs for agent 3: q_loss: 82.47540283203125, p_loss: 3.7711498737335205, mean_rew: -0.2397802899196233, variance: 5.570268851280212, cvar: 2.766374349594116, v: -2.2128727436065674, mean_q: -3.971285104751587, std_q: 3.183687925338745, lamda: 1.363215446472168

steps: 599975, episodes: 24000, mean episode reward: -4.365437925738622, agent episode reward: [1.33, 1.33, -0.8645516786714654, -6.160886247067156], time: 68.132
steps: 599975, episodes: 24000, mean episode variance: 1.5496922808717937, agent episode variance: [0.05417373621463776, 0.13701880115270615, 0.117326785011217, 1.2411729584932327], time: 68.132
steps: 599975, episodes: 24000, mean episode cvar: 14.640609878882765, agent episode cvar: [7.071850952148438, 7.05028643989563, -0.02044061590731144, 0.5389131027460098], time: 68.133
Running avgs for agent 0: q_loss: 3.969956636428833, p_loss: -21.713016510009766, mean_rew: 0.106015625, variance: 0.21669494485855104, cvar: 28.287405014038086, v: 25.471742630004883, mean_q: 21.70482063293457, std_q: 2.8027915954589844, lamda: 1.3586279153823853
Running avgs for agent 1: q_loss: 4.1355695724487305, p_loss: -20.245182037353516, mean_rew: 0.1081640625, variance: 0.5480752046108246, cvar: 28.20114517211914, v: 24.907085418701172, mean_q: 20.221580505371094, std_q: 3.4888579845428467, lamda: 1.4127570390701294
Running avgs for agent 2: q_loss: 0.4371536374092102, p_loss: 0.20564578473567963, mean_rew: -0.09218902493138682, variance: 0.469307140044868, cvar: -0.08176246285438538, v: -0.09816429018974304, mean_q: -0.26139238476753235, std_q: 1.5557643175125122, lamda: 1.0564556121826172
Running avgs for agent 3: q_loss: 78.25068664550781, p_loss: 5.2493510246276855, mean_rew: -0.2429248444938421, variance: 4.964691833972931, cvar: 2.1556522846221924, v: -2.6039836406707764, mean_q: -5.510655403137207, std_q: 4.022510528564453, lamda: 1.3801888227462769

steps: 624975, episodes: 25000, mean episode reward: -9.217526277145991, agent episode reward: [1.86, 1.86, -0.9077388570864234, -12.029787420059568], time: 68.834
steps: 624975, episodes: 25000, mean episode variance: 2.5612265082169325, agent episode variance: [0.08555132752656937, 0.08925251126289367, 0.11747557772882283, 2.2689470916986467], time: 68.834
steps: 624975, episodes: 25000, mean episode cvar: 14.86751545767486, agent episode cvar: [7.051152696609497, 7.041175582885742, -0.020407441094517708, 0.7955946192741394], time: 68.835
Running avgs for agent 0: q_loss: 3.000410556793213, p_loss: -21.56298065185547, mean_rew: 0.106484375, variance: 0.34220531010627747, cvar: 28.204608917236328, v: 25.336973190307617, mean_q: 21.550308227539062, std_q: 2.827751398086548, lamda: 1.3726558685302734
Running avgs for agent 1: q_loss: 3.3988232612609863, p_loss: -20.48627281188965, mean_rew: 0.106640625, variance: 0.3570100450515747, cvar: 28.164703369140625, v: 24.8227481842041, mean_q: 20.459659576416016, std_q: 3.358623504638672, lamda: 1.4263803958892822
Running avgs for agent 2: q_loss: 0.3958311676979065, p_loss: 0.2068473845720291, mean_rew: -0.09223592373990895, variance: 0.46990231091529133, cvar: -0.08162976801395416, v: -0.0958244726061821, mean_q: -0.26098230481147766, std_q: 1.699092984199524, lamda: 1.0607656240463257
Running avgs for agent 3: q_loss: 141.67816162109375, p_loss: 7.619208812713623, mean_rew: -0.2481102428454031, variance: 9.075788366794587, cvar: 3.1823785305023193, v: -3.480823278427124, mean_q: -7.90267276763916, std_q: 4.808513164520264, lamda: 1.3902348279953003

steps: 649975, episodes: 26000, mean episode reward: -5.766230190415566, agent episode reward: [1.49, 1.49, -0.8162651960869314, -7.929964994328636], time: 70.735
steps: 649975, episodes: 26000, mean episode variance: 3.502143955970183, agent episode variance: [0.05744915598630905, 0.09776173377037048, 0.10375752797164023, 3.2431755382418634], time: 70.736
steps: 649975, episodes: 26000, mean episode cvar: 14.387221838265658, agent episode cvar: [7.051197183609009, 7.044287073135376, -0.020366962879896165, 0.3121045444011688], time: 70.736
Running avgs for agent 0: q_loss: 2.7649781703948975, p_loss: -21.5007381439209, mean_rew: 0.1069921875, variance: 0.2297966239452362, cvar: 28.204788208007812, v: 25.27431297302246, mean_q: 21.491119384765625, std_q: 2.8364601135253906, lamda: 1.3880375623703003
Running avgs for agent 1: q_loss: 4.052346229553223, p_loss: -21.013206481933594, mean_rew: 0.103984375, variance: 0.3910469350814819, cvar: 28.177148818969727, v: 24.827842712402344, mean_q: 20.987945556640625, std_q: 3.0938358306884766, lamda: 1.4391132593154907
Running avgs for agent 2: q_loss: 0.3761589229106903, p_loss: 0.19841931760311127, mean_rew: -0.08821958718781114, variance: 0.4150301118865609, cvar: -0.08146785199642181, v: -0.09697835147380829, mean_q: -0.2513562738895416, std_q: 1.5116215944290161, lamda: 1.0669667720794678
Running avgs for agent 3: q_loss: 79.70318603515625, p_loss: 8.307381629943848, mean_rew: -0.2525282107548018, variance: 12.972702152967454, cvar: 1.2484182119369507, v: -4.761076927185059, mean_q: -8.547595024108887, std_q: 4.656673431396484, lamda: 1.405078649520874

steps: 674975, episodes: 27000, mean episode reward: -0.8254413365367375, agent episode reward: [2.58, 2.58, -0.9822975737370091, -5.003143762799729], time: 68.942
steps: 674975, episodes: 27000, mean episode variance: 2.9008494141679257, agent episode variance: [0.07709332364797593, 0.013868156671524048, 0.1047699716668576, 2.705117962181568], time: 68.943
steps: 674975, episodes: 27000, mean episode cvar: 14.163952258616685, agent episode cvar: [7.04417013168335, 7.057488285064697, -0.02057203409075737, 0.08286587595939636], time: 68.943
Running avgs for agent 0: q_loss: 3.152937412261963, p_loss: -21.512836456298828, mean_rew: 0.10515625, variance: 0.3083732945919037, cvar: 28.176679611206055, v: 25.22764778137207, mean_q: 21.48398780822754, std_q: 2.808945894241333, lamda: 1.4018926620483398
Running avgs for agent 1: q_loss: 3.090648889541626, p_loss: -21.186019897460938, mean_rew: 0.1078125, variance: 0.05547262668609619, cvar: 28.22995376586914, v: 24.825515747070312, mean_q: 21.15117645263672, std_q: 3.0249509811401367, lamda: 1.4533061981201172
Running avgs for agent 2: q_loss: 0.37139520049095154, p_loss: 0.19688548147678375, mean_rew: -0.08662596766979547, variance: 0.4190798866674304, cvar: -0.08228813111782074, v: -0.09787559509277344, mean_q: -0.24776460230350494, std_q: 1.5149604082107544, lamda: 1.0732457637786865
Running avgs for agent 3: q_loss: 26.69989776611328, p_loss: 8.03730297088623, mean_rew: -0.2527746668186814, variance: 10.820471848726273, cvar: 0.3314635455608368, v: -5.827523231506348, mean_q: -8.234197616577148, std_q: 4.561258316040039, lamda: 1.4232951402664185

steps: 699975, episodes: 28000, mean episode reward: -0.3487394519771503, agent episode reward: [2.01, 2.01, -0.8149445036629477, -3.5537949483142026], time: 68.755
steps: 699975, episodes: 28000, mean episode variance: 3.2597453109640626, agent episode variance: [0.07115831488370895, 0.018339991331100463, 0.10578148967213928, 3.064465515077114], time: 68.756
steps: 699975, episodes: 28000, mean episode cvar: 14.629741165414453, agent episode cvar: [7.036147035598755, 7.066974039077759, -0.01995079754292965, 0.5465708882808685], time: 68.756
Running avgs for agent 0: q_loss: 3.9678311347961426, p_loss: -21.574371337890625, mean_rew: 0.1080859375, variance: 0.2846332595348358, cvar: 28.144588470458984, v: 25.119932174682617, mean_q: 21.566179275512695, std_q: 2.737422466278076, lamda: 1.4163037538528442
Running avgs for agent 1: q_loss: 3.435225009918213, p_loss: -21.287206649780273, mean_rew: 0.1027734375, variance: 0.07335996532440185, cvar: 28.26789665222168, v: 24.70929527282715, mean_q: 21.25766372680664, std_q: 2.945788621902466, lamda: 1.466987133026123
Running avgs for agent 2: q_loss: 0.39947307109832764, p_loss: 0.19768762588500977, mean_rew: -0.08869145098245204, variance: 0.4231259586885571, cvar: -0.07980319112539291, v: -0.09439527243375778, mean_q: -0.2493523508310318, std_q: 1.5511302947998047, lamda: 1.0796958208084106
Running avgs for agent 3: q_loss: 118.35807800292969, p_loss: 9.45701789855957, mean_rew: -0.24999806511252948, variance: 12.257862060308456, cvar: 2.186283588409424, v: -6.646829605102539, mean_q: -9.665549278259277, std_q: 5.540125846862793, lamda: 1.4418185949325562

steps: 724975, episodes: 29000, mean episode reward: -0.9056109271985243, agent episode reward: [1.98, 1.98, -0.43976300691979503, -4.42584792027873], time: 65.7
steps: 724975, episodes: 29000, mean episode variance: 4.27380965778511, agent episode variance: [0.0006820170283317566, 0.01687128722667694, 0.09938404766004533, 4.156872305870056], time: 65.7
steps: 724975, episodes: 29000, mean episode cvar: 14.734978562962263, agent episode cvar: [7.04534433555603, 7.070312780380249, -0.019695656169205905, 0.6390171031951905], time: 65.701
Running avgs for agent 0: q_loss: 2.7987797260284424, p_loss: -21.743947982788086, mean_rew: 0.1005078125, variance: 0.0027280681133270263, cvar: 28.181377410888672, v: 25.130475997924805, mean_q: 21.73858070373535, std_q: 2.686098098754883, lamda: 1.4293599128723145
Running avgs for agent 1: q_loss: 3.367664098739624, p_loss: -21.218957901000977, mean_rew: 0.102421875, variance: 0.06748514890670776, cvar: 28.281251907348633, v: 24.633346557617188, mean_q: 21.193584442138672, std_q: 2.935175895690918, lamda: 1.4823906421661377
Running avgs for agent 2: q_loss: 0.3494579493999481, p_loss: 0.19416071474552155, mean_rew: -0.08256490081565959, variance: 0.3975361906401813, cvar: -0.07878262549638748, v: -0.0965193435549736, mean_q: -0.24204279482364655, std_q: 1.4714677333831787, lamda: 1.0865800380706787
Running avgs for agent 3: q_loss: 177.32855224609375, p_loss: 9.466558456420898, mean_rew: -0.24992765348467955, variance: 16.627489223480225, cvar: 2.5560686588287354, v: -7.130308628082275, mean_q: -9.665756225585938, std_q: 5.5654473304748535, lamda: 1.4619567394256592

steps: 749975, episodes: 30000, mean episode reward: -3.6620018555326723, agent episode reward: [2.34, 2.34, -0.8357817106309858, -7.506220144901687], time: 67.869
steps: 749975, episodes: 30000, mean episode variance: 6.404740793457255, agent episode variance: [0.01071528959274292, 0.020788917243480684, 0.09927910654805601, 6.273957480072975], time: 67.869
steps: 749975, episodes: 30000, mean episode cvar: 15.89981699135527, agent episode cvar: [7.059240522384644, 7.060103830337525, -0.020068789932876824, 1.800541428565979], time: 67.87
Running avgs for agent 0: q_loss: 3.4671151638031006, p_loss: -21.94324493408203, mean_rew: 0.10296875, variance: 0.04286115837097168, cvar: 28.236963272094727, v: 25.16604232788086, mean_q: 21.909208297729492, std_q: 2.6208384037017822, lamda: 1.4425987005233765
Running avgs for agent 1: q_loss: 3.7470455169677734, p_loss: -21.321245193481445, mean_rew: 0.1045703125, variance: 0.08315566897392274, cvar: 28.240415573120117, v: 24.54872703552246, mean_q: 21.28705406188965, std_q: 2.851685047149658, lamda: 1.4976847171783447
Running avgs for agent 2: q_loss: 0.3443789482116699, p_loss: 0.19535765051841736, mean_rew: -0.08044651195982647, variance: 0.39711642619222404, cvar: -0.08027516305446625, v: -0.0975048616528511, mean_q: -0.2415434569120407, std_q: 1.487427830696106, lamda: 1.0930436849594116
Running avgs for agent 3: q_loss: 989.5682373046875, p_loss: 10.176156997680664, mean_rew: -0.24935768653544238, variance: 25.0958299202919, cvar: 7.2021660804748535, v: -7.428524494171143, mean_q: -10.413963317871094, std_q: 6.1697001457214355, lamda: 1.4825434684753418

steps: 774975, episodes: 31000, mean episode reward: -32.96510737454882, agent episode reward: [2.77, 2.77, -1.9134574437725174, -36.5916499307763], time: 68.979
steps: 774975, episodes: 31000, mean episode variance: 14.634160406496376, agent episode variance: [0.14401875549554824, 0.0372444257736206, 0.10310951229557395, 14.349787712931633], time: 68.979
steps: 774975, episodes: 31000, mean episode cvar: 14.812434497263283, agent episode cvar: [7.054376792907715, 7.048375019073486, -0.01914597186818719, 0.7288286571502686], time: 68.98
Running avgs for agent 0: q_loss: 2.955832004547119, p_loss: -22.027637481689453, mean_rew: 0.1012890625, variance: 0.576075021982193, cvar: 28.21750831604004, v: 25.10456657409668, mean_q: 21.96270751953125, std_q: 2.5663161277770996, lamda: 1.4544048309326172
Running avgs for agent 1: q_loss: 4.438553810119629, p_loss: -21.542797088623047, mean_rew: 0.1049609375, variance: 0.1489777030944824, cvar: 28.193500518798828, v: 24.42837142944336, mean_q: 21.50466537475586, std_q: 2.732510805130005, lamda: 1.5120811462402344
Running avgs for agent 2: q_loss: 0.3521404564380646, p_loss: 0.18866831064224243, mean_rew: -0.08055201154041729, variance: 0.4124380491822958, cvar: -0.07658388465642929, v: -0.0962400957942009, mean_q: -0.23539972305297852, std_q: 1.4016430377960205, lamda: 1.1049418449401855
Running avgs for agent 3: q_loss: 261.1448059082031, p_loss: 15.218235969543457, mean_rew: -0.2588957742677052, variance: 57.39915085172653, cvar: 2.9153146743774414, v: -7.454229831695557, mean_q: -15.651132583618164, std_q: 7.922236442565918, lamda: 1.5018373727798462

steps: 799975, episodes: 32000, mean episode reward: -43.75992719094539, agent episode reward: [2.62, 2.62, -1.291263830211684, -47.70866336073371], time: 65.671
steps: 799975, episodes: 32000, mean episode variance: 18.33172143498063, agent episode variance: [0.08003277611732483, 0.15951334261894226, 0.09159074345231057, 18.000584572792054], time: 65.672
steps: 799975, episodes: 32000, mean episode cvar: 14.231096914563327, agent episode cvar: [7.061467514038086, 7.047068603515625, -0.019792095866054297, 0.14235289287567138], time: 65.672
Running avgs for agent 0: q_loss: 3.234959363937378, p_loss: -21.926136016845703, mean_rew: 0.10203125, variance: 0.3201311044692993, cvar: 28.245868682861328, v: 25.03433609008789, mean_q: 21.884939193725586, std_q: 2.593926191329956, lamda: 1.4670644998550415
Running avgs for agent 1: q_loss: 5.585654258728027, p_loss: -21.973562240600586, mean_rew: 0.106796875, variance: 0.638053370475769, cvar: 28.188274383544922, v: 24.4453182220459, mean_q: 21.92637062072754, std_q: 2.5250375270843506, lamda: 1.526663064956665
Running avgs for agent 2: q_loss: 0.32284799218177795, p_loss: 0.19529478251934052, mean_rew: -0.08189825923315427, variance: 0.36636297380924226, cvar: -0.07916837930679321, v: -0.09627093374729156, mean_q: -0.2432888299226761, std_q: 1.542839527130127, lamda: 1.1139774322509766
Running avgs for agent 3: q_loss: 95.62504577636719, p_loss: 18.147695541381836, mean_rew: -0.3159063763122622, variance: 72.00233829116821, cvar: 0.5694116353988647, v: -8.989265441894531, mean_q: -18.587379455566406, std_q: 8.69991397857666, lamda: 1.5178285837173462

steps: 824975, episodes: 33000, mean episode reward: -14.386374354066156, agent episode reward: [2.23, 2.23, -0.6936655053124463, -18.15270884875371], time: 65.962
steps: 824975, episodes: 33000, mean episode variance: 14.30130876513198, agent episode variance: [0.1656866919696331, 0.13193272012472151, 0.10010514264181257, 13.903584210395813], time: 65.962
steps: 824975, episodes: 33000, mean episode cvar: 14.560592115163804, agent episode cvar: [7.07175306892395, 7.052666868209839, -0.01917711043357849, 0.4553492884635925], time: 65.963
Running avgs for agent 0: q_loss: 3.4218266010284424, p_loss: -21.647592544555664, mean_rew: 0.10390625, variance: 0.6627467678785324, cvar: 28.287012100219727, v: 24.912307739257812, mean_q: 21.62859344482422, std_q: 2.6910436153411865, lamda: 1.483270525932312
Running avgs for agent 1: q_loss: 5.323932647705078, p_loss: -22.21556854248047, mean_rew: 0.1015625, variance: 0.5277308804988861, cvar: 28.210668563842773, v: 24.32634162902832, mean_q: 22.173397064208984, std_q: 2.421243667602539, lamda: 1.5426276922225952
Running avgs for agent 2: q_loss: 0.3713839650154114, p_loss: 0.19753439724445343, mean_rew: -0.08085492973436297, variance: 0.4004205705672503, cvar: -0.07670845091342926, v: -0.09085515141487122, mean_q: -0.24493157863616943, std_q: 1.5472829341888428, lamda: 1.122450828552246
Running avgs for agent 3: q_loss: 235.52410888671875, p_loss: 17.93220329284668, mean_rew: -0.34208100393616225, variance: 55.61433684158325, cvar: 1.8213971853256226, v: -10.518684387207031, mean_q: -18.267107009887695, std_q: 8.822931289672852, lamda: 1.5336463451385498

steps: 849975, episodes: 34000, mean episode reward: -7.843126869331625, agent episode reward: [3.29, 3.29, -1.0904878923179215, -13.332638977013703], time: 69.381
steps: 849975, episodes: 34000, mean episode variance: 11.074381417090073, agent episode variance: [0.20457949867844583, 0.04268023264408111, 0.10396798529662192, 10.723153700470924], time: 69.381
steps: 849975, episodes: 34000, mean episode cvar: 14.482622617900372, agent episode cvar: [7.069688499450684, 7.046705324172974, -0.01895172244310379, 0.3851805167198181], time: 69.382
Running avgs for agent 0: q_loss: 3.7804667949676514, p_loss: -21.385679244995117, mean_rew: 0.1030078125, variance: 0.8183179947137833, cvar: 28.27875328063965, v: 24.705581665039062, mean_q: 21.347671508789062, std_q: 2.7835333347320557, lamda: 1.5008862018585205
Running avgs for agent 1: q_loss: 4.104540824890137, p_loss: -22.469945907592773, mean_rew: 0.102890625, variance: 0.17072093057632445, cvar: 28.18682098388672, v: 24.29800033569336, mean_q: 22.432241439819336, std_q: 2.302891492843628, lamda: 1.556235671043396
Running avgs for agent 2: q_loss: 0.3824620842933655, p_loss: 0.20080913603305817, mean_rew: -0.07707453951266581, variance: 0.4158719411864877, cvar: -0.0758068859577179, v: -0.09204237163066864, mean_q: -0.2458822876214981, std_q: 1.5493128299713135, lamda: 1.1307913064956665
Running avgs for agent 3: q_loss: 133.0056610107422, p_loss: 18.044063568115234, mean_rew: -0.35493561037772353, variance: 42.892614801883695, cvar: 1.5407218933105469, v: -11.841880798339844, mean_q: -18.426969528198242, std_q: 9.265203475952148, lamda: 1.551631212234497

steps: 874975, episodes: 35000, mean episode reward: -1.7349752580735696, agent episode reward: [3.38, 3.38, -0.8428637244352116, -7.652111533638359], time: 69.296
steps: 874975, episodes: 35000, mean episode variance: 15.51075484251231, agent episode variance: [0.21126756420731543, 0.4475423554778099, 0.12330215241760016, 14.728642770409584], time: 69.297
steps: 874975, episodes: 35000, mean episode cvar: 14.640004296611995, agent episode cvar: [7.0877255420684815, 7.104564622879028, -0.0196221453435719, 0.46733627700805663], time: 69.297
Running avgs for agent 0: q_loss: 3.9280214309692383, p_loss: -21.09052276611328, mean_rew: 0.099921875, variance: 0.8450702568292617, cvar: 28.350902557373047, v: 24.51618194580078, mean_q: 21.065082550048828, std_q: 2.9136576652526855, lamda: 1.5198547840118408
Running avgs for agent 1: q_loss: 7.777291297912598, p_loss: -22.416767120361328, mean_rew: 0.104453125, variance: 1.7901694219112396, cvar: 28.41826057434082, v: 24.29802894592285, mean_q: 22.382823944091797, std_q: 2.359266996383667, lamda: 1.5710910558700562
Running avgs for agent 2: q_loss: 0.4138944149017334, p_loss: 0.20198047161102295, mean_rew: -0.07968639484664873, variance: 0.49320860967040064, cvar: -0.07848858088254929, v: -0.09590853750705719, mean_q: -0.2483907788991928, std_q: 1.536840558052063, lamda: 1.1414690017700195
Running avgs for agent 3: q_loss: 218.21653747558594, p_loss: 17.872838973999023, mean_rew: -0.35733408313415455, variance: 58.914571081638336, cvar: 1.8693450689315796, v: -12.62026596069336, mean_q: -18.24268341064453, std_q: 9.466129302978516, lamda: 1.569641351699829

steps: 899975, episodes: 36000, mean episode reward: 2.033423843469108, agent episode reward: [3.6, 3.6, -0.7534676584820784, -4.413108498048814], time: 68.381
steps: 899975, episodes: 36000, mean episode variance: 29.949502067660912, agent episode variance: [0.23173546028137207, 0.05961477506160736, 0.10232332012616098, 29.555828512191773], time: 68.381
steps: 899975, episodes: 36000, mean episode cvar: 15.549398372594267, agent episode cvar: [7.0779491214752195, 7.045167167663574, -0.018578815516084434, 1.4448608989715577], time: 68.382
Running avgs for agent 0: q_loss: 4.230063438415527, p_loss: -21.02891731262207, mean_rew: 0.1046875, variance: 0.9269418411254883, cvar: 28.311796188354492, v: 24.529712677001953, mean_q: 20.99510383605957, std_q: 2.915876865386963, lamda: 1.537779450416565
Running avgs for agent 1: q_loss: 4.138401508331299, p_loss: -22.45864486694336, mean_rew: 0.107734375, variance: 0.23845910024642944, cvar: 28.180667877197266, v: 24.186763763427734, mean_q: 22.42136573791504, std_q: 2.295170307159424, lamda: 1.5857961177825928
Running avgs for agent 2: q_loss: 0.3827603757381439, p_loss: 0.19723790884017944, mean_rew: -0.07880023441903157, variance: 0.4092932805046439, cvar: -0.07431526482105255, v: -0.0918017253279686, mean_q: -0.245025634765625, std_q: 1.5257636308670044, lamda: 1.1521306037902832
Running avgs for agent 3: q_loss: 957.3834838867188, p_loss: 19.28510856628418, mean_rew: -0.34977047537935757, variance: 118.2233140487671, cvar: 5.779443264007568, v: -12.806321144104004, mean_q: -19.765026092529297, std_q: 10.501982688903809, lamda: 1.589927077293396

steps: 924975, episodes: 37000, mean episode reward: 2.013140048560327, agent episode reward: [4.36, 4.36, -0.6381755835856425, -6.068684367854031], time: 67.505
steps: 924975, episodes: 37000, mean episode variance: 45.99080326181464, agent episode variance: [0.13886502894759178, 0.0, 0.09064104363135994, 45.761297189235684], time: 67.505
steps: 924975, episodes: 37000, mean episode cvar: 14.36733372664079, agent episode cvar: [7.098483730316162, 7.01562093925476, -0.018932552102953196, 0.27216160917282106], time: 67.506
Running avgs for agent 0: q_loss: 4.229526042938232, p_loss: -20.880342483520508, mean_rew: 0.1071484375, variance: 0.5554601157903671, cvar: 28.393936157226562, v: 24.480510711669922, mean_q: 20.842823028564453, std_q: 3.009791851043701, lamda: 1.5572060346603394
Running avgs for agent 1: q_loss: 3.4932124614715576, p_loss: -22.727550506591797, mean_rew: 0.10546875, variance: 0.0, cvar: 28.062484741210938, v: 24.024349212646484, mean_q: 22.69446563720703, std_q: 2.152872085571289, lamda: 1.597955346107483
Running avgs for agent 2: q_loss: 0.3620679974555969, p_loss: 0.19547505676746368, mean_rew: -0.0734573511801239, variance: 0.36256417452543976, cvar: -0.07573021203279495, v: -0.09108438342809677, mean_q: -0.24247516691684723, std_q: 1.4868676662445068, lamda: 1.1608120203018188
Running avgs for agent 3: q_loss: 519.7958984375, p_loss: 28.13109016418457, mean_rew: -0.3510539358258268, variance: 183.04518875694274, cvar: 1.0886465311050415, v: -13.83250617980957, mean_q: -28.7946834564209, std_q: 13.264426231384277, lamda: 1.6092092990875244

steps: 949975, episodes: 38000, mean episode reward: 3.1048104328219215, agent episode reward: [4.75, 4.75, -0.7982283404641792, -5.5969612267138995], time: 67.003
steps: 949975, episodes: 38000, mean episode variance: 22.572379696331918, agent episode variance: [0.1643982535302639, 0.17878311541676523, 0.0988301008567214, 22.13036822652817], time: 67.003
steps: 949975, episodes: 38000, mean episode cvar: 14.505874587040395, agent episode cvar: [7.115570957183838, 7.0480715675354, -0.018566914577037096, 0.36079897689819335], time: 67.004
Running avgs for agent 0: q_loss: 4.843688011169434, p_loss: -20.679006576538086, mean_rew: 0.111484375, variance: 0.6575930141210556, cvar: 28.4622859954834, v: 24.43731689453125, mean_q: 20.60354232788086, std_q: 3.1298935413360596, lamda: 1.5749635696411133
Running avgs for agent 1: q_loss: 5.038863658905029, p_loss: -22.810504913330078, mean_rew: 0.10921875, variance: 0.7151324616670609, cvar: 28.192289352416992, v: 24.036359786987305, mean_q: 22.775583267211914, std_q: 2.175814390182495, lamda: 1.6127588748931885
Running avgs for agent 2: q_loss: 0.33683088421821594, p_loss: 0.18888327479362488, mean_rew: -0.07147489229009857, variance: 0.3953204034268856, cvar: -0.07426765561103821, v: -0.09140190482139587, mean_q: -0.23395346105098724, std_q: 1.4002960920333862, lamda: 1.1745500564575195
Running avgs for agent 3: q_loss: 173.08438110351562, p_loss: 27.15201187133789, mean_rew: -0.3428085049787546, variance: 88.52147290611268, cvar: 1.4431958198547363, v: -15.515727996826172, mean_q: -27.865154266357422, std_q: 13.709407806396484, lamda: 1.627184271812439

steps: 974975, episodes: 39000, mean episode reward: 4.3914136416111775, agent episode reward: [5.34, 5.34, -0.6787601954645217, -5.609826162924301], time: 67.399
steps: 974975, episodes: 39000, mean episode variance: 21.105325801136903, agent episode variance: [0.07171134196221829, 0.01729027009010315, 0.0888647064147517, 20.927459482669832], time: 67.4
steps: 974975, episodes: 39000, mean episode cvar: 14.311070898678153, agent episode cvar: [7.103449794769287, 7.056324203491211, -0.019250150058418514, 0.1705470504760742], time: 67.4
Running avgs for agent 0: q_loss: 3.6615636348724365, p_loss: -20.46078872680664, mean_rew: 0.1075390625, variance: 0.28684536784887316, cvar: 28.413799285888672, v: 24.32711410522461, mean_q: 20.405738830566406, std_q: 3.19325852394104, lamda: 1.5929290056228638
Running avgs for agent 1: q_loss: 3.854522466659546, p_loss: -22.88892936706543, mean_rew: 0.1099609375, variance: 0.0691610803604126, cvar: 28.225296020507812, v: 23.970535278320312, mean_q: 22.859329223632812, std_q: 2.1746761798858643, lamda: 1.624071478843689
Running avgs for agent 2: q_loss: 0.3432248532772064, p_loss: 0.18772070109844208, mean_rew: -0.07517619865131159, variance: 0.3554588256590068, cvar: -0.07700059562921524, v: -0.09044475853443146, mean_q: -0.2350122332572937, std_q: 1.419992446899414, lamda: 1.1840769052505493
Running avgs for agent 3: q_loss: 151.00009155273438, p_loss: 24.59552001953125, mean_rew: -0.3372742071171352, variance: 83.70983793067933, cvar: 0.6821881532669067, v: -16.48919105529785, mean_q: -25.221027374267578, std_q: 12.64827823638916, lamda: 1.6483114957809448

steps: 999975, episodes: 40000, mean episode reward: 3.5808293549379724, agent episode reward: [4.62, 4.62, -0.8900715610346182, -4.76909908402741], time: 68.346
steps: 999975, episodes: 40000, mean episode variance: 16.525269548149314, agent episode variance: [0.11557460182905196, 0.07603848457336426, 0.08786228862544522, 16.245794173121453], time: 68.346
steps: 999975, episodes: 40000, mean episode cvar: 14.425204902183264, agent episode cvar: [7.054257350921631, 7.067752864837646, -0.01827093170955777, 0.3214656181335449], time: 68.347
Running avgs for agent 0: q_loss: 4.2350287437438965, p_loss: -20.48105812072754, mean_rew: 0.1132421875, variance: 0.46229840731620786, cvar: 28.217031478881836, v: 24.214040756225586, mean_q: 20.431949615478516, std_q: 3.1002113819122314, lamda: 1.6107430458068848
Running avgs for agent 1: q_loss: 4.762056827545166, p_loss: -22.865575790405273, mean_rew: 0.1124609375, variance: 0.30415393829345705, cvar: 28.271011352539062, v: 24.055376052856445, mean_q: 22.83502960205078, std_q: 2.199493646621704, lamda: 1.6375327110290527
Running avgs for agent 2: q_loss: 0.3402959704399109, p_loss: 0.18627403676509857, mean_rew: -0.07158107019302198, variance: 0.3514491545017809, cvar: -0.07308372855186462, v: -0.09015635401010513, mean_q: -0.23156073689460754, std_q: 1.3901687860488892, lamda: 1.1948720216751099
Running avgs for agent 3: q_loss: 226.43186950683594, p_loss: 23.562667846679688, mean_rew: -0.3350113107968671, variance: 64.98317669248581, cvar: 1.2858624458312988, v: -16.946964263916016, mean_q: -24.11343765258789, std_q: 12.239476203918457, lamda: 1.6684811115264893

steps: 1024975, episodes: 41000, mean episode reward: 3.422772484208338, agent episode reward: [3.94, 3.94, -0.8243262604785644, -3.6329012553130977], time: 69.574
steps: 1024975, episodes: 41000, mean episode variance: 12.806990287053399, agent episode variance: [0.18139032423496246, 0.03171625638008118, 0.08626663719397039, 12.507617069244384], time: 69.575
steps: 1024975, episodes: 41000, mean episode cvar: 14.31249057554081, agent episode cvar: [7.082745628356934, 7.070448354721069, -0.018041059743613006, 0.1773376522064209], time: 69.575
Running avgs for agent 0: q_loss: 5.319007396697998, p_loss: -20.543750762939453, mean_rew: 0.11109375, variance: 0.7255612969398498, cvar: 28.330982208251953, v: 24.159278869628906, mean_q: 20.467721939086914, std_q: 3.1135499477386475, lamda: 1.630414605140686
Running avgs for agent 1: q_loss: 3.487844228744507, p_loss: -22.628936767578125, mean_rew: 0.1122265625, variance: 0.12686502552032472, cvar: 28.28179359436035, v: 23.967153549194336, mean_q: 22.593976974487305, std_q: 2.299010753631592, lamda: 1.6520944833755493
Running avgs for agent 2: q_loss: 0.31036120653152466, p_loss: 0.17364095151424408, mean_rew: -0.06653508794783054, variance: 0.34506654877588155, cvar: -0.07216423749923706, v: -0.08387099951505661, mean_q: -0.21238906681537628, std_q: 1.2835298776626587, lamda: 1.2065610885620117
Running avgs for agent 3: q_loss: 117.45768737792969, p_loss: 21.983367919921875, mean_rew: -0.3316297397646276, variance: 50.03046827697754, cvar: 0.7093505859375, v: -16.690303802490234, mean_q: -22.487159729003906, std_q: 11.338940620422363, lamda: 1.6885297298431396

steps: 1049975, episodes: 42000, mean episode reward: 3.0917896649442116, agent episode reward: [3.71, 3.71, -0.636849292230793, -3.6913610428249948], time: 73.644
steps: 1049975, episodes: 42000, mean episode variance: 11.577375258957968, agent episode variance: [0.1523302114009857, 0.1015619478225708, 0.07127280840836465, 11.252210291326046], time: 73.644
steps: 1049975, episodes: 42000, mean episode cvar: 14.245991945892573, agent episode cvar: [7.102925075531006, 7.07243479347229, -0.0186196092069149, 0.08925168609619141], time: 73.645
Running avgs for agent 0: q_loss: 4.7464728355407715, p_loss: -20.54951286315918, mean_rew: 0.112890625, variance: 0.6093208456039428, cvar: 28.411699295043945, v: 24.193531036376953, mean_q: 20.469327926635742, std_q: 3.1524882316589355, lamda: 1.6492211818695068
Running avgs for agent 1: q_loss: 5.772867202758789, p_loss: -22.650880813598633, mean_rew: 0.1148046875, variance: 0.4062477912902832, cvar: 28.289737701416016, v: 23.975250244140625, mean_q: 22.62589454650879, std_q: 2.271479606628418, lamda: 1.6673320531845093
Running avgs for agent 2: q_loss: 0.2860376834869385, p_loss: 0.14344105124473572, mean_rew: -0.05563358947611146, variance: 0.2850912336334586, cvar: -0.07447843998670578, v: -0.09115687757730484, mean_q: -0.1711774319410324, std_q: 0.7344565391540527, lamda: 1.2164571285247803
Running avgs for agent 3: q_loss: 55.57150650024414, p_loss: 19.124292373657227, mean_rew: -0.3185184620861221, variance: 45.00884116530418, cvar: 0.3570067584514618, v: -15.279544830322266, mean_q: -19.547042846679688, std_q: 9.811654090881348, lamda: 1.7081215381622314

steps: 1074975, episodes: 43000, mean episode reward: 2.7420471706108693, agent episode reward: [3.72, 3.72, -0.5329060332865593, -4.165046796102572], time: 68.178
steps: 1074975, episodes: 43000, mean episode variance: 8.408651652596891, agent episode variance: [0.22256974720954895, 0.1740456556081772, 0.06979870273917914, 7.942237547039985], time: 68.178
steps: 1074975, episodes: 43000, mean episode cvar: 14.244801914304494, agent episode cvar: [7.101542512893677, 7.069252885818481, -0.018397232919931412, 0.09240374851226807], time: 68.179
Running avgs for agent 0: q_loss: 4.951740264892578, p_loss: -20.444520950317383, mean_rew: 0.1121484375, variance: 0.8902789888381958, cvar: 28.406169891357422, v: 24.157936096191406, mean_q: 20.389883041381836, std_q: 3.1625635623931885, lamda: 1.669325351715088
Running avgs for agent 1: q_loss: 4.702668190002441, p_loss: -22.417850494384766, mean_rew: 0.110859375, variance: 0.6961826224327088, cvar: 28.27701187133789, v: 24.035266876220703, mean_q: 22.391006469726562, std_q: 2.361656665802002, lamda: 1.6826444864273071
Running avgs for agent 2: q_loss: 0.27214711904525757, p_loss: 0.13728179037570953, mean_rew: -0.04963013053503468, variance: 0.27919481095671655, cvar: -0.07358893007040024, v: -0.09025205671787262, mean_q: -0.16142700612545013, std_q: 0.6391468644142151, lamda: 1.227070689201355
Running avgs for agent 3: q_loss: 41.74357986450195, p_loss: 16.595054626464844, mean_rew: -0.31192602893723415, variance: 31.76895018815994, cvar: 0.36961498856544495, v: -13.138243675231934, mean_q: -16.97965431213379, std_q: 8.533154487609863, lamda: 1.72796630859375

steps: 1099975, episodes: 44000, mean episode reward: 2.2765759630018603, agent episode reward: [3.76, 3.76, -0.6956476530936548, -4.547776383904485], time: 69.211
steps: 1099975, episodes: 44000, mean episode variance: 5.832339493324048, agent episode variance: [0.17346379297971726, 0.25736056742072105, 0.06595964076090605, 5.335555492162705], time: 69.211
steps: 1099975, episodes: 44000, mean episode cvar: 14.170622550785541, agent episode cvar: [7.072784120559692, 7.0900785427093505, -0.018329934298992157, 0.026089821815490723], time: 69.212
Running avgs for agent 0: q_loss: 5.977120876312256, p_loss: -20.530254364013672, mean_rew: 0.107734375, variance: 0.693855171918869, cvar: 28.291135787963867, v: 24.094146728515625, mean_q: 20.47223472595215, std_q: 3.07887601852417, lamda: 1.6874091625213623
Running avgs for agent 1: q_loss: 5.662928581237793, p_loss: -22.229995727539062, mean_rew: 0.1108203125, variance: 1.0294422696828842, cvar: 28.360315322875977, v: 23.896753311157227, mean_q: 22.199249267578125, std_q: 2.452500343322754, lamda: 1.7010846138000488
Running avgs for agent 2: q_loss: 0.2674557566642761, p_loss: 0.13999545574188232, mean_rew: -0.05076969542433091, variance: 0.2638385630436242, cvar: -0.07331973314285278, v: -0.0891576036810875, mean_q: -0.16499905288219452, std_q: 0.6472474336624146, lamda: 1.2381561994552612
Running avgs for agent 3: q_loss: 20.363670349121094, p_loss: 14.835528373718262, mean_rew: -0.3237520350687969, variance: 21.34222196865082, cvar: 0.10435927659273148, v: -10.649868965148926, mean_q: -15.234201431274414, std_q: 7.7269463539123535, lamda: 1.7463334798812866

steps: 1124975, episodes: 45000, mean episode reward: 2.6579575432486156, agent episode reward: [3.55, 3.55, -0.59233766820091, -3.8497047885504743], time: 68.868
steps: 1124975, episodes: 45000, mean episode variance: 2.9628182405836414, agent episode variance: [0.1224782327413559, 0.05818168723583222, 0.05993255114625208, 2.7222257694602012], time: 68.869
steps: 1124975, episodes: 45000, mean episode cvar: 14.187624188948423, agent episode cvar: [7.073647912979126, 7.074223474502563, -0.017922217320650816, 0.057675018787384034], time: 68.87
Running avgs for agent 0: q_loss: 4.817054748535156, p_loss: -20.623313903808594, mean_rew: 0.106328125, variance: 0.4899129309654236, cvar: 28.294591903686523, v: 24.020231246948242, mean_q: 20.521099090576172, std_q: 3.0641121864318848, lamda: 1.7059223651885986
Running avgs for agent 1: q_loss: 3.5733940601348877, p_loss: -21.989700317382812, mean_rew: 0.1065625, variance: 0.23272674894332887, cvar: 28.296894073486328, v: 23.863922119140625, mean_q: 21.95035743713379, std_q: 2.5349888801574707, lamda: 1.719388484954834
Running avgs for agent 2: q_loss: 0.22578759491443634, p_loss: 0.13935792446136475, mean_rew: -0.04656492275313491, variance: 0.23973020458500832, cvar: -0.07168887555599213, v: -0.09178084880113602, mean_q: -0.16027231514453888, std_q: 0.6126964688301086, lamda: 1.2503013610839844
Running avgs for agent 3: q_loss: 10.592507362365723, p_loss: 13.829144477844238, mean_rew: -0.3208003833167346, variance: 10.888903077840805, cvar: 0.2307000756263733, v: -8.577248573303223, mean_q: -14.275751113891602, std_q: 7.358268737792969, lamda: 1.763388752937317

steps: 1149975, episodes: 46000, mean episode reward: 3.0410125337951768, agent episode reward: [3.74, 3.74, -0.706624748981995, -3.732362717222828], time: 69.621
steps: 1149975, episodes: 46000, mean episode variance: 2.9379633885291403, agent episode variance: [0.13435258144140244, 0.11950996944308281, 0.05534603133471683, 2.6287548063099386], time: 69.622
steps: 1149975, episodes: 46000, mean episode cvar: 14.151225740935654, agent episode cvar: [7.072634000778198, 7.054972787857055, -0.01714729020372033, 0.04076624250411987], time: 69.623
Running avgs for agent 0: q_loss: 4.6147541999816895, p_loss: -20.65876007080078, mean_rew: 0.1081640625, variance: 0.5374103257656098, cvar: 28.29053497314453, v: 24.055906295776367, mean_q: 20.599756240844727, std_q: 3.0202832221984863, lamda: 1.724729061126709
Running avgs for agent 1: q_loss: 5.435394763946533, p_loss: -21.969446182250977, mean_rew: 0.1098046875, variance: 0.47803987777233126, cvar: 28.219892501831055, v: 23.794021606445312, mean_q: 21.930952072143555, std_q: 2.5084216594696045, lamda: 1.7375448942184448
Running avgs for agent 2: q_loss: 0.21449293196201324, p_loss: 0.13559749722480774, mean_rew: -0.04414800261452207, variance: 0.2213841253388673, cvar: -0.06858915835618973, v: -0.08525639027357101, mean_q: -0.15591713786125183, std_q: 0.636763334274292, lamda: 1.2640101909637451
Running avgs for agent 3: q_loss: 10.243844032287598, p_loss: 12.575387001037598, mean_rew: -0.32216133139031966, variance: 10.515019225239755, cvar: 0.16306497156620026, v: -6.967552185058594, mean_q: -13.074041366577148, std_q: 6.959092617034912, lamda: 1.7807211875915527

steps: 1174975, episodes: 47000, mean episode reward: 2.66374244245077, agent episode reward: [3.17, 3.17, -0.5725349431596131, -3.103722614389617], time: 69.997
steps: 1174975, episodes: 47000, mean episode variance: 2.914419626268558, agent episode variance: [0.2622657123208046, 0.1756892657279968, 0.05503684449475259, 2.4214278037250043], time: 69.998
steps: 1174975, episodes: 47000, mean episode cvar: 14.208706448925659, agent episode cvar: [7.123837856292725, 7.096171583175659, -0.016766942607238888, 0.00546395206451416], time: 69.998
Running avgs for agent 0: q_loss: 7.1658172607421875, p_loss: -20.687572479248047, mean_rew: 0.107734375, variance: 1.0490628492832184, cvar: 28.495351791381836, v: 24.11773109436035, mean_q: 20.604026794433594, std_q: 3.078911066055298, lamda: 1.745115876197815
Running avgs for agent 1: q_loss: 5.6809210777282715, p_loss: -21.9641056060791, mean_rew: 0.1066796875, variance: 0.7027570629119873, cvar: 28.384687423706055, v: 23.822349548339844, mean_q: 21.920856475830078, std_q: 2.575063467025757, lamda: 1.755422830581665
Running avgs for agent 2: q_loss: 0.21988126635551453, p_loss: 0.13121551275253296, mean_rew: -0.04097917772412192, variance: 0.22014737797901035, cvar: -0.06706776469945908, v: -0.0836164727807045, mean_q: -0.1495184600353241, std_q: 0.6075229644775391, lamda: 1.2772905826568604
Running avgs for agent 3: q_loss: 5.548761367797852, p_loss: 11.048871994018555, mean_rew: -0.3210770147762145, variance: 9.685711214900017, cvar: 0.021855808794498444, v: -5.562381744384766, mean_q: -11.562187194824219, std_q: 6.378961086273193, lamda: 1.7962148189544678

steps: 1199975, episodes: 48000, mean episode reward: 3.784671587936942, agent episode reward: [4.16, 4.16, -0.5311322836222891, -4.0041961284407686], time: 69.107
steps: 1199975, episodes: 48000, mean episode variance: 1.7649665690967813, agent episode variance: [0.20832723492383956, 0.1425649962425232, 0.04971508321259171, 1.364359254717827], time: 69.108
steps: 1199975, episodes: 48000, mean episode cvar: 14.220308714162558, agent episode cvar: [7.119023488998413, 7.087998090744018, -0.01820546768978238, 0.031492602109909056], time: 69.108
Running avgs for agent 0: q_loss: 5.157294750213623, p_loss: -20.540903091430664, mean_rew: 0.105703125, variance: 0.8333089396953582, cvar: 28.476093292236328, v: 24.09142303466797, mean_q: 20.46002769470215, std_q: 3.1324970722198486, lamda: 1.765819787979126
Running avgs for agent 1: q_loss: 5.1496357917785645, p_loss: -21.94700813293457, mean_rew: 0.1083984375, variance: 0.5702599849700928, cvar: 28.351991653442383, v: 23.786985397338867, mean_q: 21.912097930908203, std_q: 2.5507171154022217, lamda: 1.77370285987854
Running avgs for agent 2: q_loss: 0.18343974649906158, p_loss: 0.12740924954414368, mean_rew: -0.038269144580123544, variance: 0.19886033285036683, cvar: -0.07282187044620514, v: -0.08522508293390274, mean_q: -0.14392150938510895, std_q: 0.5523757338523865, lamda: 1.2902032136917114
Running avgs for agent 3: q_loss: 3.948509693145752, p_loss: 9.994343757629395, mean_rew: -0.3278947902993375, variance: 5.457437018871308, cvar: 0.12597040832042694, v: -4.405653953552246, mean_q: -10.563737869262695, std_q: 6.359286308288574, lamda: 1.8104497194290161

steps: 1224975, episodes: 49000, mean episode reward: 3.591085490741302, agent episode reward: [4.07, 4.07, -0.4988256748754648, -4.050088834383233], time: 69.938
steps: 1224975, episodes: 49000, mean episode variance: 1.4666064133476466, agent episode variance: [0.2130042372941971, 0.07014407527446746, 0.05088531587831676, 1.1325727849006653], time: 69.938
steps: 1224975, episodes: 49000, mean episode cvar: 14.154164542816579, agent episode cvar: [7.090169979095459, 7.075080505371094, -0.018257286407053472, 0.007171344757080078], time: 69.939
Running avgs for agent 0: q_loss: 4.808326244354248, p_loss: -20.4931697845459, mean_rew: 0.1071875, variance: 0.8520169491767884, cvar: 28.360679626464844, v: 23.924036026000977, mean_q: 20.40291404724121, std_q: 3.1108992099761963, lamda: 1.7858637571334839
Running avgs for agent 1: q_loss: 4.912965774536133, p_loss: -21.75931167602539, mean_rew: 0.105859375, variance: 0.28057630109786985, cvar: 28.300321578979492, v: 23.748456954956055, mean_q: 21.72386360168457, std_q: 2.6078929901123047, lamda: 1.7917885780334473
Running avgs for agent 2: q_loss: 0.19379116594791412, p_loss: 0.12633877992630005, mean_rew: -0.03754218902547918, variance: 0.20354126351326704, cvar: -0.07302914559841156, v: -0.08646897971630096, mean_q: -0.14191897213459015, std_q: 0.5403546094894409, lamda: 1.298343300819397
Running avgs for agent 3: q_loss: 2.7473409175872803, p_loss: 8.851861000061035, mean_rew: -0.3234272643827047, variance: 4.530291139602661, cvar: 0.028685379773378372, v: -3.507927179336548, mean_q: -9.446161270141602, std_q: 6.101155757904053, lamda: 1.8231059312820435

steps: 1249975, episodes: 50000, mean episode reward: 3.489828744498155, agent episode reward: [3.86, 3.86, -0.6074242995499107, -3.6227469559519347], time: 69.586
steps: 1249975, episodes: 50000, mean episode variance: 1.3049296882888302, agent episode variance: [0.06166087085008621, 0.011260478556156158, 0.044664292394183576, 1.1873440464884042], time: 69.587
steps: 1249975, episodes: 50000, mean episode cvar: 14.1523750638254, agent episode cvar: [7.099989431381226, 7.075921030044555, -0.017719698499888182, -0.005815699100494385], time: 69.587
Running avgs for agent 0: q_loss: 6.1025872230529785, p_loss: -20.613019943237305, mean_rew: 0.107421875, variance: 0.24664348340034484, cvar: 28.39995765686035, v: 23.80935287475586, mean_q: 20.53653907775879, std_q: 3.0775272846221924, lamda: 1.8051344156265259
Running avgs for agent 1: q_loss: 4.597301959991455, p_loss: -21.822181701660156, mean_rew: 0.110625, variance: 0.045041914224624634, cvar: 28.30368423461914, v: 23.763221740722656, mean_q: 21.778467178344727, std_q: 2.606498956680298, lamda: 1.808397650718689
Running avgs for agent 2: q_loss: 0.1772463321685791, p_loss: 0.12473878264427185, mean_rew: -0.03774155239454075, variance: 0.1786571695767343, cvar: -0.07087879627943039, v: -0.08451765775680542, mean_q: -0.1405753493309021, std_q: 0.5413916707038879, lamda: 1.3103227615356445
Running avgs for agent 3: q_loss: 2.240882635116577, p_loss: 7.6358866691589355, mean_rew: -0.32672920913838993, variance: 4.749376185953617, cvar: -0.02326279692351818, v: -2.848799228668213, mean_q: -8.195773124694824, std_q: 5.885859489440918, lamda: 1.8356050252914429

steps: 1274975, episodes: 51000, mean episode reward: 3.527651584798585, agent episode reward: [3.81, 3.81, -0.48661982955401745, -3.6057285856473973], time: 69.69
steps: 1274975, episodes: 51000, mean episode variance: 1.2217962341587991, agent episode variance: [0.017911682963371277, 0.055893070578575135, 0.04117102685384452, 1.1068204537630082], time: 69.691
steps: 1274975, episodes: 51000, mean episode cvar: 14.152986104287207, agent episode cvar: [7.09137681388855, 7.07908318901062, -0.01717267294973135, -0.0003012256622314453], time: 69.691
Running avgs for agent 0: q_loss: 4.535975933074951, p_loss: -20.766855239868164, mean_rew: 0.1110546875, variance: 0.07164673185348511, cvar: 28.365507125854492, v: 23.779327392578125, mean_q: 20.71413803100586, std_q: 3.0070526599884033, lamda: 1.8244086503982544
Running avgs for agent 1: q_loss: 4.461745262145996, p_loss: -21.82529640197754, mean_rew: 0.1091796875, variance: 0.22357228231430054, cvar: 28.31633186340332, v: 23.787269592285156, mean_q: 21.7850399017334, std_q: 2.6146841049194336, lamda: 1.8230345249176025
Running avgs for agent 2: q_loss: 0.1517518013715744, p_loss: 0.12336177378892899, mean_rew: -0.03391984944747209, variance: 0.1646841074153781, cvar: -0.06869068741798401, v: -0.08404631167650223, mean_q: -0.1375170350074768, std_q: 0.5120344161987305, lamda: 1.3207637071609497
Running avgs for agent 3: q_loss: 2.3771328926086426, p_loss: 6.739192485809326, mean_rew: -0.33256831704653633, variance: 4.427281815052033, cvar: -0.00120490079279989, v: -2.4083149433135986, mean_q: -7.26009464263916, std_q: 5.811760425567627, lamda: 1.8468953371047974

steps: 1299975, episodes: 52000, mean episode reward: 3.439447835104308, agent episode reward: [3.98, 3.98, -0.6822974876846347, -3.838254677211057], time: 70.075
steps: 1299975, episodes: 52000, mean episode variance: 0.9436753166485577, agent episode variance: [0.2402935390472412, 0.04256254482269287, 0.04372892366535962, 0.6170903091132641], time: 70.075
steps: 1299975, episodes: 52000, mean episode cvar: 14.16566747970134, agent episode cvar: [7.097783189773559, 7.077721893310547, -0.016402435593307018, 0.006564832210540771], time: 70.076
Running avgs for agent 0: q_loss: 6.07528018951416, p_loss: -20.788259506225586, mean_rew: 0.117109375, variance: 0.9611741561889648, cvar: 28.391132354736328, v: 23.636808395385742, mean_q: 20.72113609313965, std_q: 3.0210585594177246, lamda: 1.8419626951217651
Running avgs for agent 1: q_loss: 4.852393627166748, p_loss: -21.806018829345703, mean_rew: 0.114375, variance: 0.1702501792907715, cvar: 28.31088638305664, v: 23.787092208862305, mean_q: 21.75423812866211, std_q: 2.6221518516540527, lamda: 1.8399405479431152
Running avgs for agent 2: q_loss: 0.16519807279109955, p_loss: 0.12948420643806458, mean_rew: -0.0351573960619736, variance: 0.17491569466143847, cvar: -0.06560973823070526, v: -0.08591167628765106, mean_q: -0.14171554148197174, std_q: 0.5345657467842102, lamda: 1.3307042121887207
Running avgs for agent 3: q_loss: 2.0103225708007812, p_loss: 6.230798244476318, mean_rew: -0.33675382046536095, variance: 2.4683612364530565, cvar: 0.02625933103263378, v: -2.079463481903076, mean_q: -6.764148235321045, std_q: 5.814605236053467, lamda: 1.8573490381240845

steps: 1324975, episodes: 53000, mean episode reward: 3.0755622902187922, agent episode reward: [3.47, 3.47, -0.49931854124242586, -3.365119168538781], time: 70.025
steps: 1324975, episodes: 53000, mean episode variance: 0.7845380527935922, agent episode variance: [0.08229591876268387, 0.08252723550796509, 0.04969603663310409, 0.5700188618898392], time: 70.025
steps: 1324975, episodes: 53000, mean episode cvar: 14.176471581712365, agent episode cvar: [7.088110557556153, 7.103442161560059, -0.01735160420835018, 0.0022704668045043946], time: 70.026
Running avgs for agent 0: q_loss: 5.224520683288574, p_loss: -20.910879135131836, mean_rew: 0.114921875, variance: 0.3291836750507355, cvar: 28.352441787719727, v: 23.570096969604492, mean_q: 20.837055206298828, std_q: 2.976257085800171, lamda: 1.8603774309158325
Running avgs for agent 1: q_loss: 6.004645347595215, p_loss: -21.968917846679688, mean_rew: 0.115546875, variance: 0.33010894203186036, cvar: 28.413766860961914, v: 23.90036392211914, mean_q: 21.91758918762207, std_q: 2.6125707626342773, lamda: 1.8572601079940796
Running avgs for agent 2: q_loss: 0.18815627694129944, p_loss: 0.12872320413589478, mean_rew: -0.034259581774830676, variance: 0.19878414653241636, cvar: -0.0694064199924469, v: -0.08442500978708267, mean_q: -0.14041556417942047, std_q: 0.5503308176994324, lamda: 1.3469637632369995
Running avgs for agent 3: q_loss: 1.9552595615386963, p_loss: 5.91705846786499, mean_rew: -0.3377383737165986, variance: 2.2800754475593568, cvar: 0.009081867523491383, v: -1.844286322593689, mean_q: -6.438701152801514, std_q: 5.891769886016846, lamda: 1.8667421340942383

steps: 1349975, episodes: 54000, mean episode reward: 3.2605510596808895, agent episode reward: [3.73, 3.73, -0.612794636266546, -3.5866543040525647], time: 69.957
steps: 1349975, episodes: 54000, mean episode variance: 1.1964256709688343, agent episode variance: [0.2730456187725067, 0.21295677936077118, 0.04543264965945855, 0.6649906231760979], time: 69.958
steps: 1349975, episodes: 54000, mean episode cvar: 14.141931219696998, agent episode cvar: [7.1027904167175295, 7.0649585590362545, -0.017436113834381102, -0.00838164222240448], time: 69.958
Running avgs for agent 0: q_loss: 8.587798118591309, p_loss: -20.914175033569336, mean_rew: 0.1185546875, variance: 1.0921824750900269, cvar: 28.411161422729492, v: 23.50481414794922, mean_q: 20.84262466430664, std_q: 2.991595983505249, lamda: 1.8795424699783325
Running avgs for agent 1: q_loss: 5.9822235107421875, p_loss: -22.07143783569336, mean_rew: 0.1169921875, variance: 0.8518271174430847, cvar: 28.259836196899414, v: 23.802505493164062, mean_q: 22.02863311767578, std_q: 2.5215277671813965, lamda: 1.8736333847045898
Running avgs for agent 2: q_loss: 0.17064139246940613, p_loss: 0.12798239290714264, mean_rew: -0.032907341240319306, variance: 0.1817305986378342, cvar: -0.06974445283412933, v: -0.08774767071008682, mean_q: -0.138111874461174, std_q: 0.5180338025093079, lamda: 1.363124132156372
Running avgs for agent 3: q_loss: 1.830615520477295, p_loss: 5.399933338165283, mean_rew: -0.33747166124141825, variance: 2.6599624927043917, cvar: -0.03352656587958336, v: -1.6772056818008423, mean_q: -5.889918804168701, std_q: 5.951313018798828, lamda: 1.8767404556274414

steps: 1374975, episodes: 55000, mean episode reward: 2.5104296848765575, agent episode reward: [3.05, 3.05, -0.5462921975259182, -3.043278117597524], time: 70.305
steps: 1374975, episodes: 55000, mean episode variance: 1.0264203149348032, agent episode variance: [0.1230989660024643, 0.12072136759757995, 0.03866213072533719, 0.7439378506094217], time: 70.306
steps: 1374975, episodes: 55000, mean episode cvar: 14.158204203881324, agent episode cvar: [7.121185918807983, 7.068359153747559, -0.017182527028024197, -0.014158341646194459], time: 70.306
Running avgs for agent 0: q_loss: 7.6482157707214355, p_loss: -21.13610076904297, mean_rew: 0.1201171875, variance: 0.4923958640098572, cvar: 28.484743118286133, v: 23.415611267089844, mean_q: 21.078413009643555, std_q: 2.9570322036743164, lamda: 1.899066686630249
Running avgs for agent 1: q_loss: 5.836606025695801, p_loss: -21.98701286315918, mean_rew: 0.1190234375, variance: 0.4828854703903198, cvar: 28.2734375, v: 23.71937370300293, mean_q: 21.945636749267578, std_q: 2.577615261077881, lamda: 1.8928031921386719
Running avgs for agent 2: q_loss: 0.15095224976539612, p_loss: 0.12650303542613983, mean_rew: -0.029902233018028416, variance: 0.15464852290134876, cvar: -0.06873010843992233, v: -0.08800686150789261, mean_q: -0.13513588905334473, std_q: 0.49826034903526306, lamda: 1.3780157566070557
Running avgs for agent 3: q_loss: 1.9593244791030884, p_loss: 4.8925065994262695, mean_rew: -0.3328102421423848, variance: 2.975751402437687, cvar: -0.0566333644092083, v: -1.5239970684051514, mean_q: -5.368375778198242, std_q: 6.067331314086914, lamda: 1.8869374990463257

steps: 1399975, episodes: 56000, mean episode reward: 2.8418341532971634, agent episode reward: [3.27, 3.27, -0.2765232850039248, -3.421642561698912], time: 70.112
steps: 1399975, episodes: 56000, mean episode variance: 1.0497071031956002, agent episode variance: [0.2644008135795593, 0.012362008333206177, 0.04105799824278802, 0.7318862830400467], time: 70.112
steps: 1399975, episodes: 56000, mean episode cvar: 14.134263607610016, agent episode cvar: [7.113530694961548, 7.057553358078003, -0.018578316401690245, -0.018242129027843477], time: 70.113
Running avgs for agent 0: q_loss: 7.95754861831665, p_loss: -21.237449645996094, mean_rew: 0.123671875, variance: 1.0576032543182372, cvar: 28.454120635986328, v: 23.42486572265625, mean_q: 21.161212921142578, std_q: 2.931774854660034, lamda: 1.9188097715377808
Running avgs for agent 1: q_loss: 4.258434772491455, p_loss: -22.071279525756836, mean_rew: 0.1190625, variance: 0.04944803333282471, cvar: 28.230213165283203, v: 23.62289047241211, mean_q: 22.024982452392578, std_q: 2.5471811294555664, lamda: 1.9104758501052856
Running avgs for agent 2: q_loss: 0.15543349087238312, p_loss: 0.12602105736732483, mean_rew: -0.03034537312854748, variance: 0.16423199297115207, cvar: -0.07431326061487198, v: -0.08773524314165115, mean_q: -0.13612766563892365, std_q: 0.5257232785224915, lamda: 1.3906276226043701
Running avgs for agent 3: q_loss: 1.8852636814117432, p_loss: 4.516227722167969, mean_rew: -0.3358942726588289, variance: 2.9275451321601866, cvar: -0.0729685127735138, v: -1.4230064153671265, mean_q: -4.956464767456055, std_q: 6.076449871063232, lamda: 1.8984358310699463

steps: 1424975, episodes: 57000, mean episode reward: 2.529631855483507, agent episode reward: [3.25, 3.25, -0.803881361550364, -3.1664867829661287], time: 70.164
steps: 1424975, episodes: 57000, mean episode variance: 1.143433249537833, agent episode variance: [0.44548652303218844, 0.0977611890733242, 0.04780977263022214, 0.5523757648020983], time: 70.165
steps: 1424975, episodes: 57000, mean episode cvar: 14.133872114297002, agent episode cvar: [7.090066959381104, 7.0657360496521, -0.018113412026315927, -0.0038174827098846438], time: 70.165
Running avgs for agent 0: q_loss: 9.827293395996094, p_loss: -21.26713752746582, mean_rew: 0.120234375, variance: 1.7819460921287538, cvar: 28.360267639160156, v: 23.311832427978516, mean_q: 21.190645217895508, std_q: 2.8852384090423584, lamda: 1.93959641456604
Running avgs for agent 1: q_loss: 6.003201961517334, p_loss: -22.059629440307617, mean_rew: 0.1220703125, variance: 0.3910447562932968, cvar: 28.2629451751709, v: 23.61316680908203, mean_q: 22.00363540649414, std_q: 2.5540060997009277, lamda: 1.9270087480545044
Running avgs for agent 2: q_loss: 0.17822492122650146, p_loss: 0.12286574393510818, mean_rew: -0.030683951056417563, variance: 0.19123909052088855, cvar: -0.07245364040136337, v: -0.08532612770795822, mean_q: -0.13143083453178406, std_q: 0.4986117482185364, lamda: 1.4040859937667847
Running avgs for agent 3: q_loss: 1.8600834608078003, p_loss: 4.251436710357666, mean_rew: -0.3291553850515462, variance: 2.209503059208393, cvar: -0.015269930474460125, v: -1.2524874210357666, mean_q: -4.690186500549316, std_q: 6.076623916625977, lamda: 1.9081766605377197

steps: 1449975, episodes: 58000, mean episode reward: 2.919886840141621, agent episode reward: [3.45, 3.45, -0.6176515023501017, -3.362461657508277], time: 70.025
steps: 1449975, episodes: 58000, mean episode variance: 0.6506713686974254, agent episode variance: [0.07724460056424141, 0.04808979639410973, 0.04481218613102101, 0.48052478560805323], time: 70.025
steps: 1449975, episodes: 58000, mean episode cvar: 14.162326491840183, agent episode cvar: [7.0829620323181155, 7.096900428771972, -0.01653025221079588, -0.0010057170391082765], time: 70.026
Running avgs for agent 0: q_loss: 6.52340030670166, p_loss: -21.420175552368164, mean_rew: 0.1240625, variance: 0.30897840225696566, cvar: 28.33184814453125, v: 23.2232608795166, mean_q: 21.345022201538086, std_q: 2.8396425247192383, lamda: 1.9587831497192383
Running avgs for agent 1: q_loss: 5.9745097160339355, p_loss: -22.04093360900879, mean_rew: 0.1248046875, variance: 0.1923591855764389, cvar: 28.387601852416992, v: 23.64764404296875, mean_q: 21.98603057861328, std_q: 2.618741035461426, lamda: 1.9438152313232422
Running avgs for agent 2: q_loss: 0.16327816247940063, p_loss: 0.12486334145069122, mean_rew: -0.03007706476849391, variance: 0.17924874452408404, cvar: -0.06612101197242737, v: -0.08873649686574936, mean_q: -0.13278353214263916, std_q: 0.5005883574485779, lamda: 1.4185785055160522
Running avgs for agent 3: q_loss: 1.7936115264892578, p_loss: 4.12445068359375, mean_rew: -0.32499281198938196, variance: 1.922099142432213, cvar: -0.004022868350148201, v: -1.183245062828064, mean_q: -4.547037124633789, std_q: 6.140827178955078, lamda: 1.9183969497680664

steps: 1474975, episodes: 59000, mean episode reward: 3.1095654933525534, agent episode reward: [3.65, 3.65, -0.6378579710696933, -3.552576535577753], time: 70.19
steps: 1474975, episodes: 59000, mean episode variance: 1.0989406201611274, agent episode variance: [0.06680213189125062, 0.44164298725128176, 0.04168437648518011, 0.5488111245334149], time: 70.19
steps: 1474975, episodes: 59000, mean episode cvar: 14.147237300135195, agent episode cvar: [7.065302219390869, 7.09697885131836, -0.01813725885003805, 0.003093488276004791], time: 70.191
Running avgs for agent 0: q_loss: 6.451616287231445, p_loss: -21.557453155517578, mean_rew: 0.1249609375, variance: 0.26720852756500246, cvar: 28.26120948791504, v: 23.184316635131836, mean_q: 21.487520217895508, std_q: 2.7774877548217773, lamda: 1.9765478372573853
Running avgs for agent 1: q_loss: 8.800973892211914, p_loss: -22.026090621948242, mean_rew: 0.1274609375, variance: 1.766571949005127, cvar: 28.387916564941406, v: 23.590364456176758, mean_q: 21.96331214904785, std_q: 2.6305668354034424, lamda: 1.9618310928344727
Running avgs for agent 2: q_loss: 0.159026637673378, p_loss: 0.12491954118013382, mean_rew: -0.030248168086118486, variance: 0.16673750594072043, cvar: -0.07254903018474579, v: -0.08779983967542648, mean_q: -0.13204370439052582, std_q: 0.5001605749130249, lamda: 1.4355008602142334
Running avgs for agent 3: q_loss: 2.116208553314209, p_loss: 4.034134864807129, mean_rew: -0.3276020148302036, variance: 2.1952444981336594, cvar: 0.012373953126370907, v: -1.1150778532028198, mean_q: -4.443070411682129, std_q: 6.240353584289551, lamda: 1.9293720722198486

steps: 1499975, episodes: 60000, mean episode reward: 2.3733085185533556, agent episode reward: [2.78, 2.78, -0.5065519596134717, -2.6801395218331727], time: 70.352
steps: 1499975, episodes: 60000, mean episode variance: 0.6981936162319035, agent episode variance: [0.08994892752170562, 0.013275800466537476, 0.04084449740312993, 0.5541243908405304], time: 70.352
steps: 1499975, episodes: 60000, mean episode cvar: 14.137260377839208, agent episode cvar: [7.081164861679077, 7.067761081695557, -0.017620715901255607, 0.005955150365829468], time: 70.353
Running avgs for agent 0: q_loss: 6.451910972595215, p_loss: -21.780548095703125, mean_rew: 0.1299609375, variance: 0.3597957100868225, cvar: 28.32465934753418, v: 23.274642944335938, mean_q: 21.70968246459961, std_q: 2.7232401371002197, lamda: 1.9938466548919678
Running avgs for agent 1: q_loss: 4.597088813781738, p_loss: -22.06374168395996, mean_rew: 0.1265234375, variance: 0.053103201866149904, cvar: 28.271045684814453, v: 23.492429733276367, mean_q: 21.99742889404297, std_q: 2.610283851623535, lamda: 1.9788435697555542
Running avgs for agent 2: q_loss: 0.15178947150707245, p_loss: 0.1205764040350914, mean_rew: -0.029480682501716836, variance: 0.16337798961251973, cvar: -0.07048286497592926, v: -0.08722499012947083, mean_q: -0.1284114271402359, std_q: 0.4546271860599518, lamda: 1.4454469680786133
Running avgs for agent 3: q_loss: 2.077065944671631, p_loss: 3.934469223022461, mean_rew: -0.31528156918042394, variance: 2.216497563362122, cvar: 0.023820601403713226, v: -1.0637544393539429, mean_q: -4.314070224761963, std_q: 6.110716342926025, lamda: 1.942685842514038

steps: 1524975, episodes: 61000, mean episode reward: 2.2654257812746046, agent episode reward: [3.0, 3.0, -0.9189439879525525, -2.8156302307728427], time: 70.34
steps: 1524975, episodes: 61000, mean episode variance: 0.6871088480602484, agent episode variance: [0.10976968228816986, 0.14069835072755812, 0.038863598431227726, 0.3977772166132927], time: 70.341
steps: 1524975, episodes: 61000, mean episode cvar: 14.175631658047438, agent episode cvar: [7.103233009338379, 7.083809417724609, -0.01780194887518883, 0.006391179859638214], time: 70.341
Running avgs for agent 0: q_loss: 7.883391380310059, p_loss: -21.745737075805664, mean_rew: 0.129765625, variance: 0.4390787291526794, cvar: 28.412933349609375, v: 23.342809677124023, mean_q: 21.673152923583984, std_q: 2.7497143745422363, lamda: 2.011993646621704
Running avgs for agent 1: q_loss: 5.564236164093018, p_loss: -21.859046936035156, mean_rew: 0.129140625, variance: 0.5627934029102325, cvar: 28.335237503051758, v: 23.391279220581055, mean_q: 21.79033660888672, std_q: 2.707153797149658, lamda: 1.9965766668319702
Running avgs for agent 2: q_loss: 0.1427941918373108, p_loss: 0.12018473446369171, mean_rew: -0.028574400449815424, variance: 0.1554543937249109, cvar: -0.07120779156684875, v: -0.08574900776147842, mean_q: -0.1275944709777832, std_q: 0.4651261866092682, lamda: 1.457857370376587
Running avgs for agent 3: q_loss: 1.8414064645767212, p_loss: 3.8066563606262207, mean_rew: -0.2984317279204281, variance: 1.5911088664531707, cvar: 0.02556472085416317, v: -1.0418190956115723, mean_q: -4.174393653869629, std_q: 6.253881931304932, lamda: 1.9540178775787354

steps: 1549975, episodes: 62000, mean episode reward: 2.12038278832782, agent episode reward: [2.67, 2.67, -0.6413409643342106, -2.57827624733797], time: 70.073
steps: 1549975, episodes: 62000, mean episode variance: 0.812330368029885, agent episode variance: [0.0015262577533721924, 0.3114782232046127, 0.038876434686593714, 0.4604494523853064], time: 70.073
steps: 1549975, episodes: 62000, mean episode cvar: 14.171808609392494, agent episode cvar: [7.074703691482544, 7.115035060882568, -0.017887126717716455, -4.3016254901885985e-05], time: 70.074
Running avgs for agent 0: q_loss: 5.086663722991943, p_loss: -21.748550415039062, mean_rew: 0.1273828125, variance: 0.0061050310134887695, cvar: 28.29881477355957, v: 23.16623306274414, mean_q: 21.680967330932617, std_q: 2.725955009460449, lamda: 2.0309581756591797
Running avgs for agent 1: q_loss: 7.637345790863037, p_loss: -21.600109100341797, mean_rew: 0.1283984375, variance: 1.2459128928184509, cvar: 28.460140228271484, v: 23.194063186645508, mean_q: 21.51630973815918, std_q: 2.8353803157806396, lamda: 2.0179007053375244
Running avgs for agent 2: q_loss: 0.144300177693367, p_loss: 0.11906111240386963, mean_rew: -0.028750807189938662, variance: 0.15550573874637486, cvar: -0.07154850661754608, v: -0.0881904736161232, mean_q: -0.12707389891147614, std_q: 0.44056448340415955, lamda: 1.4715662002563477
Running avgs for agent 3: q_loss: 1.8040577173233032, p_loss: 3.7029106616973877, mean_rew: -0.28843665114218575, variance: 1.8417978095412255, cvar: -0.0001720652508083731, v: -1.0040663480758667, mean_q: -4.0563578605651855, std_q: 6.17543888092041, lamda: 1.965206503868103

steps: 1574975, episodes: 63000, mean episode reward: 2.168128122419858, agent episode reward: [2.71, 2.71, -0.5816572165740874, -2.670214661006054], time: 70.289
steps: 1574975, episodes: 63000, mean episode variance: 0.7886213973211125, agent episode variance: [0.1659021862745285, 0.08594800692796707, 0.04064940613973886, 0.496121797978878], time: 70.291
steps: 1574975, episodes: 63000, mean episode cvar: 14.179991591580212, agent episode cvar: [7.093557104110718, 7.106500061035156, -0.017805030696094036, -0.002260542869567871], time: 70.292
Running avgs for agent 0: q_loss: 7.346695423126221, p_loss: -21.780927658081055, mean_rew: 0.129765625, variance: 0.663608745098114, cvar: 28.374225616455078, v: 23.15638542175293, mean_q: 21.713546752929688, std_q: 2.7257847785949707, lamda: 2.0493364334106445
Running avgs for agent 1: q_loss: 5.833767414093018, p_loss: -21.272432327270508, mean_rew: 0.1284375, variance: 0.3437920277118683, cvar: 28.426000595092773, v: 22.961008071899414, mean_q: 21.189180374145508, std_q: 2.9578027725219727, lamda: 2.0379457473754883
Running avgs for agent 2: q_loss: 0.15254706144332886, p_loss: 0.11912475526332855, mean_rew: -0.02995270622923425, variance: 0.16259762455895543, cvar: -0.07122012227773666, v: -0.08951953053474426, mean_q: -0.12808972597122192, std_q: 0.46021685004234314, lamda: 1.4850225448608398
Running avgs for agent 3: q_loss: 1.9043618440628052, p_loss: 3.594820976257324, mean_rew: -0.27948078514432256, variance: 1.984487191915512, cvar: -0.009042171761393547, v: -0.945527970790863, mean_q: -3.933218240737915, std_q: 6.095601558685303, lamda: 1.976781964302063

steps: 1599975, episodes: 64000, mean episode reward: 2.074281827322744, agent episode reward: [2.75, 2.75, -0.7600368516082544, -2.6656813210690014], time: 70.372
steps: 1599975, episodes: 64000, mean episode variance: 1.076584176036995, agent episode variance: [0.0477896731197834, 0.508242816388607, 0.04260028484789655, 0.47795140168070793], time: 70.372
steps: 1599975, episodes: 64000, mean episode cvar: 14.232551466017961, agent episode cvar: [7.102745643615723, 7.14760410118103, -0.01689832052588463, -0.0008999582529067993], time: 70.373
Running avgs for agent 0: q_loss: 5.702738285064697, p_loss: -21.6861515045166, mean_rew: 0.132578125, variance: 0.1911586924791336, cvar: 28.410982131958008, v: 23.018476486206055, mean_q: 21.621816635131836, std_q: 2.7905538082122803, lamda: 2.067840337753296
Running avgs for agent 1: q_loss: 8.431828498840332, p_loss: -20.835161209106445, mean_rew: 0.13609375, variance: 2.032971265554428, cvar: 28.590417861938477, v: 22.9478702545166, mean_q: 20.760276794433594, std_q: 3.1459248065948486, lamda: 2.058335304260254
Running avgs for agent 2: q_loss: 0.1561206430196762, p_loss: 0.11694689095020294, mean_rew: -0.028645108417068993, variance: 0.1704011393915862, cvar: -0.0675932839512825, v: -0.08737275749444962, mean_q: -0.12475787848234177, std_q: 0.44646671414375305, lamda: 1.4999268054962158
Running avgs for agent 3: q_loss: 1.8659093379974365, p_loss: 3.4416279792785645, mean_rew: -0.2765417511976251, variance: 1.9118056067228317, cvar: -0.0035998306702822447, v: -0.8237518072128296, mean_q: -3.7827861309051514, std_q: 5.894339084625244, lamda: 1.9890481233596802

steps: 1624975, episodes: 65000, mean episode reward: 2.5545354796070843, agent episode reward: [3.32, 3.32, -1.0953929800035753, -2.9900715403893408], time: 70.187
steps: 1624975, episodes: 65000, mean episode variance: 1.151582386224065, agent episode variance: [0.3180968359708786, 0.3564924850463867, 0.042610932065639644, 0.43438213314116003], time: 70.188
steps: 1624975, episodes: 65000, mean episode cvar: 14.220513899412007, agent episode cvar: [7.117915121078491, 7.127128667831421, -0.01814882368221879, -0.006381065815687179], time: 70.188
Running avgs for agent 0: q_loss: 7.266287326812744, p_loss: -21.284391403198242, mean_rew: 0.1349609375, variance: 1.2723873438835145, cvar: 28.471660614013672, v: 22.87093162536621, mean_q: 21.22442054748535, std_q: 2.937366008758545, lamda: 2.0881569385528564
Running avgs for agent 1: q_loss: 6.211532115936279, p_loss: -20.315902709960938, mean_rew: 0.1340625, variance: 1.4259699401855468, cvar: 28.508516311645508, v: 22.904516220092773, mean_q: 20.24184799194336, std_q: 3.282371759414673, lamda: 2.079047441482544
Running avgs for agent 2: q_loss: 0.1583544760942459, p_loss: 0.12109357118606567, mean_rew: -0.028865784317892267, variance: 0.17044372826255857, cvar: -0.07259529083967209, v: -0.0856717899441719, mean_q: -0.12792348861694336, std_q: 0.47977155447006226, lamda: 1.5143259763717651
Running avgs for agent 3: q_loss: 1.7553510665893555, p_loss: 3.1827797889709473, mean_rew: -0.2720684110410846, variance: 1.7375285325646401, cvar: -0.025524264201521873, v: -0.7370760440826416, mean_q: -3.509521722793579, std_q: 5.576264381408691, lamda: 1.9989705085754395

steps: 1649975, episodes: 66000, mean episode reward: 1.6698321391778288, agent episode reward: [2.39, 2.39, -0.84466660992133, -2.265501250900841], time: 70.617
steps: 1649975, episodes: 66000, mean episode variance: 1.3084904495824594, agent episode variance: [0.601091949492693, 0.3016391792297363, 0.047549269159557296, 0.3582100517004728], time: 70.617
steps: 1649975, episodes: 66000, mean episode cvar: 14.251818720519543, agent episode cvar: [7.116502857208252, 7.157120780944824, -0.01763962468504906, -0.0041652929484844205], time: 70.618
Running avgs for agent 0: q_loss: 9.29026985168457, p_loss: -20.948604583740234, mean_rew: 0.13296875, variance: 2.404367797970772, cvar: 28.46601104736328, v: 22.719051361083984, mean_q: 20.890369415283203, std_q: 3.0297634601593018, lamda: 2.1085195541381836
Running avgs for agent 1: q_loss: 7.009438991546631, p_loss: -19.887041091918945, mean_rew: 0.1351171875, variance: 1.2065567169189453, cvar: 28.62848472595215, v: 22.80314826965332, mean_q: 19.796398162841797, std_q: 3.4837582111358643, lamda: 2.0991318225860596
Running avgs for agent 2: q_loss: 0.1739514172077179, p_loss: 0.1314116269350052, mean_rew: -0.030570559383849286, variance: 0.19019707663822918, cvar: -0.07055850327014923, v: -0.089622363448143, mean_q: -0.13943390548229218, std_q: 0.5393400192260742, lamda: 1.5293391942977905
Running avgs for agent 3: q_loss: 1.6821321249008179, p_loss: 3.0694706439971924, mean_rew: -0.2542365661673348, variance: 1.4328402068018913, cvar: -0.01666117087006569, v: -0.5853245854377747, mean_q: -3.3873496055603027, std_q: 5.369776248931885, lamda: 2.0081536769866943

steps: 1674975, episodes: 67000, mean episode reward: 2.5207572900774786, agent episode reward: [3.3, 3.3, -0.8564753295305872, -3.222767380391934], time: 70.313
steps: 1674975, episodes: 67000, mean episode variance: 2.1217011105883867, agent episode variance: [0.928836243569851, 0.8224143334031105, 0.04847692631371319, 0.32197360730171204], time: 70.313
steps: 1674975, episodes: 67000, mean episode cvar: 14.283484608270228, agent episode cvar: [7.144857194900513, 7.156236789703369, -0.017770745716989042, 0.00016136938333511354], time: 70.314
Running avgs for agent 0: q_loss: 12.21786880493164, p_loss: -20.532638549804688, mean_rew: 0.13359375, variance: 3.715344974279404, cvar: 28.579429626464844, v: 22.58074188232422, mean_q: 20.462860107421875, std_q: 3.212625503540039, lamda: 2.1309444904327393
Running avgs for agent 1: q_loss: 9.01989459991455, p_loss: -19.5476131439209, mean_rew: 0.1376171875, variance: 3.289657333612442, cvar: 28.62494659423828, v: 22.809614181518555, mean_q: 19.462512969970703, std_q: 3.5874228477478027, lamda: 2.119870901107788
Running avgs for agent 2: q_loss: 0.18046215176582336, p_loss: 0.1292324811220169, mean_rew: -0.02937793673285804, variance: 0.19390770525485276, cvar: -0.07108298689126968, v: -0.09098562598228455, mean_q: -0.13750839233398438, std_q: 0.5445854067802429, lamda: 1.5445449352264404
Running avgs for agent 3: q_loss: 1.6382914781570435, p_loss: 2.9704160690307617, mean_rew: -0.253739083677302, variance: 1.2878944292068482, cvar: 0.0006454772665165365, v: -0.5118846297264099, mean_q: -3.2765352725982666, std_q: 5.44018030166626, lamda: 2.0177857875823975

steps: 1699975, episodes: 68000, mean episode reward: 1.9275961322722721, agent episode reward: [2.99, 2.99, -1.2900265640769926, -2.762377303650735], time: 70.605
steps: 1699975, episodes: 68000, mean episode variance: 1.779908899375121, agent episode variance: [0.40792181530594823, 0.9987572709619998, 0.04703126395575236, 0.3261985491514206], time: 70.605
steps: 1699975, episodes: 68000, mean episode cvar: 14.292791947562247, agent episode cvar: [7.152767107009888, 7.152582458496093, -0.017370642136782407, 0.004813024193048477], time: 70.606
Running avgs for agent 0: q_loss: 9.3053617477417, p_loss: -20.080551147460938, mean_rew: 0.1336328125, variance: 1.631687261223793, cvar: 28.611068725585938, v: 22.452899932861328, mean_q: 20.005508422851562, std_q: 3.3537118434906006, lamda: 2.153244733810425
Running avgs for agent 1: q_loss: 9.664361953735352, p_loss: -19.100688934326172, mean_rew: 0.136171875, variance: 3.9950290838479994, cvar: 28.610328674316406, v: 22.8317813873291, mean_q: 19.015037536621094, std_q: 3.7476425170898438, lamda: 2.142998695373535
Running avgs for agent 2: q_loss: 0.17718321084976196, p_loss: 0.13314460217952728, mean_rew: -0.030885225687211068, variance: 0.18812505582300945, cvar: -0.06948257237672806, v: -0.09225025773048401, mean_q: -0.13972869515419006, std_q: 0.568220317363739, lamda: 1.557096242904663
Running avgs for agent 3: q_loss: 1.7010324001312256, p_loss: 2.9465951919555664, mean_rew: -0.25813073326778335, variance: 1.3047941966056824, cvar: 0.019252097234129906, v: -0.3504517674446106, mean_q: -3.251979112625122, std_q: 5.469930171966553, lamda: 2.025630235671997

steps: 1724975, episodes: 69000, mean episode reward: 2.2896978364604106, agent episode reward: [2.81, 2.81, -0.6612009142356927, -2.669101249303897], time: 70.814
steps: 1724975, episodes: 69000, mean episode variance: 1.7490673222849145, agent episode variance: [0.7414043501615525, 0.6806690802574158, 0.05133385887835175, 0.2756600329875946], time: 70.815
steps: 1724975, episodes: 69000, mean episode cvar: 14.293980551853776, agent episode cvar: [7.1398262062072755, 7.171678544998169, -0.018936354465782644, 0.0014121551141142844], time: 70.815
Running avgs for agent 0: q_loss: 9.53259563446045, p_loss: -19.75370979309082, mean_rew: 0.133984375, variance: 2.96561740064621, cvar: 28.55930519104004, v: 22.443246841430664, mean_q: 19.67795181274414, std_q: 3.4558656215667725, lamda: 2.176107406616211
Running avgs for agent 1: q_loss: 9.344812393188477, p_loss: -18.909269332885742, mean_rew: 0.1418359375, variance: 2.722676321029663, cvar: 28.68671417236328, v: 22.78553581237793, mean_q: 18.829851150512695, std_q: 3.846712827682495, lamda: 2.1655077934265137
Running avgs for agent 2: q_loss: 0.18775086104869843, p_loss: 0.132002592086792, mean_rew: -0.031087882874283324, variance: 0.205335435513407, cvar: -0.07574541866779327, v: -0.0880947858095169, mean_q: -0.13949526846408844, std_q: 0.570886492729187, lamda: 1.570133924484253
Running avgs for agent 3: q_loss: 1.5892419815063477, p_loss: 2.85429048538208, mean_rew: -0.25644659569702616, variance: 1.1026401319503785, cvar: 0.005648620426654816, v: -0.23727716505527496, mean_q: -3.165722131729126, std_q: 5.48057222366333, lamda: 2.031453847885132

steps: 1749975, episodes: 70000, mean episode reward: 2.9813937088751317, agent episode reward: [3.53, 3.53, -0.7992171117326706, -3.2793891793921977], time: 70.661
steps: 1749975, episodes: 70000, mean episode variance: 2.0976463138833643, agent episode variance: [1.166851562857628, 0.491472227036953, 0.04953377189487219, 0.3897887520939112], time: 70.662
steps: 1749975, episodes: 70000, mean episode cvar: 14.22419312649779, agent episode cvar: [7.113961145401001, 7.143368806838989, -0.017660478577017785, -0.015476347165182232], time: 70.662
Running avgs for agent 0: q_loss: 12.263246536254883, p_loss: -19.417543411254883, mean_rew: 0.138515625, variance: 4.667406251430512, cvar: 28.455842971801758, v: 22.300235748291016, mean_q: 19.335254669189453, std_q: 3.5291898250579834, lamda: 2.1989004611968994
Running avgs for agent 1: q_loss: 8.91980266571045, p_loss: -18.8404598236084, mean_rew: 0.138203125, variance: 1.965888908147812, cvar: 28.573476791381836, v: 22.775339126586914, mean_q: 18.75244140625, std_q: 3.838352680206299, lamda: 2.187866687774658
Running avgs for agent 2: q_loss: 0.18293599784374237, p_loss: 0.12857390940189362, mean_rew: -0.029301736334293945, variance: 0.19813508757948875, cvar: -0.0706419125199318, v: -0.08562513440847397, mean_q: -0.1370716243982315, std_q: 0.5608460307121277, lamda: 1.5814675092697144
Running avgs for agent 3: q_loss: 1.5439201593399048, p_loss: 2.4666037559509277, mean_rew: -0.24912886741950876, variance: 1.5591550083756447, cvar: -0.0619053915143013, v: -0.08176843076944351, mean_q: -2.7645435333251953, std_q: 5.209975242614746, lamda: 2.0325636863708496

steps: 1774975, episodes: 71000, mean episode reward: 2.2843075564429767, agent episode reward: [2.99, 2.99, -0.8848632465431943, -2.810829197013829], time: 70.434
steps: 1774975, episodes: 71000, mean episode variance: 1.3268780323020182, agent episode variance: [0.3996969332396984, 0.39631295335292815, 0.042040229042526335, 0.48882791666686537], time: 70.434
steps: 1774975, episodes: 71000, mean episode cvar: 14.23101989089325, agent episode cvar: [7.15178324508667, 7.1225527439117435, -0.01691522192582488, -0.0264008761793375], time: 70.435
Running avgs for agent 0: q_loss: 8.837987899780273, p_loss: -19.139806747436523, mean_rew: 0.13640625, variance: 1.5987877329587936, cvar: 28.607131958007812, v: 22.404041290283203, mean_q: 19.06609344482422, std_q: 3.6684463024139404, lamda: 2.2208809852600098
Running avgs for agent 1: q_loss: 7.132478713989258, p_loss: -18.773344039916992, mean_rew: 0.1385546875, variance: 1.5852518134117126, cvar: 28.490211486816406, v: 22.812881469726562, mean_q: 18.687206268310547, std_q: 3.8511831760406494, lamda: 2.209378719329834
Running avgs for agent 2: q_loss: 0.1533239781856537, p_loss: 0.12640267610549927, mean_rew: -0.02872837317273699, variance: 0.16816091617010534, cvar: -0.06766089051961899, v: -0.08695009350776672, mean_q: -0.13408993184566498, std_q: 0.5215381979942322, lamda: 1.5981502532958984
Running avgs for agent 3: q_loss: 1.4777426719665527, p_loss: 1.8830307722091675, mean_rew: -0.24124742254845336, variance: 1.9553116666674615, cvar: -0.10560350865125656, v: -0.10988087952136993, mean_q: -2.1579511165618896, std_q: 4.900331974029541, lamda: 2.032698154449463

steps: 1799975, episodes: 72000, mean episode reward: 2.253512722371604, agent episode reward: [2.81, 2.81, -0.7394842500807205, -2.6270030275476755], time: 70.738
steps: 1799975, episodes: 72000, mean episode variance: 0.9529112218376249, agent episode variance: [0.1739372755885124, 0.3317865264415741, 0.0436002495046705, 0.4035871703028679], time: 70.739
steps: 1799975, episodes: 72000, mean episode cvar: 14.206710707828403, agent episode cvar: [7.140138690948486, 7.1093659057617185, -0.016708900664001703, -0.026084988217800857], time: 70.739
Running avgs for agent 0: q_loss: 7.237907886505127, p_loss: -19.003753662109375, mean_rew: 0.1387109375, variance: 0.6957491023540496, cvar: 28.56055450439453, v: 22.650293350219727, mean_q: 18.92768096923828, std_q: 3.6830313205718994, lamda: 2.2424604892730713
Running avgs for agent 1: q_loss: 7.090665817260742, p_loss: -18.843555450439453, mean_rew: 0.141640625, variance: 1.3271461057662963, cvar: 28.437463760375977, v: 23.11846160888672, mean_q: 18.75301170349121, std_q: 3.815518617630005, lamda: 2.231645345687866
Running avgs for agent 2: q_loss: 0.15691760182380676, p_loss: 0.1228673979640007, mean_rew: -0.029055444987790217, variance: 0.174400998018682, cvar: -0.06683560460805893, v: -0.08425038307905197, mean_q: -0.13125255703926086, std_q: 0.5061098337173462, lamda: 1.6135318279266357
Running avgs for agent 3: q_loss: 1.3954514265060425, p_loss: 1.276888132095337, mean_rew: -0.18979636182335446, variance: 1.6143486812114716, cvar: -0.10433994978666306, v: -0.1102978065609932, mean_q: -1.4600718021392822, std_q: 2.997849225997925, lamda: 2.0329811573028564

steps: 1824975, episodes: 73000, mean episode reward: 2.6037442400447897, agent episode reward: [2.99, 2.99, -0.3825728857040669, -2.993682874251143], time: 70.515
steps: 1824975, episodes: 73000, mean episode variance: 0.9921155549059621, agent episode variance: [0.42958289235830305, 0.14297110852599143, 0.04428005274338648, 0.3752815012782812], time: 70.515
steps: 1824975, episodes: 73000, mean episode cvar: 14.248432575535029, agent episode cvar: [7.162125440597534, 7.127473733901978, -0.01729589357972145, -0.023870705384761094], time: 70.516
Running avgs for agent 0: q_loss: 10.529376983642578, p_loss: -18.956762313842773, mean_rew: 0.1401953125, variance: 1.7183315694332122, cvar: 28.648502349853516, v: 22.992084503173828, mean_q: 18.88713264465332, std_q: 3.716695785522461, lamda: 2.264305591583252
Running avgs for agent 1: q_loss: 7.159642696380615, p_loss: -18.84595489501953, mean_rew: 0.1394140625, variance: 0.5718844341039657, cvar: 28.50989532470703, v: 23.317964553833008, mean_q: 18.756851196289062, std_q: 3.8704748153686523, lamda: 2.2520556449890137
Running avgs for agent 2: q_loss: 0.15776866674423218, p_loss: 0.12764352560043335, mean_rew: -0.02884656400249002, variance: 0.1771202109735459, cvar: -0.0691835805773735, v: -0.08803752064704895, mean_q: -0.1356385201215744, std_q: 0.5091504454612732, lamda: 1.6298272609710693
Running avgs for agent 3: q_loss: 1.4021496772766113, p_loss: 0.9578174352645874, mean_rew: -0.16793679569780423, variance: 1.5011260051131248, cvar: -0.09548281878232956, v: -0.10397607088088989, mean_q: -1.0822763442993164, std_q: 2.018101215362549, lamda: 2.03338360786438

steps: 1849975, episodes: 74000, mean episode reward: 1.4661829090962806, agent episode reward: [2.67, 2.67, -1.0209060809950878, -2.852911009908632], time: 70.585
steps: 1849975, episodes: 74000, mean episode variance: 1.929921396053396, agent episode variance: [0.9819156395792961, 0.5737125914692879, 0.03840868800040335, 0.3358844770044088], time: 70.585
steps: 1849975, episodes: 74000, mean episode cvar: 14.293289949353785, agent episode cvar: [7.182464477539063, 7.149393115997315, -0.016334126528352498, -0.022233517654240132], time: 70.586
Running avgs for agent 0: q_loss: 9.586589813232422, p_loss: -18.469566345214844, mean_rew: 0.1402734375, variance: 3.9276625583171842, cvar: 28.729860305786133, v: 23.262651443481445, mean_q: 18.38877296447754, std_q: 3.9576265811920166, lamda: 2.286926507949829
Running avgs for agent 1: q_loss: 7.433233737945557, p_loss: -18.74709701538086, mean_rew: 0.1398828125, variance: 2.2948503658771515, cvar: 28.59757423400879, v: 23.554393768310547, mean_q: 18.656272888183594, std_q: 3.9692587852478027, lamda: 2.2730355262756348
Running avgs for agent 2: q_loss: 0.14562825858592987, p_loss: 0.1228913813829422, mean_rew: -0.026893278595690014, variance: 0.1536347520016134, cvar: -0.06533650308847427, v: -0.08493556082248688, mean_q: -0.12924113869667053, std_q: 0.4727570116519928, lamda: 1.6431362628936768
Running avgs for agent 3: q_loss: 1.291266918182373, p_loss: 0.7574297189712524, mean_rew: -0.15321083918267264, variance: 1.3435379080176353, cvar: -0.08893406391143799, v: -0.09867845475673676, mean_q: -0.8550053834915161, std_q: 1.7850052118301392, lamda: 2.033750534057617

steps: 1874975, episodes: 75000, mean episode reward: -0.7887612962979093, agent episode reward: [2.8, 2.8, -1.5110486281734186, -4.877712668124491], time: 70.77
steps: 1874975, episodes: 75000, mean episode variance: 1.1662212845629547, agent episode variance: [0.4768025654554367, 0.3348888140916824, 0.04078133733780123, 0.31374856767803433], time: 70.771
steps: 1874975, episodes: 75000, mean episode cvar: 14.232047364685684, agent episode cvar: [7.131315185546875, 7.141226055145264, -0.018005412820726634, -0.022488463185727597], time: 70.771
Running avgs for agent 0: q_loss: 7.947416305541992, p_loss: -18.505817413330078, mean_rew: 0.1370703125, variance: 1.9072102618217468, cvar: 28.52526092529297, v: 23.395090103149414, mean_q: 18.434215545654297, std_q: 3.8740994930267334, lamda: 2.3083689212799072
Running avgs for agent 1: q_loss: 8.876521110534668, p_loss: -18.84193992614746, mean_rew: 0.14140625, variance: 1.3395552563667297, cvar: 28.564903259277344, v: 23.640840530395508, mean_q: 18.74112892150879, std_q: 3.9456214904785156, lamda: 2.295193910598755
Running avgs for agent 2: q_loss: 0.14887166023254395, p_loss: 0.12162982672452927, mean_rew: -0.028725683781511756, variance: 0.16312534935120493, cvar: -0.07202165573835373, v: -0.08787759393453598, mean_q: -0.1271551102399826, std_q: 0.4560396373271942, lamda: 1.6574770212173462
Running avgs for agent 3: q_loss: 1.24351966381073, p_loss: 0.6205346584320068, mean_rew: -0.14172874565479623, variance: 1.2549942707121373, cvar: -0.08995384722948074, v: -0.10265806317329407, mean_q: -0.7042545080184937, std_q: 1.6292831897735596, lamda: 2.034210205078125

steps: 1899975, episodes: 76000, mean episode reward: -3.640769689008464, agent episode reward: [2.6, 2.6, -1.4775707537356715, -7.363198935272792], time: 70.525
steps: 1899975, episodes: 76000, mean episode variance: 1.4692554247954395, agent episode variance: [0.3695708775520325, 0.7329539452791214, 0.038615878103068095, 0.3281147238612175], time: 70.526
steps: 1899975, episodes: 76000, mean episode cvar: 14.258700125366449, agent episode cvar: [7.141658430099487, 7.157094537734985, -0.01795858934521675, -0.022094253122806547], time: 70.526
Running avgs for agent 0: q_loss: 6.8381805419921875, p_loss: -18.595312118530273, mean_rew: 0.1358203125, variance: 1.47828351020813, cvar: 28.566635131835938, v: 23.501602172851562, mean_q: 18.501020431518555, std_q: 3.8932385444641113, lamda: 2.329366683959961
Running avgs for agent 1: q_loss: 9.502289772033691, p_loss: -18.786890029907227, mean_rew: 0.1334375, variance: 2.9318157811164856, cvar: 28.628379821777344, v: 23.724058151245117, mean_q: 18.68132781982422, std_q: 4.030117034912109, lamda: 2.3182783126831055
Running avgs for agent 2: q_loss: 0.14883162081241608, p_loss: 0.11638148128986359, mean_rew: -0.029004763110188042, variance: 0.15446351241227238, cvar: -0.07183434814214706, v: -0.08544667065143585, mean_q: -0.12288706749677658, std_q: 0.43947914242744446, lamda: 1.6690562963485718
Running avgs for agent 3: q_loss: 1.2928574085235596, p_loss: 0.5636242628097534, mean_rew: -0.14625163951343637, variance: 1.31245889544487, cvar: -0.08837701380252838, v: -0.09968705475330353, mean_q: -0.6493514776229858, std_q: 1.6887151002883911, lamda: 2.0349690914154053

steps: 1924975, episodes: 77000, mean episode reward: -2.1748500307655263, agent episode reward: [1.86, 1.86, -1.0093716768513457, -4.885478353914181], time: 70.72
steps: 1924975, episodes: 77000, mean episode variance: 1.360302650165744, agent episode variance: [0.5435625512003899, 0.46725290817022325, 0.04626296886522323, 0.3032242219299078], time: 70.72
steps: 1924975, episodes: 77000, mean episode cvar: 14.221628362752497, agent episode cvar: [7.137896709442138, 7.1228397178649905, -0.017666789572685956, -0.021441274981945754], time: 70.721
Running avgs for agent 0: q_loss: 10.435654640197754, p_loss: -18.744543075561523, mean_rew: 0.1362109375, variance: 2.1742502048015595, cvar: 28.551586151123047, v: 23.72578239440918, mean_q: 18.649206161499023, std_q: 3.827193021774292, lamda: 2.350539445877075
Running avgs for agent 1: q_loss: 9.029099464416504, p_loss: -18.83570098876953, mean_rew: 0.1336328125, variance: 1.869011632680893, cvar: 28.49135971069336, v: 23.821414947509766, mean_q: 18.745962142944336, std_q: 3.952033519744873, lamda: 2.340710163116455
Running avgs for agent 2: q_loss: 0.1730683296918869, p_loss: 0.1255054920911789, mean_rew: -0.0313571189211571, variance: 0.1850518754608929, cvar: -0.07066716253757477, v: -0.08929182589054108, mean_q: -0.1312817931175232, std_q: 0.4946120083332062, lamda: 1.6813304424285889
Running avgs for agent 3: q_loss: 1.2407033443450928, p_loss: 0.49089518189430237, mean_rew: -0.14729278878452487, variance: 1.2128968877196311, cvar: -0.08576510846614838, v: -0.10133067518472672, mean_q: -0.583698570728302, std_q: 1.7246651649475098, lamda: 2.0359082221984863

steps: 1949975, episodes: 78000, mean episode reward: -1.4831516971259484, agent episode reward: [1.88, 1.88, -0.9672491196851836, -4.275902577440766], time: 70.705
steps: 1949975, episodes: 78000, mean episode variance: 1.3314557526132558, agent episode variance: [0.375723722755909, 0.6197130842804909, 0.04209342634142377, 0.29392551923543214], time: 70.706
steps: 1949975, episodes: 78000, mean episode cvar: 14.25666389202699, agent episode cvar: [7.139541017532348, 7.156255195617676, -0.01818732825666666, -0.020944992866367102], time: 70.706
Running avgs for agent 0: q_loss: 9.492039680480957, p_loss: -18.98513412475586, mean_rew: 0.1336328125, variance: 1.502894891023636, cvar: 28.558164596557617, v: 23.817211151123047, mean_q: 18.913833618164062, std_q: 3.7350659370422363, lamda: 2.3719351291656494/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

Running avgs for agent 1: q_loss: 10.634108543395996, p_loss: -19.10113525390625, mean_rew: 0.131015625, variance: 2.4788523371219635, cvar: 28.625019073486328, v: 23.919960021972656, mean_q: 19.02149772644043, std_q: 3.875988721847534, lamda: 2.3631577491760254
Running avgs for agent 2: q_loss: 0.15693868696689606, p_loss: 0.1287512183189392, mean_rew: -0.02971576586430487, variance: 0.16837370536569507, cvar: -0.0727493166923523, v: -0.09525828808546066, mean_q: -0.1335868239402771, std_q: 0.4185255765914917, lamda: 1.6988441944122314
Running avgs for agent 3: q_loss: 1.1831026077270508, p_loss: 0.40638265013694763, mean_rew: -0.14436531699174315, variance: 1.1757020769417286, cvar: -0.0837799683213234, v: -0.09982666373252869, mean_q: -0.4963315427303314, std_q: 1.6816623210906982, lamda: 2.037088632583618

steps: 1974975, episodes: 79000, mean episode reward: -1.22852768224076, agent episode reward: [2.07, 2.07, -1.2105348422773696, -4.1579928399633905], time: 70.753
steps: 1974975, episodes: 79000, mean episode variance: 1.1254058617649134, agent episode variance: [0.4182837746441364, 0.3819701358377933, 0.03914246153808199, 0.28600948974490165], time: 70.754
steps: 1974975, episodes: 79000, mean episode cvar: 14.205564269188791, agent episode cvar: [7.108246812820434, 7.137660423278809, -0.018588891685009004, -0.02175407522544265], time: 70.754
Running avgs for agent 0: q_loss: 9.903164863586426, p_loss: -19.137409210205078, mean_rew: 0.1295703125, variance: 1.6731350985765456, cvar: 28.4329891204834, v: 23.78849220275879, mean_q: 19.072914123535156, std_q: 3.621936559677124, lamda: 2.3932065963745117
Running avgs for agent 1: q_loss: 8.359292984008789, p_loss: -18.959125518798828, mean_rew: 0.13296875, variance: 1.5278805433511733, cvar: 28.550640106201172, v: 24.0075740814209, mean_q: 18.87855339050293, std_q: 3.9248387813568115, lamda: 2.3846161365509033
Running avgs for agent 2: q_loss: 0.14017115533351898, p_loss: 0.12308941781520844, mean_rew: -0.02950953176027633, variance: 0.15656984615232797, cvar: -0.07435555756092072, v: -0.08599109202623367, mean_q: -0.12800389528274536, std_q: 0.44538402557373047, lamda: 1.7149337530136108
Running avgs for agent 3: q_loss: 1.1535876989364624, p_loss: 0.37679940462112427, mean_rew: -0.1411652898393077, variance: 1.1440379589796066, cvar: -0.08701630681753159, v: -0.10426004230976105, mean_q: -0.45883581042289734, std_q: 1.6509205102920532, lamda: 2.0383784770965576

steps: 1999975, episodes: 80000, mean episode reward: -3.6062198919872173, agent episode reward: [1.61, 1.61, -1.2570374847664578, -5.56918240722076], time: 71.748
steps: 1999975, episodes: 80000, mean episode variance: 0.6779769741153577, agent episode variance: [0.29725267678499223, 0.06241519628465176, 0.037344766870723106, 0.28096433417499067], time: 71.749
steps: 1999975, episodes: 80000, mean episode cvar: 14.256634351316839, agent episode cvar: [7.1398861141204835, 7.156120168685913, -0.01728464896604419, -0.02208728252351284], time: 71.75
Running avgs for agent 0: q_loss: 8.15046215057373, p_loss: -19.220680236816406, mean_rew: 0.1234375, variance: 1.189010707139969, cvar: 28.559545516967773, v: 23.958303451538086, mean_q: 19.162845611572266, std_q: 3.6661949157714844, lamda: 2.414951801300049
Running avgs for agent 1: q_loss: 6.919053554534912, p_loss: -19.33420181274414, mean_rew: 0.1233203125, variance: 0.24966078513860704, cvar: 28.624481201171875, v: 24.20916175842285, mean_q: 19.260448455810547, std_q: 3.790445327758789, lamda: 2.4062674045562744
Running avgs for agent 2: q_loss: 0.15354491770267487, p_loss: 0.11968763172626495, mean_rew: -0.03054130867652172, variance: 0.14937906748289242, cvar: -0.06913859397172928, v: -0.08718196302652359, mean_q: -0.12626923620700836, std_q: 0.4332720637321472, lamda: 1.7265632152557373
Running avgs for agent 3: q_loss: 1.1250749826431274, p_loss: 0.35689419507980347, mean_rew: -0.14077806381268623, variance: 1.1238573366999627, cvar: -0.0883491262793541, v: -0.10507512092590332, mean_q: -0.4387110471725464, std_q: 1.630218267440796, lamda: 2.040224552154541

...Finished total of 80001 episodes... Now freezing policy and running for 1000 more episodes 
steps: 24975, episodes: 1000, mean episode reward: -6.51295718082363, agent episode reward: [1.5, 1.5, -1.8153731546559035, -7.697584026167727], time: 50.232
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 50.233
steps: 24975, episodes: 1000, mean episode cvar: 0.0, agent episode cvar: [0.0, 0.0, 0.0, 0.0], time: 50.233
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan, lamda: nan

steps: 49975, episodes: 2000, mean episode reward: -7.415162870813792, agent episode reward: [1.81, 1.81, -1.9486371298260747, -9.08652574098772], time: 69.829
steps: 49975, episodes: 2000, mean episode variance: 0.45042472245078535, agent episode variance: [0.0, 0.0, 0.0730965192494914, 0.37732820320129395], time: 69.829
steps: 49975, episodes: 2000, mean episode cvar: 16.033638049721716, agent episode cvar: [8.033645648956298, 8.038283870697022, -0.020841412879526615, -0.017450057052075862], time: 69.83
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.06515753073770492, variance: 0.0, cvar: 32.92477798461914, v: 26.881704330444336, mean_q: 21.4744873046875, std_q: 4.6629767417907715, lamda: 2.425917863845825
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.0649173924180328, variance: 0.0, cvar: 32.94378662109375, v: 27.137378692626953, mean_q: 21.47146224975586, std_q: 4.873978614807129, lamda: 2.4168484210968018
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.0729744759771954, variance: 0.2995758985634893, cvar: -0.08541563898324966, v: -0.09810416400432587, mean_q: -0.13094794750213623, std_q: 0.3832717835903168, lamda: 1.733943223953247
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.33079416484958085, variance: 1.5464271306991577, cvar: -0.07151663303375244, v: -0.08083625137805939, mean_q: -0.27761608362197876, std_q: 1.1098426580429077, lamda: 2.040872573852539

...Finished total of 2001 episodes with the fixed policy.
