WARNING: Logging before flag parsing goes to stderr.
W0826 16:32:09.447475 4670260672 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0826 16:32:09.447677 4670260672 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:167: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-26 16:32:09.448019: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0826 16:32:09.451279 4670260672 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/common/tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0826 16:32:09.453341 4670260672 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:228: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0826 16:32:09.453471 4670260672 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0826 16:32:09.453542 4670260672 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:85: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0826 16:32:09.774013 4670260672 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0826 16:32:09.935642 4670260672 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:152: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0826 16:32:09.944570 4670260672 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:156: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0826 16:32:10.362095 4670260672 deprecation_wrapper.py:119] From /Users/sandeep/marl/multiagent-rl/maddpg/maddpg/trainer/maddpg.py:260: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

arglist.u_estimation False
adversary agent:  -0.25
adversary agent:  -0.25
good agent:  4.2
good agent:  4.2
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -11.862003101352053, agent episode reward: [2.87, 2.87, -6.418691053566415, -11.183312047785636], time: 40.755
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 40.756
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: -6.456625920679171, agent episode reward: [5.22, 5.22, -7.797779227214505, -9.098846693464665], time: 66.578
steps: 49975, episodes: 2000, mean episode variance: 0.995067258797586, agent episode variance: [0.3201915358006954, 0.3137198346555233, 0.17017499940097333, 0.19098088894039392], time: 66.579
Running avgs for agent 0: q_loss: 1.3093810081481934, p_loss: -0.3408343195915222, mean_rew: 0.13591828893442623, variance: 1.312260392625801, mean_q: 0.31790095567703247, std_q: 0.28126975893974304
Running avgs for agent 1: q_loss: 1.2855122089385986, p_loss: -0.12309913337230682, mean_rew: 0.1328365138319672, variance: 1.2857370376586914, mean_q: 0.10696817934513092, std_q: 0.2698637843132019
Running avgs for agent 2: q_loss: 0.7094763517379761, p_loss: 0.3842310607433319, mean_rew: -0.3044961099570324, variance: 0.6974385221351366, mean_q: -0.4302768111228943, std_q: 0.9877593517303467
Running avgs for agent 3: q_loss: 0.8158354163169861, p_loss: 0.8213599920272827, mean_rew: -0.47483095477971715, variance: 0.7827085612311226, mean_q: -0.9310819506645203, std_q: 1.52840256690979

steps: 74975, episodes: 3000, mean episode reward: 5.228480719470259, agent episode reward: [6.04, 6.04, -3.5088340039145085, -3.3426852766152324], time: 68.109
steps: 74975, episodes: 3000, mean episode variance: 1.269278323546052, agent episode variance: [0.4151163130700588, 0.4139073451459408, 0.21432539508491755, 0.22592927024513482], time: 68.11
Running avgs for agent 0: q_loss: 1.6231523752212524, p_loss: -0.6408014893531799, mean_rew: 0.1758203125, variance: 1.6604653596878052, mean_q: 0.5981321334838867, std_q: 0.8023221492767334
Running avgs for agent 1: q_loss: 1.6344120502471924, p_loss: -0.5594701170921326, mean_rew: 0.1736328125, variance: 1.6556293964385986, mean_q: 0.5174825191497803, std_q: 0.6988269090652466
Running avgs for agent 2: q_loss: 0.8694711923599243, p_loss: 0.5208408236503601, mean_rew: -0.2578190173156458, variance: 0.8573015803396702, mean_q: -0.630180299282074, std_q: 1.7743875980377197
Running avgs for agent 3: q_loss: 0.9334717988967896, p_loss: 0.8343760371208191, mean_rew: -0.35155223359234244, variance: 0.9037170809805393, mean_q: -1.0202323198318481, std_q: 2.5450005531311035

steps: 99975, episodes: 4000, mean episode reward: 6.483665245228704, agent episode reward: [7.01, 7.01, -4.2598220940124385, -3.2765126607588573], time: 67.572
steps: 99975, episodes: 4000, mean episode variance: 1.4653165314421057, agent episode variance: [0.4773445480465889, 0.47935939159989355, 0.24470802703499794, 0.26390456476062535], time: 67.572
Running avgs for agent 0: q_loss: 1.8751749992370605, p_loss: -1.084534764289856, mean_rew: 0.197890625, variance: 1.909378170967102, mean_q: 1.0307151079177856, std_q: 1.4014447927474976
Running avgs for agent 1: q_loss: 1.8860843181610107, p_loss: -1.0030969381332397, mean_rew: 0.19703125, variance: 1.9174375534057617, mean_q: 0.9536229372024536, std_q: 1.2076160907745361
Running avgs for agent 2: q_loss: 1.0000791549682617, p_loss: 0.6185965538024902, mean_rew: -0.22684531230336727, variance: 0.9788321081399918, mean_q: -0.7447680830955505, std_q: 2.1476073265075684
Running avgs for agent 3: q_loss: 1.0561954975128174, p_loss: 0.8508517742156982, mean_rew: -0.2889510570049596, variance: 1.0556182590425014, mean_q: -1.0476919412612915, std_q: 2.9335665702819824

steps: 124975, episodes: 5000, mean episode reward: 8.144835754609389, agent episode reward: [8.75, 8.75, -4.303855148230507, -5.051309097160103], time: 68.056
steps: 124975, episodes: 5000, mean episode variance: 1.7337102824896573, agent episode variance: [0.5834337373301387, 0.5693530263900757, 0.2895416123494506, 0.29138190641999245], time: 68.057
Running avgs for agent 0: q_loss: 2.2899346351623535, p_loss: -1.6221764087677002, mean_rew: 0.2249609375, variance: 2.3337347507476807, mean_q: 1.5514253377914429, std_q: 1.9939404726028442
Running avgs for agent 1: q_loss: 2.24092435836792, p_loss: -1.5200270414352417, mean_rew: 0.2253515625, variance: 2.2774121055603027, mean_q: 1.4595388174057007, std_q: 1.6594526767730713
Running avgs for agent 2: q_loss: 1.142978549003601, p_loss: 0.6703060865402222, mean_rew: -0.2138453199376522, variance: 1.1581664493978023, mean_q: -0.8011671304702759, std_q: 2.237945318222046
Running avgs for agent 3: q_loss: 1.1814167499542236, p_loss: 0.8588704466819763, mean_rew: -0.2590522678622231, variance: 1.1655276256799698, mean_q: -1.0286473035812378, std_q: 2.9695887565612793

steps: 149975, episodes: 6000, mean episode reward: 10.441224564476132, agent episode reward: [11.14, 11.14, -5.171875073446392, -6.6669003620774765], time: 68.516
steps: 149975, episodes: 6000, mean episode variance: 2.0030441554933787, agent episode variance: [0.6903533089160919, 0.6568434077203273, 0.31464624412357806, 0.3412011947333813], time: 68.516
Running avgs for agent 0: q_loss: 2.764524459838867, p_loss: -2.288018226623535, mean_rew: 0.253515625, variance: 2.761413335800171, mean_q: 2.1924989223480225, std_q: 2.639400005340576
Running avgs for agent 1: q_loss: 2.6194043159484863, p_loss: -2.159266710281372, mean_rew: 0.2555078125, variance: 2.627373456954956, mean_q: 2.082273006439209, std_q: 2.148784875869751
Running avgs for agent 2: q_loss: 1.2662367820739746, p_loss: 0.7576744556427002, mean_rew: -0.2102991354429184, variance: 1.2585849764943122, mean_q: -0.8825857639312744, std_q: 2.139681339263916
Running avgs for agent 3: q_loss: 1.377609372138977, p_loss: 0.9863763451576233, mean_rew: -0.2525346572074035, variance: 1.3648047789335251, mean_q: -1.1542569398880005, std_q: 3.062608003616333

steps: 174975, episodes: 7000, mean episode reward: 12.377979811993413, agent episode reward: [13.69, 13.69, -6.612037824629064, -8.389982363377523], time: 63.3
steps: 174975, episodes: 7000, mean episode variance: 2.3573743921369315, agent episode variance: [0.8193681928217411, 0.7840033219754696, 0.3498348724246025, 0.40416800491511823], time: 63.301
Running avgs for agent 0: q_loss: 3.2662103176116943, p_loss: -3.093538999557495, mean_rew: 0.2888671875, variance: 3.2774727712869645, mean_q: 2.9643383026123047, std_q: 3.195925235748291
Running avgs for agent 1: q_loss: 3.1522128582000732, p_loss: -2.995346784591675, mean_rew: 0.2957421875, variance: 3.1360132879018785, mean_q: 2.864380359649658, std_q: 2.7555391788482666
Running avgs for agent 2: q_loss: 1.3998103141784668, p_loss: 0.9364290833473206, mean_rew: -0.21810147443774233, variance: 1.39933948969841, mean_q: -1.0577946901321411, std_q: 2.24576473236084
Running avgs for agent 3: q_loss: 1.588772177696228, p_loss: 1.204351544380188, mean_rew: -0.25933262472160756, variance: 1.616672019660473, mean_q: -1.3774150609970093, std_q: 3.1496028900146484

steps: 199975, episodes: 8000, mean episode reward: 16.338437266658598, agent episode reward: [18.2, 18.2, -7.907500666733625, -12.154062066607775], time: 63.507
steps: 199975, episodes: 8000, mean episode variance: 2.7658355559408663, agent episode variance: [0.968519294410944, 0.941295310869813, 0.38627443136274814, 0.4697465192973614], time: 63.507
Running avgs for agent 0: q_loss: 3.9420104026794434, p_loss: -4.233384609222412, mean_rew: 0.33828125, variance: 3.874077177643776, mean_q: 4.048424243927002, std_q: 4.065336227416992
Running avgs for agent 1: q_loss: 3.775188446044922, p_loss: -4.005250930786133, mean_rew: 0.3420703125, variance: 3.765181243479252, mean_q: 3.7925453186035156, std_q: 3.4707491397857666
Running avgs for agent 2: q_loss: 1.4957126379013062, p_loss: 1.1492793560028076, mean_rew: -0.22074545841788096, variance: 1.545097827911377, mean_q: -1.2885804176330566, std_q: 2.2787415981292725
Running avgs for agent 3: q_loss: 1.8199795484542847, p_loss: 1.3604512214660645, mean_rew: -0.2777990761396804, variance: 1.8789860771894455, mean_q: -1.567447543144226, std_q: 3.263638973236084

steps: 224975, episodes: 9000, mean episode reward: 14.555334799010202, agent episode reward: [19.04, 19.04, -5.306593521725108, -18.218071679264693], time: 63.14
steps: 224975, episodes: 9000, mean episode variance: 3.202481327816844, agent episode variance: [1.180101308375597, 1.0511475066542626, 0.42430095706880094, 0.5469315557181835], time: 63.141
Running avgs for agent 0: q_loss: 4.724827289581299, p_loss: -5.521541118621826, mean_rew: 0.3887890625, variance: 4.720405233502388, mean_q: 5.288984298706055, std_q: 5.041015148162842
Running avgs for agent 1: q_loss: 4.32275390625, p_loss: -4.987785339355469, mean_rew: 0.3874609375, variance: 4.20459002661705, mean_q: 4.7354278564453125, std_q: 4.131637096405029
Running avgs for agent 2: q_loss: 1.6350353956222534, p_loss: 1.3772550821304321, mean_rew: -0.2285552604483196, variance: 1.6972038282752038, mean_q: -1.5650312900543213, std_q: 2.664515733718872
Running avgs for agent 3: q_loss: 2.1264936923980713, p_loss: 1.5800405740737915, mean_rew: -0.32217150971780334, variance: 2.187726222872734, mean_q: -1.8260422945022583, std_q: 3.7008378505706787

steps: 249975, episodes: 10000, mean episode reward: 5.103521535531574, agent episode reward: [12.07, 12.07, -7.243175013244745, -11.79330345122368], time: 63.853
steps: 249975, episodes: 10000, mean episode variance: 3.686812534093857, agent episode variance: [1.3678575776219368, 1.1778748277425766, 0.47656509786844253, 0.6645150308609009], time: 63.854
Running avgs for agent 0: q_loss: 5.421267509460449, p_loss: -6.5936760902404785, mean_rew: 0.423046875, variance: 5.471430310487747, mean_q: 6.318809509277344, std_q: 5.891386032104492
Running avgs for agent 1: q_loss: 4.71836519241333, p_loss: -5.807978630065918, mean_rew: 0.408125, variance: 4.711499310970306, mean_q: 5.504802227020264, std_q: 4.584054470062256
Running avgs for agent 2: q_loss: 1.8105226755142212, p_loss: 1.5298259258270264, mean_rew: -0.23099772285591413, variance: 1.9062603914737701, mean_q: -1.7669998407363892, std_q: 2.978278160095215
Running avgs for agent 3: q_loss: 2.564439058303833, p_loss: 1.830684781074524, mean_rew: -0.35134370013827015, variance: 2.6580601234436037, mean_q: -2.1669349670410156, std_q: 4.379012584686279

steps: 274975, episodes: 11000, mean episode reward: 2.75310945168894, agent episode reward: [7.42, 7.42, -6.8768885078483954, -5.210002040462665], time: 63.681
steps: 274975, episodes: 11000, mean episode variance: 3.8235010483153165, agent episode variance: [1.3758787516355515, 1.2173842878341674, 0.4980913695730269, 0.7321466392725706], time: 63.682
Running avgs for agent 0: q_loss: 5.5256876945495605, p_loss: -7.250932693481445, mean_rew: 0.410703125, variance: 5.503515006542206, mean_q: 6.944881916046143, std_q: 6.058946132659912
Running avgs for agent 1: q_loss: 4.954817295074463, p_loss: -6.4552001953125, mean_rew: 0.413359375, variance: 4.86953715133667, mean_q: 6.111090660095215, std_q: 4.773045063018799
Running avgs for agent 2: q_loss: 1.9204236268997192, p_loss: 1.624004602432251, mean_rew: -0.23660086048518833, variance: 1.9923654782921076, mean_q: -1.90264892578125, std_q: 3.328578233718872
Running avgs for agent 3: q_loss: 2.9325501918792725, p_loss: 1.988587737083435, mean_rew: -0.3529327050094374, variance: 2.9285865570902825, mean_q: -2.396427631378174, std_q: 4.856268882751465

steps: 299975, episodes: 12000, mean episode reward: 2.615259561430583, agent episode reward: [7.15, 7.15, -6.823385438472941, -4.861355000096477], time: 63.533
steps: 299975, episodes: 12000, mean episode variance: 3.9100751578062773, agent episode variance: [1.4231277419924735, 1.237016706854105, 0.5233888069987297, 0.726541901960969], time: 63.534
Running avgs for agent 0: q_loss: 5.62852144241333, p_loss: -7.740701198577881, mean_rew: 0.4047265625, variance: 5.692510967969894, mean_q: 7.431654930114746, std_q: 5.977576732635498
Running avgs for agent 1: q_loss: 5.0519185066223145, p_loss: -6.992226600646973, mean_rew: 0.4033984375, variance: 4.94806682741642, mean_q: 6.661219596862793, std_q: 4.724639415740967
Running avgs for agent 2: q_loss: 2.1174824237823486, p_loss: 1.6832892894744873, mean_rew: -0.24024876129537023, variance: 2.0935552279949188, mean_q: -2.0106189250946045, std_q: 3.6887168884277344
Running avgs for agent 3: q_loss: 2.878814458847046, p_loss: 2.0091536045074463, mean_rew: -0.33389806398641486, variance: 2.906167607843876, mean_q: -2.4242377281188965, std_q: 4.781967639923096

steps: 324975, episodes: 13000, mean episode reward: 2.799646925849991, agent episode reward: [5.91, 5.91, -4.759333952569375, -4.261019121580634], time: 63.655
steps: 324975, episodes: 13000, mean episode variance: 3.891081477507949, agent episode variance: [1.3957554199695588, 1.2523023526668549, 0.5402825520485639, 0.7027411528229713], time: 63.656
Running avgs for agent 0: q_loss: 5.489194393157959, p_loss: -7.996778964996338, mean_rew: 0.3881640625, variance: 5.583021679878235, mean_q: 7.7140421867370605, std_q: 5.589604377746582
Running avgs for agent 1: q_loss: 4.9609198570251465, p_loss: -7.322505950927734, mean_rew: 0.3902734375, variance: 5.009209410667419, mean_q: 7.004387855529785, std_q: 4.520935535430908
Running avgs for agent 2: q_loss: 2.283008098602295, p_loss: 1.6630327701568604, mean_rew: -0.23822657984609066, variance: 2.1611302081942556, mean_q: -2.0066096782684326, std_q: 3.8094120025634766
Running avgs for agent 3: q_loss: 2.8573272228240967, p_loss: 1.9947224855422974, mean_rew: -0.32743195983887996, variance: 2.8109646112918854, mean_q: -2.4142560958862305, std_q: 4.555109977722168

steps: 349975, episodes: 14000, mean episode reward: 2.679231049477726, agent episode reward: [5.82, 5.82, -4.513228281404286, -4.447540669117986], time: 64.42
steps: 349975, episodes: 14000, mean episode variance: 3.683153248652816, agent episode variance: [1.2683392555415631, 1.1979763869643212, 0.5369210900217295, 0.6799165161252022], time: 64.42
Running avgs for agent 0: q_loss: 5.103018283843994, p_loss: -8.128870964050293, mean_rew: 0.3758203125, variance: 5.0733570221662525, mean_q: 7.866222858428955, std_q: 5.053659439086914
Running avgs for agent 1: q_loss: 4.843199253082275, p_loss: -7.56544303894043, mean_rew: 0.381875, variance: 4.791905547857285, mean_q: 7.276758193969727, std_q: 4.2407917976379395
Running avgs for agent 2: q_loss: 2.2573211193084717, p_loss: 1.5927659273147583, mean_rew: -0.23133592969571015, variance: 2.147684360086918, mean_q: -1.9300554990768433, std_q: 3.8518495559692383
Running avgs for agent 3: q_loss: 2.753551483154297, p_loss: 1.9412897825241089, mean_rew: -0.31836187998321425, variance: 2.719666064500809, mean_q: -2.3528573513031006, std_q: 4.279289245605469

steps: 374975, episodes: 15000, mean episode reward: 2.3211528207932033, agent episode reward: [5.42, 5.42, -4.399714911780426, -4.119132267426371], time: 64.18
steps: 374975, episodes: 15000, mean episode variance: 3.5723325739353897, agent episode variance: [1.1938287545740605, 1.1644205301702022, 0.5569057053625583, 0.6571775838285685], time: 64.181
Running avgs for agent 0: q_loss: 4.804680824279785, p_loss: -8.239459991455078, mean_rew: 0.3669140625, variance: 4.775315018296242, mean_q: 7.996949672698975, std_q: 4.685049057006836
Running avgs for agent 1: q_loss: 4.658836841583252, p_loss: -7.6587910652160645, mean_rew: 0.3722265625, variance: 4.657682120680809, mean_q: 7.4011054039001465, std_q: 3.9158878326416016
Running avgs for agent 2: q_loss: 2.2659077644348145, p_loss: 1.4995639324188232, mean_rew: -0.23259192964230216, variance: 2.2276228214502334, mean_q: -1.8257551193237305, std_q: 3.7653021812438965
Running avgs for agent 3: q_loss: 2.7056078910827637, p_loss: 1.9240407943725586, mean_rew: -0.3008047139275952, variance: 2.628710335314274, mean_q: -2.304272413253784, std_q: 4.076334476470947

steps: 399975, episodes: 16000, mean episode reward: 1.9475752044741965, agent episode reward: [5.25, 5.25, -4.227058717854787, -4.325366077671017], time: 63.916
steps: 399975, episodes: 16000, mean episode variance: 3.4224415571168065, agent episode variance: [1.1826344304680825, 1.102432337641716, 0.5261868284493685, 0.6111879605576396], time: 63.916
Running avgs for agent 0: q_loss: 4.556328296661377, p_loss: -8.236661911010742, mean_rew: 0.361015625, variance: 4.73053772187233, mean_q: 8.014778137207031, std_q: 4.3929667472839355
Running avgs for agent 1: q_loss: 4.34512186050415, p_loss: -7.628026485443115, mean_rew: 0.3564453125, variance: 4.409729350566864, mean_q: 7.404921054840088, std_q: 3.656297206878662
Running avgs for agent 2: q_loss: 2.1431992053985596, p_loss: 1.3798145055770874, mean_rew: -0.22538544668108304, variance: 2.104747313797474, mean_q: -1.7007114887237549, std_q: 3.688694715499878
Running avgs for agent 3: q_loss: 2.4798569679260254, p_loss: 1.9198219776153564, mean_rew: -0.29256663742563216, variance: 2.4447518422305583, mean_q: -2.2670047283172607, std_q: 3.9714038372039795

steps: 424975, episodes: 17000, mean episode reward: 1.6587698188477682, agent episode reward: [5.04, 5.04, -3.8878803617181164, -4.5333498194341155], time: 63.656
steps: 424975, episodes: 17000, mean episode variance: 3.219991664916277, agent episode variance: [1.0951685357391834, 1.0081121217906475, 0.5078588688671589, 0.6088521385192871], time: 63.657
Running avgs for agent 0: q_loss: 4.349661827087402, p_loss: -8.177166938781738, mean_rew: 0.344921875, variance: 4.3806741429567335, mean_q: 7.967392921447754, std_q: 4.1256632804870605
Running avgs for agent 1: q_loss: 4.12941312789917, p_loss: -7.618713855743408, mean_rew: 0.349453125, variance: 4.03244848716259, mean_q: 7.401357650756836, std_q: 3.5431573390960693
Running avgs for agent 2: q_loss: 2.1065664291381836, p_loss: 1.2751779556274414, mean_rew: -0.2224800509302604, variance: 2.0314354754686357, mean_q: -1.5746873617172241, std_q: 3.5485239028930664
Running avgs for agent 3: q_loss: 2.4947025775909424, p_loss: 1.9109336137771606, mean_rew: -0.28757971762855894, variance: 2.4354085540771484, mean_q: -2.237661600112915, std_q: 3.8560678958892822

steps: 449975, episodes: 18000, mean episode reward: 1.372021791672817, agent episode reward: [4.6, 4.6, -3.4610094253492627, -4.36696878297792], time: 63.736
steps: 449975, episodes: 18000, mean episode variance: 3.1672847466915846, agent episode variance: [1.0909940158724785, 0.9996808182150125, 0.48048793798685074, 0.5961219746172428], time: 63.737
Running avgs for agent 0: q_loss: 4.312561511993408, p_loss: -8.024090766906738, mean_rew: 0.3415625, variance: 4.363976063489914, mean_q: 7.834094524383545, std_q: 3.948711395263672
Running avgs for agent 1: q_loss: 4.004708766937256, p_loss: -7.552531719207764, mean_rew: 0.3430859375, variance: 3.99872327286005, mean_q: 7.344609260559082, std_q: 3.4249160289764404
Running avgs for agent 2: q_loss: 1.951267123222351, p_loss: 1.2342760562896729, mean_rew: -0.21779446594808044, variance: 1.921951751947403, mean_q: -1.4995993375778198, std_q: 3.3265390396118164
Running avgs for agent 3: q_loss: 2.403872013092041, p_loss: 1.9369406700134277, mean_rew: -0.2869395595505898, variance: 2.3844878984689712, mean_q: -2.253126382827759, std_q: 3.888542890548706

steps: 474975, episodes: 19000, mean episode reward: 1.296847054845848, agent episode reward: [3.98, 3.98, -3.1222607668462734, -3.5408921783078786], time: 63.998
steps: 474975, episodes: 19000, mean episode variance: 3.0200398619174957, agent episode variance: [1.0452294208109378, 0.934059399753809, 0.4517222063243389, 0.58902883502841], time: 63.998
Running avgs for agent 0: q_loss: 4.1156840324401855, p_loss: -7.883130073547363, mean_rew: 0.32671875, variance: 4.180917683243751, mean_q: 7.6998515129089355, std_q: 3.8047292232513428
Running avgs for agent 1: q_loss: 3.792739152908325, p_loss: -7.439070224761963, mean_rew: 0.32546875, variance: 3.736237599015236, mean_q: 7.238500118255615, std_q: 3.320507764816284
Running avgs for agent 2: q_loss: 1.8784348964691162, p_loss: 1.217703938484192, mean_rew: -0.21364693053035536, variance: 1.8068888252973556, mean_q: -1.4725483655929565, std_q: 3.1765575408935547
Running avgs for agent 3: q_loss: 2.379163980484009, p_loss: 1.9496569633483887, mean_rew: -0.2768899607974948, variance: 2.35611534011364, mean_q: -2.248525857925415, std_q: 3.8900318145751953

steps: 499975, episodes: 20000, mean episode reward: 1.5541415920095967, agent episode reward: [4.06, 4.06, -2.95854944390904, -3.6073089640813634], time: 63.609
steps: 499975, episodes: 20000, mean episode variance: 2.9274998705238104, agent episode variance: [0.9965139223635197, 0.9152722188681364, 0.4368426199555397, 0.5788711093366146], time: 63.61
Running avgs for agent 0: q_loss: 4.009937763214111, p_loss: -7.725342750549316, mean_rew: 0.3219921875, variance: 3.9860556894540786, mean_q: 7.557155609130859, std_q: 3.6561646461486816
Running avgs for agent 1: q_loss: 3.6735265254974365, p_loss: -7.310225486755371, mean_rew: 0.315859375, variance: 3.6610888754725455, mean_q: 7.123196601867676, std_q: 3.2462995052337646
Running avgs for agent 2: q_loss: 1.772905707359314, p_loss: 1.2270662784576416, mean_rew: -0.20292012687455374, variance: 1.7473704798221588, mean_q: -1.460667371749878, std_q: 2.9982962608337402
Running avgs for agent 3: q_loss: 2.3649091720581055, p_loss: 1.9100874662399292, mean_rew: -0.2725464822105102, variance: 2.3154844373464583, mean_q: -2.1976425647735596, std_q: 3.8152544498443604

steps: 524975, episodes: 21000, mean episode reward: 1.7357439003299293, agent episode reward: [3.97, 3.97, -2.775909888269651, -3.428346211400419], time: 64.105
steps: 524975, episodes: 21000, mean episode variance: 2.9033813300579787, agent episode variance: [0.9969609797596931, 0.9163707122802734, 0.4291660239249468, 0.5608836140930653], time: 64.106
Running avgs for agent 0: q_loss: 3.9586503505706787, p_loss: -7.534038066864014, mean_rew: 0.3160546875, variance: 3.9878439190387724, mean_q: 7.374038219451904, std_q: 3.531001567840576
Running avgs for agent 1: q_loss: 3.635295867919922, p_loss: -7.212634563446045, mean_rew: 0.310390625, variance: 3.665482849121094, mean_q: 7.041046619415283, std_q: 3.1844234466552734
Running avgs for agent 2: q_loss: 1.752805233001709, p_loss: 1.2593889236450195, mean_rew: -0.20662589063308906, variance: 1.7166640956997872, mean_q: -1.4871339797973633, std_q: 2.8790345191955566
Running avgs for agent 3: q_loss: 2.2436118125915527, p_loss: 1.8550721406936646, mean_rew: -0.2616976369696324, variance: 2.2435344563722612, mean_q: -2.1328306198120117, std_q: 3.7195279598236084

steps: 549975, episodes: 22000, mean episode reward: 1.1546939929807463, agent episode reward: [3.67, 3.67, -2.7705247389127305, -3.4147812681065224], time: 64.024
steps: 549975, episodes: 22000, mean episode variance: 2.8440987691879274, agent episode variance: [0.9770973431766034, 0.8919692008495331, 0.42455427926778794, 0.5504779458940029], time: 64.024
Running avgs for agent 0: q_loss: 3.947239637374878, p_loss: -7.380735874176025, mean_rew: 0.3149609375, variance: 3.9083893727064134, mean_q: 7.229739665985107, std_q: 3.4399478435516357
Running avgs for agent 1: q_loss: 3.598287343978882, p_loss: -7.148892402648926, mean_rew: 0.3078515625, variance: 3.5678768033981325, mean_q: 6.990876197814941, std_q: 3.183023452758789
Running avgs for agent 2: q_loss: 1.7131538391113281, p_loss: 1.294986605644226, mean_rew: -0.19990452696180327, variance: 1.6982171170711517, mean_q: -1.5102788209915161, std_q: 2.826059103012085
Running avgs for agent 3: q_loss: 2.237255334854126, p_loss: 1.8051074743270874, mean_rew: -0.25893773935502584, variance: 2.2019117835760116, mean_q: -2.0770962238311768, std_q: 3.6246936321258545

steps: 574975, episodes: 23000, mean episode reward: 1.8347363615098433, agent episode reward: [3.9, 3.9, -2.7298925157167315, -3.235371122773425], time: 66.816
steps: 574975, episodes: 23000, mean episode variance: 2.7506583203822372, agent episode variance: [0.9265785118639469, 0.8892913592457772, 0.41486717550456526, 0.5199212737679482], time: 66.816
Running avgs for agent 0: q_loss: 3.759873151779175, p_loss: -7.204198837280273, mean_rew: 0.296328125, variance: 3.7063140474557876, mean_q: 7.064803600311279, std_q: 3.3506572246551514
Running avgs for agent 1: q_loss: 3.532525062561035, p_loss: -7.047115802764893, mean_rew: 0.3010546875, variance: 3.5571654369831087, mean_q: 6.8941144943237305, std_q: 3.1932432651519775
Running avgs for agent 2: q_loss: 1.7659177780151367, p_loss: 1.3187283277511597, mean_rew: -0.19878194823115633, variance: 1.659468702018261, mean_q: -1.5305427312850952, std_q: 2.8063607215881348
Running avgs for agent 3: q_loss: 2.1590559482574463, p_loss: 1.7734673023223877, mean_rew: -0.24798172876262653, variance: 2.0796850950717927, mean_q: -2.0329959392547607, std_q: 3.5171761512756348

steps: 599975, episodes: 24000, mean episode reward: 1.1251682784911377, agent episode reward: [3.39, 3.39, -2.630005656726413, -3.0248260647824496], time: 68.587
steps: 599975, episodes: 24000, mean episode variance: 2.73461540254578, agent episode variance: [0.9189959246516227, 0.8728107647001744, 0.41796690892800686, 0.5248418042659759], time: 68.587
Running avgs for agent 0: q_loss: 3.6851372718811035, p_loss: -7.0309062004089355, mean_rew: 0.2914453125, variance: 3.675983698606491, mean_q: 6.886553764343262, std_q: 3.2734570503234863
Running avgs for agent 1: q_loss: 3.4822163581848145, p_loss: -6.944566249847412, mean_rew: 0.2903125, variance: 3.4912430588006975, mean_q: 6.799998760223389, std_q: 3.2406811714172363
Running avgs for agent 2: q_loss: 1.6870583295822144, p_loss: 1.316559910774231, mean_rew: -0.1919031229765475, variance: 1.6718676357120275, mean_q: -1.5132092237472534, std_q: 2.6925103664398193
Running avgs for agent 3: q_loss: 2.1504530906677246, p_loss: 1.7885318994522095, mean_rew: -0.24194889611354456, variance: 2.0993672170639037, mean_q: -2.0357680320739746, std_q: 3.5458755493164062

steps: 624975, episodes: 25000, mean episode reward: 1.2019983866588686, agent episode reward: [3.72, 3.72, -2.7611708071915317, -3.476830806149599], time: 71.001
steps: 624975, episodes: 25000, mean episode variance: 2.7476860722452403, agent episode variance: [0.9245406350940466, 0.8741737075448036, 0.4216917397081852, 0.5272799898982048], time: 71.002
Running avgs for agent 0: q_loss: 3.731393575668335, p_loss: -6.862819194793701, mean_rew: 0.29078125, variance: 3.6981625403761864, mean_q: 6.726478576660156, std_q: 3.2477781772613525
Running avgs for agent 1: q_loss: 3.5090365409851074, p_loss: -6.808652400970459, mean_rew: 0.2899609375, variance: 3.4966948301792145, mean_q: 6.669729232788086, std_q: 3.285635471343994
Running avgs for agent 2: q_loss: 1.6885576248168945, p_loss: 1.3369090557098389, mean_rew: -0.19197612924519727, variance: 1.6867669588327407, mean_q: -1.53739333152771, std_q: 2.66434383392334
Running avgs for agent 3: q_loss: 2.1937856674194336, p_loss: 1.8221033811569214, mean_rew: -0.24511685702928918, variance: 2.109119959592819, mean_q: -2.0812084674835205, std_q: 3.577667713165283

steps: 649975, episodes: 26000, mean episode reward: 1.0706328512438552, agent episode reward: [3.38, 3.38, -2.64741821009894, -3.041948938657205], time: 64.149
steps: 649975, episodes: 26000, mean episode variance: 2.7524861419051887, agent episode variance: [0.8973077354133129, 0.8775567595362663, 0.41815908851474526, 0.5594625584408641], time: 64.149
Running avgs for agent 0: q_loss: 3.675208806991577, p_loss: -6.676123142242432, mean_rew: 0.28296875, variance: 3.5892309416532515, mean_q: 6.54379940032959, std_q: 3.0984439849853516
Running avgs for agent 1: q_loss: 3.5143189430236816, p_loss: -6.641480445861816, mean_rew: 0.28234375, variance: 3.5102270381450653, mean_q: 6.508302688598633, std_q: 3.236736536026001
Running avgs for agent 2: q_loss: 1.7076371908187866, p_loss: 1.3608827590942383, mean_rew: -0.18882849321250347, variance: 1.672636354058981, mean_q: -1.5528463125228882, std_q: 2.693206548690796
Running avgs for agent 3: q_loss: 2.2639427185058594, p_loss: 1.8117338418960571, mean_rew: -0.24138092344205433, variance: 2.2378502337634565, mean_q: -2.0605521202087402, std_q: 3.552964448928833

steps: 674975, episodes: 27000, mean episode reward: 1.263617790309368, agent episode reward: [3.75, 3.75, -2.616960585582827, -3.6194216241078054], time: 65.392
steps: 674975, episodes: 27000, mean episode variance: 2.6285810322761534, agent episode variance: [0.8634695829451084, 0.8423636875450611, 0.4039150990843773, 0.5188326627016068], time: 65.393
Running avgs for agent 0: q_loss: 3.4736452102661133, p_loss: -6.438706398010254, mean_rew: 0.2741796875, variance: 3.453878331780434, mean_q: 6.318709373474121, std_q: 3.0025463104248047
Running avgs for agent 1: q_loss: 3.4053354263305664, p_loss: -6.472874164581299, mean_rew: 0.2769921875, variance: 3.3694547501802443, mean_q: 6.3479180335998535, std_q: 3.1960058212280273
Running avgs for agent 2: q_loss: 1.6199504137039185, p_loss: 1.3485424518585205, mean_rew: -0.18002984839990296, variance: 1.6156603963375091, mean_q: -1.5401650667190552, std_q: 2.612384080886841
Running avgs for agent 3: q_loss: 2.186537981033325, p_loss: 1.8062881231307983, mean_rew: -0.2390331205899988, variance: 2.075330650806427, mean_q: -2.050574541091919, std_q: 3.5448338985443115

steps: 699975, episodes: 28000, mean episode reward: 1.545356646771249, agent episode reward: [3.67, 3.67, -2.594971195427733, -3.1996721578010185], time: 66.559
steps: 699975, episodes: 28000, mean episode variance: 2.569861282348633, agent episode variance: [0.8367607962787151, 0.8255755731761456, 0.3909108399152756, 0.5166140729784966], time: 66.559
Running avgs for agent 0: q_loss: 3.3803563117980957, p_loss: -6.215116500854492, mean_rew: 0.2669140625, variance: 3.3470431851148605, mean_q: 6.0913920402526855, std_q: 2.92189359664917
Running avgs for agent 1: q_loss: 3.338524341583252, p_loss: -6.277076721191406, mean_rew: 0.2664453125, variance: 3.3023022927045824, mean_q: 6.161109447479248, std_q: 3.041186571121216
Running avgs for agent 2: q_loss: 1.6159988641738892, p_loss: 1.3554450273513794, mean_rew: -0.17874143510293336, variance: 1.5636433596611023, mean_q: -1.5365502834320068, std_q: 2.523292303085327
Running avgs for agent 3: q_loss: 2.1030211448669434, p_loss: 1.7828266620635986, mean_rew: -0.22937623177555566, variance: 2.0664562919139864, mean_q: -2.008166790008545, std_q: 3.4177682399749756

steps: 724975, episodes: 29000, mean episode reward: 1.2788490458286819, agent episode reward: [3.39, 3.39, -2.4509302779668904, -3.050220676204427], time: 64.74
steps: 724975, episodes: 29000, mean episode variance: 2.5175937450528143, agent episode variance: [0.8511612649559974, 0.806758804410696, 0.3633829085528851, 0.49629076713323594], time: 64.74
Running avgs for agent 0: q_loss: 3.3957438468933105, p_loss: -6.0064287185668945, mean_rew: 0.264453125, variance: 3.4046450598239897, mean_q: 5.891654014587402, std_q: 2.8530197143554688
Running avgs for agent 1: q_loss: 3.2873311042785645, p_loss: -6.08952522277832, mean_rew: 0.26859375, variance: 3.227035217642784, mean_q: 5.981832981109619, std_q: 2.955925703048706
Running avgs for agent 2: q_loss: 1.50149667263031, p_loss: 1.356170415878296, mean_rew: -0.175162565767491, variance: 1.4535316342115403, mean_q: -1.5284255743026733, std_q: 2.5068466663360596
Running avgs for agent 3: q_loss: 1.9941184520721436, p_loss: 1.7798162698745728, mean_rew: -0.22264542423730632, variance: 1.9851630685329438, mean_q: -2.000729560852051, std_q: 3.4129326343536377

steps: 749975, episodes: 30000, mean episode reward: 2.201720798507071, agent episode reward: [4.15, 4.15, -2.8278773675061237, -3.2704018339868055], time: 64.493
steps: 749975, episodes: 30000, mean episode variance: 2.534530142389238, agent episode variance: [0.8239975466132164, 0.8059483675360679, 0.38752375461161137, 0.5170604736283422], time: 64.493
Running avgs for agent 0: q_loss: 3.3436717987060547, p_loss: -5.850688457489014, mean_rew: 0.2596875, variance: 3.295990186452866, mean_q: 5.7340545654296875, std_q: 2.849824905395508
Running avgs for agent 1: q_loss: 3.235198736190796, p_loss: -5.94423246383667, mean_rew: 0.26421875, variance: 3.2237934701442716, mean_q: 5.836653232574463, std_q: 2.8998920917510986
Running avgs for agent 2: q_loss: 1.5698211193084717, p_loss: 1.3663733005523682, mean_rew: -0.17685800654252115, variance: 1.5500950184464455, mean_q: -1.5355533361434937, std_q: 2.4994823932647705
Running avgs for agent 3: q_loss: 2.078148365020752, p_loss: 1.7821789979934692, mean_rew: -0.22078749083603738, variance: 2.0682418945133687, mean_q: -1.9951348304748535, std_q: 3.2521064281463623

steps: 774975, episodes: 31000, mean episode reward: 1.386529024814094, agent episode reward: [3.6, 3.6, -2.486113287832648, -3.327357687353258], time: 63.999
steps: 774975, episodes: 31000, mean episode variance: 2.5015416670441626, agent episode variance: [0.8409781613647938, 0.797055912643671, 0.37013864317536355, 0.4933689498603344], time: 64.0
Running avgs for agent 0: q_loss: 3.380613327026367, p_loss: -5.6740498542785645, mean_rew: 0.2582421875, variance: 3.363912645459175, mean_q: 5.567593097686768, std_q: 2.830099582672119
Running avgs for agent 1: q_loss: 3.200570583343506, p_loss: -5.771725177764893, mean_rew: 0.2556640625, variance: 3.188223650574684, mean_q: 5.668295860290527, std_q: 2.853071689605713
Running avgs for agent 2: q_loss: 1.520034670829773, p_loss: 1.348497748374939, mean_rew: -0.16921005356986982, variance: 1.4805545727014542, mean_q: -1.5110714435577393, std_q: 2.4821951389312744
Running avgs for agent 3: q_loss: 1.9789834022521973, p_loss: 1.8251675367355347, mean_rew: -0.21795933704939985, variance: 1.9734757994413377, mean_q: -2.0337207317352295, std_q: 3.2190256118774414

steps: 799975, episodes: 32000, mean episode reward: 1.2167511537455853, agent episode reward: [3.18, 3.18, -2.090203530052944, -3.05304531620147], time: 64.371
steps: 799975, episodes: 32000, mean episode variance: 2.4412496988475323, agent episode variance: [0.7965588334202767, 0.7837127548754215, 0.3828499796390533, 0.4781281309127808], time: 64.372
Running avgs for agent 0: q_loss: 3.295708417892456, p_loss: -5.467455863952637, mean_rew: 0.2534375, variance: 3.1862353336811067, mean_q: 5.36565637588501, std_q: 2.769519805908203
Running avgs for agent 1: q_loss: 3.2203590869903564, p_loss: -5.614757061004639, mean_rew: 0.254140625, variance: 3.134851019501686, mean_q: 5.518803596496582, std_q: 2.822338819503784
Running avgs for agent 2: q_loss: 1.5503759384155273, p_loss: 1.3483898639678955, mean_rew: -0.17121073065478049, variance: 1.5313999185562133, mean_q: -1.5101003646850586, std_q: 2.5365045070648193
Running avgs for agent 3: q_loss: 2.001027822494507, p_loss: 1.8576291799545288, mean_rew: -0.21653224257707815, variance: 1.9125125236511231, mean_q: -2.0590155124664307, std_q: 3.2505476474761963

steps: 824975, episodes: 33000, mean episode reward: 1.7428458994969223, agent episode reward: [3.53, 3.53, -1.9531792863500357, -3.3639748141530417], time: 64.431
steps: 824975, episodes: 33000, mean episode variance: 2.4789714927226303, agent episode variance: [0.7938241562843322, 0.7846684581786394, 0.40908744804561137, 0.4913914302140474], time: 64.432
Running avgs for agent 0: q_loss: 3.2232019901275635, p_loss: -5.261589050292969, mean_rew: 0.250078125, variance: 3.175296625137329, mean_q: 5.1645002365112305, std_q: 2.6800434589385986
Running avgs for agent 1: q_loss: 3.2135117053985596, p_loss: -5.415502071380615, mean_rew: 0.251796875, variance: 3.1386738327145576, mean_q: 5.326798915863037, std_q: 2.772087812423706
Running avgs for agent 2: q_loss: 1.6376413106918335, p_loss: 1.3322409391403198, mean_rew: -0.17001436045878357, variance: 1.6363497921824455, mean_q: -1.498498797416687, std_q: 2.549792766571045
Running avgs for agent 3: q_loss: 2.0216610431671143, p_loss: 1.8643808364868164, mean_rew: -0.21242257519563856, variance: 1.9655657208561896, mean_q: -2.0621533393859863, std_q: 3.126038074493408

steps: 849975, episodes: 34000, mean episode reward: 1.2905117489346325, agent episode reward: [3.43, 3.43, -1.9750772994206884, -3.594410951644679], time: 64.538
steps: 849975, episodes: 34000, mean episode variance: 2.377758915066719, agent episode variance: [0.7540915860831737, 0.7771627088487149, 0.38416195642948153, 0.462342663705349], time: 64.538
Running avgs for agent 0: q_loss: 3.136681079864502, p_loss: -5.045956611633301, mean_rew: 0.2433203125, variance: 3.016366344332695, mean_q: 4.957576274871826, std_q: 2.6643624305725098
Running avgs for agent 1: q_loss: 3.1538379192352295, p_loss: -5.2104363441467285, mean_rew: 0.250078125, variance: 3.1086508353948594, mean_q: 5.121957778930664, std_q: 2.7290666103363037
Running avgs for agent 2: q_loss: 1.5549428462982178, p_loss: 1.2803404331207275, mean_rew: -0.16256054264977712, variance: 1.5366478257179261, mean_q: -1.4387030601501465, std_q: 2.5020105838775635
Running avgs for agent 3: q_loss: 1.8774123191833496, p_loss: 1.839108943939209, mean_rew: -0.2093065061099232, variance: 1.849370654821396, mean_q: -2.026615858078003, std_q: 3.0461225509643555

steps: 874975, episodes: 35000, mean episode reward: 1.6313133805021058, agent episode reward: [3.57, 3.57, -2.3921743731488885, -3.1165122463490054], time: 64.942
steps: 874975, episodes: 35000, mean episode variance: 2.331937164187431, agent episode variance: [0.7673109854161739, 0.756768076390028, 0.37174702306091784, 0.43611107932031157], time: 64.943
Running avgs for agent 0: q_loss: 3.1333892345428467, p_loss: -4.8693156242370605, mean_rew: 0.242734375, variance: 3.0692439416646957, mean_q: 4.778707027435303, std_q: 2.612710952758789
Running avgs for agent 1: q_loss: 3.077563524246216, p_loss: -5.018591403961182, mean_rew: 0.246328125, variance: 3.027072305560112, mean_q: 4.933607578277588, std_q: 2.668339252471924
Running avgs for agent 2: q_loss: 1.5303534269332886, p_loss: 1.2049020528793335, mean_rew: -0.1586879568665, variance: 1.4869880922436713, mean_q: -1.3664093017578125, std_q: 2.444540500640869
Running avgs for agent 3: q_loss: 1.7837860584259033, p_loss: 1.8365434408187866, mean_rew: -0.20479158957628402, variance: 1.7444443172812463, mean_q: -2.009615421295166, std_q: 2.986386299133301

steps: 899975, episodes: 36000, mean episode reward: 1.631012242143584, agent episode reward: [3.43, 3.43, -2.286023659043816, -2.9429640988126], time: 64.678
steps: 899975, episodes: 36000, mean episode variance: 2.349860285937786, agent episode variance: [0.772975988984108, 0.768212236315012, 0.3631058437228203, 0.44556621691584586], time: 64.679
Running avgs for agent 0: q_loss: 3.1464881896972656, p_loss: -4.700499057769775, mean_rew: 0.2372265625, variance: 3.091903955936432, mean_q: 4.6117634773254395, std_q: 2.5763027667999268
Running avgs for agent 1: q_loss: 3.1358587741851807, p_loss: -4.872985363006592, mean_rew: 0.2459375, variance: 3.072848945260048, mean_q: 4.789122104644775, std_q: 2.6579267978668213
Running avgs for agent 2: q_loss: 1.4847869873046875, p_loss: 1.1613662242889404, mean_rew: -0.1617410782040335, variance: 1.4524233748912811, mean_q: -1.3198012113571167, std_q: 2.4104952812194824
Running avgs for agent 3: q_loss: 1.8236591815948486, p_loss: 1.8242828845977783, mean_rew: -0.20781681359547202, variance: 1.7822648676633834, mean_q: -2.0020997524261475, std_q: 2.974437952041626

steps: 924975, episodes: 37000, mean episode reward: 1.7389558721130414, agent episode reward: [3.59, 3.59, -2.437395347964085, -3.0036487799228735], time: 64.975
steps: 924975, episodes: 37000, mean episode variance: 2.3490345580279826, agent episode variance: [0.7652281486392021, 0.7507558544278145, 0.3818725958168507, 0.45117795914411546], time: 64.976
Running avgs for agent 0: q_loss: 3.0905420780181885, p_loss: -4.5584492683410645, mean_rew: 0.2376171875, variance: 3.0609125945568083, mean_q: 4.467993259429932, std_q: 2.561535596847534
Running avgs for agent 1: q_loss: 3.0503132343292236, p_loss: -4.751835823059082, mean_rew: 0.236484375, variance: 3.003023417711258, mean_q: 4.66685152053833, std_q: 2.616363525390625
Running avgs for agent 2: q_loss: 1.5328171253204346, p_loss: 1.1241313219070435, mean_rew: -0.1613082125903381, variance: 1.5274903832674027, mean_q: -1.2785465717315674, std_q: 2.468769073486328
Running avgs for agent 3: q_loss: 1.8526424169540405, p_loss: 1.8073781728744507, mean_rew: -0.206066530364374, variance: 1.8047118365764618, mean_q: -1.984918475151062, std_q: 2.9386239051818848

steps: 949975, episodes: 38000, mean episode reward: 1.5037555575023211, agent episode reward: [3.62, 3.62, -2.5694558802035425, -3.166788562294136], time: 64.224
steps: 949975, episodes: 38000, mean episode variance: 2.3364379037506877, agent episode variance: [0.7464104035794735, 0.7412415923029184, 0.38338261263445017, 0.4654032952338457], time: 64.224
Running avgs for agent 0: q_loss: 3.0668752193450928, p_loss: -4.440934658050537, mean_rew: 0.23421875, variance: 2.985641614317894, mean_q: 4.3560895919799805, std_q: 2.5217902660369873
Running avgs for agent 1: q_loss: 3.0428102016448975, p_loss: -4.618636131286621, mean_rew: 0.2365625, variance: 2.9649663692116737, mean_q: 4.54029655456543, std_q: 2.554142951965332
Running avgs for agent 2: q_loss: 1.564640998840332, p_loss: 1.076989769935608, mean_rew: -0.15551484672655408, variance: 1.5335304505378007, mean_q: -1.2300620079040527, std_q: 2.4444241523742676
Running avgs for agent 3: q_loss: 1.90769362449646, p_loss: 1.8296598196029663, mean_rew: -0.20571871713587322, variance: 1.8616131809353829, mean_q: -2.0032968521118164, std_q: 3.0275588035583496

steps: 974975, episodes: 39000, mean episode reward: 1.5128083174719964, agent episode reward: [3.43, 3.43, -2.3891619597414455, -2.9580297227865584], time: 64.648
steps: 974975, episodes: 39000, mean episode variance: 2.325031713321805, agent episode variance: [0.7542928509116172, 0.7423846500217914, 0.367812960550189, 0.46054125183820727], time: 64.648
Running avgs for agent 0: q_loss: 3.066481351852417, p_loss: -4.316651344299316, mean_rew: 0.2351953125, variance: 3.017171403646469, mean_q: 4.230893611907959, std_q: 2.503990650177002
Running avgs for agent 1: q_loss: 3.052326202392578, p_loss: -4.5361199378967285, mean_rew: 0.238359375, variance: 2.9695386000871657, mean_q: 4.4556355476379395, std_q: 2.5950047969818115
Running avgs for agent 2: q_loss: 1.5057002305984497, p_loss: 1.0292363166809082, mean_rew: -0.15585670415908037, variance: 1.471251842200756, mean_q: -1.1883293390274048, std_q: 2.474085807800293
Running avgs for agent 3: q_loss: 1.897491455078125, p_loss: 1.8108772039413452, mean_rew: -0.2020556039702264, variance: 1.842165007352829, mean_q: -1.9866061210632324, std_q: 3.0536386966705322

steps: 999975, episodes: 40000, mean episode reward: 1.9501291100195082, agent episode reward: [3.95, 3.95, -2.4257912306404616, -3.5240796593400305], time: 64.413
steps: 999975, episodes: 40000, mean episode variance: 2.3061440761238337, agent episode variance: [0.7278451616317034, 0.7358005772531032, 0.3688664466291666, 0.47363189060986044], time: 64.414
Running avgs for agent 0: q_loss: 2.9509902000427246, p_loss: -4.205652236938477, mean_rew: 0.227890625, variance: 2.9113806465268137, mean_q: 4.123232364654541, std_q: 2.449129343032837
Running avgs for agent 1: q_loss: 2.9847352504730225, p_loss: -4.457851886749268, mean_rew: 0.2307421875, variance: 2.943202309012413, mean_q: 4.380849838256836, std_q: 2.6073009967803955
Running avgs for agent 2: q_loss: 1.505467414855957, p_loss: 0.9931153655052185, mean_rew: -0.15316125304715225, variance: 1.4754657865166665, mean_q: -1.1421152353286743, std_q: 2.4576995372772217
Running avgs for agent 3: q_loss: 1.9351682662963867, p_loss: 1.809026837348938, mean_rew: -0.19979180357305631, variance: 1.8945275624394418, mean_q: -1.9819786548614502, std_q: 3.0140366554260254

steps: 1024975, episodes: 41000, mean episode reward: 2.174701631065547, agent episode reward: [4.29, 4.29, -2.366301961213684, -4.038996407720768], time: 64.877
steps: 1024975, episodes: 41000, mean episode variance: 2.319240638576448, agent episode variance: [0.7278875508308411, 0.7375034332573414, 0.3768508905991912, 0.4769987638890743], time: 64.878
Running avgs for agent 0: q_loss: 3.002692937850952, p_loss: -4.163375377655029, mean_rew: 0.2332421875, variance: 2.9115502033233644, mean_q: 4.080300331115723, std_q: 2.5066018104553223
Running avgs for agent 1: q_loss: 3.0224344730377197, p_loss: -4.394137859344482, mean_rew: 0.2338671875, variance: 2.9500137330293654, mean_q: 4.316764831542969, std_q: 2.6331818103790283
Running avgs for agent 2: q_loss: 1.5199965238571167, p_loss: 0.9627789855003357, mean_rew: -0.15097639924608952, variance: 1.5074035623967648, mean_q: -1.1119097471237183, std_q: 2.5096263885498047
Running avgs for agent 3: q_loss: 1.859277367591858, p_loss: 1.8145250082015991, mean_rew: -0.19447325633500315, variance: 1.9079950555562972, mean_q: -1.978341817855835, std_q: 2.984055280685425

steps: 1049975, episodes: 42000, mean episode reward: 1.8781879293947938, agent episode reward: [4.39, 4.39, -2.606643735102456, -4.2951683355027495], time: 64.874
steps: 1049975, episodes: 42000, mean episode variance: 2.3550661233365537, agent episode variance: [0.7640480897128582, 0.7428595891296863, 0.370347496971488, 0.477810947522521], time: 64.875
Running avgs for agent 0: q_loss: 3.064138174057007, p_loss: -4.122470855712891, mean_rew: 0.2366015625, variance: 3.056192358851433, mean_q: 4.047516345977783, std_q: 2.5446064472198486
Running avgs for agent 1: q_loss: 3.0457661151885986, p_loss: -4.317190647125244, mean_rew: 0.2314453125, variance: 2.971438356518745, mean_q: 4.24772834777832, std_q: 2.5891289710998535
Running avgs for agent 2: q_loss: 1.518917202949524, p_loss: 0.9262378811836243, mean_rew: -0.14733124924136626, variance: 1.481389987885952, mean_q: -1.0627166032791138, std_q: 2.370321750640869
Running avgs for agent 3: q_loss: 1.965752124786377, p_loss: 1.8570044040679932, mean_rew: -0.1875912857529771, variance: 1.911243790090084, mean_q: -2.0122599601745605, std_q: 2.794236898422241

steps: 1074975, episodes: 43000, mean episode reward: 2.3059727669119496, agent episode reward: [4.59, 4.59, -2.2433277493504664, -4.630699483737584], time: 64.856
steps: 1074975, episodes: 43000, mean episode variance: 2.370314665168524, agent episode variance: [0.7546861489713192, 0.7500685524344445, 0.3682252272367477, 0.4973347365260124], time: 64.857
Running avgs for agent 0: q_loss: 3.0806689262390137, p_loss: -4.075031280517578, mean_rew: 0.234765625, variance: 3.018744595885277, mean_q: 3.9988934993743896, std_q: 2.542667865753174
Running avgs for agent 1: q_loss: 3.0562877655029297, p_loss: -4.2522196769714355, mean_rew: 0.229921875, variance: 3.000274209737778, mean_q: 4.18669319152832, std_q: 2.5941646099090576
Running avgs for agent 2: q_loss: 1.5208684206008911, p_loss: 0.8747743964195251, mean_rew: -0.1448058356771755, variance: 1.4729009089469909, mean_q: -1.0064605474472046, std_q: 2.364154815673828
Running avgs for agent 3: q_loss: 1.9905608892440796, p_loss: 1.9297727346420288, mean_rew: -0.18764076977150154, variance: 1.9893389461040496, mean_q: -2.0840790271759033, std_q: 2.80796217918396

steps: 1099975, episodes: 44000, mean episode reward: 1.9723184285121647, agent episode reward: [4.06, 4.06, -1.839174002886915, -4.308507568600921], time: 64.638
steps: 1099975, episodes: 44000, mean episode variance: 2.3298625753447415, agent episode variance: [0.7549515354931354, 0.7367873573750258, 0.3673273658081889, 0.4707963166683912], time: 64.638
Running avgs for agent 0: q_loss: 3.0760557651519775, p_loss: -4.041872978210449, mean_rew: 0.22859375, variance: 3.0198061419725417, mean_q: 3.968116044998169, std_q: 2.5235326290130615
Running avgs for agent 1: q_loss: 2.9539003372192383, p_loss: -4.20169734954834, mean_rew: 0.2228515625, variance: 2.947149429500103, mean_q: 4.139120101928711, std_q: 2.642301559448242
Running avgs for agent 2: q_loss: 1.4804589748382568, p_loss: 0.8325905203819275, mean_rew: -0.13844525408794506, variance: 1.4693094632327557, mean_q: -0.9554632902145386, std_q: 2.322953224182129
Running avgs for agent 3: q_loss: 1.9390690326690674, p_loss: 1.9808752536773682, mean_rew: -0.1875202142679457, variance: 1.883185266673565, mean_q: -2.1338109970092773, std_q: 2.7878470420837402

steps: 1124975, episodes: 45000, mean episode reward: 2.7244546272542842, agent episode reward: [4.81, 4.81, -1.9713779535728695, -4.9241674191728455], time: 64.873
steps: 1124975, episodes: 45000, mean episode variance: 2.353785720899701, agent episode variance: [0.7572143879234791, 0.7563209035098553, 0.35411093962192536, 0.48613948984444144], time: 64.874
Running avgs for agent 0: q_loss: 3.103626251220703, p_loss: -4.0696187019348145, mean_rew: 0.2318359375, variance: 3.0288575516939162, mean_q: 3.9912641048431396, std_q: 2.61696720123291
Running avgs for agent 1: q_loss: 3.084810495376587, p_loss: -4.16552734375, mean_rew: 0.2266796875, variance: 3.025283614039421, mean_q: 4.103410720825195, std_q: 2.7146151065826416
Running avgs for agent 2: q_loss: 1.4493906497955322, p_loss: 0.7819260358810425, mean_rew: -0.13753678231231198, variance: 1.4164437584877014, mean_q: -0.9017501473426819, std_q: 2.3094961643218994
Running avgs for agent 3: q_loss: 1.9685174226760864, p_loss: 2.059983491897583, mean_rew: -0.18812490118220807, variance: 1.9445579593777658, mean_q: -2.21368408203125, std_q: 2.795668601989746

steps: 1149975, episodes: 46000, mean episode reward: 2.592741214833547, agent episode reward: [4.96, 4.96, -2.041969246816224, -5.285289538350229], time: 67.814
steps: 1149975, episodes: 46000, mean episode variance: 2.3111915733367203, agent episode variance: [0.7431727794408798, 0.7490613296627998, 0.3304850279539824, 0.48847243627905845], time: 67.815
Running avgs for agent 0: q_loss: 3.025765895843506, p_loss: -4.0323381423950195, mean_rew: 0.219296875, variance: 2.972691117763519, mean_q: 3.9597458839416504, std_q: 2.560852527618408
Running avgs for agent 1: q_loss: 3.020763635635376, p_loss: -4.109887599945068, mean_rew: 0.2173046875, variance: 2.9962453186511993, mean_q: 4.054120063781738, std_q: 2.69014310836792
Running avgs for agent 2: q_loss: 1.3399931192398071, p_loss: 0.7306591868400574, mean_rew: -0.13413541795582956, variance: 1.3219401118159295, mean_q: -0.8445989489555359, std_q: 2.2288100719451904
Running avgs for agent 3: q_loss: 2.001084804534912, p_loss: 2.1519887447357178, mean_rew: -0.18620300445598678, variance: 1.9538897451162338, mean_q: -2.297689914703369, std_q: 2.8573057651519775

steps: 1174975, episodes: 47000, mean episode reward: 3.838820395799669, agent episode reward: [6.16, 6.16, -1.515198352868095, -6.965981251332236], time: 2788.432
steps: 1174975, episodes: 47000, mean episode variance: 2.261927634328604, agent episode variance: [0.7162228393554687, 0.7247630532830953, 0.32385727953910826, 0.4970844621509314], time: 2788.433
Running avgs for agent 0: q_loss: 2.9073662757873535, p_loss: -4.049190044403076, mean_rew: 0.21546875, variance: 2.864891357421875, mean_q: 3.9744503498077393, std_q: 2.573052406311035
Running avgs for agent 1: q_loss: 2.955885410308838, p_loss: -4.048701286315918, mean_rew: 0.2137109375, variance: 2.8990522131323813, mean_q: 4.000468730926514, std_q: 2.6414437294006348
Running avgs for agent 2: q_loss: 1.3250477313995361, p_loss: 0.6997126340866089, mean_rew: -0.13628045646044495, variance: 1.295429118156433, mean_q: -0.8020256161689758, std_q: 2.2533535957336426
Running avgs for agent 3: q_loss: 2.01470685005188, p_loss: 2.2212975025177, mean_rew: -0.18386231946641218, variance: 1.9883378486037255, mean_q: -2.3653783798217773, std_q: 2.875326156616211

steps: 1199975, episodes: 48000, mean episode reward: 3.56854498465378, agent episode reward: [6.0, 6.0, -1.964137769439289, -6.467317245906931], time: 67.76
steps: 1199975, episodes: 48000, mean episode variance: 2.2371154420226813, agent episode variance: [0.7086468199342489, 0.7067851230800152, 0.3133271892517805, 0.5083563097566366], time: 67.76
Running avgs for agent 0: q_loss: 2.904503107070923, p_loss: -4.056951522827148, mean_rew: 0.2016015625, variance: 2.8345872797369958, mean_q: 3.987304925918579, std_q: 2.5730409622192383
Running avgs for agent 1: q_loss: 2.9098730087280273, p_loss: -3.9988269805908203, mean_rew: 0.2023046875, variance: 2.8271404923200607, mean_q: 3.961547374725342, std_q: 2.594663143157959
Running avgs for agent 2: q_loss: 1.2844233512878418, p_loss: 0.6483098268508911, mean_rew: -0.1263593131596297, variance: 1.253308757007122, mean_q: -0.7355504035949707, std_q: 2.188532829284668
Running avgs for agent 3: q_loss: 2.057798147201538, p_loss: 2.3220298290252686, mean_rew: -0.18421138141127413, variance: 2.0334252390265464, mean_q: -2.4572887420654297, std_q: 3.007481098175049

steps: 1224975, episodes: 49000, mean episode reward: 3.523455340785296, agent episode reward: [6.05, 6.05, -1.6297703649952202, -6.946774294219484], time: 65.567
steps: 1224975, episodes: 49000, mean episode variance: 2.194345695659518, agent episode variance: [0.6798895885050297, 0.7040966737866402, 0.3050486394017935, 0.5053107939660549], time: 65.568
Running avgs for agent 0: q_loss: 2.7933499813079834, p_loss: -4.014153480529785, mean_rew: 0.18921875, variance: 2.7195583540201187, mean_q: 3.9479637145996094, std_q: 2.4986371994018555
Running avgs for agent 1: q_loss: 2.8647782802581787, p_loss: -3.9423184394836426, mean_rew: 0.191640625, variance: 2.816386695146561, mean_q: 3.914520025253296, std_q: 2.5594911575317383
Running avgs for agent 2: q_loss: 1.2509831190109253, p_loss: 0.619096040725708, mean_rew: -0.1226933680783931, variance: 1.220194557607174, mean_q: -0.6987966299057007, std_q: 2.1524152755737305
Running avgs for agent 3: q_loss: 2.090726852416992, p_loss: 2.4194223880767822, mean_rew: -0.17225982064146572, variance: 2.0212431758642198, mean_q: -2.5427122116088867, std_q: 3.0573930740356445

steps: 1249975, episodes: 50000, mean episode reward: 4.38947294747567, agent episode reward: [7.03, 7.03, -1.7957877768632753, -7.874739275661055], time: 65.158
steps: 1249975, episodes: 50000, mean episode variance: 2.1312504620179533, agent episode variance: [0.6591797275245189, 0.6584226866364479, 0.284197138004005, 0.5294509098529816], time: 65.159
Running avgs for agent 0: q_loss: 2.708700656890869, p_loss: -4.001187324523926, mean_rew: 0.18171875, variance: 2.6367189100980757, mean_q: 3.9336888790130615, std_q: 2.4237241744995117
Running avgs for agent 1: q_loss: 2.6822898387908936, p_loss: -3.902989387512207, mean_rew: 0.1775390625, variance: 2.6336907465457915, mean_q: 3.8762588500976562, std_q: 2.475224018096924
Running avgs for agent 2: q_loss: 1.1780864000320435, p_loss: 0.6050600409507751, mean_rew: -0.11964460821754916, variance: 1.13678855201602, mean_q: -0.6763381361961365, std_q: 2.1025009155273438
Running avgs for agent 3: q_loss: 2.116497278213501, p_loss: 2.4876961708068848, mean_rew: -0.16623086011277305, variance: 2.1178036394119264, mean_q: -2.5985710620880127, std_q: 3.0247771739959717

steps: 1274975, episodes: 51000, mean episode reward: 5.477904969581229, agent episode reward: [8.13, 8.13, -1.6242402996090628, -9.157854730809708], time: 66.974
steps: 1274975, episodes: 51000, mean episode variance: 2.165258291386068, agent episode variance: [0.6745507976263762, 0.6701273218095303, 0.27678197801858184, 0.5437981939315796], time: 66.975
Running avgs for agent 0: q_loss: 2.7571663856506348, p_loss: -4.046456813812256, mean_rew: 0.181953125, variance: 2.6982031905055046, mean_q: 3.971482276916504, std_q: 2.469177722930908
Running avgs for agent 1: q_loss: 2.749912977218628, p_loss: -3.970710515975952, mean_rew: 0.1830078125, variance: 2.680509287238121, mean_q: 3.9379067420959473, std_q: 2.539512872695923
Running avgs for agent 2: q_loss: 1.157943606376648, p_loss: 0.5715545415878296, mean_rew: -0.11506457382849572, variance: 1.1071279120743274, mean_q: -0.632691502571106, std_q: 2.045698404312134
Running avgs for agent 3: q_loss: 2.1774091720581055, p_loss: 2.600289821624756, mean_rew: -0.16700699718651413, variance: 2.1751927757263183, mean_q: -2.7146358489990234, std_q: 3.1405739784240723

steps: 1299975, episodes: 52000, mean episode reward: 4.886573950851625, agent episode reward: [7.44, 7.44, -1.7361282222924872, -8.257297826855888], time: 65.454
steps: 1299975, episodes: 52000, mean episode variance: 2.153491680495441, agent episode variance: [0.664594577729702, 0.6705409950315953, 0.26548770812898875, 0.552868399605155], time: 65.454
Running avgs for agent 0: q_loss: 2.6815528869628906, p_loss: -4.1402459144592285, mean_rew: 0.1817578125, variance: 2.658378310918808, mean_q: 4.056092262268066, std_q: 2.50108003616333
Running avgs for agent 1: q_loss: 2.714625358581543, p_loss: -4.03495979309082, mean_rew: 0.177734375, variance: 2.682163980126381, mean_q: 4.00266170501709, std_q: 2.531859874725342
Running avgs for agent 2: q_loss: 1.086980938911438, p_loss: 0.5304127335548401, mean_rew: -0.10809380995450431, variance: 1.061950832515955, mean_q: -0.5842849016189575, std_q: 1.9082437753677368
Running avgs for agent 3: q_loss: 2.2721505165100098, p_loss: 2.7349343299865723, mean_rew: -0.16920638308649982, variance: 2.21147359842062, mean_q: -2.8574130535125732, std_q: 3.201831579208374

steps: 1324975, episodes: 53000, mean episode reward: 6.550533007820817, agent episode reward: [9.24, 9.24, -1.5428301961811426, -10.386636795998042], time: 65.412
steps: 1324975, episodes: 53000, mean episode variance: 2.15649251973629, agent episode variance: [0.6751832527518272, 0.6649966906905175, 0.25470861823856833, 0.561603958055377], time: 65.413
Running avgs for agent 0: q_loss: 2.6896703243255615, p_loss: -4.2436137199401855, mean_rew: 0.1794921875, variance: 2.700733011007309, mean_q: 4.156440734863281, std_q: 2.522291421890259
Running avgs for agent 1: q_loss: 2.704779624938965, p_loss: -4.163601398468018, mean_rew: 0.18109375, variance: 2.65998676276207, mean_q: 4.1248369216918945, std_q: 2.5968148708343506
Running avgs for agent 2: q_loss: 1.0644209384918213, p_loss: 0.47726085782051086, mean_rew: -0.10319557921341051, variance: 1.0188344729542733, mean_q: -0.5274240970611572, std_q: 1.764622688293457
Running avgs for agent 3: q_loss: 2.293982744216919, p_loss: 2.8849072456359863, mean_rew: -0.17475629285853686, variance: 2.246415832221508, mean_q: -3.006470203399658, std_q: 3.2486302852630615

steps: 1349975, episodes: 54000, mean episode reward: 6.0022763737574385, agent episode reward: [8.44, 8.44, -1.219809973377956, -9.657913652864606], time: 65.45
steps: 1349975, episodes: 54000, mean episode variance: 2.186220538124442, agent episode variance: [0.6873376930058003, 0.686890139773488, 0.23187565743923189, 0.5801170479059219], time: 65.451
Running avgs for agent 0: q_loss: 2.7793514728546143, p_loss: -4.3923563957214355, mean_rew: 0.187265625, variance: 2.749350772023201, mean_q: 4.295804500579834, std_q: 2.572939395904541
Running avgs for agent 1: q_loss: 2.8064424991607666, p_loss: -4.311478614807129, mean_rew: 0.1853125, variance: 2.747560559093952, mean_q: 4.2729034423828125, std_q: 2.70164155960083
Running avgs for agent 2: q_loss: 0.9372270703315735, p_loss: 0.4317150413990021, mean_rew: -0.09920764414200042, variance: 0.9275026297569275, mean_q: -0.476809561252594, std_q: 1.6286345720291138
Running avgs for agent 3: q_loss: 2.325338840484619, p_loss: 3.036158561706543, mean_rew: -0.17618271900095303, variance: 2.3204681916236876, mean_q: -3.155991315841675, std_q: 3.31141996383667

steps: 1374975, episodes: 55000, mean episode reward: 4.944127175011627, agent episode reward: [7.79, 7.79, -1.1708799528668106, -9.464992872121561], time: 65.166
steps: 1374975, episodes: 55000, mean episode variance: 2.261062494121492, agent episode variance: [0.7191430909484625, 0.6822104132473469, 0.2145184177979827, 0.6451905721276998], time: 65.167
Running avgs for agent 0: q_loss: 2.8752803802490234, p_loss: -4.550548076629639, mean_rew: 0.192578125, variance: 2.87657236379385, mean_q: 4.452550411224365, std_q: 2.6801953315734863
Running avgs for agent 1: q_loss: 2.7598187923431396, p_loss: -4.473904132843018, mean_rew: 0.1873046875, variance: 2.7288416529893875, mean_q: 4.429717540740967, std_q: 2.7779452800750732
Running avgs for agent 2: q_loss: 0.8819886445999146, p_loss: 0.39145323634147644, mean_rew: -0.09551464836693277, variance: 0.8580736711919308, mean_q: -0.4342443346977234, std_q: 1.561345100402832
Running avgs for agent 3: q_loss: 2.5910282135009766, p_loss: 3.258026123046875, mean_rew: -0.1890137155384284, variance: 2.5807622885107993, mean_q: -3.3798511028289795, std_q: 3.4770090579986572

steps: 1399975, episodes: 56000, mean episode reward: 4.798115436984573, agent episode reward: [7.62, 7.62, -1.3387303601695393, -9.103154202845888], time: 65.395
steps: 1399975, episodes: 56000, mean episode variance: 2.2541582196578385, agent episode variance: [0.7266901741921902, 0.6910547138080001, 0.21004161708056926, 0.6263717145770789], time: 65.395
Running avgs for agent 0: q_loss: 2.9648091793060303, p_loss: -4.728728294372559, mean_rew: 0.1893359375, variance: 2.9067606967687607, mean_q: 4.628616809844971, std_q: 2.759050130844116
Running avgs for agent 1: q_loss: 2.8023037910461426, p_loss: -4.708002090454102, mean_rew: 0.1851171875, variance: 2.7642188552320004, mean_q: 4.657232284545898, std_q: 2.885186195373535
Running avgs for agent 2: q_loss: 0.8556153774261475, p_loss: 0.3634495735168457, mean_rew: -0.09201974787939639, variance: 0.840166468322277, mean_q: -0.404701292514801, std_q: 1.494869351387024
Running avgs for agent 3: q_loss: 2.5437474250793457, p_loss: 3.444962501525879, mean_rew: -0.18861166857301287, variance: 2.5054868583083154, mean_q: -3.573093891143799, std_q: 3.601560115814209

steps: 1424975, episodes: 57000, mean episode reward: 5.401623427349732, agent episode reward: [7.94, 7.94, -1.3900481413539256, -9.088328431296343], time: 65.362
steps: 1424975, episodes: 57000, mean episode variance: 2.3604628312066196, agent episode variance: [0.7323509388566017, 0.7226077573299408, 0.20225222376734017, 0.703251911252737], time: 65.363
Running avgs for agent 0: q_loss: 2.9666266441345215, p_loss: -4.931769847869873, mean_rew: 0.1915234375, variance: 2.929403755426407, mean_q: 4.821352005004883, std_q: 2.824099540710449
Running avgs for agent 1: q_loss: 2.919175386428833, p_loss: -4.974625587463379, mean_rew: 0.194609375, variance: 2.890431029319763, mean_q: 4.9203619956970215, std_q: 2.9854726791381836
Running avgs for agent 2: q_loss: 0.826776385307312, p_loss: 0.3279498219490051, mean_rew: -0.0888544670005123, variance: 0.8090088950693607, mean_q: -0.36859330534935, std_q: 1.4387909173965454
Running avgs for agent 3: q_loss: 2.8376073837280273, p_loss: 3.6698715686798096, mean_rew: -0.19854386434466226, variance: 2.813007645010948, mean_q: -3.795225143432617, std_q: 3.7635750770568848

steps: 1449975, episodes: 58000, mean episode reward: 4.365121526145007, agent episode reward: [7.15, 7.15, -1.2681023083302734, -8.666776165524722], time: 65.635
steps: 1449975, episodes: 58000, mean episode variance: 2.3820426337383687, agent episode variance: [0.7333954188227654, 0.7224185380935669, 0.20198941388353706, 0.7242392629384995], time: 65.636
Running avgs for agent 0: q_loss: 2.909644842147827, p_loss: -5.153956890106201, mean_rew: 0.1963671875, variance: 2.9335816752910615, mean_q: 5.038612365722656, std_q: 2.8762857913970947
Running avgs for agent 1: q_loss: 2.942599058151245, p_loss: -5.18214750289917, mean_rew: 0.19390625, variance: 2.8896741523742677, mean_q: 5.123602390289307, std_q: 2.9635088443756104
Running avgs for agent 2: q_loss: 0.822750985622406, p_loss: 0.2960703670978546, mean_rew: -0.0875782288144078, variance: 0.8079576555341482, mean_q: -0.33470410108566284, std_q: 1.4171245098114014
Running avgs for agent 3: q_loss: 2.9196267127990723, p_loss: 3.8678128719329834, mean_rew: -0.1991609987999305, variance: 2.896957051753998, mean_q: -3.9956204891204834, std_q: 3.7838432788848877

steps: 1474975, episodes: 59000, mean episode reward: 4.325762522443004, agent episode reward: [7.66, 7.66, -1.1351766245954868, -9.859060852961509], time: 65.579
steps: 1474975, episodes: 59000, mean episode variance: 2.365214164979756, agent episode variance: [0.7245625668913126, 0.7175325689986348, 0.19163846446573735, 0.7314805646240711], time: 65.58
Running avgs for agent 0: q_loss: 2.9195897579193115, p_loss: -5.346845626831055, mean_rew: 0.1971484375, variance: 2.8982502675652504, mean_q: 5.234350204467773, std_q: 2.870816230773926
Running avgs for agent 1: q_loss: 2.8673322200775146, p_loss: -5.368628025054932, mean_rew: 0.19671875, variance: 2.8701302759945393, mean_q: 5.306148052215576, std_q: 2.9182567596435547
Running avgs for agent 2: q_loss: 0.7805179953575134, p_loss: 0.24647139012813568, mean_rew: -0.08541214180749007, variance: 0.7665538578629494, mean_q: -0.2854158282279968, std_q: 1.3704913854599
Running avgs for agent 3: q_loss: 2.929356336593628, p_loss: 4.081651210784912, mean_rew: -0.20318733718785822, variance: 2.9259222584962843, mean_q: -4.208827972412109, std_q: 3.83819580078125

steps: 1499975, episodes: 60000, mean episode reward: 4.818850054933742, agent episode reward: [7.4, 7.4, -0.9748372653662352, -9.006312679700022], time: 65.282
steps: 1499975, episodes: 60000, mean episode variance: 2.3946680039204655, agent episode variance: [0.7652350603640079, 0.7310067114681006, 0.18747318487241865, 0.7109530472159385], time: 65.282
Running avgs for agent 0: q_loss: 3.05197811126709, p_loss: -5.557345867156982, mean_rew: 0.2019921875, variance: 3.0609402414560316, mean_q: 5.445770740509033, std_q: 2.909480094909668
Running avgs for agent 1: q_loss: 2.908320426940918, p_loss: -5.56740140914917, mean_rew: 0.200859375, variance: 2.9240268458724024, mean_q: 5.50964879989624, std_q: 2.88751220703125
Running avgs for agent 2: q_loss: 0.7404124736785889, p_loss: 0.19409610331058502, mean_rew: -0.08223030846653037, variance: 0.7498927394896746, mean_q: -0.23343431949615479, std_q: 1.3221955299377441
Running avgs for agent 3: q_loss: 2.9052939414978027, p_loss: 4.26021671295166, mean_rew: -0.2146005712054181, variance: 2.843812188863754, mean_q: -4.386798858642578, std_q: 3.841167449951172

steps: 1524975, episodes: 61000, mean episode reward: 4.722172846071218, agent episode reward: [7.67, 7.67, -1.0180179370238907, -9.599809216904891], time: 66.141
steps: 1524975, episodes: 61000, mean episode variance: 2.4355214859321714, agent episode variance: [0.7587962456941605, 0.7467627316564321, 0.18252726104110478, 0.747435247540474], time: 66.142
Running avgs for agent 0: q_loss: 3.0113308429718018, p_loss: -5.7519001960754395, mean_rew: 0.205390625, variance: 3.035184982776642, mean_q: 5.644498825073242, std_q: 2.923445701599121
Running avgs for agent 1: q_loss: 2.9590940475463867, p_loss: -5.751408100128174, mean_rew: 0.2051953125, variance: 2.9870509266257286, mean_q: 5.69767427444458, std_q: 2.8551690578460693
Running avgs for agent 2: q_loss: 0.7500722408294678, p_loss: 0.15296389162540436, mean_rew: -0.08311169670851967, variance: 0.7301090441644191, mean_q: -0.19336703419685364, std_q: 1.2726197242736816
Running avgs for agent 3: q_loss: 3.041741132736206, p_loss: 4.434165000915527, mean_rew: -0.2135168185822494, variance: 2.989740990161896, mean_q: -4.561994552612305, std_q: 3.900925636291504

steps: 1549975, episodes: 62000, mean episode reward: 3.3960147098082087, agent episode reward: [6.49, 6.49, -1.1310887554307387, -8.452896534761052], time: 65.394
steps: 1549975, episodes: 62000, mean episode variance: 2.4618308953046797, agent episode variance: [0.7718762947916985, 0.7473152319788933, 0.1776639574766159, 0.7649754110574722], time: 65.395
Running avgs for agent 0: q_loss: 3.066357374191284, p_loss: -5.925528526306152, mean_rew: 0.2085546875, variance: 3.087505179166794, mean_q: 5.818862438201904, std_q: 2.9265425205230713
Running avgs for agent 1: q_loss: 2.9985756874084473, p_loss: -5.927984237670898, mean_rew: 0.207109375, variance: 2.989260927915573, mean_q: 5.875030994415283, std_q: 2.8787152767181396
Running avgs for agent 2: q_loss: 0.7128381133079529, p_loss: 0.11344298720359802, mean_rew: -0.08177680044253029, variance: 0.7106558299064636, mean_q: -0.15493932366371155, std_q: 1.266133189201355
Running avgs for agent 3: q_loss: 3.110748529434204, p_loss: 4.635439395904541, mean_rew: -0.22361723970394054, variance: 3.059901644229889, mean_q: -4.758594036102295, std_q: 3.954505681991577

steps: 1574975, episodes: 63000, mean episode reward: 4.196360986289454, agent episode reward: [6.86, 6.86, -1.1599545655638543, -8.36368444814669], time: 65.714
steps: 1574975, episodes: 63000, mean episode variance: 2.519635414112359, agent episode variance: [0.8046101580262184, 0.766267563790083, 0.17408296217396854, 0.7746747301220894], time: 65.714
Running avgs for agent 0: q_loss: 3.1344289779663086, p_loss: -6.107150554656982, mean_rew: 0.2137109375, variance: 3.2184406321048735, mean_q: 6.003103733062744, std_q: 2.955024242401123
Running avgs for agent 1: q_loss: 3.1050639152526855, p_loss: -6.068507671356201, mean_rew: 0.212265625, variance: 3.065070255160332, mean_q: 6.017213821411133, std_q: 2.942453384399414
Running avgs for agent 2: q_loss: 0.6992591619491577, p_loss: 0.07118213921785355, mean_rew: -0.08062854403777482, variance: 0.6963318486958742, mean_q: -0.11328992247581482, std_q: 1.29391348361969
Running avgs for agent 3: q_loss: 3.023326873779297, p_loss: 4.802110195159912, mean_rew: -0.2259481698140754, variance: 3.0986989204883577, mean_q: -4.923017501831055, std_q: 3.9608616828918457

steps: 1599975, episodes: 64000, mean episode reward: 3.716507103431406, agent episode reward: [6.73, 6.73, -1.2394176785743525, -8.504075217994243], time: 66.098
steps: 1599975, episodes: 64000, mean episode variance: 2.5322649300172926, agent episode variance: [0.8197414748668671, 0.7882287364006042, 0.1600307797268033, 0.7642639390230179], time: 66.098
Running avgs for agent 0: q_loss: 3.2265868186950684, p_loss: -6.272787094116211, mean_rew: 0.213046875, variance: 3.2789658994674684, mean_q: 6.171350479125977, std_q: 3.0103988647460938
Running avgs for agent 1: q_loss: 3.1800005435943604, p_loss: -6.24350643157959, mean_rew: 0.2144921875, variance: 3.152914945602417, mean_q: 6.191850662231445, std_q: 3.0277490615844727
Running avgs for agent 2: q_loss: 0.6613851189613342, p_loss: 0.022975048050284386, mean_rew: -0.07557078306069936, variance: 0.6401231189072132, mean_q: -0.06768417358398438, std_q: 1.2713067531585693
Running avgs for agent 3: q_loss: 3.092494249343872, p_loss: 4.978379726409912, mean_rew: -0.23674141507124863, variance: 3.0570557560920717, mean_q: -5.0986199378967285, std_q: 4.064518451690674

steps: 1624975, episodes: 65000, mean episode reward: 3.731492337378215, agent episode reward: [6.49, 6.49, -1.144136463425884, -8.104371199195901], time: 67.168
steps: 1624975, episodes: 65000, mean episode variance: 2.570193141270429, agent episode variance: [0.827886224552989, 0.8010263472795487, 0.1502422343529761, 0.7910383350849152], time: 67.169
Running avgs for agent 0: q_loss: 3.2283222675323486, p_loss: -6.39169979095459, mean_rew: 0.2166015625, variance: 3.311544898211956, mean_q: 6.294264316558838, std_q: 3.025378942489624
Running avgs for agent 1: q_loss: 3.2317793369293213, p_loss: -6.386568546295166, mean_rew: 0.2149609375, variance: 3.2041053891181948, mean_q: 6.339515686035156, std_q: 3.0792734622955322
Running avgs for agent 2: q_loss: 0.609574556350708, p_loss: -0.017011379823088646, mean_rew: -0.07597980070943526, variance: 0.6009689374119044, mean_q: -0.02815130166709423, std_q: 1.228520154953003
Running avgs for agent 3: q_loss: 3.214815855026245, p_loss: 5.131168842315674, mean_rew: -0.23636631980041692, variance: 3.1641533403396607, mean_q: -5.256948471069336, std_q: 4.121199131011963

steps: 1649975, episodes: 66000, mean episode reward: 3.350635298315951, agent episode reward: [6.11, 6.11, -1.5813573338788989, -7.28800736780515], time: 65.539
steps: 1649975, episodes: 66000, mean episode variance: 2.657294537447393, agent episode variance: [0.8621495357006789, 0.8399097011983394, 0.16465803084522485, 0.7905772697031498], time: 65.539
Running avgs for agent 0: q_loss: 3.347777843475342, p_loss: -6.487612724304199, mean_rew: 0.220859375, variance: 3.4485981428027155, mean_q: 6.394980430603027, std_q: 3.0742685794830322
Running avgs for agent 1: q_loss: 3.3067708015441895, p_loss: -6.526660919189453, mean_rew: 0.2253515625, variance: 3.3596388047933576, mean_q: 6.482899188995361, std_q: 3.171173334121704
Running avgs for agent 2: q_loss: 0.6567813754081726, p_loss: -0.051278311759233475, mean_rew: -0.0764245524671504, variance: 0.6586321233808994, mean_q: 0.007942100055515766, std_q: 1.2472819089889526
Running avgs for agent 3: q_loss: 3.19950008392334, p_loss: 5.225676536560059, mean_rew: -0.2425302021093327, variance: 3.162309078812599, mean_q: -5.351409435272217, std_q: 4.1319260597229

steps: 1674975, episodes: 67000, mean episode reward: 2.8075199322363114, agent episode reward: [5.58, 5.58, -0.9925384457006646, -7.359941622063024], time: 65.967
steps: 1674975, episodes: 67000, mean episode variance: 2.6576700526326893, agent episode variance: [0.832315730780363, 0.8285493946671486, 0.1558813100606203, 0.8409236171245575], time: 65.968
Running avgs for agent 0: q_loss: 3.3634305000305176, p_loss: -6.5768866539001465, mean_rew: 0.2208203125, variance: 3.329262923121452, mean_q: 6.491904258728027, std_q: 3.1111199855804443
Running avgs for agent 1: q_loss: 3.3226847648620605, p_loss: -6.608476638793945, mean_rew: 0.222890625, variance: 3.3141975786685944, mean_q: 6.567713260650635, std_q: 3.242147445678711
Running avgs for agent 2: q_loss: 0.6158924698829651, p_loss: -0.0872356966137886, mean_rew: -0.07401427089040084, variance: 0.6235252402424812, mean_q: 0.04415121674537659, std_q: 1.205633282661438
Running avgs for agent 3: q_loss: 3.2445967197418213, p_loss: 5.309781074523926, mean_rew: -0.24516928457627615, variance: 3.36369446849823, mean_q: -5.437547206878662, std_q: 4.108478546142578

steps: 1699975, episodes: 68000, mean episode reward: 3.875733170412672, agent episode reward: [6.66, 6.66, -1.33231941144834, -8.111947418138987], time: 65.939
steps: 1699975, episodes: 68000, mean episode variance: 2.7139813893269746, agent episode variance: [0.8605611396729946, 0.8811763307750226, 0.1523571053277701, 0.8198868135511875], time: 65.94
Running avgs for agent 0: q_loss: 3.4490458965301514, p_loss: -6.651369094848633, mean_rew: 0.224609375, variance: 3.4422445586919785, mean_q: 6.5701375007629395, std_q: 3.176572561264038
Running avgs for agent 1: q_loss: 3.5003609657287598, p_loss: -6.692111492156982, mean_rew: 0.2244140625, variance: 3.52470532310009, mean_q: 6.651571750640869, std_q: 3.3135623931884766
Running avgs for agent 2: q_loss: 0.6134233474731445, p_loss: -0.12269120663404465, mean_rew: -0.07295083248195558, variance: 0.6094284213110804, mean_q: 0.07866764813661575, std_q: 1.2136497497558594
Running avgs for agent 3: q_loss: 3.2363760471343994, p_loss: 5.37128210067749, mean_rew: -0.24767565361873428, variance: 3.27954725420475, mean_q: -5.497616291046143, std_q: 4.125346660614014

steps: 1724975, episodes: 69000, mean episode reward: 3.8105089397684435, agent episode reward: [6.71, 6.71, -1.3478324249440767, -8.26165863528748], time: 66.052
steps: 1724975, episodes: 69000, mean episode variance: 2.7642253045402465, agent episode variance: [0.886108515560627, 0.9063584288060665, 0.14793164179846643, 0.8238267183750868], time: 66.053
Running avgs for agent 0: q_loss: 3.594743013381958, p_loss: -6.707804203033447, mean_rew: 0.227578125, variance: 3.544434062242508, mean_q: 6.6300153732299805, std_q: 3.2216973304748535
Running avgs for agent 1: q_loss: 3.5509486198425293, p_loss: -6.794290065765381, mean_rew: 0.2259375, variance: 3.625433715224266, mean_q: 6.75365686416626, std_q: 3.4140520095825195
Running avgs for agent 2: q_loss: 0.5896539092063904, p_loss: -0.15413065254688263, mean_rew: -0.0705901997632233, variance: 0.5917265671938657, mean_q: 0.11046905815601349, std_q: 1.2232719659805298
Running avgs for agent 3: q_loss: 3.2876205444335938, p_loss: 5.422763347625732, mean_rew: -0.2532043917846637, variance: 3.2953068735003472, mean_q: -5.544139862060547, std_q: 4.115126132965088

steps: 1749975, episodes: 70000, mean episode reward: 3.135866637120285, agent episode reward: [5.93, 5.93, -1.2295462757750093, -7.494587087104707], time: 65.873
steps: 1749975, episodes: 70000, mean episode variance: 2.8070792878828943, agent episode variance: [0.8599431672692299, 0.9539398406744003, 0.14413075115904211, 0.8490655287802219], time: 65.874
Running avgs for agent 0: q_loss: 3.461289167404175, p_loss: -6.77469539642334, mean_rew: 0.2315234375, variance: 3.4397726690769197, mean_q: 6.696399211883545, std_q: 3.257122755050659
Running avgs for agent 1: q_loss: 3.745455503463745, p_loss: -6.894339084625244, mean_rew: 0.234453125, variance: 3.815759362697601, mean_q: 6.8552165031433105, std_q: 3.5099472999572754
Running avgs for agent 2: q_loss: 0.5814341306686401, p_loss: -0.20651192963123322, mean_rew: -0.06937539040012236, variance: 0.5765230046361685, mean_q: 0.15700840950012207, std_q: 1.2009270191192627
Running avgs for agent 3: q_loss: 3.3539633750915527, p_loss: 5.480278015136719, mean_rew: -0.2610512436501516, variance: 3.3962621151208876, mean_q: -5.605776309967041, std_q: 4.229465961456299

steps: 1774975, episodes: 71000, mean episode reward: 3.078156558322534, agent episode reward: [5.9, 5.9, -1.2938331627237598, -7.428010278953707], time: 65.753
steps: 1774975, episodes: 71000, mean episode variance: 2.9073277829140425, agent episode variance: [0.911484109133482, 0.9614148179888725, 0.149493857935071, 0.884934997856617], time: 65.754
Running avgs for agent 0: q_loss: 3.644566297531128, p_loss: -6.861570358276367, mean_rew: 0.2430078125, variance: 3.645936436533928, mean_q: 6.786581993103027, std_q: 3.3107621669769287
Running avgs for agent 1: q_loss: 3.779576063156128, p_loss: -6.93132209777832, mean_rew: 0.23515625, variance: 3.84565927195549, mean_q: 6.896316051483154, std_q: 3.4556777477264404
Running avgs for agent 2: q_loss: 0.5883310437202454, p_loss: -0.2505919933319092, mean_rew: -0.06997761993461088, variance: 0.597975431740284, mean_q: 0.20035366714000702, std_q: 1.2152385711669922
Running avgs for agent 3: q_loss: 3.5105512142181396, p_loss: 5.483018398284912, mean_rew: -0.26982007605917946, variance: 3.539739991426468, mean_q: -5.611995697021484, std_q: 4.246541500091553

steps: 1799975, episodes: 72000, mean episode reward: 2.12962746295123, agent episode reward: [5.4, 5.4, -1.0143385418573583, -7.656033995191412], time: 66.051
steps: 1799975, episodes: 72000, mean episode variance: 2.805097984984517, agent episode variance: [0.8910945244431495, 0.9299353489279747, 0.14507229961454868, 0.8389958119988441], time: 66.052
Running avgs for agent 0: q_loss: 3.6376545429229736, p_loss: -6.8805437088012695, mean_rew: 0.2360546875, variance: 3.564378097772598, mean_q: 6.809232711791992, std_q: 3.2906551361083984
Running avgs for agent 1: q_loss: 3.6705634593963623, p_loss: -6.909470558166504, mean_rew: 0.23046875, variance: 3.7197413957118988, mean_q: 6.878589153289795, std_q: 3.3787758350372314
Running avgs for agent 2: q_loss: 0.5845009088516235, p_loss: -0.28543227910995483, mean_rew: -0.06866055684931818, variance: 0.5802891984581947, mean_q: 0.236872136592865, std_q: 1.2295234203338623
Running avgs for agent 3: q_loss: 3.3718044757843018, p_loss: 5.45088005065918, mean_rew: -0.26946751979479483, variance: 3.3559832479953764, mean_q: -5.575895309448242, std_q: 4.185197830200195

steps: 1824975, episodes: 73000, mean episode reward: 0.6677417277786305, agent episode reward: [4.58, 4.58, -1.3258488124732626, -7.166409459748107], time: 66.167
steps: 1824975, episodes: 73000, mean episode variance: 2.9138794064708056, agent episode variance: [0.9269160932302475, 0.9520122135579586, 0.14379595455899835, 0.891155145123601], time: 66.167
Running avgs for agent 0: q_loss: 3.702373504638672, p_loss: -6.923504829406738, mean_rew: 0.239453125, variance: 3.70766437292099, mean_q: 6.853036880493164, std_q: 3.2665278911590576
Running avgs for agent 1: q_loss: 3.8081624507904053, p_loss: -6.843473434448242, mean_rew: 0.239765625, variance: 3.8080488542318345, mean_q: 6.814621925354004, std_q: 3.389558792114258
Running avgs for agent 2: q_loss: 0.5744984149932861, p_loss: -0.3127591609954834, mean_rew: -0.06483357269644995, variance: 0.5751838182359934, mean_q: 0.2637312114238739, std_q: 1.206576943397522
Running avgs for agent 3: q_loss: 3.5355331897735596, p_loss: 5.430055618286133, mean_rew: -0.27386747519453913, variance: 3.564620580494404, mean_q: -5.559494972229004, std_q: 4.250942230224609

steps: 1849975, episodes: 74000, mean episode reward: 2.6461191758347504, agent episode reward: [5.58, 5.58, -1.0000251622512055, -7.513855661914044], time: 65.764
steps: 1849975, episodes: 74000, mean episode variance: 2.8559900514231993, agent episode variance: [0.9402420066446066, 0.9292010715007782, 0.12992561717797071, 0.856621356099844], time: 65.764
Running avgs for agent 0: q_loss: 3.674931049346924, p_loss: -6.972653388977051, mean_rew: 0.2374609375, variance: 3.760968026578426, mean_q: 6.907503128051758, std_q: 3.260814666748047
Running avgs for agent 1: q_loss: 3.7843470573425293, p_loss: -6.729172706604004, mean_rew: 0.236953125, variance: 3.716804286003113, mean_q: 6.704563617706299, std_q: 3.387042999267578
Running avgs for agent 2: q_loss: 0.5146650671958923, p_loss: -0.34161826968193054, mean_rew: -0.06322627789217482, variance: 0.5197024687118829, mean_q: 0.2878422439098358, std_q: 1.2014273405075073
Running avgs for agent 3: q_loss: 3.547356367111206, p_loss: 5.4442901611328125, mean_rew: -0.2755957691285708, variance: 3.426485424399376, mean_q: -5.576906204223633, std_q: 4.370394706726074

steps: 1874975, episodes: 75000, mean episode reward: 2.848173122012854, agent episode reward: [5.77, 5.77, -0.9828165426589826, -7.709010335328163], time: 66.024
steps: 1874975, episodes: 75000, mean episode variance: 2.990517345003784, agent episode variance: [0.9393879867494106, 0.9755875996649265, 0.13782977186888457, 0.937711986720562], time: 66.025
Running avgs for agent 0: q_loss: 3.817506790161133, p_loss: -7.0237812995910645, mean_rew: 0.24453125, variance: 3.7575519469976424, mean_q: 6.961753845214844, std_q: 3.2798173427581787
Running avgs for agent 1: q_loss: 3.8278489112854004, p_loss: -6.65590763092041, mean_rew: 0.2387109375, variance: 3.902350398659706, mean_q: 6.627912521362305, std_q: 3.394108533859253
Running avgs for agent 2: q_loss: 0.5473977327346802, p_loss: -0.35408416390419006, mean_rew: -0.06448162126637144, variance: 0.5513190874755383, mean_q: 0.304672509431839, std_q: 1.1963571310043335
Running avgs for agent 3: q_loss: 3.72082781791687, p_loss: 5.43449592590332, mean_rew: -0.27878700787077415, variance: 3.750847946882248, mean_q: -5.5626678466796875, std_q: 4.432888507843018

steps: 1899975, episodes: 76000, mean episode reward: 3.0158476415030826, agent episode reward: [5.59, 5.59, -1.119249968303965, -7.044902390192952], time: 66.021
steps: 1899975, episodes: 76000, mean episode variance: 3.016771318530664, agent episode variance: [0.9394297432601452, 1.0207343847602606, 0.12294716375134886, 0.9336600267589092], time: 66.022
Running avgs for agent 0: q_loss: 3.776373863220215, p_loss: -7.07859992980957, mean_rew: 0.245390625, variance: 3.7577189730405807, mean_q: 7.019780158996582, std_q: 3.258601188659668
Running avgs for agent 1: q_loss: 4.126934051513672, p_loss: -6.687997817993164, mean_rew: 0.2448046875, variance: 4.082937539041042, mean_q: 6.656721115112305, std_q: 3.5127274990081787
Running avgs for agent 2: q_loss: 0.5007264018058777, p_loss: -0.36838629841804504, mean_rew: -0.06280355797130727, variance: 0.4917886550053954, mean_q: 0.31652331352233887, std_q: 1.2137172222137451
Running avgs for agent 3: q_loss: 3.7770135402679443, p_loss: 5.4847588539123535, mean_rew: -0.28427264363058347, variance: 3.734640107035637, mean_q: -5.625837326049805, std_q: 4.528357982635498

steps: 1924975, episodes: 77000, mean episode reward: 3.7774812544100658, agent episode reward: [6.34, 6.34, -1.6216205747817158, -7.280898170808217], time: 66.09
steps: 1924975, episodes: 77000, mean episode variance: 3.1601186284497382, agent episode variance: [0.9740604701340199, 1.0692627483606338, 0.14415311045199633, 0.972642299503088], time: 66.09
Running avgs for agent 0: q_loss: 3.7955048084259033, p_loss: -7.154556751251221, mean_rew: 0.2463671875, variance: 3.8962418805360794, mean_q: 7.095662593841553, std_q: 3.2875967025756836
Running avgs for agent 1: q_loss: 4.237988471984863, p_loss: -6.726536273956299, mean_rew: 0.2490625, variance: 4.277050993442535, mean_q: 6.694817543029785, std_q: 3.5499534606933594
Running avgs for agent 2: q_loss: 0.5593708157539368, p_loss: -0.38651880621910095, mean_rew: -0.06231991039195997, variance: 0.5766124418079853, mean_q: 0.33733251690864563, std_q: 1.2564735412597656
Running avgs for agent 3: q_loss: 3.8193137645721436, p_loss: 5.565464019775391, mean_rew: -0.2852203113173057, variance: 3.890569198012352, mean_q: -5.7090935707092285, std_q: 4.547021389007568

steps: 1949975, episodes: 78000, mean episode reward: 4.083188325683872, agent episode reward: [6.81, 6.81, -1.5183901997526028, -8.018421474563526], time: 65.888
steps: 1949975, episodes: 78000, mean episode variance: 3.1917576779536905, agent episode variance: [1.0092544844895601, 1.0531135175824164, 0.13281707834824918, 0.9965725975334644], time: 65.888
Running avgs for agent 0: q_loss: 3.9700145721435547, p_loss: -7.236108779907227, mean_rew: 0.2523046875, variance: 4.0370179379582405, mean_q: 7.1757330894470215, std_q: 3.382833242416382
Running avgs for agent 1: q_loss: 4.2928290367126465, p_loss: -6.76327657699585, mean_rew: 0.249296875, variance: 4.212454070329666, mean_q: 6.734341144561768, std_q: 3.6469974517822266
Running avgs for agent 2: q_loss: 0.5320308804512024, p_loss: -0.40816444158554077, mean_rew: -0.060086519286092044, variance: 0.5312683133929967, mean_q: 0.36219558119773865, std_q: 1.1968942880630493
Running avgs for agent 3: q_loss: 4.086952209472656, p_loss: 5.684535980224609, mean_rew: -0.293127810408095, variance: 3.9862903901338576, mean_q: -5.82227897644043, std_q: 4.535799026489258

steps: 1974975, episodes: 79000, mean episode reward: 2.8513171573389746, agent episode reward: [5.61, 5.61, -1.7839473333899236, -6.584735509271101], time: 65.983
steps: 1974975, episodes: 79000, mean episode variance: 3.243805097993463, agent episode variance: [0.9459227037429809, 1.1035274664759636, 0.1258996631167829, 1.0684552646577359], time: 65.983
Running avgs for agent 0: q_loss: 3.8250997066497803, p_loss: -7.275288105010986, mean_rew: 0.2416796875, variance: 3.7836908149719237, mean_q: 7.203273296356201, std_q: 3.4112777709960938
Running avgs for agent 1: q_loss: 4.426765441894531, p_loss: -6.794856071472168, mean_rew: 0.251875, variance: 4.414109865903854, mean_q: 6.76278829574585, std_q: 3.6440160274505615
Running avgs for agent 2: q_loss: 0.5091300010681152, p_loss: -0.41945844888687134, mean_rew: -0.06055216553791416, variance: 0.5035986524671316, mean_q: 0.37356817722320557, std_q: 1.1781450510025024
Running avgs for agent 3: q_loss: 4.209610462188721, p_loss: 5.818018436431885, mean_rew: -0.2996324872477567, variance: 4.2738210586309435, mean_q: -5.962599754333496, std_q: 4.62744140625

steps: 1999975, episodes: 80000, mean episode reward: 2.3902899143993572, agent episode reward: [5.12, 5.12, -1.3736591464527892, -6.476050939147854], time: 66.244
steps: 1999975, episodes: 80000, mean episode variance: 3.308590566696599, agent episode variance: [1.041916008591652, 1.11599052298069, 0.11781169301457703, 1.0328723421096802], time: 66.244
Running avgs for agent 0: q_loss: 4.007108688354492, p_loss: -7.30621862411499, mean_rew: 0.248046875, variance: 4.167664034366608, mean_q: 7.23988151550293, std_q: 3.4382596015930176
Running avgs for agent 1: q_loss: 4.443031311035156, p_loss: -6.793811321258545, mean_rew: 0.2532421875, variance: 4.46396209192276, mean_q: 6.7580366134643555, std_q: 3.622157335281372
Running avgs for agent 2: q_loss: 0.46333321928977966, p_loss: -0.4387072026729584, mean_rew: -0.057888463090544004, variance: 0.4712467720583081, mean_q: 0.39246416091918945, std_q: 1.110079288482666
Running avgs for agent 3: q_loss: 4.124553680419922, p_loss: 5.930846691131592, mean_rew: -0.29826224560839953, variance: 4.131489368438721, mean_q: -6.078835964202881, std_q: 4.657423973083496

...Finished total of 80001 episodes... Now freezing policy and running for 10000 more episodes to get cvar estimation
steps: 24975, episodes: 1000, mean episode reward: 2.2419454210233196, agent episode reward: [5.27, 5.27, -1.6417899515079497, -6.65626462746873], time: 46.156
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 46.156
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: 1.5806147016510204, agent episode reward: [4.64, 4.64, -1.476783079780137, -6.222602218568843], time: 58.677
steps: 49975, episodes: 2000, mean episode variance: 4.174867133529856, agent episode variance: [0.8584802202880383, 1.2983710540533067, 0.16736921674199404, 1.850646642446518], time: 58.677
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.20239658043032788, variance: 3.5183615585575336, cvar: 27.646638870239258, v: 5.997931957244873, mean_q: 7.247665882110596, std_q: 3.2217016220092773
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.20904040727459017, variance: 5.321193218231201, cvar: 24.060848236083984, v: 5.996983051300049, mean_q: 6.555360317230225, std_q: 3.465479850769043
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06561156941266728, variance: 0.6859394128770246, cvar: 1.3267571926116943, v: 1.1274255514144897, mean_q: 0.4570111930370331, std_q: 0.976398766040802
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.26453193184046137, variance: 7.584617387075893, cvar: -1.3537663221359253, v: -1.3756864070892334, mean_q: -5.750561237335205, std_q: 4.413787841796875

steps: 74975, episodes: 3000, mean episode reward: 1.1434721718801324, agent episode reward: [4.18, 4.18, -1.3513479730080444, -5.865179855111823], time: 57.573
steps: 74975, episodes: 3000, mean episode variance: 3.847422450002283, agent episode variance: [0.7124253911226988, 1.221439098060131, 0.1584831852428615, 1.7550747755765914], time: 57.573
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.18625, variance: 2.849701564490795, cvar: 14.510041236877441, v: 10.77004337310791, mean_q: 7.238790035247803, std_q: 3.2046098709106445
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1935546875, variance: 4.885756392240524, cvar: 14.468295097351074, v: 9.994726181030273, mean_q: 6.538932800292969, std_q: 3.411287546157837
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.062362352942351464, variance: 0.633932740971446, cvar: 1.315292477607727, v: 1.1167607307434082, mean_q: 0.44940710067749023, std_q: 0.9734298586845398
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25415834599275916, variance: 7.020299434661865, cvar: -1.587099552154541, v: -1.5960032939910889, mean_q: -5.718559265136719, std_q: 4.369232654571533

steps: 99975, episodes: 4000, mean episode reward: 1.9745414996328736, agent episode reward: [4.98, 4.98, -1.4561610290330513, -6.5292974713340755], time: 59.864
steps: 99975, episodes: 4000, mean episode variance: 3.8546184287853538, agent episode variance: [0.7662068648338318, 1.1367090270221234, 0.14881368945911527, 1.8028888474702836], time: 59.865
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1923046875, variance: 3.0648274593353273, cvar: 14.412296295166016, v: 11.19631576538086, mean_q: 7.23885440826416, std_q: 3.206857919692993
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.190078125, variance: 4.546836108088494, cvar: 14.4453125, v: 10.158475875854492, mean_q: 6.533322334289551, std_q: 3.4234418869018555
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05917288602173461, variance: 0.5952547578364611, cvar: 1.3147237300872803, v: 1.1126291751861572, mean_q: 0.450643926858902, std_q: 0.9633302688598633
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2523833613148713, variance: 7.211555004119873, cvar: -1.5878151655197144, v: -1.59404718875885, mean_q: -5.753032684326172, std_q: 4.424156188964844

steps: 124975, episodes: 5000, mean episode reward: 1.8480711773744094, agent episode reward: [5.06, 5.06, -1.3154100360647203, -6.956518786560871], time: 59.181
steps: 124975, episodes: 5000, mean episode variance: 3.9577261734902858, agent episode variance: [0.7588530974984169, 1.1968732697963715, 0.14352298697829247, 1.858476819217205], time: 59.181
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1953515625, variance: 3.0354123899936676, cvar: 14.47850513458252, v: 11.244770050048828, mean_q: 7.263611316680908, std_q: 3.2251803874969482
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.193203125, variance: 4.787493079185486, cvar: 14.532827377319336, v: 10.23796272277832, mean_q: 6.565710544586182, std_q: 3.4471371173858643
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.056574286936005067, variance: 0.5740919479131699, cvar: 1.319344401359558, v: 1.11599600315094, mean_q: 0.46104341745376587, std_q: 0.9287951588630676
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2600408103655626, variance: 7.4339070320129395, cvar: -1.5875903367996216, v: -1.59394371509552, mean_q: -5.776535987854004, std_q: 4.466684341430664

steps: 149975, episodes: 6000, mean episode reward: 1.6205992473648758, agent episode reward: [4.56, 4.56, -1.553007612326894, -5.9463931403082295], time: 58.873
steps: 149975, episodes: 6000, mean episode variance: 3.9319472249206155, agent episode variance: [0.7343346680402756, 1.2335680954754353, 0.13802103975974023, 1.8260234216451645], time: 58.873
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1944140625, variance: 2.9373386721611023, cvar: 14.5097074508667, v: 11.253133773803711, mean_q: 7.263286113739014, std_q: 3.2400801181793213
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.195234375, variance: 4.934272381901741, cvar: 14.53492546081543, v: 10.23425579071045, mean_q: 6.556283473968506, std_q: 3.4458723068237305
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05686783548466253, variance: 0.5520841590389609, cvar: 1.314963936805725, v: 1.1119773387908936, mean_q: 0.4585350453853607, std_q: 0.9281734824180603
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25628636514671327, variance: 7.304093837738037, cvar: -1.5886106491088867, v: -1.5936003923416138, mean_q: -5.773178577423096, std_q: 4.466701984405518

steps: 174975, episodes: 7000, mean episode reward: 1.8841411766224776, agent episode reward: [4.84, 4.84, -1.6543752974362855, -6.141483525941237], time: 58.679
steps: 174975, episodes: 7000, mean episode variance: 3.908017201386392, agent episode variance: [0.8233291736245155, 1.1930095896720887, 0.138835546515882, 1.752842891573906], time: 58.679
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19640625, variance: 3.293316694498062, cvar: 14.48441219329834, v: 11.229753494262695, mean_q: 7.25056266784668, std_q: 3.232517957687378
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1954296875, variance: 4.772037982940674, cvar: 14.573208808898926, v: 10.240904808044434, mean_q: 6.552684783935547, std_q: 3.4549384117126465
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05806494470965254, variance: 0.555342186063528, cvar: 1.3137776851654053, v: 1.1090312004089355, mean_q: 0.46004363894462585, std_q: 0.9185179471969604
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.24936948910523868, variance: 7.011371566295624, cvar: -1.5885632038116455, v: -1.593194842338562, mean_q: -5.752069473266602, std_q: 4.440659046173096

steps: 199975, episodes: 8000, mean episode reward: 1.9599697564751182, agent episode reward: [5.23, 5.23, -1.5623339559866631, -6.937696287538219], time: 58.654
steps: 199975, episodes: 8000, mean episode variance: 3.8248761811628937, agent episode variance: [0.6895413409024477, 1.1543215849995614, 0.13491888370364905, 1.8460943715572358], time: 58.654
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1915234375, variance: 2.7581653636097907, cvar: 14.48125171661377, v: 11.236062049865723, mean_q: 7.259326934814453, std_q: 3.231252908706665
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.186484375, variance: 4.617286339998246, cvar: 14.527572631835938, v: 10.23768424987793, mean_q: 6.556019306182861, std_q: 3.434718132019043
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05765276764751767, variance: 0.5396755348145962, cvar: 1.3151482343673706, v: 1.1098161935806274, mean_q: 0.4595155119895935, std_q: 0.913848340511322
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25682266330802844, variance: 7.384377479553223, cvar: -1.5884604454040527, v: -1.5934059619903564, mean_q: -5.783015727996826, std_q: 4.4781718254089355

steps: 224975, episodes: 9000, mean episode reward: 2.292069718319245, agent episode reward: [5.17, 5.17, -1.6280438470705234, -6.419886434610231], time: 58.652
steps: 224975, episodes: 9000, mean episode variance: 4.021923729076981, agent episode variance: [0.7855956008136272, 1.2225769721865654, 0.1447120511382818, 1.8690391049385071], time: 58.653
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19578125, variance: 3.142382403254509, cvar: 14.429875373840332, v: 11.231124877929688, mean_q: 7.247946739196777, std_q: 3.211669445037842
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1923046875, variance: 4.890307888746261, cvar: 14.54504108428955, v: 10.242352485656738, mean_q: 6.557717323303223, std_q: 3.4380908012390137
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05997121648656593, variance: 0.5788482045531272, cvar: 1.3077346086502075, v: 1.1077282428741455, mean_q: 0.45605626702308655, std_q: 0.9259297847747803
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25774254956617504, variance: 7.476156234741211, cvar: -1.5882295370101929, v: -1.5934542417526245, mean_q: -5.779962539672852, std_q: 4.451930999755859

steps: 249975, episodes: 10000, mean episode reward: 1.9504170891168497, agent episode reward: [4.89, 4.89, -1.5124622019838034, -6.317120708899347], time: 58.766
steps: 249975, episodes: 10000, mean episode variance: 3.848442322751507, agent episode variance: [0.6793542422354222, 1.222792818903923, 0.14195504151843488, 1.804340220093727], time: 58.767
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.194921875, variance: 2.7174169689416887, cvar: 14.457584381103516, v: 11.237846374511719, mean_q: 7.258111476898193, std_q: 3.2193782329559326
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1974609375, variance: 4.891171455383301, cvar: 14.415199279785156, v: 10.175326347351074, mean_q: 6.541865348815918, std_q: 3.3964712619781494
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.060616145483178355, variance: 0.5678201660737395, cvar: 1.3093607425689697, v: 1.1075589656829834, mean_q: 0.45922476053237915, std_q: 0.9124671816825867
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2560286439446793, variance: 7.217360973358154, cvar: -1.5881587266921997, v: -1.5934569835662842, mean_q: -5.783108234405518, std_q: 4.4555559158325195

steps: 274975, episodes: 11000, mean episode reward: 1.6033541670535634, agent episode reward: [4.8, 4.8, -1.7292732222539504, -6.267372610692486], time: 58.844
steps: 274975, episodes: 11000, mean episode variance: 3.856752258335706, agent episode variance: [0.7376309761852026, 1.179104448750615, 0.14240327165042982, 1.7976135617494584], time: 58.844
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1962109375, variance: 2.9505239047408103, cvar: 14.384801864624023, v: 11.178948402404785, mean_q: 7.24661111831665, std_q: 3.1998608112335205
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.195390625, variance: 4.7164177894592285, cvar: 14.494805335998535, v: 10.199980735778809, mean_q: 6.547384738922119, std_q: 3.4237029552459717
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05898130014940486, variance: 0.5696130866017193, cvar: 1.306876540184021, v: 1.1048147678375244, mean_q: 0.4541250467300415, std_q: 0.9276077151298523
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25606815796446664, variance: 7.190454006195068, cvar: -1.5879147052764893, v: -1.593084454536438, mean_q: -5.776864051818848, std_q: 4.43756628036499

steps: 299975, episodes: 12000, mean episode reward: 1.784516493475539, agent episode reward: [4.81, 4.81, -1.5069946206928362, -6.328488885831624], time: 58.943
steps: 299975, episodes: 12000, mean episode variance: 3.8840901409201325, agent episode variance: [0.7592629348635673, 1.203863914012909, 0.1478474280051887, 1.7731158640384674], time: 58.943
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1942578125, variance: 3.0370517394542693, cvar: 14.435273170471191, v: 11.223353385925293, mean_q: 7.253610610961914, std_q: 3.2093188762664795
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1953125, variance: 4.815455913543701, cvar: 14.61208724975586, v: 10.251029014587402, mean_q: 6.571953773498535, std_q: 3.4607090950012207
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.061078179471439155, variance: 0.5913897120207549, cvar: 1.3077490329742432, v: 1.106182336807251, mean_q: 0.4530574381351471, std_q: 0.9327464699745178
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2541091165180892, variance: 7.092463493347168, cvar: -1.5879290103912354, v: -1.5929707288742065, mean_q: -5.78828763961792, std_q: 4.457967758178711

steps: 324975, episodes: 13000, mean episode reward: 2.5658118433724453, agent episode reward: [5.67, 5.67, -1.736125487712263, -7.038062668915292], time: 59.313
steps: 324975, episodes: 13000, mean episode variance: 4.083206358727067, agent episode variance: [0.7749668648838997, 1.2903961778134108, 0.16284482664987446, 1.8549984893798828], time: 59.314
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.198125, variance: 3.0998674595355986, cvar: 14.50357437133789, v: 11.239679336547852, mean_q: 7.2749199867248535, std_q: 3.235774517059326
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19703125, variance: 5.161584711253643, cvar: 14.54031753540039, v: 10.246908187866211, mean_q: 6.568374156951904, std_q: 3.4397151470184326
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06213614055193854, variance: 0.6513793065994978, cvar: 1.3070732355117798, v: 1.106006145477295, mean_q: 0.4496285021305084, std_q: 0.9461340308189392
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25665006005525726, variance: 7.419994354248047, cvar: -1.5879297256469727, v: -1.5934470891952515, mean_q: -5.800171852111816, std_q: 4.462483882904053

steps: 349975, episodes: 14000, mean episode reward: 2.136512403658591, agent episode reward: [4.89, 4.89, -1.5128476205010197, -6.130639975840389], time: 58.76
steps: 349975, episodes: 14000, mean episode variance: 3.9779531994704156, agent episode variance: [0.8035330703854561, 1.1707353665828706, 0.1542972433809191, 1.84938751912117], time: 58.761
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.20046875, variance: 3.2141322815418243, cvar: 14.468345642089844, v: 11.260972023010254, mean_q: 7.262509822845459, std_q: 3.2234721183776855
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19609375, variance: 4.682941466331482, cvar: 14.584388732910156, v: 10.261548042297363, mean_q: 6.569613456726074, std_q: 3.447375535964966
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06107251941834026, variance: 0.6171889735236764, cvar: 1.311693549156189, v: 1.1053353548049927, mean_q: 0.4493505656719208, std_q: 0.9462668299674988
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25778085683405244, variance: 7.397549629211426, cvar: -1.5880926847457886, v: -1.593321442604065, mean_q: -5.799166202545166, std_q: 4.455063343048096

steps: 374975, episodes: 15000, mean episode reward: 1.1563905211218162, agent episode reward: [4.37, 4.37, -1.7091267755097872, -5.874482703368397], time: 58.907
steps: 374975, episodes: 15000, mean episode variance: 3.921412658851594, agent episode variance: [0.7326356456130743, 1.2505225435197354, 0.15136559876427055, 1.7868888709545137], time: 58.907
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19484375, variance: 2.930542582452297, cvar: 14.458407402038574, v: 11.256999015808105, mean_q: 7.264832019805908, std_q: 3.2143445014953613
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.203046875, variance: 5.002090174078941, cvar: 14.607511520385742, v: 10.275236129760742, mean_q: 6.572335720062256, std_q: 3.451364755630493
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06007855287074618, variance: 0.6054623950570822, cvar: 1.3117406368255615, v: 1.1056439876556396, mean_q: 0.45037510991096497, std_q: 0.9329649806022644
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25405782473920707, variance: 7.147555351257324, cvar: -1.588819146156311, v: -1.5931769609451294, mean_q: -5.790022373199463, std_q: 4.438177108764648

steps: 399975, episodes: 16000, mean episode reward: 1.7990411918852707, agent episode reward: [4.92, 4.92, -1.517259886671578, -6.5236989214431516], time: 58.755
steps: 399975, episodes: 16000, mean episode variance: 3.9283628677502276, agent episode variance: [0.8198755921423435, 1.2056648198366164, 0.15218305230885745, 1.75063940346241], time: 58.755
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.205, variance: 3.279502368569374, cvar: 14.464625358581543, v: 11.216509819030762, mean_q: 7.2608418464660645, std_q: 3.21879243850708
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.192890625, variance: 4.822659015655518, cvar: 14.482064247131348, v: 10.24516487121582, mean_q: 6.556735515594482, std_q: 3.412306070327759
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06187381942150134, variance: 0.6087322092354298, cvar: 1.3121896982192993, v: 1.1072771549224854, mean_q: 0.4500438868999481, std_q: 0.9363742470741272
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2561767097153711, variance: 7.002557754516602, cvar: -1.588102102279663, v: -1.5935587882995605, mean_q: -5.782347202301025, std_q: 4.421476364135742

steps: 424975, episodes: 17000, mean episode reward: 2.6350640387430992, agent episode reward: [5.28, 5.28, -1.4571082282859542, -6.467827732970947], time: 58.689
steps: 424975, episodes: 17000, mean episode variance: 3.964660892892629, agent episode variance: [0.8039162688851357, 1.2210449404418469, 0.1482666330523789, 1.7914330505132676], time: 58.689
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.201953125, variance: 3.2156650755405427, cvar: 14.445113182067871, v: 11.261364936828613, mean_q: 7.265967845916748, std_q: 3.214672088623047
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1930078125, variance: 4.884179761767387, cvar: 14.585773468017578, v: 10.258554458618164, mean_q: 6.5675153732299805, std_q: 3.4520933628082275
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05984239359792792, variance: 0.5930665322095156, cvar: 1.3136342763900757, v: 1.1082124710083008, mean_q: 0.44990888237953186, std_q: 0.9429916739463806
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2550957485345448, variance: 7.1657322020530705, cvar: -1.5883370637893677, v: -1.5931377410888672, mean_q: -5.769248962402344, std_q: 4.431629657745361

steps: 449975, episodes: 18000, mean episode reward: 1.5478412798522418, agent episode reward: [4.66, 4.66, -1.5454581169704857, -6.226700603177273], time: 59.075
steps: 449975, episodes: 18000, mean episode variance: 3.8970382797792555, agent episode variance: [0.7611307634413242, 1.202034834086895, 0.15769761864095927, 1.7761750636100768], time: 59.075
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1940234375, variance: 3.044523053765297, cvar: 14.428385734558105, v: 11.219815254211426, mean_q: 7.252750873565674, std_q: 3.2096474170684814
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1967578125, variance: 4.808139801025391, cvar: 14.589545249938965, v: 10.2603759765625, mean_q: 6.557638168334961, std_q: 3.457920789718628
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.062325138066255135, variance: 0.6307904745638371, cvar: 1.3067389726638794, v: 1.1051732301712036, mean_q: 0.44822368025779724, std_q: 0.94756019115448
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25141099937949096, variance: 7.104700088500977, cvar: -1.5883517265319824, v: -1.5928517580032349, mean_q: -5.755969047546387, std_q: 4.404130935668945

steps: 474975, episodes: 19000, mean episode reward: 1.922839303943277, agent episode reward: [5.18, 5.18, -1.6658146793570745, -6.771346016699648], time: 58.913
steps: 474975, episodes: 19000, mean episode variance: 3.9665599280036985, agent episode variance: [0.7780720037072897, 1.2594458846449852, 0.16578742792084813, 1.7632546117305756], time: 58.913
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19671875, variance: 3.1122880148291587, cvar: 14.441152572631836, v: 11.26613998413086, mean_q: 7.260044574737549, std_q: 3.2093441486358643
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1994921875, variance: 5.037783622741699, cvar: 14.554670333862305, v: 10.257087707519531, mean_q: 6.565763473510742, std_q: 3.442594051361084
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06407037650424936, variance: 0.6631497116833925, cvar: 1.3092972040176392, v: 1.1059216260910034, mean_q: 0.4462546110153198, std_q: 0.9554071426391602
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25688653485031365, variance: 7.053018569946289, cvar: -1.5883240699768066, v: -1.5932368040084839, mean_q: -5.7930908203125, std_q: 4.446808338165283

steps: 499975, episodes: 20000, mean episode reward: 1.9128573474592785, agent episode reward: [4.65, 4.65, -1.3379341662605315, -6.0492084862801905], time: 58.87
steps: 499975, episodes: 20000, mean episode variance: 3.9798942386098206, agent episode variance: [0.7641550382822752, 1.2558306044042111, 0.15154945981875062, 1.8083591361045837], time: 58.87
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19640625, variance: 3.0566201531291006, cvar: 14.423828125, v: 11.242332458496094, mean_q: 7.260805606842041, std_q: 3.2063515186309814
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.196484375, variance: 5.023322105407715, cvar: 14.548502922058105, v: 10.277390480041504, mean_q: 6.5610857009887695, std_q: 3.4367432594299316
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.061478123060699344, variance: 0.6061978392750025, cvar: 1.308842658996582, v: 1.10518479347229, mean_q: 0.45019835233688354, std_q: 0.9359709620475769
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.26027439598461366, variance: 7.233436584472656, cvar: -1.5880584716796875, v: -1.5936784744262695, mean_q: -5.780308723449707, std_q: 4.425093173980713

steps: 524975, episodes: 21000, mean episode reward: 2.745591146460907, agent episode reward: [5.68, 5.68, -1.6347936496557864, -6.979615203883306], time: 58.772
steps: 524975, episodes: 21000, mean episode variance: 3.9531608334612103, agent episode variance: [0.7621653769612312, 1.209459573507309, 0.16206020085327327, 1.8194756821393967], time: 58.772
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1997265625, variance: 3.048661507844925, cvar: 14.417848587036133, v: 11.274449348449707, mean_q: 7.257167339324951, std_q: 3.202255964279175
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1964453125, variance: 4.837838294029236, cvar: 14.577929496765137, v: 10.284481048583984, mean_q: 6.56850004196167, std_q: 3.4452295303344727
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06216106012589785, variance: 0.6482408034130931, cvar: 1.3076168298721313, v: 1.1034501791000366, mean_q: 0.4485422968864441, std_q: 0.9370726346969604
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25639977920290347, variance: 7.277902603149414, cvar: -1.588046908378601, v: -1.59336519241333, mean_q: -5.797248840332031, std_q: 4.435474872589111

steps: 549975, episodes: 22000, mean episode reward: 1.7321420174688924, agent episode reward: [4.73, 4.73, -1.421612201555164, -6.306245780975943], time: 58.885
steps: 549975, episodes: 22000, mean episode variance: 3.938032518338412, agent episode variance: [0.7910914998948574, 1.2647882494330407, 0.15082633630558848, 1.7313264327049256], time: 58.885
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.198984375, variance: 3.1643659995794295, cvar: 14.446198463439941, v: 11.268095970153809, mean_q: 7.2605695724487305, std_q: 3.2126572132110596
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1986328125, variance: 5.0591535568237305, cvar: 14.555419921875, v: 10.287446975708008, mean_q: 6.5601277351379395, std_q: 3.4303500652313232
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06149502944146455, variance: 0.6033053452223539, cvar: 1.3103511333465576, v: 1.1065566539764404, mean_q: 0.44770684838294983, std_q: 0.9564304351806641
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25169874392051733, variance: 6.9253058433532715, cvar: -1.5883188247680664, v: -1.5934462547302246, mean_q: -5.771153450012207, std_q: 4.389931678771973

steps: 574975, episodes: 23000, mean episode reward: 1.8140270727500147, agent episode reward: [4.86, 4.86, -1.4030131239662151, -6.50295980328377], time: 59.216
steps: 574975, episodes: 23000, mean episode variance: 3.8110905059315265, agent episode variance: [0.7079106663167477, 1.2175893014371395, 0.14703736630454659, 1.7385531718730927], time: 59.217
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.195546875, variance: 2.831642665266991, cvar: 14.45737361907959, v: 11.294357299804688, mean_q: 7.267825126647949, std_q: 3.2177481651306152
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1960546875, variance: 4.870357205748558, cvar: 14.557339668273926, v: 10.245789527893066, mean_q: 6.554821968078613, std_q: 3.442345142364502
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06044797616487372, variance: 0.5881494652181863, cvar: 1.309690237045288, v: 1.1065890789031982, mean_q: 0.44892618060112, std_q: 0.93697190284729
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25545286732489025, variance: 6.954212188720703, cvar: -1.5880308151245117, v: -1.593363881111145, mean_q: -5.778848648071289, std_q: 4.386620044708252

steps: 599975, episodes: 24000, mean episode reward: 1.631460468916795, agent episode reward: [4.57, 4.57, -1.6423325485971858, -5.866206982486019], time: 59.008
steps: 599975, episodes: 24000, mean episode variance: 3.8364813080271705, agent episode variance: [0.7400690269768238, 1.2168250118494033, 0.15498346663918347, 1.72460380256176], time: 59.008
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1991796875, variance: 2.960276107907295, cvar: 14.47301197052002, v: 11.274237632751465, mean_q: 7.264594078063965, std_q: 3.2178783416748047
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.198125, variance: 4.867300047397613, cvar: 14.605568885803223, v: 10.312128067016602, mean_q: 6.577243328094482, std_q: 3.4489009380340576
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06061444680510414, variance: 0.6199338665567339, cvar: 1.3109222650527954, v: 1.1067949533462524, mean_q: 0.4492478370666504, std_q: 0.946654736995697
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2507139271275968, variance: 6.898415565490723, cvar: -1.5880826711654663, v: -1.5935351848602295, mean_q: -5.755225658416748, std_q: 4.363790035247803

steps: 624975, episodes: 25000, mean episode reward: 1.5818843060476677, agent episode reward: [4.82, 4.82, -1.6670861088964675, -6.391029585055865], time: 59.398
steps: 624975, episodes: 25000, mean episode variance: 3.8218596457391976, agent episode variance: [0.7195759745836258, 1.2422114123702048, 0.16053804983198644, 1.6995342089533805], time: 59.398
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1989453125, variance: 2.878303898334503, cvar: 14.410700798034668, v: 11.245660781860352, mean_q: 7.2562713623046875, std_q: 3.20273756980896
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1941796875, variance: 4.968845649480819, cvar: 14.509523391723633, v: 10.273679733276367, mean_q: 6.554986476898193, std_q: 3.41586971282959
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06373145760752326, variance: 0.6421521993279458, cvar: 1.307739496231079, v: 1.1065177917480469, mean_q: 0.44849860668182373, std_q: 0.9509972929954529
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2551836103266708, variance: 6.798136835813522, cvar: -1.5878324508666992, v: -1.5938559770584106, mean_q: -5.778390884399414, std_q: 4.411656379699707

steps: 649975, episodes: 26000, mean episode reward: 2.5009590509616384, agent episode reward: [5.3, 5.3, -1.602496972750191, -6.496543976288169], time: 58.864
steps: 649975, episodes: 26000, mean episode variance: 3.8274967537336053, agent episode variance: [0.7054712778925896, 1.2091159303188324, 0.1524641722254455, 1.7604453732967378], time: 58.864
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1932421875, variance: 2.8218851115703583, cvar: 14.403411865234375, v: 11.243671417236328, mean_q: 7.255390167236328, std_q: 3.197444438934326
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1987109375, variance: 4.836463721275329, cvar: 14.551509857177734, v: 10.281639099121094, mean_q: 6.566204071044922, std_q: 3.4311132431030273
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06175865260495658, variance: 0.609856688901782, cvar: 1.3097041845321655, v: 1.104587435722351, mean_q: 0.45148828625679016, std_q: 0.9316023588180542
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2528450174305975, variance: 7.041781425476074, cvar: -1.5881084203720093, v: -1.5932382345199585, mean_q: -5.769179821014404, std_q: 4.379233360290527

steps: 674975, episodes: 27000, mean episode reward: 1.650038706890858, agent episode reward: [4.67, 4.67, -1.2247825467779485, -6.465178746331194], time: 58.984
steps: 674975, episodes: 27000, mean episode variance: 3.894544578704983, agent episode variance: [0.70715309882164, 1.2672709623575211, 0.15751074269786478, 1.7626097748279572], time: 58.984
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.195234375, variance: 2.82861239528656, cvar: 14.414024353027344, v: 11.229137420654297, mean_q: 7.25259256362915, std_q: 3.2041516304016113
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1991015625, variance: 5.069084167480469, cvar: 14.533862113952637, v: 10.26478099822998, mean_q: 6.552254676818848, std_q: 3.4333651065826416
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06077352698059522, variance: 0.6300429707914591, cvar: 1.3118535280227661, v: 1.1062177419662476, mean_q: 0.4520703852176666, std_q: 0.9239018559455872
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25637798296915987, variance: 7.050439357757568, cvar: -1.5880029201507568, v: -1.5932785272598267, mean_q: -5.774787902832031, std_q: 4.40353536605835

steps: 699975, episodes: 28000, mean episode reward: 1.699059665206743, agent episode reward: [4.86, 4.86, -1.5023695899569274, -6.518570744836329], time: 59.024
steps: 699975, episodes: 28000, mean episode variance: 3.864323461249471, agent episode variance: [0.7263831160515547, 1.1917415056228637, 0.15228298714756966, 1.7939158524274825], time: 59.024
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1949609375, variance: 2.905532464206219, cvar: 14.448641777038574, v: 11.257786750793457, mean_q: 7.2576584815979, std_q: 3.2102606296539307
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1933203125, variance: 4.766966022491455, cvar: 14.567724227905273, v: 10.277819633483887, mean_q: 6.566030979156494, std_q: 3.440467119216919
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.060551720425872826, variance: 0.6091319485902786, cvar: 1.3078196048736572, v: 1.1053779125213623, mean_q: 0.44954171776771545, std_q: 0.9359630942344666
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2572242929006917, variance: 7.175662994384766, cvar: -1.5880733728408813, v: -1.5937517881393433, mean_q: -5.772214889526367, std_q: 4.363430023193359

steps: 724975, episodes: 29000, mean episode reward: 1.6651678891264996, agent episode reward: [4.54, 4.54, -1.4100515341062523, -6.0047805767672475], time: 58.934
steps: 724975, episodes: 29000, mean episode variance: 3.838381340608001, agent episode variance: [0.7273275842368603, 1.1663793149590491, 0.1529269349128008, 1.7917475064992905], time: 58.935
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1980859375, variance: 2.909310336947441, cvar: 14.427765846252441, v: 11.255722999572754, mean_q: 7.259419918060303, std_q: 3.2054903507232666
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1951171875, variance: 4.665517807006836, cvar: 14.562033653259277, v: 10.287934303283691, mean_q: 6.570969104766846, std_q: 3.4372596740722656
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06109107927339171, variance: 0.6117077396512032, cvar: 1.3133900165557861, v: 1.109497308731079, mean_q: 0.4514685571193695, std_q: 0.9410995841026306
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2587089471938991, variance: 7.166989803314209, cvar: -1.5884710550308228, v: -1.5928125381469727, mean_q: -5.768167018890381, std_q: 4.385775566101074

steps: 749975, episodes: 30000, mean episode reward: 2.8156935100791944, agent episode reward: [5.74, 5.74, -2.0557389375355952, -6.608567552385211], time: 58.799
steps: 749975, episodes: 30000, mean episode variance: 3.8464341034069656, agent episode variance: [0.7120466573536396, 1.204721579015255, 0.15393986851722002, 1.7757259985208511], time: 58.799
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1948828125, variance: 2.8481866294145584, cvar: 14.433152198791504, v: 11.251603126525879, mean_q: 7.2526726722717285, std_q: 3.2074358463287354
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1959765625, variance: 4.818886756896973, cvar: 14.557908058166504, v: 10.294999122619629, mean_q: 6.562778472900391, std_q: 3.4311978816986084
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06211623095372917, variance: 0.6157594740688801, cvar: 1.310621738433838, v: 1.1068147420883179, mean_q: 0.45039132237434387, std_q: 0.9437640309333801
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2569119929669006, variance: 7.102904319763184, cvar: -1.5883369445800781, v: -1.5938457250595093, mean_q: -5.765775203704834, std_q: 4.379555702209473

steps: 774975, episodes: 31000, mean episode reward: 1.5249805719519631, agent episode reward: [4.88, 4.88, -1.8473745412672837, -6.387644886780752], time: 59.06
steps: 774975, episodes: 31000, mean episode variance: 3.8493683754876256, agent episode variance: [0.7447211368978024, 1.225430735886097, 0.1572685696259141, 1.7219479330778122], time: 59.06
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19578125, variance: 2.9788845475912096, cvar: 14.461845397949219, v: 11.265515327453613, mean_q: 7.260071277618408, std_q: 3.212454080581665
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1984765625, variance: 4.901722943544388, cvar: 14.490072250366211, v: 10.293732643127441, mean_q: 6.560405731201172, std_q: 3.411494731903076
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.0625580485461766, variance: 0.6290742785036564, cvar: 1.3064535856246948, v: 1.1047924757003784, mean_q: 0.4506732225418091, std_q: 0.9354735612869263
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25456612441103327, variance: 6.887791732311249, cvar: -1.5875715017318726, v: -1.5934463739395142, mean_q: -5.77694845199585, std_q: 4.389187812805176

steps: 799975, episodes: 32000, mean episode reward: 2.2752263150827257, agent episode reward: [5.27, 5.27, -1.4573678519144464, -6.807405833002829], time: 58.974
steps: 799975, episodes: 32000, mean episode variance: 3.865338697772473, agent episode variance: [0.7223291779607535, 1.2195362811088561, 0.1656235419474542, 1.7578496967554091], time: 58.975
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.195625, variance: 2.889316711843014, cvar: 14.413979530334473, v: 11.264374732971191, mean_q: 7.258395671844482, std_q: 3.200002908706665
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1970703125, variance: 4.878145124435425, cvar: 14.504273414611816, v: 10.262568473815918, mean_q: 6.557262897491455, std_q: 3.4155073165893555
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06283353692608856, variance: 0.6624941677898168, cvar: 1.310201644897461, v: 1.1059949398040771, mean_q: 0.4493023157119751, std_q: 0.9370558261871338
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25881600735636556, variance: 7.031398296356201, cvar: -1.588266372680664, v: -1.5934661626815796, mean_q: -5.777481555938721, std_q: 4.415279388427734

steps: 824975, episodes: 33000, mean episode reward: 1.908793947666477, agent episode reward: [5.03, 5.03, -1.5076664675887386, -6.643539584744785], time: 59.055
steps: 824975, episodes: 33000, mean episode variance: 3.9974593033865093, agent episode variance: [0.7729897491335869, 1.2992716056406497, 0.16460871241241692, 1.7605892361998559], time: 59.056
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1978515625, variance: 3.0919589965343475, cvar: 14.436752319335938, v: 11.261828422546387, mean_q: 7.2587361335754395, std_q: 3.2119362354278564
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.202890625, variance: 5.197086422562599, cvar: 14.566218376159668, v: 10.279109001159668, mean_q: 6.569339752197266, std_q: 3.4366915225982666
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06363008709918162, variance: 0.6584348496496677, cvar: 1.3105729818344116, v: 1.1049727201461792, mean_q: 0.44925642013549805, std_q: 0.9430784583091736
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25689521497415396, variance: 7.042356967926025, cvar: -1.5880472660064697, v: -1.5934101343154907, mean_q: -5.770392417907715, std_q: 4.374432563781738

steps: 849975, episodes: 34000, mean episode reward: 2.2157960328609625, agent episode reward: [5.17, 5.17, -1.6604270770181249, -6.463776890120913], time: 59.056
steps: 849975, episodes: 34000, mean episode variance: 3.9265622060373424, agent episode variance: [0.776048354536295, 1.2399920656085015, 0.1652894341722131, 1.7452323517203332], time: 59.057
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.199140625, variance: 3.10419341814518, cvar: 14.42017650604248, v: 11.238579750061035, mean_q: 7.256448268890381, std_q: 3.2006752490997314
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.2019140625, variance: 4.959967613220215, cvar: 14.582582473754883, v: 10.29822063446045, mean_q: 6.574069499969482, std_q: 3.441596508026123
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.0628035435480148, variance: 0.6611577366888524, cvar: 1.3108553886413574, v: 1.1076419353485107, mean_q: 0.45093753933906555, std_q: 0.949622631072998
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2583658328780417, variance: 6.980929374694824, cvar: -1.5879324674606323, v: -1.5930893421173096, mean_q: -5.781657695770264, std_q: 4.401524543762207

steps: 874975, episodes: 35000, mean episode reward: 1.5982830529113137, agent episode reward: [4.46, 4.46, -1.4195024518771542, -5.902214495211532], time: 59.016
steps: 874975, episodes: 35000, mean episode variance: 3.9278296684361993, agent episode variance: [0.714006712257862, 1.245262704819441, 0.16757313859835268, 1.800987112760544], time: 59.016
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1991796875, variance: 2.856026849031448, cvar: 14.449630737304688, v: 11.263321876525879, mean_q: 7.270448207855225, std_q: 3.208319664001465
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1986328125, variance: 4.981050819277764, cvar: 14.51577377319336, v: 10.2843599319458, mean_q: 6.566710472106934, std_q: 3.4221413135528564
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06414069507580186, variance: 0.6702925543934107, cvar: 1.3125178813934326, v: 1.1055129766464233, mean_q: 0.4486338496208191, std_q: 0.9480324983596802
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2600256169569512, variance: 7.203948020935059, cvar: -1.5883148908615112, v: -1.5935918092727661, mean_q: -5.78101110458374, std_q: 4.403478622436523

steps: 899975, episodes: 36000, mean episode reward: 1.663157276149574, agent episode reward: [4.75, 4.75, -1.3936857545118837, -6.443156969338543], time: 59.148
steps: 899975, episodes: 36000, mean episode variance: 3.849698353935033, agent episode variance: [0.7784323996305466, 1.178652902752161, 0.16257684518769383, 1.7300362063646317], time: 59.148
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1985546875, variance: 3.1137295985221862, cvar: 14.414984703063965, v: 11.23763656616211, mean_q: 7.258358001708984, std_q: 3.202547311782837
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1928515625, variance: 4.714611611008644, cvar: 14.50014877319336, v: 10.270931243896484, mean_q: 6.5625457763671875, std_q: 3.416193962097168
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06325270749497874, variance: 0.6503073807507753, cvar: 1.312955379486084, v: 1.1062211990356445, mean_q: 0.44906893372535706, std_q: 0.941675066947937
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2558434330047126, variance: 6.920144557952881, cvar: -1.587867259979248, v: -1.5933101177215576, mean_q: -5.783990859985352, std_q: 4.402517318725586

steps: 924975, episodes: 37000, mean episode reward: 1.3810596270446962, agent episode reward: [4.31, 4.31, -1.4394313289866463, -5.799509043968658], time: 59.261
steps: 924975, episodes: 37000, mean episode variance: 3.840665997672826, agent episode variance: [0.7613415233194828, 1.2086362709999086, 0.16537954511120914, 1.7053086582422257], time: 59.262
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1977734375, variance: 3.045366093277931, cvar: 14.445732116699219, v: 11.230738639831543, mean_q: 7.263171672821045, std_q: 3.2114243507385254
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.2007421875, variance: 4.834545083999634, cvar: 14.55078411102295, v: 10.301263809204102, mean_q: 6.5758585929870605, std_q: 3.426389455795288
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06381382958847323, variance: 0.6615181804448366, cvar: 1.3124016523361206, v: 1.1088296175003052, mean_q: 0.4529913365840912, std_q: 0.9368785619735718
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25446625781153415, variance: 6.821234226226807, cvar: -1.58830988407135, v: -1.5938407182693481, mean_q: -5.778419017791748, std_q: 4.380590915679932

steps: 949975, episodes: 38000, mean episode reward: 1.9248768940794216, agent episode reward: [5.08, 5.08, -1.4762632955546342, -6.758859810365944], time: 59.012
steps: 949975, episodes: 38000, mean episode variance: 3.878072705101222, agent episode variance: [0.7614105838835239, 1.1849508640170097, 0.14923749450966717, 1.782473762691021], time: 59.013
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.196875, variance: 3.0456423355340956, cvar: 14.446721076965332, v: 11.258310317993164, mean_q: 7.266427516937256, std_q: 3.212379217147827
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1928515625, variance: 4.739803314208984, cvar: 14.539053916931152, v: 10.274951934814453, mean_q: 6.565217971801758, std_q: 3.4228029251098633
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.059023680574682005, variance: 0.5969499780386687, cvar: 1.3118895292282104, v: 1.108160376548767, mean_q: 0.4561741352081299, std_q: 0.9150662422180176
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25633300504682477, variance: 7.129895210266113, cvar: -1.5876572132110596, v: -1.5933630466461182, mean_q: -5.779669761657715, std_q: 4.380241394042969

steps: 974975, episodes: 39000, mean episode reward: 1.2320524893307967, agent episode reward: [4.46, 4.46, -1.3241646614278, -6.363782849241403], time: 58.896
steps: 974975, episodes: 39000, mean episode variance: 3.861679981481284, agent episode variance: [0.7595134582966566, 1.2117761090397834, 0.15906959500536322, 1.7313208191394807], time: 58.897
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.200703125, variance: 3.0380538331866265, cvar: 14.482561111450195, v: 11.29728889465332, mean_q: 7.272455215454102, std_q: 3.222243547439575
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.195859375, variance: 4.847104549407959, cvar: 14.55101203918457, v: 10.295544624328613, mean_q: 6.576147079467773, std_q: 3.428084373474121
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06181693564762018, variance: 0.6362783800214529, cvar: 1.3114680051803589, v: 1.1069741249084473, mean_q: 0.4527249038219452, std_q: 0.9307526350021362
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25042019471284754, variance: 6.925283432006836, cvar: -1.58848237991333, v: -1.5933314561843872, mean_q: -5.749502182006836, std_q: 4.353015422821045

steps: 999975, episodes: 40000, mean episode reward: 1.7163869942086576, agent episode reward: [4.59, 4.59, -1.4052131070925982, -6.058399898698744], time: 59.294
steps: 999975, episodes: 40000, mean episode variance: 3.843045545771718, agent episode variance: [0.6945944453775883, 1.1928199599981308, 0.15582791756093503, 1.799803222835064], time: 59.294
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1934375, variance: 2.778377781510353, cvar: 14.48355484008789, v: 11.271565437316895, mean_q: 7.274013519287109, std_q: 3.221497058868408
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.193515625, variance: 4.771279839992523, cvar: 14.470108985900879, v: 10.247108459472656, mean_q: 6.557785987854004, std_q: 3.4127895832061768
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.0610109007271143, variance: 0.6233116702437401, cvar: 1.3136515617370605, v: 1.1091227531433105, mean_q: 0.4555385708808899, std_q: 0.9163135886192322
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25867910255116827, variance: 7.199213027954102, cvar: -1.5880769491195679, v: -1.5935163497924805, mean_q: -5.772496223449707, std_q: 4.386806964874268

steps: 1024975, episodes: 41000, mean episode reward: 1.5224387075419814, agent episode reward: [4.36, 4.36, -1.4382457609226833, -5.759315531535336], time: 59.553
steps: 1024975, episodes: 41000, mean episode variance: 3.94562728780508, agent episode variance: [0.7442756413817405, 1.2631030357480049, 0.1631902576982975, 1.7750583529770374], time: 59.554
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1961328125, variance: 2.977102565526962, cvar: 14.43398666381836, v: 11.280887603759766, mean_q: 7.265462875366211, std_q: 3.20034122467041
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19796875, variance: 5.052412033081055, cvar: 14.502593994140625, v: 10.298483848571777, mean_q: 6.567110061645508, std_q: 3.4123291969299316
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.0646306574819737, variance: 0.65276103079319, cvar: 1.3128950595855713, v: 1.1088401079177856, mean_q: 0.4524669349193573, std_q: 0.9393136501312256
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2560352087392286, variance: 7.100233554840088, cvar: -1.5881394147872925, v: -1.5935536623001099, mean_q: -5.7755608558654785, std_q: 4.37500524520874

steps: 1049975, episodes: 42000, mean episode reward: 1.4177607176781526, agent episode reward: [4.67, 4.67, -1.5529838061250356, -6.369255476196812], time: 59.424
steps: 1049975, episodes: 42000, mean episode variance: 3.8555158756524324, agent episode variance: [0.7490370258688926, 1.2629536421298981, 0.1607141802459955, 1.6828110274076462], time: 59.424
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.198515625, variance: 2.9961481034755706, cvar: 14.406661033630371, v: 11.265294075012207, mean_q: 7.2598748207092285, std_q: 3.195077657699585
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.196640625, variance: 5.0518145685195925, cvar: 14.461989402770996, v: 10.274687767028809, mean_q: 6.565332889556885, std_q: 3.4050607681274414
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.061817652066393565, variance: 0.642856720983982, cvar: 1.3105237483978271, v: 1.10530424118042, mean_q: 0.4522891342639923, std_q: 0.9333972334861755
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2509872186615807, variance: 6.731244087219238, cvar: -1.588627815246582, v: -1.5938563346862793, mean_q: -5.768590927124023, std_q: 4.367988109588623

steps: 1074975, episodes: 43000, mean episode reward: 1.1504076457837933, agent episode reward: [4.49, 4.49, -1.4805912122247396, -6.349001141991467], time: 59.337
steps: 1074975, episodes: 43000, mean episode variance: 3.829004322119057, agent episode variance: [0.7637255310714245, 1.2013551495671273, 0.16502691308408976, 1.6988967283964158], time: 59.337
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1984765625, variance: 3.054902124285698, cvar: 14.425908088684082, v: 11.270268440246582, mean_q: 7.269630432128906, std_q: 3.2030904293060303
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1948046875, variance: 4.805420598268509, cvar: 14.481971740722656, v: 10.272390365600586, mean_q: 6.5588531494140625, std_q: 3.405250310897827
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06329460715907548, variance: 0.660107652336359, cvar: 1.3110668659210205, v: 1.1056416034698486, mean_q: 0.450676828622818, std_q: 0.9544574022293091
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2556525450542047, variance: 6.795586913585663, cvar: -1.5881339311599731, v: -1.5937798023223877, mean_q: -5.762742519378662, std_q: 4.369668483734131

steps: 1099975, episodes: 44000, mean episode reward: 1.5075392655638629, agent episode reward: [4.29, 4.29, -1.3636262232873315, -5.7088345111488055], time: 59.305
steps: 1099975, episodes: 44000, mean episode variance: 3.9188717991150916, agent episode variance: [0.7677551920711995, 1.251312657058239, 0.15690766385570168, 1.7428962861299515], time: 59.305
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.2002734375, variance: 3.071020768284798, cvar: 14.45671558380127, v: 11.270308494567871, mean_q: 7.264543056488037, std_q: 3.2105154991149902
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1993359375, variance: 5.005250453948975, cvar: 14.53134536743164, v: 10.300859451293945, mean_q: 6.566181659698486, std_q: 3.4260036945343018
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06241370277613373, variance: 0.6276306554228067, cvar: 1.3120074272155762, v: 1.10577392578125, mean_q: 0.45460522174835205, std_q: 0.9397275447845459
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25320987958749475, variance: 6.971584796905518, cvar: -1.5879416465759277, v: -1.5937385559082031, mean_q: -5.789971828460693, std_q: 4.380786895751953

steps: 1124975, episodes: 45000, mean episode reward: 1.8732463307210745, agent episode reward: [4.85, 4.85, -1.419138342020272, -6.407615327258654], time: 59.27
steps: 1124975, episodes: 45000, mean episode variance: 3.740271637399681, agent episode variance: [0.6769564501047134, 1.1519257882237435, 0.15667911036778243, 1.7547102887034416], time: 59.27
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1929296875, variance: 2.7078258004188536, cvar: 14.426871299743652, v: 11.271968841552734, mean_q: 7.26626443862915, std_q: 3.199357271194458
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.190546875, variance: 4.607703152894974, cvar: 14.447174072265625, v: 10.250704765319824, mean_q: 6.550548076629639, std_q: 3.396244764328003
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.061504700856858906, variance: 0.6267164414711297, cvar: 1.3096628189086914, v: 1.1033732891082764, mean_q: 0.45149317383766174, std_q: 0.9460080862045288
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.257524774678783, variance: 7.018840789794922, cvar: -1.5878561735153198, v: -1.593831181526184, mean_q: -5.77358865737915, std_q: 4.351821422576904

steps: 1149975, episodes: 46000, mean episode reward: 1.113039882907284, agent episode reward: [4.42, 4.42, -1.3601895961789496, -6.366770520913767], time: 59.31
steps: 1149975, episodes: 46000, mean episode variance: 3.864135536648333, agent episode variance: [0.7232668879032135, 1.1630106990933418, 0.1693892366066575, 1.8084687130451202], time: 59.31
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19796875, variance: 2.893067551612854, cvar: 14.402623176574707, v: 11.269265174865723, mean_q: 7.262911319732666, std_q: 3.188582420349121
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1942578125, variance: 4.652042796373367, cvar: 14.520466804504395, v: 10.295940399169922, mean_q: 6.566516399383545, std_q: 3.4197864532470703
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06291818160654629, variance: 0.67755694642663, cvar: 1.3121482133865356, v: 1.104939579963684, mean_q: 0.45303675532341003, std_q: 0.9221518635749817
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25924172524273886, variance: 7.233874797821045, cvar: -1.588097095489502, v: -1.5939242839813232, mean_q: -5.790363311767578, std_q: 4.376337051391602

steps: 1174975, episodes: 47000, mean episode reward: 1.2422803528539186, agent episode reward: [4.05, 4.05, -1.1334649484191732, -5.724254698726908], time: 58.946
steps: 1174975, episodes: 47000, mean episode variance: 3.817521666660905, agent episode variance: [0.6903326670229435, 1.2312995586395263, 0.16164010928571224, 1.7342493317127228], time: 58.947
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.193125, variance: 2.761330668091774, cvar: 14.389886856079102, v: 11.2463960647583, mean_q: 7.256335735321045, std_q: 3.1896860599517822
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1936328125, variance: 4.925198078155518, cvar: 14.41328239440918, v: 10.265989303588867, mean_q: 6.554834842681885, std_q: 3.386699914932251
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06053571037017871, variance: 0.6465604371428489, cvar: 1.3099323511123657, v: 1.1049031019210815, mean_q: 0.4529569149017334, std_q: 0.930173933506012
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2535002159021484, variance: 6.936996936798096, cvar: -1.5884169340133667, v: -1.5927717685699463, mean_q: -5.767261505126953, std_q: 4.354968547821045

steps: 1199975, episodes: 48000, mean episode reward: 1.4954282915354034, agent episode reward: [4.41, 4.41, -1.2783005036834998, -6.046271204781098], time: 59.287
steps: 1199975, episodes: 48000, mean episode variance: 3.7046168925277887, agent episode variance: [0.629265744715929, 1.2232783956229687, 0.15210049862787128, 1.69997225356102], time: 59.287
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.189765625, variance: 2.517062978863716, cvar: 14.392707824707031, v: 11.241494178771973, mean_q: 7.248242378234863, std_q: 3.1900601387023926
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1953125, variance: 4.893113582491875, cvar: 14.47651481628418, v: 10.272744178771973, mean_q: 6.560936450958252, std_q: 3.403918504714966
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06082101960339648, variance: 0.6084019945114851, cvar: 1.3096332550048828, v: 1.104738712310791, mean_q: 0.4492299258708954, std_q: 0.9531151056289673
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2574087498194759, variance: 6.799889087677002, cvar: -1.5885236263275146, v: -1.5928850173950195, mean_q: -5.779339790344238, std_q: 4.3727898597717285

steps: 1224975, episodes: 49000, mean episode reward: 1.6425100193230033, agent episode reward: [4.6, 4.6, -1.536288460434106, -6.02120152024289], time: 59.54
steps: 1224975, episodes: 49000, mean episode variance: 3.745101776562631, agent episode variance: [0.6870176551342011, 1.2058155485987663, 0.15215146639198066, 1.7001171064376832], time: 59.54
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.189453125, variance: 2.7480706205368044, cvar: 14.378632545471191, v: 11.249716758728027, mean_q: 7.252922058105469, std_q: 3.187832832336426
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1966015625, variance: 4.8232622146606445, cvar: 14.449956893920898, v: 10.26015853881836, mean_q: 6.553191184997559, std_q: 3.3939602375030518
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.060266398388513935, variance: 0.6086058655679226, cvar: 1.3147128820419312, v: 1.1074100732803345, mean_q: 0.4539482295513153, std_q: 0.9253309369087219
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25232308708711765, variance: 6.800468921661377, cvar: -1.5874868631362915, v: -1.593674659729004, mean_q: -5.760371208190918, std_q: 4.340552806854248

steps: 1249975, episodes: 50000, mean episode reward: 1.938810746037427, agent episode reward: [4.81, 4.81, -1.4839749015614816, -6.197214352401092], time: 59.531
steps: 1249975, episodes: 50000, mean episode variance: 3.655217868126929, agent episode variance: [0.6733869843333959, 1.1576104187071323, 0.15676541884988546, 1.667455046236515], time: 59.532
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.191484375, variance: 2.6935479373335838, cvar: 14.400789260864258, v: 11.291449546813965, mean_q: 7.257292747497559, std_q: 3.18646240234375
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1936328125, variance: 4.630441674828529, cvar: 14.539443969726562, v: 10.314596176147461, mean_q: 6.5709075927734375, std_q: 3.4204697608947754
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06122155340963809, variance: 0.6270616753995418, cvar: 1.3110390901565552, v: 1.1064640283584595, mean_q: 0.4548013508319855, std_q: 0.9252959489822388
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2528191314131064, variance: 6.669820308685303, cvar: -1.587878942489624, v: -1.5937427282333374, mean_q: -5.75942850112915, std_q: 4.336926460266113

steps: 1274975, episodes: 51000, mean episode reward: 1.543915166250566, agent episode reward: [4.67, 4.67, -1.343413154771654, -6.45267167897778], time: 59.373
steps: 1274975, episodes: 51000, mean episode variance: 3.757111592950299, agent episode variance: [0.7520862227529287, 1.1766934021115303, 0.16063388206250967, 1.6676980860233306], time: 59.374
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19421875, variance: 3.0083448910117148, cvar: 14.399308204650879, v: 11.254853248596191, mean_q: 7.263003826141357, std_q: 3.1882734298706055
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19203125, variance: 4.706773608446121, cvar: 14.443812370300293, v: 10.278818130493164, mean_q: 6.552018165588379, std_q: 3.39398193359375
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.060368148296915745, variance: 0.6425355282500387, cvar: 1.312530755996704, v: 1.1046909093856812, mean_q: 0.455563485622406, std_q: 0.9112164974212646
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2534475147634341, variance: 6.670792344093322, cvar: -1.5877450704574585, v: -1.5937161445617676, mean_q: -5.774550914764404, std_q: 4.356346607208252

steps: 1299975, episodes: 52000, mean episode reward: 2.0112822785375752, agent episode reward: [4.97, 4.97, -1.5977210365006567, -6.330996684961768], time: 59.307
steps: 1299975, episodes: 52000, mean episode variance: 3.7482524732127787, agent episode variance: [0.6803620852530002, 1.2333622477054595, 0.15829554111510516, 1.6762325991392135], time: 59.307
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19171875, variance: 2.721448341012001, cvar: 14.398429870605469, v: 11.254353523254395, mean_q: 7.257562637329102, std_q: 3.1887500286102295
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.194453125, variance: 4.9334492683410645, cvar: 14.494614601135254, v: 10.288249969482422, mean_q: 6.562372207641602, std_q: 3.407196521759033
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.060233352636913255, variance: 0.6331821644604206, cvar: 1.315096378326416, v: 1.1071655750274658, mean_q: 0.4527829587459564, std_q: 0.9302696585655212
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25065174181165406, variance: 6.704930305480957, cvar: -1.5880722999572754, v: -1.5934295654296875, mean_q: -5.763829231262207, std_q: 4.345320224761963

steps: 1324975, episodes: 53000, mean episode reward: 1.681633124095898, agent episode reward: [4.69, 4.69, -1.4088427957410699, -6.289524080163032], time: 59.249
steps: 1324975, episodes: 53000, mean episode variance: 3.667647974219173, agent episode variance: [0.6917202433347702, 1.1508030571341514, 0.1536432974599302, 1.6714813762903213], time: 59.25
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1905859375, variance: 2.766880973339081, cvar: 14.38070297241211, v: 11.237542152404785, mean_q: 7.253098487854004, std_q: 3.1806464195251465
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.188515625, variance: 4.603212228536606, cvar: 14.434130668640137, v: 10.250513076782227, mean_q: 6.540846824645996, std_q: 3.393662929534912
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05889284259694585, variance: 0.6145731898397208, cvar: 1.3137246370315552, v: 1.1064672470092773, mean_q: 0.4567584693431854, std_q: 0.9263573884963989
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.252522867625002, variance: 6.685925483703613, cvar: -1.5881015062332153, v: -1.5936862230300903, mean_q: -5.756472587585449, std_q: 4.342954635620117

steps: 1349975, episodes: 54000, mean episode reward: 2.1109848620428697, agent episode reward: [5.04, 5.04, -1.5151705062606675, -6.4538446316964615], time: 59.578
steps: 1349975, episodes: 54000, mean episode variance: 3.7922197731025515, agent episode variance: [0.7386934387385845, 1.1758128915429116, 0.15303150441125035, 1.7246819384098053], time: 59.579
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1955078125, variance: 2.954773754954338, cvar: 14.397761344909668, v: 11.260175704956055, mean_q: 7.25618314743042, std_q: 3.192495107650757
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1926171875, variance: 4.703251361846924, cvar: 14.45573616027832, v: 10.263742446899414, mean_q: 6.554408073425293, std_q: 3.3958754539489746
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06073864238148995, variance: 0.6121260176450014, cvar: 1.31392502784729, v: 1.1086933612823486, mean_q: 0.4564104378223419, std_q: 0.9328791499137878
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2539156901491966, variance: 6.8987274169921875, cvar: -1.5883392095565796, v: -1.5937429666519165, mean_q: -5.758249282836914, std_q: 4.339603900909424

steps: 1374975, episodes: 55000, mean episode reward: 1.373321610113223, agent episode reward: [4.37, 4.37, -1.2569569872701636, -6.109721402616613], time: 59.567
steps: 1374975, episodes: 55000, mean episode variance: 3.6211193054653705, agent episode variance: [0.6445584576427936, 1.194919265806675, 0.14234572148695587, 1.6392958605289458], time: 59.567
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.18359375, variance: 2.5782338305711745, cvar: 14.334402084350586, v: 11.230584144592285, mean_q: 7.236142635345459, std_q: 3.1720542907714844
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19, variance: 4.7796770632267, cvar: 14.438871383666992, v: 10.268211364746094, mean_q: 6.552920818328857, std_q: 3.393185615539551
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.0572793994611974, variance: 0.5693828859478235, cvar: 1.309613823890686, v: 1.1058344841003418, mean_q: 0.45943310856819153, std_q: 0.9108243584632874
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.250846593243393, variance: 6.557183742523193, cvar: -1.5880833864212036, v: -1.5933133363723755, mean_q: -5.761125087738037, std_q: 4.339463710784912

steps: 1399975, episodes: 56000, mean episode reward: 1.7120620402180362, agent episode reward: [4.75, 4.75, -1.5403438591346335, -6.247594100647331], time: 59.49
steps: 1399975, episodes: 56000, mean episode variance: 3.7649869437515737, agent episode variance: [0.7705316133797169, 1.159322633743286, 0.13173175859451294, 1.7034009380340576], time: 59.49
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19578125, variance: 3.0821264535188675, cvar: 14.388753890991211, v: 11.246665954589844, mean_q: 7.25403356552124, std_q: 3.1891238689422607
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1943359375, variance: 4.637290534973144, cvar: 14.45663833618164, v: 10.27573299407959, mean_q: 6.564573287963867, std_q: 3.393747568130493
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05675684732710842, variance: 0.5269270343780518, cvar: 1.3145207166671753, v: 1.1082302331924438, mean_q: 0.4622395634651184, std_q: 0.898966908454895
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25500499160869605, variance: 6.813603401184082, cvar: -1.587887167930603, v: -1.5929570198059082, mean_q: -5.773967742919922, std_q: 4.3568644523620605

steps: 1424975, episodes: 57000, mean episode reward: 1.7440255414529866, agent episode reward: [4.8, 4.8, -1.6830925993609702, -6.1728818591860435], time: 59.378
steps: 1424975, episodes: 57000, mean episode variance: 3.619469759956002, agent episode variance: [0.6714562888145447, 1.1178064951598645, 0.13636928503215312, 1.69383769094944], time: 59.379
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.189453125, variance: 2.6858251552581787, cvar: 14.398982048034668, v: 11.24393367767334, mean_q: 7.250563621520996, std_q: 3.18930721282959
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.188359375, variance: 4.471225980639458, cvar: 14.445029258728027, v: 10.273161888122559, mean_q: 6.557196617126465, std_q: 3.3940067291259766
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05698926913949977, variance: 0.5454771401286125, cvar: 1.3127667903900146, v: 1.1063190698623657, mean_q: 0.45942187309265137, std_q: 0.906405508518219
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2535731257142495, variance: 6.775350570678711, cvar: -1.5882002115249634, v: -1.5930182933807373, mean_q: -5.762002944946289, std_q: 4.325706481933594

steps: 1449975, episodes: 58000, mean episode reward: 1.8640528302905353, agent episode reward: [4.8, 4.8, -1.5376199626750842, -6.198327207034381], time: 59.366
steps: 1449975, episodes: 58000, mean episode variance: 3.7813100125826895, agent episode variance: [0.729627839833498, 1.162986501455307, 0.1534966180063784, 1.7351990532875061], time: 59.367
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.195234375, variance: 2.918511359333992, cvar: 14.387789726257324, v: 11.24703598022461, mean_q: 7.254965782165527, std_q: 3.1852407455444336
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1934375, variance: 4.6519455909729, cvar: 14.430310249328613, v: 10.235732078552246, mean_q: 6.54046106338501, std_q: 3.394763708114624
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05945776518296004, variance: 0.6139864720255136, cvar: 1.3120352029800415, v: 1.1060651540756226, mean_q: 0.4579564929008484, std_q: 0.9253876805305481
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25204442748739003, variance: 6.9407962131500245, cvar: -1.5880111455917358, v: -1.5940821170806885, mean_q: -5.766029357910156, std_q: 4.3552327156066895

steps: 1474975, episodes: 59000, mean episode reward: 2.000026483867054, agent episode reward: [4.94, 4.94, -1.4506881526912057, -6.42928536344174], time: 59.517
steps: 1474975, episodes: 59000, mean episode variance: 3.673331573165953, agent episode variance: [0.6597380571365357, 1.1613558281064034, 0.15141593111306428, 1.70082175680995], time: 59.518
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.190234375, variance: 2.6389522285461426, cvar: 14.34202766418457, v: 11.226399421691895, mean_q: 7.234619140625, std_q: 3.1792168617248535
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1875390625, variance: 4.6454233124256135, cvar: 14.368374824523926, v: 10.243133544921875, mean_q: 6.5358076095581055, std_q: 3.367734432220459
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.0604937646723492, variance: 0.6056637244522571, cvar: 1.3119685649871826, v: 1.1034899950027466, mean_q: 0.4591445326805115, std_q: 0.9084601998329163
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25683800923856437, variance: 6.803287029266357, cvar: -1.5877223014831543, v: -1.5933700799942017, mean_q: -5.761466979980469, std_q: 4.354326248168945

steps: 1499975, episodes: 60000, mean episode reward: 1.8333677827639607, agent episode reward: [5.03, 5.03, -1.7736681681237165, -6.452964049112323], time: 59.438
steps: 1499975, episodes: 60000, mean episode variance: 3.753599964596331, agent episode variance: [0.6993086040019989, 1.2197491936981677, 0.1396223945096135, 1.694919772386551], time: 59.439
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19078125, variance: 2.7972344160079956, cvar: 14.330000877380371, v: 11.24320125579834, mean_q: 7.244325160980225, std_q: 3.166567087173462
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1946484375, variance: 4.878996774792671, cvar: 14.390812873840332, v: 10.215171813964844, mean_q: 6.540097236633301, std_q: 3.383835554122925
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05896949169102343, variance: 0.558489578038454, cvar: 1.3168232440948486, v: 1.1098144054412842, mean_q: 0.45739179849624634, std_q: 0.9235028624534607
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2537885034353348, variance: 6.779679298400879, cvar: -1.5881879329681396, v: -1.5935226678848267, mean_q: -5.766610622406006, std_q: 4.358168125152588

steps: 1524975, episodes: 61000, mean episode reward: 2.362154649914152, agent episode reward: [5.32, 5.32, -1.8491042707790184, -6.428741079306829], time: 59.651
steps: 1524975, episodes: 61000, mean episode variance: 3.692859028942883, agent episode variance: [0.6685633465945721, 1.1705429880917073, 0.1462144683673978, 1.707538225889206], time: 59.652
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19234375, variance: 2.6742533863782882, cvar: 14.360607147216797, v: 11.200406074523926, mean_q: 7.24225378036499, std_q: 3.1817996501922607
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.188828125, variance: 4.682171952366829, cvar: 14.489680290222168, v: 10.259089469909668, mean_q: 6.5592265129089355, std_q: 3.4103827476501465
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05884238531989746, variance: 0.5848578734695912, cvar: 1.3149027824401855, v: 1.109498143196106, mean_q: 0.45931723713874817, std_q: 0.9156391024589539
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25119467256456657, variance: 6.830153465270996, cvar: -1.5879542827606201, v: -1.5936152935028076, mean_q: -5.760634899139404, std_q: 4.32151460647583

steps: 1549975, episodes: 62000, mean episode reward: 1.226841381183275, agent episode reward: [4.3, 4.3, -1.4217626084604031, -5.951396010356322], time: 59.779
steps: 1549975, episodes: 62000, mean episode variance: 3.6754942460879683, agent episode variance: [0.6925371126532555, 1.0937320433855058, 0.1689031896814704, 1.7203219003677368], time: 59.78
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.193828125, variance: 2.770148450613022, cvar: 14.400115013122559, v: 11.280111312866211, mean_q: 7.261138439178467, std_q: 3.1957015991210938
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1844921875, variance: 4.374928173542023, cvar: 14.349699020385742, v: 10.231298446655273, mean_q: 6.545458793640137, std_q: 3.3680531978607178
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.0602222800563224, variance: 0.6756127587258816, cvar: 1.3139921426773071, v: 1.1065313816070557, mean_q: 0.45602378249168396, std_q: 0.9361067414283752
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2521161805496689, variance: 6.881287574768066, cvar: -1.588515043258667, v: -1.593145728111267, mean_q: -5.763181686401367, std_q: 4.352818489074707

steps: 1574975, episodes: 63000, mean episode reward: 1.4300705803257032, agent episode reward: [4.57, 4.57, -1.4887593424846086, -6.221170077189688], time: 59.737
steps: 1574975, episodes: 63000, mean episode variance: 3.73491498555243, agent episode variance: [0.6563739775121212, 1.1444283232986927, 0.1526895246654749, 1.7814231600761414], time: 59.738
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1859765625, variance: 2.625495910048485, cvar: 14.358994483947754, v: 11.243816375732422, mean_q: 7.248132228851318, std_q: 3.1774814128875732
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19421875, variance: 4.577713489532471, cvar: 14.404681205749512, v: 10.228717803955078, mean_q: 6.547405242919922, std_q: 3.388681173324585
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.059417682657657435, variance: 0.6107580986618996, cvar: 1.3183093070983887, v: 1.1109118461608887, mean_q: 0.45885249972343445, std_q: 0.9261809587478638
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2539516152548289, variance: 7.125692367553711, cvar: -1.588096022605896, v: -1.5930004119873047, mean_q: -5.758593559265137, std_q: 4.324052810668945

steps: 1599975, episodes: 64000, mean episode reward: 1.8252635620963407, agent episode reward: [4.92, 4.92, -1.5560960602560598, -6.4586403776476], time: 62.085
steps: 1599975, episodes: 64000, mean episode variance: 3.7431089748479427, agent episode variance: [0.7060731794834137, 1.1531356612443924, 0.15121055141463877, 1.7326895827054978], time: 62.086
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19171875, variance: 2.824292717933655, cvar: 14.34298324584961, v: 11.221338272094727, mean_q: 7.242164134979248, std_q: 3.178579092025757
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1889453125, variance: 4.612542629241943, cvar: 14.415363311767578, v: 10.233111381530762, mean_q: 6.54749059677124, std_q: 3.391219139099121
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05992026411314715, variance: 0.6048422056585551, cvar: 1.31581711769104, v: 1.1101024150848389, mean_q: 0.46052631735801697, std_q: 0.9250319600105286
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25534785500518625, variance: 6.930758476257324, cvar: -1.5882090330123901, v: -1.5929584503173828, mean_q: -5.76796817779541, std_q: 4.35914945602417

steps: 1624975, episodes: 65000, mean episode reward: 2.307199508669404, agent episode reward: [5.35, 5.35, -1.5428226114475139, -6.849977879883082], time: 61.104
steps: 1624975, episodes: 65000, mean episode variance: 3.738623739896342, agent episode variance: [0.7215249353945256, 1.192609760314226, 0.15434599602036178, 1.6701430481672288], time: 61.104
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.192265625, variance: 2.8860997415781022, cvar: 14.354743957519531, v: 11.226167678833008, mean_q: 7.24472188949585, std_q: 3.180964469909668
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1946875, variance: 4.770439147949219, cvar: 14.498920440673828, v: 10.27962875366211, mean_q: 6.559610843658447, std_q: 3.418792724609375
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.0602914402879649, variance: 0.6173839840814471, cvar: 1.3175181150436401, v: 1.1093401908874512, mean_q: 0.4581570327281952, std_q: 0.9278395771980286
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.24990637964941678, variance: 6.680572032928467, cvar: -1.587476372718811, v: -1.5936708450317383, mean_q: -5.766251087188721, std_q: 4.345938682556152

steps: 1649975, episodes: 66000, mean episode reward: 1.8086693026378151, agent episode reward: [4.72, 4.72, -1.455403533129241, -6.175927164232944], time: 61.773
steps: 1649975, episodes: 66000, mean episode variance: 3.680912831183523, agent episode variance: [0.6920417536199093, 1.1551139231920242, 0.14420908996090293, 1.6895480644106864], time: 61.774
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1912890625, variance: 2.768167014479637, cvar: 14.35094928741455, v: 11.235628128051758, mean_q: 7.256548881530762, std_q: 3.168708086013794
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1883984375, variance: 4.620456218719482, cvar: 14.442877769470215, v: 10.266631126403809, mean_q: 6.556428909301758, std_q: 3.3973915576934814
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.058195741670516624, variance: 0.5768363598436117, cvar: 1.3144053220748901, v: 1.1075862646102905, mean_q: 0.4578010141849518, std_q: 0.9221669435501099
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2517388753331913, variance: 6.758192539215088, cvar: -1.588150978088379, v: -1.5931344032287598, mean_q: -5.760408401489258, std_q: 4.346029281616211

steps: 1674975, episodes: 67000, mean episode reward: 1.6197394583413587, agent episode reward: [4.5, 4.5, -1.4039469657394625, -5.976313575919179], time: 61.685
steps: 1674975, episodes: 67000, mean episode variance: 3.7540072721047326, agent episode variance: [0.6573077132403851, 1.1713294649720192, 0.16172641683649272, 1.7636436770558357], time: 61.685
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1883984375, variance: 2.6292308529615402, cvar: 14.374415397644043, v: 11.226486206054688, mean_q: 7.253365993499756, std_q: 3.1836349964141846
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.187421875, variance: 4.685317859888077, cvar: 14.441086769104004, v: 10.255906105041504, mean_q: 6.558330059051514, std_q: 3.3997368812561035
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.059296122934066046, variance: 0.6469056673459709, cvar: 1.3166464567184448, v: 1.1078331470489502, mean_q: 0.4583950340747833, std_q: 0.9303951859474182
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2532630430719802, variance: 7.054574489593506, cvar: -1.5881731510162354, v: -1.5940515995025635, mean_q: -5.771322250366211, std_q: 4.332028388977051

steps: 1699975, episodes: 68000, mean episode reward: 1.5463926066246851, agent episode reward: [4.68, 4.68, -1.528568733541177, -6.285038659834138], time: 61.613
steps: 1699975, episodes: 68000, mean episode variance: 3.6910558883249758, agent episode variance: [0.6567330626249314, 1.189065456032753, 0.14826341155171394, 1.6969939581155777], time: 61.614
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.192265625, variance: 2.6269322504997255, cvar: 14.409170150756836, v: 11.259716987609863, mean_q: 7.255053997039795, std_q: 3.1925723552703857
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19546875, variance: 4.756261824131012, cvar: 14.433446884155273, v: 10.270000457763672, mean_q: 6.561952114105225, std_q: 3.3915717601776123
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05908350451366409, variance: 0.5930536462068557, cvar: 1.3162707090377808, v: 1.1065971851348877, mean_q: 0.45767003297805786, std_q: 0.9149404168128967
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.24799906215365713, variance: 6.787976264953613, cvar: -1.5883347988128662, v: -1.5932353734970093, mean_q: -5.762913703918457, std_q: 4.367640018463135

steps: 1724975, episodes: 69000, mean episode reward: 1.6550361180112554, agent episode reward: [4.64, 4.64, -1.572597822607351, -6.052366059381394], time: 61.772
steps: 1724975, episodes: 69000, mean episode variance: 3.7128196877427397, agent episode variance: [0.728601156771183, 1.1734542248845101, 0.1604070221967995, 1.6503572838902474], time: 61.772
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.192578125, variance: 2.914404627084732, cvar: 14.397574424743652, v: 11.241984367370605, mean_q: 7.257376194000244, std_q: 3.1888856887817383
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1925, variance: 4.6938168995380405, cvar: 14.45439338684082, v: 10.244134902954102, mean_q: 6.547246932983398, std_q: 3.4013895988464355
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06140268273620854, variance: 0.641628088787198, cvar: 1.315541386604309, v: 1.108699917793274, mean_q: 0.45521464943885803, std_q: 0.9379751086235046
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2528395826349473, variance: 6.601428508758545, cvar: -1.5879887342453003, v: -1.593916416168213, mean_q: -5.769261360168457, std_q: 4.357043743133545

steps: 1749975, episodes: 70000, mean episode reward: 2.2600045001516746, agent episode reward: [5.31, 5.31, -1.6182039699599005, -6.741791529888426], time: 61.959
steps: 1749975, episodes: 70000, mean episode variance: 3.7288120196573438, agent episode variance: [0.7138012689650058, 1.1906845338940621, 0.16560897014662623, 1.6587172466516495], time: 61.959
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.194921875, variance: 2.8552050758600234, cvar: 14.364012718200684, v: 11.223316192626953, mean_q: 7.256608009338379, std_q: 3.1803667545318604
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.193984375, variance: 4.762738135576249, cvar: 14.41646671295166, v: 10.247672080993652, mean_q: 6.546674728393555, std_q: 3.3850717544555664
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06271608468979656, variance: 0.6624358805865049, cvar: 1.3182603120803833, v: 1.1092079877853394, mean_q: 0.45750707387924194, std_q: 0.9337306022644043
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.24599725280789408, variance: 6.63486909866333, cvar: -1.587997317314148, v: -1.5938594341278076, mean_q: -5.757007598876953, std_q: 4.324446201324463

steps: 1774975, episodes: 71000, mean episode reward: 1.7830883281751726, agent episode reward: [4.75, 4.75, -1.5673335681962939, -6.149578103628533], time: 62.33
steps: 1774975, episodes: 71000, mean episode variance: 3.74162232895568, agent episode variance: [0.6568072081208229, 1.1952981544435024, 0.15537074034288526, 1.7341462260484695], time: 62.33
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1898046875, variance: 2.6272288324832918, cvar: 14.373851776123047, v: 11.23948860168457, mean_q: 7.251660346984863, std_q: 3.183793067932129
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.191875, variance: 4.781192302703857, cvar: 14.444129943847656, v: 10.24355697631836, mean_q: 6.553898334503174, std_q: 3.398684024810791
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05904646076124177, variance: 0.621482961371541, cvar: 1.3181583881378174, v: 1.1097418069839478, mean_q: 0.4585743546485901, std_q: 0.9310965538024902
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25511104343530133, variance: 6.936584949493408, cvar: -1.5884348154067993, v: -1.5933700799942017, mean_q: -5.763665199279785, std_q: 4.3346638679504395

steps: 1799975, episodes: 72000, mean episode reward: 2.3591688487711777, agent episode reward: [5.52, 5.52, -1.6811993232516547, -6.999631827977168], time: 61.977
steps: 1799975, episodes: 72000, mean episode variance: 3.726402190696448, agent episode variance: [0.7067358428835869, 1.113973249733448, 0.15887677575275302, 1.7468163223266602], time: 61.977
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.190234375, variance: 2.8269433715343477, cvar: 14.364882469177246, v: 11.209704399108887, mean_q: 7.242298603057861, std_q: 3.180346727371216
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1884765625, variance: 4.455892998933792, cvar: 14.45376205444336, v: 10.2568359375, mean_q: 6.5568437576293945, std_q: 3.4073355197906494
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05943040156473733, variance: 0.6355071030110121, cvar: 1.3193011283874512, v: 1.1113218069076538, mean_q: 0.46238046884536743, std_q: 0.9137386679649353
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25221187327524863, variance: 6.987265289306641, cvar: -1.586703896522522, v: -1.5938847064971924, mean_q: -5.750670433044434, std_q: 4.320687294006348

steps: 1824975, episodes: 73000, mean episode reward: 1.722379003966078, agent episode reward: [4.91, 4.91, -1.4991194767956904, -6.598501519238233], time: 62.273
steps: 1824975, episodes: 73000, mean episode variance: 3.6915571163520218, agent episode variance: [0.6648128749579191, 1.1950573254227639, 0.1475245648100972, 1.6841623511612416], time: 62.273
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1857421875, variance: 2.6592514998316763, cvar: 14.328176498413086, v: 11.213232040405273, mean_q: 7.236111164093018, std_q: 3.1711816787719727
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1913671875, variance: 4.780229568481445, cvar: 14.447942733764648, v: 10.254014015197754, mean_q: 6.552063465118408, std_q: 3.393876075744629
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05819966212709629, variance: 0.5900982592403888, cvar: 1.3184142112731934, v: 1.1109274625778198, mean_q: 0.46039700508117676, std_q: 0.916512668132782
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.24425752471121231, variance: 6.736649513244629, cvar: -1.5886355638504028, v: -1.593030571937561, mean_q: -5.750272750854492, std_q: 4.340565204620361

steps: 1849975, episodes: 74000, mean episode reward: 0.9886688181290556, agent episode reward: [4.16, 4.16, -1.318127112793877, -6.013204069077068], time: 62.145
steps: 1849975, episodes: 74000, mean episode variance: 3.73443120791018, agent episode variance: [0.6649887681156397, 1.2260842260718345, 0.1499834685921669, 1.693374745130539], time: 62.146
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1896484375, variance: 2.6599550724625587, cvar: 14.327994346618652, v: 11.2236909866333, mean_q: 7.240414142608643, std_q: 3.173769474029541
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.193671875, variance: 4.904336929321289, cvar: 14.341124534606934, v: 10.221183776855469, mean_q: 6.545684337615967, std_q: 3.3655035495758057
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.059057334771894815, variance: 0.5999338743686676, cvar: 1.3178337812423706, v: 1.110743522644043, mean_q: 0.4598711133003235, std_q: 0.9175967574119568
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2525116566105644, variance: 6.773499011993408, cvar: -1.5877492427825928, v: -1.5933140516281128, mean_q: -5.751470565795898, std_q: 4.329841613769531

steps: 1874975, episodes: 75000, mean episode reward: 2.0216299582944455, agent episode reward: [5.08, 5.08, -1.5778634699904146, -6.560506571715141], time: 62.337
steps: 1874975, episodes: 75000, mean episode variance: 3.713266109151766, agent episode variance: [0.6738388525098562, 1.1730339698195458, 0.15196385466866194, 1.7144294321537017], time: 62.338
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.191328125, variance: 2.695355410039425, cvar: 14.357990264892578, v: 11.22019100189209, mean_q: 7.250036239624023, std_q: 3.17755126953125
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1884765625, variance: 4.692135879278183, cvar: 14.369412422180176, v: 10.237103462219238, mean_q: 6.541694164276123, std_q: 3.3775761127471924
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.058913384561645646, variance: 0.6078554186746478, cvar: 1.3186181783676147, v: 1.1107330322265625, mean_q: 0.4632885158061981, std_q: 0.8995758295059204
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25011115854554866, variance: 6.857717990875244, cvar: -1.5883116722106934, v: -1.5931020975112915, mean_q: -5.753006935119629, std_q: 4.333452224731445

steps: 1899975, episodes: 76000, mean episode reward: 1.8060175575023858, agent episode reward: [4.87, 4.87, -1.4677254645631739, -6.4662569779344405], time: 62.261
steps: 1899975, episodes: 76000, mean episode variance: 3.708668684426695, agent episode variance: [0.7055835274755955, 1.1270366380810737, 0.1639590593315661, 1.7120894595384597], time: 62.262
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.190859375, variance: 2.822334109902382, cvar: 14.352168083190918, v: 11.234801292419434, mean_q: 7.249645709991455, std_q: 3.170417547225952
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1871484375, variance: 4.508146552324295, cvar: 14.414977073669434, v: 10.257076263427734, mean_q: 6.550990104675293, std_q: 3.3894364833831787
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06012444957878736, variance: 0.6558362373262644, cvar: 1.319143533706665, v: 1.1106125116348267, mean_q: 0.45949962735176086, std_q: 0.9165560007095337
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2511575420355007, variance: 6.848357677459717, cvar: -1.5873703956604004, v: -1.5940361022949219, mean_q: -5.753390789031982, std_q: 4.3403167724609375

steps: 1924975, episodes: 77000, mean episode reward: 1.5301558070695365, agent episode reward: [4.51, 4.51, -1.4776023899017092, -6.012241803028754], time: 62.413
steps: 1924975, episodes: 77000, mean episode variance: 3.6008701655901967, agent episode variance: [0.6629442478120326, 1.1325286386311055, 0.15623459685221314, 1.6491626822948455], time: 62.413
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1879296875, variance: 2.6517769912481306, cvar: 14.341585159301758, v: 11.215887069702148, mean_q: 7.245028495788574, std_q: 3.168315887451172
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.190625, variance: 4.530114650726318, cvar: 14.41655445098877, v: 10.25024700164795, mean_q: 6.5520734786987305, std_q: 3.3877758979797363
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06108283240582125, variance: 0.6249383874088525, cvar: 1.3185969591140747, v: 1.1084855794906616, mean_q: 0.457491397857666, std_q: 0.9209406971931458
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2510784103491885, variance: 6.59665060043335, cvar: -1.5878520011901855, v: -1.593618631362915, mean_q: -5.76077127456665, std_q: 4.357151508331299

steps: 1949975, episodes: 78000, mean episode reward: 2.49326128913796, agent episode reward: [5.47, 5.47, -1.710495397501944, -6.736243313360096], time: 62.589
steps: 1949975, episodes: 78000, mean episode variance: 3.8326605435833336, agent episode variance: [0.6910649222284555, 1.2197143628299236, 0.15444243947416544, 1.7674388190507888], time: 62.589
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1866015625, variance: 2.764259688913822, cvar: 14.300501823425293, v: 11.228293418884277, mean_q: 7.232962131500244, std_q: 3.157106399536133
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1959375, variance: 4.8788574513196945, cvar: 14.43442440032959, v: 10.25084114074707, mean_q: 6.55158805847168, std_q: 3.3992135524749756
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05862081302245771, variance: 0.6177697578966618, cvar: 1.317163348197937, v: 1.1097983121871948, mean_q: 0.45986485481262207, std_q: 0.9087064862251282
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2580358852073099, variance: 7.069755554199219, cvar: -1.5876129865646362, v: -1.5936689376831055, mean_q: -5.7681169509887695, std_q: 4.3391618728637695

steps: 1974975, episodes: 79000, mean episode reward: 1.8317035725110384, agent episode reward: [5.09, 5.09, -1.600303486937459, -6.747992940551503], time: 62.551
steps: 1974975, episodes: 79000, mean episode variance: 3.791099425334483, agent episode variance: [0.7470478127002717, 1.1747410174012185, 0.1555721628256142, 1.7137384324073792], time: 62.552
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.194296875, variance: 2.9881912508010866, cvar: 14.324560165405273, v: 11.210963249206543, mean_q: 7.247551918029785, std_q: 3.168168544769287
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.18796875, variance: 4.698964069604874, cvar: 14.413365364074707, v: 10.255304336547852, mean_q: 6.5514726638793945, std_q: 3.3899195194244385
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06019052995593024, variance: 0.6222886513024568, cvar: 1.3192851543426514, v: 1.1103829145431519, mean_q: 0.4589717984199524, std_q: 0.9208592176437378
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25127484913305553, variance: 6.854954242706299, cvar: -1.5883610248565674, v: -1.5932258367538452, mean_q: -5.759280681610107, std_q: 4.343101978302002

steps: 1999975, episodes: 80000, mean episode reward: 1.701072997435527, agent episode reward: [4.8, 4.8, -1.5396369573270452, -6.359290045237427], time: 62.465
steps: 1999975, episodes: 80000, mean episode variance: 3.729945524595678, agent episode variance: [0.6788803705573082, 1.1640350083708764, 0.16323184993118048, 1.723798295736313], time: 62.466
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.190625, variance: 2.715521482229233, cvar: 14.354730606079102, v: 11.197578430175781, mean_q: 7.25043249130249, std_q: 3.1774044036865234
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.185546875, variance: 4.6561400334835055, cvar: 14.382635116577148, v: 10.240500450134277, mean_q: 6.5403547286987305, std_q: 3.378994941711426
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06041169975635044, variance: 0.6529273997247219, cvar: 1.3192075490951538, v: 1.1116372346878052, mean_q: 0.4575691223144531, std_q: 0.9247133731842041
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25088939938502425, variance: 6.895193182945252, cvar: -1.588091254234314, v: -1.5934603214263916, mean_q: -5.767287254333496, std_q: 4.3464860916137695

steps: 2024975, episodes: 81000, mean episode reward: 1.0042690499844256, agent episode reward: [4.0, 4.0, -1.2337410624087943, -5.76198988760678], time: 62.465
steps: 2024975, episodes: 81000, mean episode variance: 3.7412198812551796, agent episode variance: [0.6819410515874624, 1.2028032536506652, 0.15552434584870933, 1.7009512301683425], time: 62.466
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19078125, variance: 2.7277642063498497, cvar: 14.335814476013184, v: 11.211572647094727, mean_q: 7.244181156158447, std_q: 3.169074296951294
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1930078125, variance: 4.811213014602661, cvar: 14.440040588378906, v: 10.265101432800293, mean_q: 6.5520710945129395, std_q: 3.3945045471191406
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.05891591642730186, variance: 0.6220973833948373, cvar: 1.317299723625183, v: 1.109866976737976, mean_q: 0.4552777409553528, std_q: 0.9232251048088074
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25270885948893695, variance: 6.803804874420166, cvar: -1.5884325504302979, v: -1.5928088426589966, mean_q: -5.763706207275391, std_q: 4.362024784088135

steps: 2049975, episodes: 82000, mean episode reward: 2.24456706346328, agent episode reward: [5.09, 5.09, -1.601799194546594, -6.333633741990126], time: 62.531
steps: 2049975, episodes: 82000, mean episode variance: 3.864739621132612, agent episode variance: [0.7470117606222629, 1.1510325898230076, 0.16430406656861304, 1.8023912041187287], time: 62.531
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.192421875, variance: 2.9880470424890517, cvar: 14.353582382202148, v: 11.2246732711792, mean_q: 7.244114398956299, std_q: 3.176093578338623
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.188828125, variance: 4.60413035929203, cvar: 14.428194999694824, v: 10.247798919677734, mean_q: 6.556187629699707, std_q: 3.3915815353393555
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.062491794042820986, variance: 0.6572162662744522, cvar: 1.3200114965438843, v: 1.1096571683883667, mean_q: 0.45189324021339417, std_q: 0.9478898048400879
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25456360496142877, variance: 7.209564685821533, cvar: -1.5878052711486816, v: -1.5936362743377686, mean_q: -5.760800838470459, std_q: 4.3302435874938965

steps: 2074975, episodes: 83000, mean episode reward: 1.3858934356420518, agent episode reward: [4.65, 4.65, -1.4999202917974503, -6.414186272560499], time: 62.717
steps: 2074975, episodes: 83000, mean episode variance: 3.7169795638397334, agent episode variance: [0.7159338946789503, 1.129797337293625, 0.16580161794275045, 1.705446713924408], time: 62.718
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1916015625, variance: 2.863735578715801, cvar: 14.34095287322998, v: 11.19179916381836, mean_q: 7.239657878875732, std_q: 3.171132802963257
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.19078125, variance: 4.5191893491745, cvar: 14.39667797088623, v: 10.232107162475586, mean_q: 6.544209957122803, std_q: 3.3850326538085938
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.060939191860632534, variance: 0.6632064717710018, cvar: 1.3164976835250854, v: 1.1081740856170654, mean_q: 0.4563619792461395, std_q: 0.9220206141471863
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25109749494626077, variance: 6.821786880493164, cvar: -1.5881422758102417, v: -1.5933462381362915, mean_q: -5.757939338684082, std_q: 4.357120037078857

steps: 2099975, episodes: 84000, mean episode reward: 1.4704693090006173, agent episode reward: [4.53, 4.53, -1.747603831741979, -5.841926859257405], time: 62.709
steps: 2099975, episodes: 84000, mean episode variance: 3.7471159974243493, agent episode variance: [0.7010099773406983, 1.1356922625005246, 0.15381398727558554, 1.7565997703075409], time: 62.709
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1899609375, variance: 2.804039909362793, cvar: 14.340197563171387, v: 11.208189010620117, mean_q: 7.244767665863037, std_q: 3.1730189323425293
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.190390625, variance: 4.542768955230713, cvar: 14.383055686950684, v: 10.236602783203125, mean_q: 6.545799255371094, std_q: 3.3816890716552734
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.059026069995521055, variance: 0.6152559491023422, cvar: 1.3199355602264404, v: 1.1123950481414795, mean_q: 0.4586016833782196, std_q: 0.9195074439048767
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2547828348477908, variance: 7.0263991355896, cvar: -1.5874580144882202, v: -1.593780517578125, mean_q: -5.761100769042969, std_q: 4.368735313415527

steps: 2124975, episodes: 85000, mean episode reward: 2.1096519075630775, agent episode reward: [4.97, 4.97, -1.4510016730414157, -6.379346419395506], time: 62.701
steps: 2124975, episodes: 85000, mean episode variance: 3.7914026972316206, agent episode variance: [0.7449380428791046, 1.1936180096268654, 0.1626047263406217, 1.6902419183850288], time: 62.701
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1929296875, variance: 2.9797521715164184, cvar: 14.351808547973633, v: 11.209511756896973, mean_q: 7.245963096618652, std_q: 3.178797721862793
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1944921875, variance: 4.7744720385074615, cvar: 14.45931625366211, v: 10.229146957397461, mean_q: 6.558864593505859, std_q: 3.403059959411621
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06030825923777883, variance: 0.6504189053624868, cvar: 1.3179599046707153, v: 1.1101477146148682, mean_q: 0.45670485496520996, std_q: 0.9176105856895447
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2521113509874845, variance: 6.76096773147583, cvar: -1.58784818649292, v: -1.5937098264694214, mean_q: -5.7574286460876465, std_q: 4.365109443664551

steps: 2149975, episodes: 86000, mean episode reward: 2.0068720297880427, agent episode reward: [4.92, 4.92, -1.539837041146694, -6.293290929065264], time: 62.546
steps: 2149975, episodes: 86000, mean episode variance: 3.75939848786965, agent episode variance: [0.6589324329346419, 1.1832640320956707, 0.16243883751705288, 1.7547631853222847], time: 62.547
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.190625, variance: 2.6357297317385675, cvar: 14.313455581665039, v: 11.218092918395996, mean_q: 7.242629528045654, std_q: 3.1654162406921387
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1937109375, variance: 4.733056128382683, cvar: 14.42956256866455, v: 10.25192928314209, mean_q: 6.553083896636963, std_q: 3.3977670669555664
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.060739733580900575, variance: 0.6497553500682115, cvar: 1.3170933723449707, v: 1.1121901273727417, mean_q: 0.4564467668533325, std_q: 0.9260683655738831
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25028792040846265, variance: 7.019052505493164, cvar: -1.5874931812286377, v: -1.5937011241912842, mean_q: -5.748354434967041, std_q: 4.332889556884766

steps: 2174975, episodes: 87000, mean episode reward: 1.042760947733264, agent episode reward: [4.28, 4.28, -1.5816341821570399, -5.935604870109697], time: 62.87
steps: 2174975, episodes: 87000, mean episode variance: 3.777137892626226, agent episode variance: [0.7378408240824937, 1.1760517075061798, 0.16194877380877734, 1.701296587228775], time: 62.871
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.195703125, variance: 2.951363296329975, cvar: 14.377083778381348, v: 11.228857040405273, mean_q: 7.257083892822266, std_q: 3.1806089878082275
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.192109375, variance: 4.704206943511963, cvar: 14.4744234085083, v: 10.287099838256836, mean_q: 6.562110900878906, std_q: 3.410310745239258
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06293592163498148, variance: 0.6477950952351094, cvar: 1.3165606260299683, v: 1.1101164817810059, mean_q: 0.4523969292640686, std_q: 0.936504065990448
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.24914472489707115, variance: 6.8051862716674805, cvar: -1.5874000787734985, v: -1.5938957929611206, mean_q: -5.7378692626953125, std_q: 4.320068359375

steps: 2199975, episodes: 88000, mean episode reward: 1.5176678094690244, agent episode reward: [4.75, 4.75, -1.58931170994752, -6.393020480583456], time: 62.71
steps: 2199975, episodes: 88000, mean episode variance: 3.715323304446414, agent episode variance: [0.6961516277194023, 1.149816796541214, 0.16548747536726297, 1.703867404818535], time: 62.711
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.188046875, variance: 2.7846065108776092, cvar: 14.32457160949707, v: 11.19717788696289, mean_q: 7.2407917976379395, std_q: 3.1657681465148926
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.189765625, variance: 4.599267186164856, cvar: 14.443014144897461, v: 10.267304420471191, mean_q: 6.554348468780518, std_q: 3.4045350551605225
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06110097629990139, variance: 0.6619499014690519, cvar: 1.3178484439849854, v: 1.111258864402771, mean_q: 0.45812079310417175, std_q: 0.9200224876403809
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25410944816107334, variance: 6.815469741821289, cvar: -1.5875728130340576, v: -1.5937860012054443, mean_q: -5.758272647857666, std_q: 4.360849380493164

steps: 2224975, episodes: 89000, mean episode reward: 1.7501846625435666, agent episode reward: [4.89, 4.89, -1.55499265146136, -6.474822685995074], time: 62.646
steps: 2224975, episodes: 89000, mean episode variance: 3.8586013616099955, agent episode variance: [0.7579127528965474, 1.2076468489468097, 0.15656840196996927, 1.7364733577966691], time: 62.647
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.1953125, variance: 3.0316510115861894, cvar: 14.380171775817871, v: 11.241878509521484, mean_q: 7.256544589996338, std_q: 3.184495210647583
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.192734375, variance: 4.830587395787239, cvar: 14.482246398925781, v: 10.254398345947266, mean_q: 6.560534954071045, std_q: 3.4154911041259766
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06190277816557338, variance: 0.6262736078798771, cvar: 1.3151674270629883, v: 1.1079462766647339, mean_q: 0.45665302872657776, std_q: 0.9231981635093689
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.2543901571249577, variance: 6.94589376449585, cvar: -1.5869182348251343, v: -1.5930871963500977, mean_q: -5.749462127685547, std_q: 4.344537258148193

steps: 2249975, episodes: 90000, mean episode reward: 1.618510815283725, agent episode reward: [4.62, 4.62, -1.583663153782642, -6.037826030933632], time: 62.682
steps: 2249975, episodes: 90000, mean episode variance: 3.742244287971407, agent episode variance: [0.7090633667707443, 1.136396238207817, 0.1646364715434611, 1.7321482114493847], time: 62.682
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.194140625, variance: 2.8362534670829773, cvar: 14.375654220581055, v: 11.228824615478516, mean_q: 7.248553276062012, std_q: 3.1857500076293945
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.186796875, variance: 4.5455851554870605, cvar: 14.432848930358887, v: 10.265457153320312, mean_q: 6.560391426086426, std_q: 3.3976664543151855
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06024253570254568, variance: 0.6585458861738444, cvar: 1.3155053853988647, v: 1.1105188131332397, mean_q: 0.45539677143096924, std_q: 0.9248749613761902
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.25610638851749873, variance: 6.928593158721924, cvar: -1.5873301029205322, v: -1.5933635234832764, mean_q: -5.779956817626953, std_q: 4.373960971832275

...Finished total of 90001 episodes... Now freezing policy and running for 1000 more episodes
steps: 24975, episodes: 1000, mean episode reward: 1.5706579395609626, agent episode reward: [4.72, 4.72, -1.5660722388303419, -6.303269821608696], time: 46.566
steps: 24975, episodes: 1000, mean episode variance: 0.0, agent episode variance: [0.0, 0.0, 0.0, 0.0], time: 46.566
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, variance: nan, cvar: nan, v: nan, mean_q: nan, std_q: nan

steps: 49975, episodes: 2000, mean episode reward: 1.35256862844322, agent episode reward: [4.44, 4.44, -1.5917629800027961, -5.935668391553984], time: 58.46
steps: 49975, episodes: 2000, mean episode variance: 3.452073140591383, agent episode variance: [0.6052836797088385, 1.0809825030863285, 0.16370741839706898, 1.602099539399147], time: 58.46
Running avgs for agent 0: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.18514664446721313, variance: 2.480670818478846, cvar: 14.346129417419434, v: 11.272704124450684, mean_q: 7.2153754234313965, std_q: 3.1890785694122314
Running avgs for agent 1: q_loss: 0.0, p_loss: 0.0, mean_rew: 0.18662749743852458, variance: 4.430256160189871, cvar: 14.380011558532715, v: 10.255392074584961, mean_q: 6.511885166168213, std_q: 3.373867988586426
Running avgs for agent 2: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.06566628111428918, variance: 0.6709320426109384, cvar: 1.3249874114990234, v: 1.1162587404251099, mean_q: 0.448320597410202, std_q: 0.9368553757667542
Running avgs for agent 3: q_loss: 0.0, p_loss: 0.0, mean_rew: -0.23960182617890954, variance: 6.565981864929199, cvar: -1.5873911380767822, v: -1.5931098461151123, mean_q: -5.718259811401367, std_q: 4.419591903686523

...Finished total of 2001 episodes with the fixed policy.
